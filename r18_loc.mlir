#dram = #ttnn.buffer_type<dram>
#loc = loc(unknown)
#loc22 = loc("Sequential[classifier]|Linear[getattr(classifier, '1')]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:329|forward|347|xla__device_data")
#loc23 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[3].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data")
#loc24 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|Conv2d[resnet.encoder.stages[3].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|xla__device_data")
#loc25 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[2].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data")
#loc26 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|Conv2d[resnet.encoder.stages[2].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|xla__device_data")
#loc27 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[1].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data")
#loc28 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|Conv2d[resnet.encoder.stages[1].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|xla__device_data")
#loc29 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|BatchNorm2d[resnet.embedder.embedder.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc30 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc31 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc32 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc33 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc34 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc35 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc36 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc37 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc38 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc39 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc40 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc41 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc42 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc43 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc44 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc45 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc46 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc47 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc48 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc49 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc50 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc51 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc52 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc53 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc54 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc55 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc56 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc57 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc58 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc59 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc60 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#loc61 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data")
#loc62 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")
#system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 9x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 101664, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073064064, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_physical_size_tiles = 16, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0], [1 : i32], [ 0x0x0x0]>
#system_memory = #ttnn.buffer_type<system_memory>
#ttnn_layout = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout1 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout2 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 16384 + d1 * 32 + d2, d3), <1x1>, memref<512x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout3 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout4 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 32 + d2, d3), <1x1>, memref<64x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout5 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x8x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout6 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 32 + d2, d3), <1x1>, memref<256x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout7 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout8 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 32 + d2, d3), <1x1>, memref<128x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout9 = #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout10 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout11 = #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<32x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout12 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), <1x1>, memref<131072x1xf32, #system_memory>>
#ttnn_layout13 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 + d2, d3), <1x1>, memref<32768x1xf32, #system_memory>>
#ttnn_layout14 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), <1x1>, memref<8192x1xf32, #system_memory>>
#ttnn_layout15 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 21 + d1 * 7 + d2, d3), <1x1>, memref<1344x7xf32, #system_memory>>
#ttnn_layout16 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), <1x1>, memref<21x7x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout17 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 192 + d1 * 3 + d2, d3), <1x1>, memref<12288x3xf32, #system_memory>>
#ttnn_layout18 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 3 + d2, d3), <1x1>, memref<49152x3xf32, #system_memory>>
#ttnn_layout19 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 192 + d1 * 3 + d2, d3), <1x1>, memref<24576x3xf32, #system_memory>>
#ttnn_layout20 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 3 + d2, d3), <1x1>, memref<196608x3xf32, #system_memory>>
#ttnn_layout21 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 3 + d2, d3), <1x1>, memref<98304x3xf32, #system_memory>>
#ttnn_layout22 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 3 + d2, d3), <1x1>, memref<786432x3xf32, #system_memory>>
#ttnn_layout23 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 3 + d2, d3), <1x1>, memref<393216x3xf32, #system_memory>>
#ttnn_layout24 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 224 + d2, d3), <1x1>, memref<1568x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout25 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 50176 + d2, d3), <1x1>, memref<1568x1x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout26 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 50176 + d2, d3), <1x1>, memref<1568x1x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout27 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 50176 + d2, d3), <1x1>, memref<50176x3xf32, #system_memory>>
#ttnn_layout28 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 50176 + d2, d3), <1x1>, memref<50176x3xf32, #dram>, <interleaved>>
#ttnn_layout29 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12544 + d1 * 12544 + d2, d3), <1x1>, memref<392x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout30 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 128 + d2, d3), <1x1>, memref<448x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout31 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 8192 + d1 * 128 + d2, d3), <1x1>, memref<256x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout32 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12544 + d1 * 12544 + d2, d3), <1x1>, memref<392x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout33 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12544 + d1 * 12544 + d2, d3), <1x1>, memref<12544x64xbf16, #dram>, <interleaved>>
#ttnn_layout34 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x64xbf16, #dram>, <interleaved>>
#ttnn_layout35 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<98x2x!ttcore.tile<32x32, bf16>, #dram>, <interleaved>>
#ttnn_layout36 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<98x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout37 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x64xbf16, #system_memory>>
#ttnn_layout38 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x64xf32, #system_memory>>
#ttnn_layout39 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<3136x64xf32, #dram>, <interleaved>>
#ttnn_layout40 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 64 + d2, d3), <1x1>, memref<112x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout41 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 4096 + d1 * 64 + d2, d3), <1x1>, memref<128x2x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout42 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 3136 + d2, d3), <1x1>, memref<98x2x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout43 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout44 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 896 + d1 * 32 + d2, d3), <1x1>, memref<28x4x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout45 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 800 + d1 * 800 + d2, d3), <1x1>, memref<25x4x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout46 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x128xf32, #system_memory>>
#ttnn_layout47 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 784 + d2, d3), <1x1>, memref<784x128xf32, #dram>, <interleaved>>
#ttnn_layout48 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 224 + d2, d3), <1x1>, memref<7x8x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout49 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 448 + d1 * 32 + d2, d3), <1x1>, memref<14x8x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout50 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 224 + d2, d3), <1x1>, memref<7x8x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout51 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 196 + d1 * 196 + d2, d3), <1x1>, memref<196x256xf32, #system_memory>>
#ttnn_layout52 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 196 + d1 * 196 + d2, d3), <1x1>, memref<196x256xf32, #dram>, <interleaved>>
#ttnn_layout53 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout54 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x16x!ttcore.tile<32x32, f32>, #dram>, <interleaved>>
#ttnn_layout55 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 * 64 + d2, d3), <1x1>, memref<2x16x!ttcore.tile<32x32, f32>, #system_memory>>
#ttnn_layout56 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49 + d1 * 49 + d2, d3), <1x1>, memref<49x512xf32, #system_memory>>
#ttnn_layout57 = #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 49 + d1 * 49 + d2, d3), <1x1>, memref<49x512xf32, #dram>, <interleaved>>
module @SyncTensorsGraph.778 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.778 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #system_desc} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<9x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5, s6] -> (0, 0, (((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) mod 12, ((((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) floordiv s4) floordiv 12) * s4 + ((d0 * s1) * (s2 * (s3 * s6)) + d1 * (s2 * (s3 * s6)) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]> loc(#loc)
      func.func @main_const_eval_0() -> tensor<1x512xf32, #ttnn_layout> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 0.0204081628 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x512>}> : (!ttnn.device) -> tensor<1x512xf32, #ttnn_layout> loc(#loc)
        return %1 : tensor<1x512xf32, #ttnn_layout> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_1(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc1)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_2(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc2)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_3(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc3)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_4(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc2)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_5(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc3)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_6(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc4)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_7(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc5)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_8(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc6)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_9(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc2)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_10(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc7)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_11(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc8)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_12(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc9)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_13(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc10)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_14(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc11)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_15(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc10)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_16(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc12)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_17(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc6)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_18(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc13)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_19(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc6)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_20(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc14)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_21(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc10)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_22(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc15)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_23(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc7)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_24(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc1)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_25(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc8)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_26(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc3)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_27(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc16)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_28(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc15)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_29(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc17)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_30(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc13)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_31(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc15)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_32(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc18)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_33(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc15)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_34(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc1)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_35(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc14)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_36(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc13)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_37(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc12)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_38(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc19)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_39(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc18)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_40(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc17)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_41(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc4)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_42(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc12)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_43(%arg0: tensor<1000xf32, #ttnn_layout9> loc(unknown)) -> tensor<1x1000xf32, #ttnn_layout10> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 1000 : i32]}> : (tensor<1000xf32, #ttnn_layout9>) -> tensor<1x1000xf32, #ttnn_layout10> loc(#loc20)
        return %0 : tensor<1x1000xf32, #ttnn_layout10> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_44(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc16)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_45(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc2)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_46(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc7)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_47(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc21)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_48(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc14)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_49(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc8)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_50(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc9)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_51(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc1)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_52(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc4)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_53(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc17)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_54(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc3)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_55(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc9)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_56(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc11)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_57(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc11)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_58(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc19)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_59(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc11)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_60(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc8)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_61(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc19)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_62(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc4)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_63(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc21)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_64(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc7)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_65(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc19)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_66(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc18)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_67(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc13)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_68(%arg0: tensor<256xf32, #ttnn_layout5> loc(unknown)) -> tensor<1x256x1x1xf32, #ttnn_layout6> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 256 : i32, 1 : i32, 1 : i32]}> : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc12)
        return %0 : tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_69(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc5)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_70(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc16)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_71(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc5)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_72(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc21)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_73(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc17)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_74(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc14)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_75(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc16)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_76(%arg0: tensor<512xf32, #ttnn_layout1> loc(unknown)) -> tensor<1x512x1x1xf32, #ttnn_layout2> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 512 : i32, 1 : i32, 1 : i32]}> : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc18)
        return %0 : tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_77(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc9)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_78(%arg0: tensor<64xf32, #ttnn_layout3> loc(unknown)) -> tensor<1x64x1x1xf32, #ttnn_layout4> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 64 : i32, 1 : i32, 1 : i32]}> : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc5)
        return %0 : tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_79(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc6)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_80(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc21)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main_const_eval_81(%arg0: tensor<128xf32, #ttnn_layout7> loc(unknown)) -> tensor<1x128x1x1xf32, #ttnn_layout8> attributes {const_eval} {
        %0 = "ttnn.reshape"(%arg0) <{shape = [1 : i32, 128 : i32, 1 : i32, 1 : i32]}> : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc10)
        return %0 : tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
      } loc(#loc)
      func.func @main(%arg0: tensor<1000xf32, #ttnn_layout9> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___classifier_1_bias"} loc("Sequential[classifier]|Linear[getattr(classifier, '1')]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:329|forward|347|xla__device_data"), %arg1: tensor<1000x512xf32, #ttnn_layout11> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___classifier_1_weight"} loc("Sequential[classifier]|Linear[getattr(classifier, '1')]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:329|forward|347|xla__device_data"), %arg2: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[3].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg3: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[3].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg4: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[3].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg5: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[3].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg6: tensor<512x256x1x1xf32, #ttnn_layout12> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_encoder_stages_3_layers_0_shortcut_convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|Conv2d[resnet.encoder.stages[3].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|xla__device_data"), %arg7: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[2].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg8: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[2].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg9: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[2].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg10: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[2].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg11: tensor<256x128x1x1xf32, #ttnn_layout13> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_encoder_stages_2_layers_0_shortcut_convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|Conv2d[resnet.encoder.stages[2].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|xla__device_data"), %arg12: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[1].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg13: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[1].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg14: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[1].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg15: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[1].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|xla__device_data"), %arg16: tensor<128x64x1x1xf32, #ttnn_layout14> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_encoder_stages_1_layers_0_shortcut_convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|Conv2d[resnet.encoder.stages[1].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|xla__device_data"), %arg17: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_running_var"} loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|BatchNorm2d[resnet.embedder.embedder.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg18: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|BatchNorm2d[resnet.embedder.embedder.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg19: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_bias"} loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|BatchNorm2d[resnet.embedder.embedder.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg20: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___resnet_embedder_embedder_normalization_weight"} loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|BatchNorm2d[resnet.embedder.embedder.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg21: tensor<64x3x7x7xf32, #ttnn_layout15> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "l__self___resnet_embedder_embedder_convolution_weight"} loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg22: tensor<1x3x224x224xf32, #ttnn_layout16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg23: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg24: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg25: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg26: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg27: tensor<64x64x3x3xf32, #ttnn_layout17> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg28: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg29: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg30: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg31: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg32: tensor<64x64x3x3xf32, #ttnn_layout17> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_0_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg33: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg34: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg35: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg36: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg37: tensor<64x64x3x3xf32, #ttnn_layout17> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg38: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg39: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg40: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg41: tensor<64xf32, #ttnn_layout3> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg42: tensor<64x64x3x3xf32, #ttnn_layout17> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_0_layers_1_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg43: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg44: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg45: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg46: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg47: tensor<128x128x3x3xf32, #ttnn_layout18> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg48: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg49: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg50: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg51: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg52: tensor<128x64x3x3xf32, #ttnn_layout19> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_0_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg53: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg54: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg55: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg56: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg57: tensor<128x128x3x3xf32, #ttnn_layout18> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg58: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg59: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg60: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg61: tensor<128xf32, #ttnn_layout7> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg62: tensor<128x128x3x3xf32, #ttnn_layout18> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_1_layers_1_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg63: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg64: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg65: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg66: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg67: tensor<256x256x3x3xf32, #ttnn_layout20> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg68: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg69: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg70: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg71: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg72: tensor<256x128x3x3xf32, #ttnn_layout21> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_0_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg73: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg74: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg75: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg76: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg77: tensor<256x256x3x3xf32, #ttnn_layout20> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg78: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg79: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg80: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg81: tensor<256xf32, #ttnn_layout5> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg82: tensor<256x256x3x3xf32, #ttnn_layout20> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_2_layers_1_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg83: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg84: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg85: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg86: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg87: tensor<512x512x3x3xf32, #ttnn_layout22> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg88: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg89: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg90: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg91: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg92: tensor<512x256x3x3xf32, #ttnn_layout23> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_0_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg93: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg94: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg95: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg96: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg97: tensor<512x512x3x3xf32, #ttnn_layout22> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___1___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data"), %arg98: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_running_var"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg99: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_running_mean"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg100: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_bias"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg101: tensor<512xf32, #ttnn_layout1> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___normalization_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|xla__device_data"), %arg102: tensor<512x512x3x3xf32, #ttnn_layout22> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.conv2d_weight, ttir.name = "getattr_l__self___resnet_encoder_stages_3_layers_1_layer___0___convolution_weight"} loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|xla__device_data")) -> (tensor<1x1000xf32, #ttnn_layout10> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x512xf32, #ttnn_layout> loc(#loc)
        %1 = ttcore.load_cached(@main_const_eval_1, [%arg93]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg93) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %2 = ttcore.load_cached(@main_const_eval_2, [%arg99]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg99) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %3 = ttcore.load_cached(@main_const_eval_3, [%arg31]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg31) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %4 = ttcore.load_cached(@main_const_eval_4, [%arg100]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg100) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %5 = ttcore.load_cached(@main_const_eval_5, [%arg28]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg28) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %6 = ttcore.load_cached(@main_const_eval_6, [%arg73]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg73) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %7 = ttcore.load_cached(@main_const_eval_7, [%arg40]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg40) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %8 = ttcore.load_cached(@main_const_eval_8, [%arg46]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg46) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %9 = ttcore.load_cached(@main_const_eval_9, [%arg101]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg101) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %10 = ttcore.load_cached(@main_const_eval_10, [%arg33]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg33) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %11 = ttcore.load_cached(@main_const_eval_11, [%arg7]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %12 = ttcore.load_cached(@main_const_eval_12, [%arg12]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %13 = ttcore.load_cached(@main_const_eval_13, [%arg56]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg56) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %14 = ttcore.load_cached(@main_const_eval_14, [%arg66]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg66) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %15 = ttcore.load_cached(@main_const_eval_15, [%arg53]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg53) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %16 = ttcore.load_cached(@main_const_eval_16, [%arg80]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg80) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %17 = ttcore.load_cached(@main_const_eval_17, [%arg43]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg43) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %18 = ttcore.load_cached(@main_const_eval_18, [%arg19]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %19 = ttcore.load_cached(@main_const_eval_19, [%arg44]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg44) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %20 = ttcore.load_cached(@main_const_eval_20, [%arg25]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg25) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %21 = ttcore.load_cached(@main_const_eval_21, [%arg54]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg54) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %22 = ttcore.load_cached(@main_const_eval_22, [%arg69]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg69) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %23 = ttcore.load_cached(@main_const_eval_23, [%arg35]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg35) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %24 = ttcore.load_cached(@main_const_eval_24, [%arg94]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg94) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %25 = ttcore.load_cached(@main_const_eval_25, [%arg10]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %26 = ttcore.load_cached(@main_const_eval_26, [%arg29]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg29) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %27 = ttcore.load_cached(@main_const_eval_27, [%arg86]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg86) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %28 = ttcore.load_cached(@main_const_eval_28, [%arg70]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg70) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %29 = ttcore.load_cached(@main_const_eval_29, [%arg3]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %30 = ttcore.load_cached(@main_const_eval_30, [%arg20]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %31 = ttcore.load_cached(@main_const_eval_31, [%arg68]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg68) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %32 = ttcore.load_cached(@main_const_eval_32, [%arg88]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg88) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %33 = ttcore.load_cached(@main_const_eval_33, [%arg71]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg71) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %34 = ttcore.load_cached(@main_const_eval_34, [%arg95]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg95) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %35 = ttcore.load_cached(@main_const_eval_35, [%arg23]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg23) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %36 = ttcore.load_cached(@main_const_eval_36, [%arg17]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %37 = ttcore.load_cached(@main_const_eval_37, [%arg78]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg78) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %38 = ttcore.load_cached(@main_const_eval_38, [%arg50]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg50) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %39 = ttcore.load_cached(@main_const_eval_39, [%arg90]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg90) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %40 = ttcore.load_cached(@main_const_eval_40, [%arg4]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %41 = ttcore.load_cached(@main_const_eval_41, [%arg75]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg75) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %42 = ttcore.load_cached(@main_const_eval_42, [%arg81]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg81) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %43 = ttcore.load_cached(@main_const_eval_43, [%arg0]) : (tensor<1000xf32, #ttnn_layout9>) -> tensor<1x1000xf32, #ttnn_layout10> loc(#loc)
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1000xf32, #ttnn_layout9>) -> () loc(#loc)
        %44 = ttcore.load_cached(@main_const_eval_44, [%arg85]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg85) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %45 = ttcore.load_cached(@main_const_eval_45, [%arg98]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg98) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %46 = ttcore.load_cached(@main_const_eval_46, [%arg36]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg36) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %47 = ttcore.load_cached(@main_const_eval_47, [%arg61]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg61) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %48 = ttcore.load_cached(@main_const_eval_48, [%arg24]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg24) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %49 = ttcore.load_cached(@main_const_eval_49, [%arg9]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %50 = ttcore.load_cached(@main_const_eval_50, [%arg14]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %51 = ttcore.load_cached(@main_const_eval_51, [%arg96]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg96) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %52 = ttcore.load_cached(@main_const_eval_52, [%arg76]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg76) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %53 = ttcore.load_cached(@main_const_eval_53, [%arg5]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %54 = ttcore.load_cached(@main_const_eval_54, [%arg30]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg30) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %55 = ttcore.load_cached(@main_const_eval_55, [%arg13]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %56 = ttcore.load_cached(@main_const_eval_56, [%arg65]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg65) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %57 = ttcore.load_cached(@main_const_eval_57, [%arg63]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg63) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %58 = ttcore.load_cached(@main_const_eval_58, [%arg48]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg48) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %59 = ttcore.load_cached(@main_const_eval_59, [%arg64]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg64) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %60 = ttcore.load_cached(@main_const_eval_60, [%arg8]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %61 = ttcore.load_cached(@main_const_eval_61, [%arg51]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg51) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %62 = ttcore.load_cached(@main_const_eval_62, [%arg74]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg74) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %63 = ttcore.load_cached(@main_const_eval_63, [%arg59]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg59) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %64 = ttcore.load_cached(@main_const_eval_64, [%arg34]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg34) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %65 = ttcore.load_cached(@main_const_eval_65, [%arg49]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg49) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %66 = ttcore.load_cached(@main_const_eval_66, [%arg91]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg91) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %67 = ttcore.load_cached(@main_const_eval_67, [%arg18]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %68 = ttcore.load_cached(@main_const_eval_68, [%arg79]) : (tensor<256xf32, #ttnn_layout5>) -> tensor<1x256x1x1xf32, #ttnn_layout6> loc(#loc)
        "ttnn.deallocate"(%arg79) <{force = false}> : (tensor<256xf32, #ttnn_layout5>) -> () loc(#loc)
        %69 = ttcore.load_cached(@main_const_eval_69, [%arg38]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg38) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %70 = ttcore.load_cached(@main_const_eval_70, [%arg83]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg83) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %71 = ttcore.load_cached(@main_const_eval_71, [%arg39]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg39) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %72 = ttcore.load_cached(@main_const_eval_72, [%arg58]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg58) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %73 = ttcore.load_cached(@main_const_eval_73, [%arg2]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %74 = ttcore.load_cached(@main_const_eval_74, [%arg26]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg26) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %75 = ttcore.load_cached(@main_const_eval_75, [%arg84]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg84) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %76 = ttcore.load_cached(@main_const_eval_76, [%arg89]) : (tensor<512xf32, #ttnn_layout1>) -> tensor<1x512x1x1xf32, #ttnn_layout2> loc(#loc)
        "ttnn.deallocate"(%arg89) <{force = false}> : (tensor<512xf32, #ttnn_layout1>) -> () loc(#loc)
        %77 = ttcore.load_cached(@main_const_eval_77, [%arg15]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %78 = ttcore.load_cached(@main_const_eval_78, [%arg41]) : (tensor<64xf32, #ttnn_layout3>) -> tensor<1x64x1x1xf32, #ttnn_layout4> loc(#loc)
        "ttnn.deallocate"(%arg41) <{force = false}> : (tensor<64xf32, #ttnn_layout3>) -> () loc(#loc)
        %79 = ttcore.load_cached(@main_const_eval_79, [%arg45]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg45) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %80 = ttcore.load_cached(@main_const_eval_80, [%arg60]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg60) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %81 = ttcore.load_cached(@main_const_eval_81, [%arg55]) : (tensor<128xf32, #ttnn_layout7>) -> tensor<1x128x1x1xf32, #ttnn_layout8> loc(#loc)
        "ttnn.deallocate"(%arg55) <{force = false}> : (tensor<128xf32, #ttnn_layout7>) -> () loc(#loc)
        %82 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device loc(#loc)
        %83 = "ttnn.permute"(%arg22) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x3x224x224xf32, #ttnn_layout16>) -> tensor<1x224x224x3xf32, #ttnn_layout24> loc(#loc63)
        "ttnn.deallocate"(%arg22) <{force = false}> : (tensor<1x3x224x224xf32, #ttnn_layout16>) -> () loc(#loc63)
        %84 = "ttnn.reshape"(%83) <{shape = [1 : i32, 1 : i32, 50176 : i32, 3 : i32]}> : (tensor<1x224x224x3xf32, #ttnn_layout24>) -> tensor<1x1x50176x3xf32, #ttnn_layout25> loc(#loc64)
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<1x224x224x3xf32, #ttnn_layout24>) -> () loc(#loc64)
        %85 = "ttnn.from_device"(%84) : (tensor<1x1x50176x3xf32, #ttnn_layout25>) -> tensor<1x1x50176x3xf32, #ttnn_layout26> loc(#loc65)
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<1x1x50176x3xf32, #ttnn_layout25>) -> () loc(#loc65)
        %86 = "ttnn.to_layout"(%85) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x50176x3xf32, #ttnn_layout26>) -> tensor<1x1x50176x3xf32, #ttnn_layout27> loc(#loc65)
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x1x50176x3xf32, #ttnn_layout26>) -> () loc(#loc65)
        %87 = "ttnn.to_device"(%86, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x50176x3xf32, #ttnn_layout27>, !ttnn.device) -> tensor<1x1x50176x3xf32, #ttnn_layout28> loc(#loc65)
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<1x1x50176x3xf32, #ttnn_layout27>) -> () loc(#loc65)
        %88 = "ttnn.conv2d"(%87, %arg21, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 3 : i32, input_height = 224 : i32, input_width = 224 : i32, kernel_size = array<i32: 7, 7>, out_channels = 64 : i32, padding = array<i32: 3, 3, 3, 3>, stride = array<i32: 2, 2>}> : (tensor<1x1x50176x3xf32, #ttnn_layout28>, tensor<64x3x7x7xf32, #ttnn_layout15>, !ttnn.device) -> tensor<1x1x12544x64xf32, #ttnn_layout29> loc(#loc66)
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x1x50176x3xf32, #ttnn_layout28>) -> () loc(#loc66)
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<64x3x7x7xf32, #ttnn_layout15>) -> () loc(#loc66)
        %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 112 : i32, 112 : i32, 64 : i32]}> : (tensor<1x1x12544x64xf32, #ttnn_layout29>) -> tensor<1x112x112x64xf32, #ttnn_layout30> loc(#loc67)
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x1x12544x64xf32, #ttnn_layout29>) -> () loc(#loc67)
        %90 = "ttnn.permute"(%89) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x112x112x64xf32, #ttnn_layout30>) -> tensor<1x64x112x112xf32, #ttnn_layout31> loc(#loc66)
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x112x112x64xf32, #ttnn_layout30>) -> () loc(#loc66)
        %91 = "ttnn.batch_norm_inference"(%90, %67, %36, %30, %18) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x112x112xf32, #ttnn_layout31>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>) -> tensor<1x64x112x112xf32, #ttnn_layout31> loc(#loc13)
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x64x112x112xf32, #ttnn_layout31>) -> () loc(#loc13)
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc13)
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc13)
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc13)
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc13)
        %92 = "ttnn.relu"(%91) : (tensor<1x64x112x112xf32, #ttnn_layout31>) -> tensor<1x64x112x112xf32, #ttnn_layout31> loc(#loc68)
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x64x112x112xf32, #ttnn_layout31>) -> () loc(#loc68)
        %93 = "ttnn.permute"(%92) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x112x112xf32, #ttnn_layout31>) -> tensor<1x112x112x64xf32, #ttnn_layout30> loc(#loc68)
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x64x112x112xf32, #ttnn_layout31>) -> () loc(#loc68)
        %94 = "ttnn.reshape"(%93) <{shape = [1 : i32, 1 : i32, 12544 : i32, 64 : i32]}> : (tensor<1x112x112x64xf32, #ttnn_layout30>) -> tensor<1x1x12544x64xf32, #ttnn_layout29> loc(#loc68)
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<1x112x112x64xf32, #ttnn_layout30>) -> () loc(#loc68)
        %95 = "ttnn.typecast"(%94) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x12544x64xf32, #ttnn_layout29>) -> tensor<1x1x12544x64xbf16, #ttnn_layout32> loc(#loc69)
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x1x12544x64xf32, #ttnn_layout29>) -> () loc(#loc69)
        %96 = "ttnn.to_layout"(%95) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x12544x64xbf16, #ttnn_layout32>) -> tensor<1x1x12544x64xbf16, #ttnn_layout33> loc(#loc69)
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x12544x64xbf16, #ttnn_layout32>) -> () loc(#loc69)
        %97 = "ttnn.max_pool2d"(%96) <{batch_size = 1 : si32, ceil_mode = false, channels = 64 : si32, dilation = array<i32: 1, 1>, in_place_halo = false, input_height = 112 : si32, input_width = 112 : si32, kernel_size = array<i32: 3, 3>, padding = array<i32: 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x12544x64xbf16, #ttnn_layout33>) -> tensor<1x1x3136x64xbf16, #ttnn_layout34> loc(#loc70)
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x1x12544x64xbf16, #ttnn_layout33>) -> () loc(#loc70)
        %98 = "ttnn.to_layout"(%97) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x3136x64xbf16, #ttnn_layout34>) -> tensor<1x1x3136x64xbf16, #ttnn_layout35> loc(#loc69)
        %99 = "ttnn.typecast"(%98) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x3136x64xbf16, #ttnn_layout35>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc69)
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x1x3136x64xbf16, #ttnn_layout35>) -> () loc(#loc69)
        %100 = "ttnn.from_device"(%97) : (tensor<1x1x3136x64xbf16, #ttnn_layout34>) -> tensor<1x1x3136x64xbf16, #ttnn_layout37> loc(#loc71)
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x3136x64xbf16, #ttnn_layout34>) -> () loc(#loc71)
        %101 = "ttnn.to_dtype"(%100) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x3136x64xbf16, #ttnn_layout37>) -> tensor<1x1x3136x64xf32, #ttnn_layout38> loc(#loc71)
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x1x3136x64xbf16, #ttnn_layout37>) -> () loc(#loc71)
        %102 = "ttnn.to_device"(%101, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout39> loc(#loc71)
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>) -> () loc(#loc71)
        %103 = "ttnn.conv2d"(%102, %arg32, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>, tensor<64x64x3x3xf32, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc72)
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>) -> () loc(#loc72)
        "ttnn.deallocate"(%arg32) <{force = false}> : (tensor<64x64x3x3xf32, #ttnn_layout17>) -> () loc(#loc72)
        %104 = "ttnn.reshape"(%103) <{shape = [1 : i32, 56 : i32, 56 : i32, 64 : i32]}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc73)
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc73)
        %105 = "ttnn.permute"(%104) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc72)
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc72)
        %106 = "ttnn.batch_norm_inference"(%105, %26, %5, %3, %54) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc3)
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc3)
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc3)
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc3)
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc3)
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc3)
        %107 = "ttnn.relu"(%106) : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc74)
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc74)
        %108 = "ttnn.permute"(%107) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc74)
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc74)
        %109 = "ttnn.reshape"(%108) <{shape = [1 : i32, 1 : i32, 3136 : i32, 64 : i32]}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc74)
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc74)
        %110 = "ttnn.from_device"(%109) : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout42> loc(#loc75)
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc75)
        %111 = "ttnn.to_layout"(%110) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> tensor<1x1x3136x64xf32, #ttnn_layout38> loc(#loc75)
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> () loc(#loc75)
        %112 = "ttnn.to_device"(%111, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout39> loc(#loc75)
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>) -> () loc(#loc75)
        %113 = "ttnn.conv2d"(%112, %arg27, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>, tensor<64x64x3x3xf32, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc76)
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>) -> () loc(#loc76)
        "ttnn.deallocate"(%arg27) <{force = false}> : (tensor<64x64x3x3xf32, #ttnn_layout17>) -> () loc(#loc76)
        %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 56 : i32, 56 : i32, 64 : i32]}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc77)
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc77)
        %115 = "ttnn.permute"(%114) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc76)
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc76)
        %116 = "ttnn.batch_norm_inference"(%115, %48, %35, %74, %20) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc14)
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc14)
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc14)
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc14)
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc14)
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc14)
        %117 = "ttnn.permute"(%116) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc78)
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc78)
        %118 = "ttnn.reshape"(%117) <{shape = [1 : i32, 1 : i32, 3136 : i32, 64 : i32]}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc79)
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc79)
        %119 = "ttnn.add"(%118, %99) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>, tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc80)
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc80)
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc80)
        %120 = "ttnn.relu"(%119) : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc81)
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc81)
        %121 = "ttnn.from_device"(%120) : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout42> loc(#loc82)
        %122 = "ttnn.to_layout"(%121) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> tensor<1x1x3136x64xf32, #ttnn_layout38> loc(#loc82)
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> () loc(#loc82)
        %123 = "ttnn.to_device"(%122, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout39> loc(#loc82)
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>) -> () loc(#loc82)
        %124 = "ttnn.conv2d"(%123, %arg42, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>, tensor<64x64x3x3xf32, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc83)
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>) -> () loc(#loc83)
        "ttnn.deallocate"(%arg42) <{force = false}> : (tensor<64x64x3x3xf32, #ttnn_layout17>) -> () loc(#loc83)
        %125 = "ttnn.reshape"(%124) <{shape = [1 : i32, 56 : i32, 56 : i32, 64 : i32]}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc84)
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc84)
        %126 = "ttnn.permute"(%125) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc83)
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc83)
        %127 = "ttnn.batch_norm_inference"(%126, %71, %69, %78, %7) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc5)
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc5)
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc5)
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc5)
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc5)
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc5)
        %128 = "ttnn.relu"(%127) : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc85)
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc85)
        %129 = "ttnn.permute"(%128) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc85)
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc85)
        %130 = "ttnn.reshape"(%129) <{shape = [1 : i32, 1 : i32, 3136 : i32, 64 : i32]}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc85)
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc85)
        %131 = "ttnn.from_device"(%130) : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout42> loc(#loc86)
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc86)
        %132 = "ttnn.to_layout"(%131) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> tensor<1x1x3136x64xf32, #ttnn_layout38> loc(#loc86)
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> () loc(#loc86)
        %133 = "ttnn.to_device"(%132, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout39> loc(#loc86)
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>) -> () loc(#loc86)
        %134 = "ttnn.conv2d"(%133, %arg37, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 64 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>, tensor<64x64x3x3xf32, #ttnn_layout17>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc87)
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>) -> () loc(#loc87)
        "ttnn.deallocate"(%arg37) <{force = false}> : (tensor<64x64x3x3xf32, #ttnn_layout17>) -> () loc(#loc87)
        %135 = "ttnn.reshape"(%134) <{shape = [1 : i32, 56 : i32, 56 : i32, 64 : i32]}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc88)
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc88)
        %136 = "ttnn.permute"(%135) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc87)
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc87)
        %137 = "ttnn.batch_norm_inference"(%136, %64, %10, %46, %23) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>, tensor<1x64x1x1xf32, #ttnn_layout4>) -> tensor<1x64x56x56xf32, #ttnn_layout41> loc(#loc7)
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc7)
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc7)
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc7)
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc7)
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x64x1x1xf32, #ttnn_layout4>) -> () loc(#loc7)
        %138 = "ttnn.permute"(%137) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> tensor<1x56x56x64xf32, #ttnn_layout40> loc(#loc89)
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x64x56x56xf32, #ttnn_layout41>) -> () loc(#loc89)
        %139 = "ttnn.reshape"(%138) <{shape = [1 : i32, 1 : i32, 3136 : i32, 64 : i32]}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc90)
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x56x56x64xf32, #ttnn_layout40>) -> () loc(#loc90)
        %140 = "ttnn.add"(%139, %120) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>, tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc91)
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc91)
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc91)
        %141 = "ttnn.relu"(%140) : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout36> loc(#loc92)
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc92)
        %142 = "ttnn.from_device"(%141) : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout42> loc(#loc93)
        %143 = "ttnn.to_layout"(%142) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> tensor<1x1x3136x64xf32, #ttnn_layout38> loc(#loc93)
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> () loc(#loc93)
        %144 = "ttnn.to_device"(%143, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout39> loc(#loc93)
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>) -> () loc(#loc93)
        %145 = "ttnn.conv2d"(%144, %arg52, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>, tensor<128x64x3x3xf32, #ttnn_layout19>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc94)
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>) -> () loc(#loc94)
        "ttnn.deallocate"(%arg52) <{force = false}> : (tensor<128x64x3x3xf32, #ttnn_layout19>) -> () loc(#loc94)
        %146 = "ttnn.reshape"(%145) <{shape = [1 : i32, 28 : i32, 28 : i32, 128 : i32]}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc95)
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc95)
        %147 = "ttnn.permute"(%146) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc94)
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc94)
        %148 = "ttnn.batch_norm_inference"(%147, %65, %58, %61, %38) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc19)
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc19)
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc19)
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc19)
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc19)
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc19)
        %149 = "ttnn.relu"(%148) : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc96)
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc96)
        %150 = "ttnn.permute"(%149) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc96)
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc96)
        %151 = "ttnn.reshape"(%150) <{shape = [1 : i32, 1 : i32, 784 : i32, 128 : i32]}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc96)
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc96)
        %152 = "ttnn.from_device"(%151) : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x1x784x128xf32, #ttnn_layout45> loc(#loc97)
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc97)
        %153 = "ttnn.to_layout"(%152) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> tensor<1x1x784x128xf32, #ttnn_layout46> loc(#loc97)
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> () loc(#loc97)
        %154 = "ttnn.to_device"(%153, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x784x128xf32, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout47> loc(#loc97)
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout46>) -> () loc(#loc97)
        %155 = "ttnn.conv2d"(%154, %arg47, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x128xf32, #ttnn_layout47>, tensor<128x128x3x3xf32, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc98)
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout47>) -> () loc(#loc98)
        "ttnn.deallocate"(%arg47) <{force = false}> : (tensor<128x128x3x3xf32, #ttnn_layout18>) -> () loc(#loc98)
        %156 = "ttnn.reshape"(%155) <{shape = [1 : i32, 28 : i32, 28 : i32, 128 : i32]}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc99)
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc99)
        %157 = "ttnn.permute"(%156) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc98)
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc98)
        %158 = "ttnn.batch_norm_inference"(%157, %19, %17, %8, %79) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc6)
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc6)
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc6)
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc6)
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc6)
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc6)
        %159 = "ttnn.from_device"(%141) : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> tensor<1x1x3136x64xf32, #ttnn_layout42> loc(#loc100)
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout36>) -> () loc(#loc100)
        %160 = "ttnn.to_layout"(%159) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> tensor<1x1x3136x64xf32, #ttnn_layout38> loc(#loc100)
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout42>) -> () loc(#loc100)
        %161 = "ttnn.to_device"(%160, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>, !ttnn.device) -> tensor<1x1x3136x64xf32, #ttnn_layout39> loc(#loc100)
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout38>) -> () loc(#loc100)
        %162 = "ttnn.conv2d"(%161, %arg16, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_size = array<i32: 1, 1>, out_channels = 128 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>, tensor<128x64x1x1xf32, #ttnn_layout14>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc101)
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x1x3136x64xf32, #ttnn_layout39>) -> () loc(#loc101)
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<128x64x1x1xf32, #ttnn_layout14>) -> () loc(#loc101)
        %163 = "ttnn.reshape"(%162) <{shape = [1 : i32, 28 : i32, 28 : i32, 128 : i32]}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc102)
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc102)
        %164 = "ttnn.permute"(%163) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc101)
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc101)
        %165 = "ttnn.batch_norm_inference"(%164, %55, %12, %77, %50) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc9)
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc9)
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc9)
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc9)
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc9)
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc9)
        %166 = "ttnn.add"(%158, %165) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>, tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc103)
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc103)
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc103)
        %167 = "ttnn.relu"(%166) : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc104)
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc104)
        %168 = "ttnn.permute"(%167) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc104)
        %169 = "ttnn.reshape"(%168) <{shape = [1 : i32, 1 : i32, 784 : i32, 128 : i32]}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc104)
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc104)
        %170 = "ttnn.from_device"(%169) : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x1x784x128xf32, #ttnn_layout45> loc(#loc105)
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc105)
        %171 = "ttnn.to_layout"(%170) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> tensor<1x1x784x128xf32, #ttnn_layout46> loc(#loc105)
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> () loc(#loc105)
        %172 = "ttnn.to_device"(%171, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x784x128xf32, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout47> loc(#loc105)
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout46>) -> () loc(#loc105)
        %173 = "ttnn.conv2d"(%172, %arg62, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x128xf32, #ttnn_layout47>, tensor<128x128x3x3xf32, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc106)
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout47>) -> () loc(#loc106)
        "ttnn.deallocate"(%arg62) <{force = false}> : (tensor<128x128x3x3xf32, #ttnn_layout18>) -> () loc(#loc106)
        %174 = "ttnn.reshape"(%173) <{shape = [1 : i32, 28 : i32, 28 : i32, 128 : i32]}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc107)
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc107)
        %175 = "ttnn.permute"(%174) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc106)
        "ttnn.deallocate"(%174) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc106)
        %176 = "ttnn.batch_norm_inference"(%175, %63, %72, %47, %80) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc21)
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc21)
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc21)
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc21)
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc21)
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc21)
        %177 = "ttnn.relu"(%176) : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc108)
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc108)
        %178 = "ttnn.permute"(%177) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc108)
        "ttnn.deallocate"(%177) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc108)
        %179 = "ttnn.reshape"(%178) <{shape = [1 : i32, 1 : i32, 784 : i32, 128 : i32]}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc108)
        "ttnn.deallocate"(%178) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc108)
        %180 = "ttnn.from_device"(%179) : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x1x784x128xf32, #ttnn_layout45> loc(#loc109)
        "ttnn.deallocate"(%179) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc109)
        %181 = "ttnn.to_layout"(%180) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> tensor<1x1x784x128xf32, #ttnn_layout46> loc(#loc109)
        "ttnn.deallocate"(%180) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> () loc(#loc109)
        %182 = "ttnn.to_device"(%181, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x784x128xf32, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout47> loc(#loc109)
        "ttnn.deallocate"(%181) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout46>) -> () loc(#loc109)
        %183 = "ttnn.conv2d"(%182, %arg57, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x784x128xf32, #ttnn_layout47>, tensor<128x128x3x3xf32, #ttnn_layout18>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc110)
        "ttnn.deallocate"(%182) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout47>) -> () loc(#loc110)
        "ttnn.deallocate"(%arg57) <{force = false}> : (tensor<128x128x3x3xf32, #ttnn_layout18>) -> () loc(#loc110)
        %184 = "ttnn.reshape"(%183) <{shape = [1 : i32, 28 : i32, 28 : i32, 128 : i32]}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc111)
        "ttnn.deallocate"(%183) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc111)
        %185 = "ttnn.permute"(%184) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc110)
        "ttnn.deallocate"(%184) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc110)
        %186 = "ttnn.batch_norm_inference"(%185, %21, %15, %13, %81) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>, tensor<1x128x1x1xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc10)
        "ttnn.deallocate"(%185) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc10)
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc10)
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc10)
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc10)
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x128x1x1xf32, #ttnn_layout8>) -> () loc(#loc10)
        %187 = "ttnn.add"(%186, %167) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>, tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc112)
        "ttnn.deallocate"(%186) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc112)
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc112)
        %188 = "ttnn.relu"(%187) : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x128x28x28xf32, #ttnn_layout8> loc(#loc113)
        "ttnn.deallocate"(%187) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc113)
        %189 = "ttnn.permute"(%188) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> tensor<1x28x28x128xf32, #ttnn_layout44> loc(#loc113)
        "ttnn.deallocate"(%188) <{force = false}> : (tensor<1x128x28x28xf32, #ttnn_layout8>) -> () loc(#loc113)
        %190 = "ttnn.reshape"(%189) <{shape = [1 : i32, 1 : i32, 784 : i32, 128 : i32]}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> tensor<1x1x784x128xf32, #ttnn_layout43> loc(#loc113)
        "ttnn.deallocate"(%189) <{force = false}> : (tensor<1x28x28x128xf32, #ttnn_layout44>) -> () loc(#loc113)
        %191 = "ttnn.from_device"(%190) : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x1x784x128xf32, #ttnn_layout45> loc(#loc114)
        %192 = "ttnn.to_layout"(%191) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> tensor<1x1x784x128xf32, #ttnn_layout46> loc(#loc114)
        "ttnn.deallocate"(%191) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> () loc(#loc114)
        %193 = "ttnn.to_device"(%192, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x784x128xf32, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout47> loc(#loc114)
        "ttnn.deallocate"(%192) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout46>) -> () loc(#loc114)
        %194 = "ttnn.conv2d"(%193, %arg72, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x784x128xf32, #ttnn_layout47>, tensor<256x128x3x3xf32, #ttnn_layout21>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc115)
        "ttnn.deallocate"(%193) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout47>) -> () loc(#loc115)
        "ttnn.deallocate"(%arg72) <{force = false}> : (tensor<256x128x3x3xf32, #ttnn_layout21>) -> () loc(#loc115)
        %195 = "ttnn.reshape"(%194) <{shape = [1 : i32, 14 : i32, 14 : i32, 256 : i32]}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc116)
        "ttnn.deallocate"(%194) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc116)
        %196 = "ttnn.permute"(%195) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc115)
        "ttnn.deallocate"(%195) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc115)
        %197 = "ttnn.batch_norm_inference"(%196, %22, %31, %33, %28) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc15)
        "ttnn.deallocate"(%196) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc15)
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc15)
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc15)
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc15)
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc15)
        %198 = "ttnn.relu"(%197) : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc117)
        "ttnn.deallocate"(%197) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc117)
        %199 = "ttnn.permute"(%198) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc117)
        "ttnn.deallocate"(%198) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc117)
        %200 = "ttnn.reshape"(%199) <{shape = [1 : i32, 1 : i32, 196 : i32, 256 : i32]}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc117)
        "ttnn.deallocate"(%199) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc117)
        %201 = "ttnn.from_device"(%200) : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x1x196x256xf32, #ttnn_layout50> loc(#loc118)
        "ttnn.deallocate"(%200) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc118)
        %202 = "ttnn.to_layout"(%201) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> tensor<1x1x196x256xf32, #ttnn_layout51> loc(#loc118)
        "ttnn.deallocate"(%201) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> () loc(#loc118)
        %203 = "ttnn.to_device"(%202, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x196x256xf32, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout52> loc(#loc118)
        "ttnn.deallocate"(%202) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout51>) -> () loc(#loc118)
        %204 = "ttnn.conv2d"(%203, %arg67, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x256xf32, #ttnn_layout52>, tensor<256x256x3x3xf32, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc119)
        "ttnn.deallocate"(%203) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout52>) -> () loc(#loc119)
        "ttnn.deallocate"(%arg67) <{force = false}> : (tensor<256x256x3x3xf32, #ttnn_layout20>) -> () loc(#loc119)
        %205 = "ttnn.reshape"(%204) <{shape = [1 : i32, 14 : i32, 14 : i32, 256 : i32]}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc120)
        "ttnn.deallocate"(%204) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc120)
        %206 = "ttnn.permute"(%205) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc119)
        "ttnn.deallocate"(%205) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc119)
        %207 = "ttnn.batch_norm_inference"(%206, %59, %57, %14, %56) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc11)
        "ttnn.deallocate"(%206) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc11)
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc11)
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc11)
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc11)
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc11)
        %208 = "ttnn.from_device"(%190) : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> tensor<1x1x784x128xf32, #ttnn_layout45> loc(#loc121)
        "ttnn.deallocate"(%190) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout43>) -> () loc(#loc121)
        %209 = "ttnn.to_layout"(%208) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> tensor<1x1x784x128xf32, #ttnn_layout46> loc(#loc121)
        "ttnn.deallocate"(%208) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout45>) -> () loc(#loc121)
        %210 = "ttnn.to_device"(%209, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x784x128xf32, #ttnn_layout46>, !ttnn.device) -> tensor<1x1x784x128xf32, #ttnn_layout47> loc(#loc121)
        "ttnn.deallocate"(%209) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout46>) -> () loc(#loc121)
        %211 = "ttnn.conv2d"(%210, %arg11, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_size = array<i32: 1, 1>, out_channels = 256 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x784x128xf32, #ttnn_layout47>, tensor<256x128x1x1xf32, #ttnn_layout13>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc122)
        "ttnn.deallocate"(%210) <{force = false}> : (tensor<1x1x784x128xf32, #ttnn_layout47>) -> () loc(#loc122)
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<256x128x1x1xf32, #ttnn_layout13>) -> () loc(#loc122)
        %212 = "ttnn.reshape"(%211) <{shape = [1 : i32, 14 : i32, 14 : i32, 256 : i32]}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc123)
        "ttnn.deallocate"(%211) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc123)
        %213 = "ttnn.permute"(%212) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc122)
        "ttnn.deallocate"(%212) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc122)
        %214 = "ttnn.batch_norm_inference"(%213, %60, %11, %25, %49) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc8)
        "ttnn.deallocate"(%213) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc8)
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc8)
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc8)
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc8)
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc8)
        %215 = "ttnn.add"(%207, %214) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>, tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc124)
        "ttnn.deallocate"(%214) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc124)
        "ttnn.deallocate"(%207) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc124)
        %216 = "ttnn.relu"(%215) : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc125)
        "ttnn.deallocate"(%215) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc125)
        %217 = "ttnn.permute"(%216) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc125)
        %218 = "ttnn.reshape"(%217) <{shape = [1 : i32, 1 : i32, 196 : i32, 256 : i32]}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc125)
        "ttnn.deallocate"(%217) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc125)
        %219 = "ttnn.from_device"(%218) : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x1x196x256xf32, #ttnn_layout50> loc(#loc126)
        "ttnn.deallocate"(%218) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc126)
        %220 = "ttnn.to_layout"(%219) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> tensor<1x1x196x256xf32, #ttnn_layout51> loc(#loc126)
        "ttnn.deallocate"(%219) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> () loc(#loc126)
        %221 = "ttnn.to_device"(%220, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x196x256xf32, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout52> loc(#loc126)
        "ttnn.deallocate"(%220) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout51>) -> () loc(#loc126)
        %222 = "ttnn.conv2d"(%221, %arg82, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x256xf32, #ttnn_layout52>, tensor<256x256x3x3xf32, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc127)
        "ttnn.deallocate"(%221) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout52>) -> () loc(#loc127)
        "ttnn.deallocate"(%arg82) <{force = false}> : (tensor<256x256x3x3xf32, #ttnn_layout20>) -> () loc(#loc127)
        %223 = "ttnn.reshape"(%222) <{shape = [1 : i32, 14 : i32, 14 : i32, 256 : i32]}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc128)
        "ttnn.deallocate"(%222) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc128)
        %224 = "ttnn.permute"(%223) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc127)
        "ttnn.deallocate"(%223) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc127)
        %225 = "ttnn.batch_norm_inference"(%224, %68, %37, %42, %16) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc12)
        "ttnn.deallocate"(%224) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc12)
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc12)
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc12)
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc12)
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc12)
        %226 = "ttnn.relu"(%225) : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc129)
        "ttnn.deallocate"(%225) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc129)
        %227 = "ttnn.permute"(%226) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc129)
        "ttnn.deallocate"(%226) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc129)
        %228 = "ttnn.reshape"(%227) <{shape = [1 : i32, 1 : i32, 196 : i32, 256 : i32]}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc129)
        "ttnn.deallocate"(%227) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc129)
        %229 = "ttnn.from_device"(%228) : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x1x196x256xf32, #ttnn_layout50> loc(#loc130)
        "ttnn.deallocate"(%228) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc130)
        %230 = "ttnn.to_layout"(%229) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> tensor<1x1x196x256xf32, #ttnn_layout51> loc(#loc130)
        "ttnn.deallocate"(%229) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> () loc(#loc130)
        %231 = "ttnn.to_device"(%230, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x196x256xf32, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout52> loc(#loc130)
        "ttnn.deallocate"(%230) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout51>) -> () loc(#loc130)
        %232 = "ttnn.conv2d"(%231, %arg77, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x196x256xf32, #ttnn_layout52>, tensor<256x256x3x3xf32, #ttnn_layout20>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc131)
        "ttnn.deallocate"(%231) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout52>) -> () loc(#loc131)
        "ttnn.deallocate"(%arg77) <{force = false}> : (tensor<256x256x3x3xf32, #ttnn_layout20>) -> () loc(#loc131)
        %233 = "ttnn.reshape"(%232) <{shape = [1 : i32, 14 : i32, 14 : i32, 256 : i32]}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc132)
        "ttnn.deallocate"(%232) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc132)
        %234 = "ttnn.permute"(%233) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc131)
        "ttnn.deallocate"(%233) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc131)
        %235 = "ttnn.batch_norm_inference"(%234, %62, %6, %52, %41) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>, tensor<1x256x1x1xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc4)
        "ttnn.deallocate"(%234) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc4)
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc4)
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc4)
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc4)
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<1x256x1x1xf32, #ttnn_layout6>) -> () loc(#loc4)
        %236 = "ttnn.add"(%235, %216) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>, tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc133)
        "ttnn.deallocate"(%235) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc133)
        "ttnn.deallocate"(%216) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc133)
        %237 = "ttnn.relu"(%236) : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x256x14x14xf32, #ttnn_layout6> loc(#loc134)
        "ttnn.deallocate"(%236) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc134)
        %238 = "ttnn.permute"(%237) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> tensor<1x14x14x256xf32, #ttnn_layout49> loc(#loc134)
        "ttnn.deallocate"(%237) <{force = false}> : (tensor<1x256x14x14xf32, #ttnn_layout6>) -> () loc(#loc134)
        %239 = "ttnn.reshape"(%238) <{shape = [1 : i32, 1 : i32, 196 : i32, 256 : i32]}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> tensor<1x1x196x256xf32, #ttnn_layout48> loc(#loc134)
        "ttnn.deallocate"(%238) <{force = false}> : (tensor<1x14x14x256xf32, #ttnn_layout49>) -> () loc(#loc134)
        %240 = "ttnn.from_device"(%239) : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x1x196x256xf32, #ttnn_layout50> loc(#loc135)
        %241 = "ttnn.to_layout"(%240) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> tensor<1x1x196x256xf32, #ttnn_layout51> loc(#loc135)
        "ttnn.deallocate"(%240) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> () loc(#loc135)
        %242 = "ttnn.to_device"(%241, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x196x256xf32, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout52> loc(#loc135)
        "ttnn.deallocate"(%241) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout51>) -> () loc(#loc135)
        %243 = "ttnn.conv2d"(%242, %arg92, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> : (tensor<1x1x196x256xf32, #ttnn_layout52>, tensor<512x256x3x3xf32, #ttnn_layout23>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc136)
        "ttnn.deallocate"(%242) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout52>) -> () loc(#loc136)
        "ttnn.deallocate"(%arg92) <{force = false}> : (tensor<512x256x3x3xf32, #ttnn_layout23>) -> () loc(#loc136)
        %244 = "ttnn.reshape"(%243) <{shape = [1 : i32, 7 : i32, 7 : i32, 512 : i32]}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc137)
        "ttnn.deallocate"(%243) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc137)
        %245 = "ttnn.permute"(%244) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc136)
        "ttnn.deallocate"(%244) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc136)
        %246 = "ttnn.batch_norm_inference"(%245, %76, %32, %66, %39) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc18)
        "ttnn.deallocate"(%245) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc18)
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc18)
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc18)
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc18)
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc18)
        %247 = "ttnn.relu"(%246) : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc138)
        "ttnn.deallocate"(%246) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc138)
        %248 = "ttnn.permute"(%247) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc138)
        "ttnn.deallocate"(%247) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc138)
        %249 = "ttnn.reshape"(%248) <{shape = [1 : i32, 1 : i32, 49 : i32, 512 : i32]}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc138)
        "ttnn.deallocate"(%248) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc138)
        %250 = "ttnn.from_device"(%249) : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x1x49x512xf32, #ttnn_layout55> loc(#loc139)
        "ttnn.deallocate"(%249) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc139)
        %251 = "ttnn.to_layout"(%250) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x512xf32, #ttnn_layout55>) -> tensor<1x1x49x512xf32, #ttnn_layout56> loc(#loc139)
        "ttnn.deallocate"(%250) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout55>) -> () loc(#loc139)
        %252 = "ttnn.to_device"(%251, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x49x512xf32, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout57> loc(#loc139)
        "ttnn.deallocate"(%251) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout56>) -> () loc(#loc139)
        %253 = "ttnn.conv2d"(%252, %arg87, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x512xf32, #ttnn_layout57>, tensor<512x512x3x3xf32, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc140)
        "ttnn.deallocate"(%252) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout57>) -> () loc(#loc140)
        "ttnn.deallocate"(%arg87) <{force = false}> : (tensor<512x512x3x3xf32, #ttnn_layout22>) -> () loc(#loc140)
        %254 = "ttnn.reshape"(%253) <{shape = [1 : i32, 7 : i32, 7 : i32, 512 : i32]}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc141)
        "ttnn.deallocate"(%253) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc141)
        %255 = "ttnn.permute"(%254) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc140)
        "ttnn.deallocate"(%254) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc140)
        %256 = "ttnn.batch_norm_inference"(%255, %75, %70, %27, %44) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc16)
        "ttnn.deallocate"(%255) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc16)
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc16)
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc16)
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc16)
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc16)
        %257 = "ttnn.from_device"(%239) : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> tensor<1x1x196x256xf32, #ttnn_layout50> loc(#loc142)
        "ttnn.deallocate"(%239) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout48>) -> () loc(#loc142)
        %258 = "ttnn.to_layout"(%257) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> tensor<1x1x196x256xf32, #ttnn_layout51> loc(#loc142)
        "ttnn.deallocate"(%257) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout50>) -> () loc(#loc142)
        %259 = "ttnn.to_device"(%258, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x196x256xf32, #ttnn_layout51>, !ttnn.device) -> tensor<1x1x196x256xf32, #ttnn_layout52> loc(#loc142)
        "ttnn.deallocate"(%258) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout51>) -> () loc(#loc142)
        %260 = "ttnn.conv2d"(%259, %arg6, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_size = array<i32: 1, 1>, out_channels = 512 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> : (tensor<1x1x196x256xf32, #ttnn_layout52>, tensor<512x256x1x1xf32, #ttnn_layout12>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc143)
        "ttnn.deallocate"(%259) <{force = false}> : (tensor<1x1x196x256xf32, #ttnn_layout52>) -> () loc(#loc143)
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<512x256x1x1xf32, #ttnn_layout12>) -> () loc(#loc143)
        %261 = "ttnn.reshape"(%260) <{shape = [1 : i32, 7 : i32, 7 : i32, 512 : i32]}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc144)
        "ttnn.deallocate"(%260) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc144)
        %262 = "ttnn.permute"(%261) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc143)
        "ttnn.deallocate"(%261) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc143)
        %263 = "ttnn.batch_norm_inference"(%262, %29, %73, %53, %40) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc17)
        "ttnn.deallocate"(%262) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc17)
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc17)
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc17)
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc17)
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc17)
        %264 = "ttnn.add"(%256, %263) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>, tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc145)
        "ttnn.deallocate"(%263) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc145)
        "ttnn.deallocate"(%256) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc145)
        %265 = "ttnn.relu"(%264) : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc146)
        "ttnn.deallocate"(%264) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc146)
        %266 = "ttnn.permute"(%265) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc147)
        %267 = "ttnn.reshape"(%266) <{shape = [1 : i32, 1 : i32, 49 : i32, 512 : i32]}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc148)
        "ttnn.deallocate"(%266) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc148)
        %268 = "ttnn.from_device"(%267) : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x1x49x512xf32, #ttnn_layout55> loc(#loc149)
        "ttnn.deallocate"(%267) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc149)
        %269 = "ttnn.to_layout"(%268) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x512xf32, #ttnn_layout55>) -> tensor<1x1x49x512xf32, #ttnn_layout56> loc(#loc149)
        "ttnn.deallocate"(%268) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout55>) -> () loc(#loc149)
        %270 = "ttnn.to_device"(%269, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x49x512xf32, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout57> loc(#loc149)
        "ttnn.deallocate"(%269) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout56>) -> () loc(#loc149)
        %271 = "ttnn.conv2d"(%270, %arg102, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x512xf32, #ttnn_layout57>, tensor<512x512x3x3xf32, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc150)
        "ttnn.deallocate"(%270) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout57>) -> () loc(#loc150)
        "ttnn.deallocate"(%arg102) <{force = false}> : (tensor<512x512x3x3xf32, #ttnn_layout22>) -> () loc(#loc150)
        %272 = "ttnn.reshape"(%271) <{shape = [1 : i32, 7 : i32, 7 : i32, 512 : i32]}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc151)
        "ttnn.deallocate"(%271) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc151)
        %273 = "ttnn.permute"(%272) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc150)
        "ttnn.deallocate"(%272) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc150)
        %274 = "ttnn.batch_norm_inference"(%273, %2, %45, %9, %4) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc2)
        "ttnn.deallocate"(%273) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc2)
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc2)
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc2)
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc2)
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc2)
        %275 = "ttnn.relu"(%274) : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc152)
        "ttnn.deallocate"(%274) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc152)
        %276 = "ttnn.permute"(%275) <{permutation = array<i64: 0, 2, 3, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc152)
        "ttnn.deallocate"(%275) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc152)
        %277 = "ttnn.reshape"(%276) <{shape = [1 : i32, 1 : i32, 49 : i32, 512 : i32]}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc152)
        "ttnn.deallocate"(%276) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc152)
        %278 = "ttnn.from_device"(%277) : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x1x49x512xf32, #ttnn_layout55> loc(#loc153)
        "ttnn.deallocate"(%277) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc153)
        %279 = "ttnn.to_layout"(%278) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x49x512xf32, #ttnn_layout55>) -> tensor<1x1x49x512xf32, #ttnn_layout56> loc(#loc153)
        "ttnn.deallocate"(%278) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout55>) -> () loc(#loc153)
        %280 = "ttnn.to_device"(%279, %82) <{memory_config = #ttnn.memory_config<#dram, <interleaved>>}> : (tensor<1x1x49x512xf32, #ttnn_layout56>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout57> loc(#loc153)
        "ttnn.deallocate"(%279) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout56>) -> () loc(#loc153)
        %281 = "ttnn.conv2d"(%280, %arg97, %82) <{batch_size = 1 : i32, conv2d_config = #ttnn.conv2d_config<weights_dtype = bf16, deallocate_activation = false, reallocate_halo_output = false, act_block_h_override = 0, act_block_w_div = 1, reshard_if_not_optimal = false, override_sharding_config = false, transpose_shards = false, output_layout = tile, enable_act_double_buffer = false, enable_weights_double_buffer = false, in_place = false, enable_kernel_stride_folding = false>, conv2d_slice_config = #ttnn.conv2d_slice_config<l1_full, 0>, dilation = array<i32: 1, 1>, dtype = #ttcore.supportedDataTypes<f32>, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_size = array<i32: 3, 3>, out_channels = 512 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> : (tensor<1x1x49x512xf32, #ttnn_layout57>, tensor<512x512x3x3xf32, #ttnn_layout22>, !ttnn.device) -> tensor<1x1x49x512xf32, #ttnn_layout53> loc(#loc154)
        "ttnn.deallocate"(%280) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout57>) -> () loc(#loc154)
        "ttnn.deallocate"(%arg97) <{force = false}> : (tensor<512x512x3x3xf32, #ttnn_layout22>) -> () loc(#loc154)
        %282 = "ttnn.reshape"(%281) <{shape = [1 : i32, 7 : i32, 7 : i32, 512 : i32]}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> tensor<1x7x7x512xf32, #ttnn_layout54> loc(#loc155)
        "ttnn.deallocate"(%281) <{force = false}> : (tensor<1x1x49x512xf32, #ttnn_layout53>) -> () loc(#loc155)
        %283 = "ttnn.permute"(%282) <{permutation = array<i64: 0, 3, 1, 2>}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc154)
        "ttnn.deallocate"(%282) <{force = false}> : (tensor<1x7x7x512xf32, #ttnn_layout54>) -> () loc(#loc154)
        %284 = "ttnn.batch_norm_inference"(%283, %24, %1, %51, %34) <{epsilon = 9.99999974E-6 : f32, operandSegmentSizes = array<i32: 1, 1, 1, 1, 1>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>, tensor<1x512x1x1xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc1)
        "ttnn.deallocate"(%283) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc1)
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc1)
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc1)
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc1)
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x512x1x1xf32, #ttnn_layout2>) -> () loc(#loc1)
        %285 = "ttnn.add"(%284, %265) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512x7x7xf32, #ttnn_layout2>, tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc156)
        "ttnn.deallocate"(%284) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc156)
        "ttnn.deallocate"(%265) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc156)
        %286 = "ttnn.relu"(%285) : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x512x7x7xf32, #ttnn_layout2> loc(#loc157)
        "ttnn.deallocate"(%285) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc157)
        %287 = "ttnn.sum"(%286) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> tensor<1x512xf32, #ttnn_layout> loc(#loc158)
        "ttnn.deallocate"(%286) <{force = false}> : (tensor<1x512x7x7xf32, #ttnn_layout2>) -> () loc(#loc158)
        %288 = "ttnn.multiply"(%287, %0) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x512xf32, #ttnn_layout>, tensor<1x512xf32, #ttnn_layout>) -> tensor<1x512xf32, #ttnn_layout> loc(#loc158)
        "ttnn.deallocate"(%287) <{force = false}> : (tensor<1x512xf32, #ttnn_layout>) -> () loc(#loc158)
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x512xf32, #ttnn_layout>) -> () loc(#loc158)
        %289 = "ttnn.linear"(%288, %arg1, %43) <{transpose_a = false, transpose_b = true}> : (tensor<1x512xf32, #ttnn_layout>, tensor<1000x512xf32, #ttnn_layout11>, tensor<1x1000xf32, #ttnn_layout10>) -> tensor<1x1000xf32, #ttnn_layout10> loc(#loc20)
        "ttnn.deallocate"(%288) <{force = false}> : (tensor<1x512xf32, #ttnn_layout>) -> () loc(#loc20)
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x1000xf32, #ttnn_layout10>) -> () loc(#loc20)
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<1000x512xf32, #ttnn_layout11>) -> () loc(#loc20)
        return %289 : tensor<1x1000xf32, #ttnn_layout10> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc2 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc3 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc4 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc5 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc6 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc7 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc8 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[2].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|aten__native_batch_norm")
#loc9 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[1].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|aten__native_batch_norm")
#loc10 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc11 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc12 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc13 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|BatchNorm2d[resnet.embedder.embedder.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc14 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc15 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc16 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc17 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|BatchNorm2d[resnet.encoder.stages[3].layers[0].shortcut.normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|96|aten__native_batch_norm")
#loc18 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc19 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc20 = loc("Sequential[classifier]|Linear[getattr(classifier, '1')]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:329|forward|347|aten__add")
#loc21 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|BatchNorm2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').normalization]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|54|aten__native_batch_norm")
#loc63 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_input")
#loc64 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_input_reshape")
#loc65 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc66 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc67 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|Conv2d[resnet.embedder.embedder.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc68 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|ResNetConvLayer[resnet.embedder.embedder]|ReLU[resnet.embedder.embedder.activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc69 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|MaxPool2d[resnet.embedder.pooler]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:72|forward|79|aten__max_pool2d_workaround")
#loc70 = loc("ResNetModel[resnet]|ResNetEmbeddings[resnet.embedder]|MaxPool2d[resnet.embedder.pooler]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:72|forward|79|aten__max_pool2d")
#loc71 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc72 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc73 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc74 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '0')]|ReLU[getattr(resnet.encoder.stages[0].layers[0].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc75 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc76 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc77 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|Sequential[resnet.encoder.stages[0].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc78 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|Conv2d[resnet.encoder.stages[1].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_input_tm0_tm1_tm0_tm0")
#loc79 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_input_reshape_tm0_tm0")
#loc80 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc81 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[0]]|ReLU[resnet.encoder.stages[0].layers[0].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc82 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc83 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc84 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc85 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '0')]|ReLU[getattr(resnet.encoder.stages[0].layers[1].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc86 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc87 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc88 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|Sequential[resnet.encoder.stages[0].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[0].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[0].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc89 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|Conv2d[resnet.encoder.stages[1].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_input_tm0_tm0")
#loc90 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_input_reshape_tm0_tm0")
#loc91 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc92 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[0]]|ResNetBasicLayer[resnet.encoder.stages[0].layers[1]]|ReLU[resnet.encoder.stages[0].layers[1].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc93 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc94 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc95 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc96 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '0')]|ReLU[getattr(resnet.encoder.stages[1].layers[0].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc97 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc98 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc99 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|Sequential[resnet.encoder.stages[1].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc100 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|Conv2d[resnet.encoder.stages[1].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_workaround")
#loc101 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|Conv2d[resnet.encoder.stages[1].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable")
#loc102 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ResNetShortCut[resnet.encoder.stages[1].layers[0].shortcut]|Conv2d[resnet.encoder.stages[1].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_reshape")
#loc103 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc104 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[0]]|ReLU[resnet.encoder.stages[1].layers[0].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc105 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc106 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc107 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc108 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '0')]|ReLU[getattr(resnet.encoder.stages[1].layers[1].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc109 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc110 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc111 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|Sequential[resnet.encoder.stages[1].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[1].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[1].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc112 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc113 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[1]]|ResNetBasicLayer[resnet.encoder.stages[1].layers[1]]|ReLU[resnet.encoder.stages[1].layers[1].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc114 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc115 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc116 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc117 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '0')]|ReLU[getattr(resnet.encoder.stages[2].layers[0].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc118 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc119 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc120 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|Sequential[resnet.encoder.stages[2].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc121 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|Conv2d[resnet.encoder.stages[2].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_workaround")
#loc122 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|Conv2d[resnet.encoder.stages[2].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable")
#loc123 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ResNetShortCut[resnet.encoder.stages[2].layers[0].shortcut]|Conv2d[resnet.encoder.stages[2].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_reshape")
#loc124 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc125 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[0]]|ReLU[resnet.encoder.stages[2].layers[0].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc126 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc127 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc128 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc129 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '0')]|ReLU[getattr(resnet.encoder.stages[2].layers[1].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc130 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc131 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc132 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|Sequential[resnet.encoder.stages[2].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[2].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[2].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc133 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc134 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[2]]|ResNetBasicLayer[resnet.encoder.stages[2].layers[1]]|ReLU[resnet.encoder.stages[2].layers[1].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc135 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc136 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc137 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc138 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '0')]|ReLU[getattr(resnet.encoder.stages[3].layers[0].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc139 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc140 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc141 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|Sequential[resnet.encoder.stages[3].layers[0].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[0].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[0].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc142 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|Conv2d[resnet.encoder.stages[3].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_workaround")
#loc143 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|Conv2d[resnet.encoder.stages[3].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable")
#loc144 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ResNetShortCut[resnet.encoder.stages[3].layers[0].shortcut]|Conv2d[resnet.encoder.stages[3].layers[0].shortcut.convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:94|forward|95|aten__convolution_overrideable_reshape")
#loc145 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc146 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[0]]|ReLU[resnet.encoder.stages[3].layers[0].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc147 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_input")
#loc148 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_input_reshape")
#loc149 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc150 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc151 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '0').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc152 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '0')]|ReLU[getattr(resnet.encoder.stages[3].layers[1].layer, '0').activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|55|aten__relu")
#loc153 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_workaround")
#loc154 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable")
#loc155 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|Sequential[resnet.encoder.stages[3].layers[1].layer]|ResNetConvLayer[getattr(resnet.encoder.stages[3].layers[1].layer, '1')]|Conv2d[getattr(resnet.encoder.stages[3].layers[1].layer, '1').convolution]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:52|forward|53|aten__convolution_overrideable_reshape")
#loc156 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|121|aten__add")
#loc157 = loc("ResNetModel[resnet]|ResNetEncoder[resnet.encoder]|ResNetStage[resnet.encoder.stages[3]]|ResNetBasicLayer[resnet.encoder.stages[3].layers[1]]|ReLU[resnet.encoder.stages[3].layers[1].activation]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:117|forward|122|aten__relu")
#loc158 = loc("ResNetModel[resnet]|AdaptiveAvgPool2d[resnet.pooler]|/usr/local/lib/python3.11/dist-packages/transformers/models/resnet/modeling_resnet.py:281|forward|297|aten__mean")
