#loc1 = loc("variables['params']['Dense_0']['bias']")
#loc2 = loc("variables['params']['Dense_0']['kernel']")
#loc3 = loc("variables['params']['Dense_1']['bias']")
#loc4 = loc("variables['params']['Dense_1']['kernel']")
#loc5 = loc("variables['params']['Dense_2']['bias']")
#loc6 = loc("variables['params']['Dense_2']['kernel']")
#loc7 = loc("args[0]")
module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @empty_mesh = <["default_updated"=1, "default"=1]> loc(#loc)
  func.func public @main(%arg0: tensor<128xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("variables['params']['Dense_0']['bias']"), %arg1: tensor<784x128xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("variables['params']['Dense_0']['kernel']"), %arg2: tensor<128xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("variables['params']['Dense_1']['bias']"), %arg3: tensor<128x128xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("variables['params']['Dense_1']['kernel']"), %arg4: tensor<10xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("variables['params']['Dense_2']['bias']"), %arg5: tensor<128x10xf32> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("variables['params']['Dense_2']['kernel']"), %arg6: tensor<1x28x28x1xf32> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("args[0]")) -> (tensor<1x10xf32> {jax.result_info = "result", ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %0 = stablehlo.reshape %arg6 : (tensor<1x28x28x1xf32>) -> tensor<1x784xf32> loc(#loc70)
    %1 = stablehlo.dot_general %0, %arg1, contracting_dims = [1] x [0] : (tensor<1x784xf32>, tensor<784x128xf32>) -> tensor<1x128xf32> loc(#loc71)
    %2 = stablehlo.reshape %arg0 : (tensor<128xf32>) -> tensor<1x128xf32> loc(#loc72)
    %3 = stablehlo.add %1, %2 : tensor<1x128xf32> loc(#loc73)
    %4 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc89)
    %5 = stablehlo.maximum %3, %4 : tensor<1x128xf32> loc(#loc89)
    %6 = stablehlo.dot_general %5, %arg3, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x128xf32>) -> tensor<1x128xf32> loc(#loc76)
    %7 = stablehlo.reshape %arg2 : (tensor<128xf32>) -> tensor<1x128xf32> loc(#loc77)
    %8 = stablehlo.add %6, %7 : tensor<1x128xf32> loc(#loc78)
    %9 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc89)
    %10 = stablehlo.maximum %8, %9 : tensor<1x128xf32> loc(#loc89)
    %11 = stablehlo.dot_general %10, %arg5, contracting_dims = [1] x [0] : (tensor<1x128xf32>, tensor<128x10xf32>) -> tensor<1x10xf32> loc(#loc79)
    %12 = stablehlo.reshape %arg4 : (tensor<10xf32>) -> tensor<1x10xf32> loc(#loc80)
    %13 = stablehlo.add %11, %12 : tensor<1x10xf32> loc(#loc81)
    %14 = stablehlo.reduce(%13 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32> loc(#loc82)
    %15 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1xf32> loc(#loc83)
    %16 = stablehlo.maximum %15, %14 : tensor<1xf32> loc(#loc83)
    %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xf32>) -> tensor<1x1xf32> loc(#loc84)
    %18 = stablehlo.broadcast_in_dim %17, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32> loc(#loc85)
    %19 = stablehlo.subtract %13, %18 : tensor<1x10xf32> loc(#loc85)
    %20 = stablehlo.exponential %19 : tensor<1x10xf32> loc(#loc86)
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1x10xf32>, tensor<f32>) -> tensor<1xf32> loc(#loc87)
    %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xf32>) -> tensor<1x1xf32> loc(#loc84)
    %23 = stablehlo.broadcast_in_dim %22, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x10xf32> loc(#loc88)
    %24 = stablehlo.divide %20, %23 : tensor<1x10xf32> loc(#loc88)
    return %24 : tensor<1x10xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc8 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/third_party/tt_forge_models/mnist/image_classification/jax/mlp/model_implementation.py":16:12 to :39)
#loc9 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":1216:14 to :44)
#loc10 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":699:13 to :57)
#loc11 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":3022:13 to :78)
#loc12 = loc("/usr/local/lib/python3.11/dist-packages/flax/core/scope.py":1079:10 to :35)
#loc13 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":2240:11 to 2245:44)
#loc14 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/python_package/jax_plugin_tt/monkeypatch.py":285:88 to 287:13)
#loc15 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/tests/infra/workloads/workload.py":70:19 to :70)
#loc16 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/tests/infra/runners/jax_device_runner.py":31:19 to :37)
#loc17 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/tests/infra/runners/device_runner.py":40:15 to :59)
#loc18 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/linear.py":271:8 to 276:5)
#loc19 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/third_party/tt_forge_models/mnist/image_classification/jax/mlp/model_implementation.py":19:16 to :39)
#loc20 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/linear.py":278:11 to :57)
#loc21 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/linear.py":278:6 to :57)
#loc22 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/third_party/tt_forge_models/mnist/image_classification/jax/mlp/model_implementation.py":20:16 to :26)
#loc23 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/third_party/tt_forge_models/mnist/image_classification/jax/mlp/model_implementation.py":22:12 to :36)
#loc24 = loc("/proj_sw/user_dev/sdjukic/tt-xla-repo/tt-xla/third_party/tt_forge_models/mnist/image_classification/jax/mlp/model_implementation.py":23:12 to :25)
#loc25 = loc("MNISTMLPModel.__call__"(#loc8))
#loc26 = loc("Module._call_wrapped_method"(#loc9))
#loc27 = loc("wrap_method_once.<locals>.wrapped_module_method"(#loc10))
#loc28 = loc("apply.<locals>.scope_fn"(#loc11))
#loc29 = loc("apply.<locals>.wrapper"(#loc12))
#loc30 = loc("Module.apply"(#loc13))
#loc31 = loc("_create_flax_apply_patch_config.<locals>.<lambda>.<locals>.<lambda>"(#loc14))
#loc32 = loc("Workload.execute"(#loc15))
#loc33 = loc("JaxDeviceRunner._run_on_device"(#loc16))
#loc34 = loc("DeviceRunner.run_on_device"(#loc17))
#loc35 = loc("Dense.__call__"(#loc18))
#loc36 = loc("MNISTMLPModel.__call__"(#loc19))
#loc37 = loc("Dense.__call__"(#loc20))
#loc38 = loc("Dense.__call__"(#loc21))
#loc39 = loc("MNISTMLPModel.__call__"(#loc22))
#loc40 = loc("MNISTMLPModel.__call__"(#loc23))
#loc41 = loc("MNISTMLPModel.__call__"(#loc24))
#loc42 = loc(callsite(#loc33 at #loc34))
#loc43 = loc(callsite(#loc30 at #loc31))
#loc44 = loc(callsite(#loc32 at #loc42))
#loc45 = loc(callsite(#loc29 at #loc43))
#loc46 = loc(callsite(#loc31 at #loc44))
#loc47 = loc(callsite(#loc28 at #loc45))
#loc48 = loc(callsite(#loc30 at #loc46))
#loc49 = loc(callsite(#loc27 at #loc47))
#loc50 = loc(callsite(#loc29 at #loc48))
#loc51 = loc(callsite(#loc26 at #loc49))
#loc52 = loc(callsite(#loc28 at #loc50))
#loc53 = loc(callsite(#loc36 at #loc51))
#loc54 = loc(callsite(#loc40 at #loc51))
#loc55 = loc(callsite(#loc27 at #loc52))
#loc56 = loc(callsite(#loc27 at #loc53))
#loc57 = loc(callsite(#loc27 at #loc54))
#loc58 = loc(callsite(#loc26 at #loc55))
#loc59 = loc(callsite(#loc26 at #loc56))
#loc60 = loc(callsite(#loc26 at #loc57))
#loc61 = loc(callsite(#loc25 at #loc58))
#loc62 = loc(callsite(#loc35 at #loc59))
#loc63 = loc(callsite(#loc37 at #loc59))
#loc64 = loc(callsite(#loc38 at #loc59))
#loc65 = loc(callsite(#loc39 at #loc58))
#loc66 = loc(callsite(#loc35 at #loc60))
#loc67 = loc(callsite(#loc37 at #loc60))
#loc68 = loc(callsite(#loc38 at #loc60))
#loc69 = loc(callsite(#loc41 at #loc58))
#loc70 = loc("jit(<lambda>)/MNISTMLPModel/reshape"(#loc61))
#loc71 = loc("jit(<lambda>)/MNISTMLPModel/Dense_0/dot_general"(#loc62))
#loc72 = loc("jit(<lambda>)/MNISTMLPModel/Dense_0/reshape"(#loc63))
#loc73 = loc("jit(<lambda>)/MNISTMLPModel/Dense_0/add"(#loc64))
#loc74 = loc("max"(#loc65))
#loc75 = loc("jit(<lambda>)/MNISTMLPModel/jit(relu)"(#loc65))
#loc76 = loc("jit(<lambda>)/MNISTMLPModel/Dense_1/dot_general"(#loc62))
#loc77 = loc("jit(<lambda>)/MNISTMLPModel/Dense_1/reshape"(#loc63))
#loc78 = loc("jit(<lambda>)/MNISTMLPModel/Dense_1/add"(#loc64))
#loc79 = loc("jit(<lambda>)/MNISTMLPModel/Dense_2/dot_general"(#loc66))
#loc80 = loc("jit(<lambda>)/MNISTMLPModel/Dense_2/reshape"(#loc67))
#loc81 = loc("jit(<lambda>)/MNISTMLPModel/Dense_2/add"(#loc68))
#loc82 = loc("jit(<lambda>)/MNISTMLPModel/reduce_max"(#loc69))
#loc83 = loc("jit(<lambda>)/MNISTMLPModel/max"(#loc69))
#loc84 = loc("jit(<lambda>)/MNISTMLPModel/broadcast_in_dim"(#loc69))
#loc85 = loc("jit(<lambda>)/MNISTMLPModel/sub"(#loc69))
#loc86 = loc("jit(<lambda>)/MNISTMLPModel/exp"(#loc69))
#loc87 = loc("jit(<lambda>)/MNISTMLPModel/reduce_sum"(#loc69))
#loc88 = loc("jit(<lambda>)/MNISTMLPModel/div"(#loc69))
#loc89 = loc(callsite(#loc74 at #loc75))
