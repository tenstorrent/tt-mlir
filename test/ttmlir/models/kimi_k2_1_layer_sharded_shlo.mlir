#loc = loc(unknown)
#loc1 = loc("p0.1")
#loc2 = loc("p1.9")
#loc3 = loc("p2.40")
#loc4 = loc("p3.51")
#loc5 = loc("p4.67")
#loc6 = loc("p5.78")
#loc7 = loc("p6.92")
#loc8 = loc("p7.121")
#loc9 = loc("p8.145")
#loc10 = loc("p9.156")
#loc11 = loc("p10.161")
#loc12 = loc("p11.170")
#loc13 = loc("p12.175")
#loc14 = loc("p13.180")
#loc15 = loc("p14.230")
#loc16 = loc("p15.240")
#loc17 = loc("p16.245")
#loc18 = loc("p17.254")
#loc19 = loc("p18.388")
#loc20 = loc("p19.397")
#loc32 = loc("reduce.20")
#loc46 = loc("dot.47")
#loc50 = loc("scatter.75")
#loc80 = loc("scatter.153")
#loc84 = loc("dot.260")
#loc146 = loc("dot.354")
#loc154 = loc("reduce.368")
#loc168 = loc("dot.403")
#loc177 = loc("dot.395")
#loc184 = loc("dot.411")
module @SyncTensorsGraph.417 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2, "_axis_1"=4]> loc(#loc)
  func.func @main(%arg0: tensor<576x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<576x3584xbf16>>, ttir.name = "l__self___self_attn_kv_a_proj_with_mqa_weight"} loc("p0.1"), %arg1: tensor<64x1x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<16x1x3584xbf16>>, ttir.name = "args_0"} loc("p1.9"), %arg2: tensor<7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<3584xbf16>>, ttir.name = "l__self___input_layernorm_weight"} loc("p2.40"), %arg3: tensor<64xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xi64>>, ttir.name = "args_2"} loc("p3.51"), %arg4: tensor<64x1x1024x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<16x1x1024x512xbf16>>, ttir.name = "args_3"} loc("p4.67"), %arg5: tensor<1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1xi64>>, ttir.name = "args_1"} loc("p5.78"), %arg6: tensor<1024x64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x64xbf16>>, ttir.name = "self___self_attn_rotary_emb__buffers__sin_cached"} loc("p6.92"), %arg7: tensor<1024x64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x64xbf16>>, ttir.name = "self___self_attn_rotary_emb__buffers__cos_cached"} loc("p7.121"), %arg8: tensor<64x1x1024x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<16x1x1024x64xbf16>>, ttir.name = "args_4"} loc("p8.145"), %arg9: tensor<7168x18432xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<3584x4608xbf16>>, ttir.name = "l__self___mlp_down_proj_weight"} loc("p9.156"), %arg10: tensor<18432x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<4608x3584xbf16>>, ttir.name = "l__self___mlp_up_proj_weight"} loc("p10.161"), %arg11: tensor<7168x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<7168x4096xbf16>>, ttir.name = "l__self___self_attn_o_proj_weight"} loc("p11.170"), %arg12: tensor<16384x512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8192x512xbf16>>, ttir.name = "l__self___self_attn_kv_b_proj_weight"} loc("p12.175"), %arg13: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<512xbf16>>, ttir.name = "l__self___self_attn_kv_a_layernorm_weight"} loc("p13.180"), %arg14: tensor<64x1x1x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<16x1x1x1024xbf16>>, ttir.name = "args_5"} loc("p14.230"), %arg15: tensor<12288x1536xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<6144x1536xbf16>>, ttir.name = "l__self___self_attn_q_b_proj_weight"} loc("p15.240"), %arg16: tensor<1536xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1536xbf16>>, ttir.name = "l__self___self_attn_q_a_layernorm_weight"} loc("p16.245"), %arg17: tensor<1536x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1536x3584xbf16>>, ttir.name = "l__self___self_attn_q_a_proj_weight"} loc("p17.254"), %arg18: tensor<7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<3584xbf16>>, ttir.name = "l__self___post_attention_layernorm_weight"} loc("p18.388"), %arg19: tensor<18432x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<4608x3584xbf16>>, ttir.name = "l__self___mlp_gate_proj_weight"} loc("p19.397")) -> (tensor<64x1x1024x512xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<64x1x1024x512xbf16>>}, tensor<64x1x1024x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<64x1x1024x64xbf16>>}, tensor<64x1x7168xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<64x1x7168xbf16>>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19) in_shardings=[<@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}, {}, {}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_1"}, {}, {}, {}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}, {}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>] out_shardings=[<@mesh, [{"_axis_1"}, {}, {}, {}]>, <@mesh, [{"_axis_1"}, {}, {}, {}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>] manual_axes={"_axis_0", "_axis_1"} (%arg20: tensor<576x3584xbf16> loc("p0.1"), %arg21: tensor<16x1x3584xbf16> loc("p1.9"), %arg22: tensor<3584xbf16> loc("p2.40"), %arg23: tensor<64xi64> loc("p3.51"), %arg24: tensor<16x1x1024x512xbf16> loc("p4.67"), %arg25: tensor<1xi64> loc("p5.78"), %arg26: tensor<1024x64xbf16> loc("p6.92"), %arg27: tensor<1024x64xbf16> loc("p7.121"), %arg28: tensor<16x1x1024x64xbf16> loc("p8.145"), %arg29: tensor<3584x4608xbf16> loc("p9.156"), %arg30: tensor<4608x3584xbf16> loc("p10.161"), %arg31: tensor<7168x4096xbf16> loc("p11.170"), %arg32: tensor<8192x512xbf16> loc("p12.175"), %arg33: tensor<512xbf16> loc("p13.180"), %arg34: tensor<16x1x1x1024xbf16> loc("p14.230"), %arg35: tensor<6144x1536xbf16> loc("p15.240"), %arg36: tensor<1536xbf16> loc("p16.245"), %arg37: tensor<1536x3584xbf16> loc("p17.254"), %arg38: tensor<3584xbf16> loc("p18.388"), %arg39: tensor<4608x3584xbf16> loc("p19.397")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
      %c = stablehlo.constant dense<0> : tensor<i64> loc(#loc)
      %c_1 = stablehlo.constant dense<1024> : tensor<i64> loc(#loc)
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
      %cst_3 = stablehlo.constant dense<1.39508935E-4> : tensor<f32> loc(#loc)
      %cst_4 = stablehlo.constant dense<9.99999974E-6> : tensor<f32> loc(#loc)
      %c_5 = stablehlo.constant dense<0> : tensor<1xi64> loc(#loc)
      %c_6 = stablehlo.constant dense<1024> : tensor<1xi64> loc(#loc)
      %cst_7 = stablehlo.constant dense<1.445310e-01> : tensor<bf16> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<16x32x1x1024xbf16> loc(#loc)
      %2 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1x1xf32> loc(#loc)
      %3 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc)
      %4 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x1x3584xf32> loc(#loc)
      %5 = stablehlo.broadcast_in_dim %c_1, dims = [] : (tensor<i64>) -> tensor<64xi64> loc(#loc)
      %6 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i64>) -> tensor<64xi64> loc(#loc)
      %7 = stablehlo.reshape %arg23 : (tensor<64xi64>) -> tensor<1x1x64xi64> loc(#loc21)
      %8 = stablehlo.reshape %7 : (tensor<1x1x64xi64>) -> tensor<64xi64> loc(#loc22)
      %9 = stablehlo.compare  LT, %8, %6 : (tensor<64xi64>, tensor<64xi64>) -> tensor<64xi1> loc(#loc23)
      %10 = stablehlo.add %8, %5 : tensor<64xi64> loc(#loc24)
      %11 = stablehlo.select %9, %10, %8 : tensor<64xi1>, tensor<64xi64> loc(#loc25)
      %12 = stablehlo.reshape %11 : (tensor<64xi64>) -> tensor<64x1xi64> loc(#loc26)
      %13 = stablehlo.reshape %arg22 : (tensor<3584xbf16>) -> tensor<1x1x3584xbf16> loc(#loc27)
      %14 = stablehlo.reshape %13 : (tensor<1x1x3584xbf16>) -> tensor<3584xbf16> loc(#loc28)
      %15 = stablehlo.broadcast_in_dim %14, dims = [2] : (tensor<3584xbf16>) -> tensor<16x1x3584xbf16> loc(#loc29)
      %16 = stablehlo.convert %arg21 : (tensor<16x1x3584xbf16>) -> tensor<16x1x3584xf32> loc(#loc30)
      %17 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x1x3584xf32> loc(#loc)
      %18 = stablehlo.power %16, %17 : tensor<16x1x3584xf32> loc(#loc31)
      %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<16x1x3584xf32>, tensor<f32>) -> tensor<16x1xf32> loc(#loc32)
      %20 = "stablehlo.all_reduce"(%19) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg40: tensor<f32> loc("reduce.20"), %arg41: tensor<f32> loc("reduce.20")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<f32> loc(#loc32)
        stablehlo.return %196 : tensor<f32> loc(#loc32)
      }) : (tensor<16x1xf32>) -> tensor<16x1xf32> loc(#loc32)
      %21 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc)
      %22 = stablehlo.multiply %20, %21 : tensor<16x1xf32> loc(#loc33)
      %23 = stablehlo.reshape %22 : (tensor<16x1xf32>) -> tensor<16x1x1xf32> loc(#loc34)
      %24 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<16x1x1xf32> loc(#loc)
      %25 = stablehlo.add %23, %24 : tensor<16x1x1xf32> loc(#loc35)
      %26 = stablehlo.rsqrt %25 : tensor<16x1x1xf32> loc(#loc36)
      %27 = stablehlo.reshape %26 : (tensor<16x1x1xf32>) -> tensor<16x1xf32> loc(#loc37)
      %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x1x3584xf32> loc(#loc38)
      %29 = stablehlo.multiply %16, %28 : tensor<16x1x3584xf32> loc(#loc39)
      %30 = stablehlo.convert %29 : (tensor<16x1x3584xf32>) -> tensor<16x1x3584xbf16> loc(#loc40)
      %31 = stablehlo.multiply %15, %30 : tensor<16x1x3584xbf16> loc(#loc41)
      %32 = stablehlo.reshape %31 : (tensor<16x1x3584xbf16>) -> tensor<16x3584xbf16> loc(#loc42)
      %33 = stablehlo.reshape %arg20 : (tensor<576x3584xbf16>) -> tensor<1x576x3584xbf16> loc(#loc43)
      %34 = stablehlo.reshape %33 : (tensor<1x576x3584xbf16>) -> tensor<576x3584xbf16> loc(#loc44)
      %35 = stablehlo.transpose %34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,576]{0,1}"} : (tensor<576x3584xbf16>) -> tensor<3584x576xbf16> loc(#loc45)
      %36 = stablehlo.dot_general %32, %35, contracting_dims = [1] x [0] : (tensor<16x3584xbf16>, tensor<3584x576xbf16>) -> tensor<16x576xbf16> loc(#loc46)
      %37 = "stablehlo.all_reduce"(%36) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg40: tensor<bf16> loc("dot.47"), %arg41: tensor<bf16> loc("dot.47")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<bf16> loc(#loc46)
        stablehlo.return %196 : tensor<bf16> loc(#loc46)
      }) : (tensor<16x576xbf16>) -> tensor<16x576xbf16> loc(#loc46)
      %38 = stablehlo.reshape %37 : (tensor<16x576xbf16>) -> tensor<16x1x576xbf16> loc(#loc47)
      %39 = stablehlo.slice %38 [0:16, 0:1, 0:512] : (tensor<16x1x576xbf16>) -> tensor<16x1x512xbf16> loc(#loc48)
      %40 = stablehlo.broadcast_in_dim %39, dims = [0, 1, 3] : (tensor<16x1x512xbf16>) -> tensor<16x1x64x512xbf16> loc(#loc49)
      %41 = "stablehlo.scatter"(%arg24, %12, %40) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg40: tensor<bf16> loc("scatter.75"), %arg41: tensor<bf16> loc("scatter.75")):
        stablehlo.return %arg41 : tensor<bf16> loc(#loc)
      }) : (tensor<16x1x1024x512xbf16>, tensor<64x1xi64>, tensor<16x1x64x512xbf16>) -> tensor<16x1x1024x512xbf16> loc(#loc50)
      %42 = stablehlo.slice %38 [0:16, 0:1, 512:576] : (tensor<16x1x576xbf16>) -> tensor<16x1x64xbf16> loc(#loc51)
      %43 = stablehlo.reshape %42 : (tensor<16x1x64xbf16>) -> tensor<16x1x1x32x2xbf16> loc(#loc52)
      %44 = stablehlo.transpose %43, dims = [0, 1, 2, 4, 3] {result_layout = dense<[3, 4, 2, 1, 0]> : tensor<5xindex>, xla_shape = "bf16[64,1,1,2,32]{3,4,2,1,0}"} : (tensor<16x1x1x32x2xbf16>) -> tensor<16x1x1x2x32xbf16> loc(#loc53)
      %45 = stablehlo.reshape %44 : (tensor<16x1x1x2x32xbf16>) -> tensor<16x1x1x64xbf16> loc(#loc54)
      %46 = stablehlo.reshape %arg27 : (tensor<1024x64xbf16>) -> tensor<1x1024x64xbf16> loc(#loc55)
      %47 = stablehlo.reshape %46 : (tensor<1x1024x64xbf16>) -> tensor<1024x64xbf16> loc(#loc56)
      %48 = stablehlo.reshape %arg25 : (tensor<1xi64>) -> tensor<1x1x1xi64> loc(#loc57)
      %49 = stablehlo.reshape %48 : (tensor<1x1x1xi64>) -> tensor<1xi64> loc(#loc58)
      %50 = stablehlo.compare  LT, %49, %c_5 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1> loc(#loc59)
      %51 = stablehlo.add %49, %c_6 : tensor<1xi64> loc(#loc60)
      %52 = stablehlo.select %50, %51, %49 : tensor<1xi1>, tensor<1xi64> loc(#loc61)
      %53 = stablehlo.reshape %52 : (tensor<1xi64>) -> tensor<1x1xi64> loc(#loc62)
      %54 = "stablehlo.gather"(%47, %53) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 64>}> : (tensor<1024x64xbf16>, tensor<1x1xi64>) -> tensor<1x64xbf16> loc(#loc63)
      %55 = stablehlo.reshape %54 : (tensor<1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc64)
      %56 = stablehlo.broadcast_in_dim %55, dims = [1, 2, 3] : (tensor<1x1x64xbf16>) -> tensor<16x1x1x64xbf16> loc(#loc65)
      %57 = stablehlo.multiply %45, %56 : tensor<16x1x1x64xbf16> loc(#loc66)
      %58 = stablehlo.slice %45 [0:16, 0:1, 0:1, 32:64] : (tensor<16x1x1x64xbf16>) -> tensor<16x1x1x32xbf16> loc(#loc67)
      %59 = stablehlo.negate %58 : tensor<16x1x1x32xbf16> loc(#loc68)
      %60 = stablehlo.slice %45 [0:16, 0:1, 0:1, 0:32] : (tensor<16x1x1x64xbf16>) -> tensor<16x1x1x32xbf16> loc(#loc69)
      %61 = stablehlo.concatenate %59, %60, dim = 3 : (tensor<16x1x1x32xbf16>, tensor<16x1x1x32xbf16>) -> tensor<16x1x1x64xbf16> loc(#loc70)
      %62 = stablehlo.reshape %arg26 : (tensor<1024x64xbf16>) -> tensor<1x1024x64xbf16> loc(#loc71)
      %63 = stablehlo.reshape %62 : (tensor<1x1024x64xbf16>) -> tensor<1024x64xbf16> loc(#loc72)
      %64 = "stablehlo.gather"(%63, %53) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 64>}> : (tensor<1024x64xbf16>, tensor<1x1xi64>) -> tensor<1x64xbf16> loc(#loc73)
      %65 = stablehlo.reshape %64 : (tensor<1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc74)
      %66 = stablehlo.broadcast_in_dim %65, dims = [1, 2, 3] : (tensor<1x1x64xbf16>) -> tensor<16x1x1x64xbf16> loc(#loc75)
      %67 = stablehlo.multiply %61, %66 : tensor<16x1x1x64xbf16> loc(#loc76)
      %68 = stablehlo.add %57, %67 : tensor<16x1x1x64xbf16> loc(#loc77)
      %69 = stablehlo.reshape %68 : (tensor<16x1x1x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc78)
      %70 = stablehlo.broadcast_in_dim %69, dims = [0, 1, 3] : (tensor<16x1x64xbf16>) -> tensor<16x1x64x64xbf16> loc(#loc79)
      %71 = "stablehlo.scatter"(%arg28, %12, %70) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg40: tensor<bf16> loc("scatter.153"), %arg41: tensor<bf16> loc("scatter.153")):
        stablehlo.return %arg41 : tensor<bf16> loc(#loc)
      }) : (tensor<16x1x1024x64xbf16>, tensor<64x1xi64>, tensor<16x1x64x64xbf16>) -> tensor<16x1x1024x64xbf16> loc(#loc80)
      %72 = stablehlo.reshape %arg37 : (tensor<1536x3584xbf16>) -> tensor<1x1536x3584xbf16> loc(#loc81)
      %73 = stablehlo.reshape %72 : (tensor<1x1536x3584xbf16>) -> tensor<1536x3584xbf16> loc(#loc82)
      %74 = stablehlo.transpose %73, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,1536]{0,1}"} : (tensor<1536x3584xbf16>) -> tensor<3584x1536xbf16> loc(#loc83)
      %75 = stablehlo.dot_general %32, %74, contracting_dims = [1] x [0] : (tensor<16x3584xbf16>, tensor<3584x1536xbf16>) -> tensor<16x1536xbf16> loc(#loc84)
      %76 = "stablehlo.all_reduce"(%75) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg40: tensor<bf16> loc("dot.260"), %arg41: tensor<bf16> loc("dot.260")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<bf16> loc(#loc84)
        stablehlo.return %196 : tensor<bf16> loc(#loc84)
      }) : (tensor<16x1536xbf16>) -> tensor<16x1536xbf16> loc(#loc84)
      %77 = stablehlo.reshape %76 : (tensor<16x1536xbf16>) -> tensor<16x1x1536xbf16> loc(#loc85)
      %78 = stablehlo.reshape %arg36 : (tensor<1536xbf16>) -> tensor<1x1x1536xbf16> loc(#loc86)
      %79 = stablehlo.reshape %78 : (tensor<1x1x1536xbf16>) -> tensor<1536xbf16> loc(#loc87)
      %80 = stablehlo.composite "tenstorrent.rms_norm" %77, %79 {composite_attributes = {epsilon = 9.99999997E-7 : f32, normalized_shape = dense<1536> : tensor<1xi64>}, decomposition = @outlined_composite_tenstorrent.rms_norm.impl_0} : (tensor<16x1x1536xbf16>, tensor<1536xbf16>) -> tensor<16x1x1536xbf16> loc(#loc)
      %81 = stablehlo.reshape %80 : (tensor<16x1x1536xbf16>) -> tensor<16x1536xbf16> loc(#loc88)
      %82 = stablehlo.reshape %arg35 : (tensor<6144x1536xbf16>) -> tensor<1x6144x1536xbf16> loc(#loc89)
      %83 = stablehlo.reshape %82 : (tensor<1x6144x1536xbf16>) -> tensor<6144x1536xbf16> loc(#loc90)
      %84 = stablehlo.transpose %83, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1536,12288]{0,1}"} : (tensor<6144x1536xbf16>) -> tensor<1536x6144xbf16> loc(#loc91)
      %85 = stablehlo.dot_general %81, %84, contracting_dims = [1] x [0] : (tensor<16x1536xbf16>, tensor<1536x6144xbf16>) -> tensor<16x6144xbf16> loc(#loc92)
      %86 = stablehlo.reshape %85 : (tensor<16x6144xbf16>) -> tensor<16x32x1x192xbf16> loc(#loc93)
      %87 = stablehlo.slice %86 [0:16, 0:32, 0:1, 0:128] : (tensor<16x32x1x192xbf16>) -> tensor<16x32x1x128xbf16> loc(#loc94)
      %88 = stablehlo.slice %86 [0:16, 0:32, 0:1, 128:192] : (tensor<16x32x1x192xbf16>) -> tensor<16x32x1x64xbf16> loc(#loc95)
      %89 = stablehlo.reshape %88 : (tensor<16x32x1x64xbf16>) -> tensor<16x32x1x32x2xbf16> loc(#loc96)
      %90 = stablehlo.transpose %89, dims = [0, 1, 2, 4, 3] {result_layout = dense<[3, 4, 2, 1, 0]> : tensor<5xindex>, xla_shape = "bf16[64,64,1,2,32]{3,4,2,1,0}"} : (tensor<16x32x1x32x2xbf16>) -> tensor<16x32x1x2x32xbf16> loc(#loc97)
      %91 = stablehlo.reshape %90 : (tensor<16x32x1x2x32xbf16>) -> tensor<16x32x1x64xbf16> loc(#loc98)
      %92 = stablehlo.reshape %56 : (tensor<16x1x1x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc99)
      %93 = stablehlo.broadcast_in_dim %92, dims = [0, 2, 3] : (tensor<16x1x64xbf16>) -> tensor<16x32x1x64xbf16> loc(#loc100)
      %94 = stablehlo.multiply %91, %93 : tensor<16x32x1x64xbf16> loc(#loc101)
      %95 = stablehlo.slice %91 [0:16, 0:32, 0:1, 32:64] : (tensor<16x32x1x64xbf16>) -> tensor<16x32x1x32xbf16> loc(#loc102)
      %96 = stablehlo.negate %95 : tensor<16x32x1x32xbf16> loc(#loc103)
      %97 = stablehlo.slice %91 [0:16, 0:32, 0:1, 0:32] : (tensor<16x32x1x64xbf16>) -> tensor<16x32x1x32xbf16> loc(#loc104)
      %98 = stablehlo.concatenate %96, %97, dim = 3 : (tensor<16x32x1x32xbf16>, tensor<16x32x1x32xbf16>) -> tensor<16x32x1x64xbf16> loc(#loc105)
      %99 = stablehlo.reshape %66 : (tensor<16x1x1x64xbf16>) -> tensor<16x1x64xbf16> loc(#loc106)
      %100 = stablehlo.broadcast_in_dim %99, dims = [0, 2, 3] : (tensor<16x1x64xbf16>) -> tensor<16x32x1x64xbf16> loc(#loc107)
      %101 = stablehlo.multiply %98, %100 : tensor<16x32x1x64xbf16> loc(#loc108)
      %102 = stablehlo.add %94, %101 : tensor<16x32x1x64xbf16> loc(#loc109)
      %103 = stablehlo.concatenate %87, %102, dim = 3 : (tensor<16x32x1x128xbf16>, tensor<16x32x1x64xbf16>) -> tensor<16x32x1x192xbf16> loc(#loc110)
      %104 = stablehlo.reshape %41 : (tensor<16x1x1024x512xbf16>) -> tensor<16x1024x512xbf16> loc(#loc111)
      %105 = stablehlo.reshape %arg33 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc112)
      %106 = stablehlo.reshape %105 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc113)
      %107 = stablehlo.composite "tenstorrent.rms_norm" %104, %106 {composite_attributes = {epsilon = 9.99999997E-7 : f32, normalized_shape = dense<512> : tensor<1xi64>}, decomposition = @outlined_composite_tenstorrent.rms_norm.impl} : (tensor<16x1024x512xbf16>, tensor<512xbf16>) -> tensor<16x1024x512xbf16> loc(#loc)
      %108 = stablehlo.reshape %107 : (tensor<16x1024x512xbf16>) -> tensor<16384x512xbf16> loc(#loc114)
      %109 = stablehlo.reshape %arg32 : (tensor<8192x512xbf16>) -> tensor<1x8192x512xbf16> loc(#loc115)
      %110 = stablehlo.reshape %109 : (tensor<1x8192x512xbf16>) -> tensor<8192x512xbf16> loc(#loc116)
      %111 = stablehlo.transpose %110, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,16384]{0,1}"} : (tensor<8192x512xbf16>) -> tensor<512x8192xbf16> loc(#loc117)
      %112 = stablehlo.dot_general %108, %111, contracting_dims = [1] x [0] : (tensor<16384x512xbf16>, tensor<512x8192xbf16>) -> tensor<16384x8192xbf16> loc(#loc118)
      %113 = stablehlo.reshape %112 : (tensor<16384x8192xbf16>) -> tensor<16x1024x32x256xbf16> loc(#loc119)
      %114 = stablehlo.transpose %113, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[64,64,1024,256]{3,1,2,0}"} : (tensor<16x1024x32x256xbf16>) -> tensor<16x32x1024x256xbf16> loc(#loc120)
      %115 = stablehlo.slice %114 [0:16, 0:32, 0:1024, 0:128] : (tensor<16x32x1024x256xbf16>) -> tensor<16x32x1024x128xbf16> loc(#loc121)
      %116 = stablehlo.reshape %71 : (tensor<16x1x1024x64xbf16>) -> tensor<16x1024x64xbf16> loc(#loc122)
      %117 = stablehlo.broadcast_in_dim %116, dims = [0, 2, 3] : (tensor<16x1024x64xbf16>) -> tensor<16x32x1024x64xbf16> loc(#loc123)
      %118 = stablehlo.concatenate %115, %117, dim = 3 : (tensor<16x32x1024x128xbf16>, tensor<16x32x1024x64xbf16>) -> tensor<16x32x1024x192xbf16> loc(#loc124)
      %119 = stablehlo.transpose %118, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[64,64,192,1024]{2,3,1,0}"} : (tensor<16x32x1024x192xbf16>) -> tensor<16x32x192x1024xbf16> loc(#loc125)
      %120 = stablehlo.dot_general %103, %119, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<16x32x1x192xbf16>, tensor<16x32x192x1024xbf16>) -> tensor<16x32x1x1024xbf16> loc(#loc126)
      %121 = stablehlo.multiply %120, %1 : tensor<16x32x1x1024xbf16> loc(#loc127)
      %122 = stablehlo.reshape %arg34 : (tensor<16x1x1x1024xbf16>) -> tensor<16x1x1024xbf16> loc(#loc128)
      %123 = stablehlo.broadcast_in_dim %122, dims = [0, 2, 3] : (tensor<16x1x1024xbf16>) -> tensor<16x32x1x1024xbf16> loc(#loc129)
      %124 = stablehlo.add %121, %123 : tensor<16x32x1x1024xbf16> loc(#loc130)
      %125 = stablehlo.convert %124 : (tensor<16x32x1x1024xbf16>) -> tensor<16x32x1x1024xf32> loc(#loc131)
      %126 = stablehlo.reduce(%125 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<16x32x1x1024xf32>, tensor<f32>) -> tensor<16x32x1xf32> loc(#loc132)
      %127 = stablehlo.broadcast_in_dim %126, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x1x1024xf32> loc(#loc133)
      %128 = stablehlo.subtract %125, %127 : tensor<16x32x1x1024xf32> loc(#loc134)
      %129 = stablehlo.exponential %128 : tensor<16x32x1x1024xf32> loc(#loc135)
      %130 = stablehlo.reduce(%129 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<16x32x1x1024xf32>, tensor<f32>) -> tensor<16x32x1xf32> loc(#loc136)
      %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1, 2] : (tensor<16x32x1xf32>) -> tensor<16x32x1x1024xf32> loc(#loc137)
      %132 = stablehlo.divide %129, %131 : tensor<16x32x1x1024xf32> loc(#loc138)
      %133 = stablehlo.convert %132 : (tensor<16x32x1x1024xf32>) -> tensor<16x32x1x1024xbf16> loc(#loc139)
      %134 = stablehlo.slice %114 [0:16, 0:32, 0:1024, 128:256] : (tensor<16x32x1024x256xbf16>) -> tensor<16x32x1024x128xbf16> loc(#loc140)
      %135 = stablehlo.dot_general %133, %134, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<16x32x1x1024xbf16>, tensor<16x32x1024x128xbf16>) -> tensor<16x32x1x128xbf16> loc(#loc141)
      %136 = stablehlo.reshape %135 : (tensor<16x32x1x128xbf16>) -> tensor<16x4096xbf16> loc(#loc142)
      %137 = stablehlo.reshape %arg31 : (tensor<7168x4096xbf16>) -> tensor<1x7168x4096xbf16> loc(#loc143)
      %138 = stablehlo.reshape %137 : (tensor<1x7168x4096xbf16>) -> tensor<7168x4096xbf16> loc(#loc144)
      %139 = stablehlo.transpose %138, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,7168]{0,1}"} : (tensor<7168x4096xbf16>) -> tensor<4096x7168xbf16> loc(#loc145)
      %140 = stablehlo.dot_general %136, %139, contracting_dims = [1] x [0] : (tensor<16x4096xbf16>, tensor<4096x7168xbf16>) -> tensor<16x7168xbf16> loc(#loc146)
      %141 = "stablehlo.all_reduce"(%140) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg40: tensor<bf16> loc("dot.354"), %arg41: tensor<bf16> loc("dot.354")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<bf16> loc(#loc146)
        stablehlo.return %196 : tensor<bf16> loc(#loc146)
      }) : (tensor<16x7168xbf16>) -> tensor<16x7168xbf16> loc(#loc146)
      %142 = stablehlo.reshape %141 : (tensor<16x7168xbf16>) -> tensor<16x2x3584xbf16> loc(#loc146)
      %143 = "stablehlo.all_to_all"(%142) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<16x2x3584xbf16>) -> tensor<16x2x3584xbf16> loc(#loc146)
      %144 = stablehlo.slice %143 [0:16, 0:1, 0:3584] : (tensor<16x2x3584xbf16>) -> tensor<16x1x3584xbf16> loc(#loc146)
      %145 = stablehlo.reshape %144 : (tensor<16x1x3584xbf16>) -> tensor<16x3584xbf16> loc(#loc146)
      %146 = stablehlo.reshape %145 : (tensor<16x3584xbf16>) -> tensor<16x1x3584xbf16> loc(#loc147)
      %147 = stablehlo.add %arg21, %146 : tensor<16x1x3584xbf16> loc(#loc148)
      %148 = stablehlo.reshape %arg38 : (tensor<3584xbf16>) -> tensor<1x1x3584xbf16> loc(#loc149)
      %149 = stablehlo.reshape %148 : (tensor<1x1x3584xbf16>) -> tensor<3584xbf16> loc(#loc150)
      %150 = stablehlo.broadcast_in_dim %149, dims = [2] : (tensor<3584xbf16>) -> tensor<16x1x3584xbf16> loc(#loc151)
      %151 = stablehlo.convert %147 : (tensor<16x1x3584xbf16>) -> tensor<16x1x3584xf32> loc(#loc152)
      %152 = stablehlo.power %151, %4 : tensor<16x1x3584xf32> loc(#loc153)
      %153 = stablehlo.reduce(%152 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<16x1x3584xf32>, tensor<f32>) -> tensor<16x1xf32> loc(#loc154)
      %154 = "stablehlo.all_reduce"(%153) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg40: tensor<f32> loc("reduce.368"), %arg41: tensor<f32> loc("reduce.368")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<f32> loc(#loc154)
        stablehlo.return %196 : tensor<f32> loc(#loc154)
      }) : (tensor<16x1xf32>) -> tensor<16x1xf32> loc(#loc154)
      %155 = stablehlo.multiply %154, %3 : tensor<16x1xf32> loc(#loc155)
      %156 = stablehlo.reshape %155 : (tensor<16x1xf32>) -> tensor<16x1x1xf32> loc(#loc156)
      %157 = stablehlo.add %156, %2 : tensor<16x1x1xf32> loc(#loc157)
      %158 = stablehlo.rsqrt %157 : tensor<16x1x1xf32> loc(#loc158)
      %159 = stablehlo.reshape %158 : (tensor<16x1x1xf32>) -> tensor<16x1xf32> loc(#loc159)
      %160 = stablehlo.broadcast_in_dim %159, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x1x3584xf32> loc(#loc160)
      %161 = stablehlo.multiply %151, %160 : tensor<16x1x3584xf32> loc(#loc161)
      %162 = stablehlo.convert %161 : (tensor<16x1x3584xf32>) -> tensor<16x1x3584xbf16> loc(#loc162)
      %163 = stablehlo.multiply %150, %162 : tensor<16x1x3584xbf16> loc(#loc163)
      %164 = stablehlo.reshape %163 : (tensor<16x1x3584xbf16>) -> tensor<16x3584xbf16> loc(#loc164)
      %165 = stablehlo.reshape %arg39 : (tensor<4608x3584xbf16>) -> tensor<1x4608x3584xbf16> loc(#loc165)
      %166 = stablehlo.reshape %165 : (tensor<1x4608x3584xbf16>) -> tensor<4608x3584xbf16> loc(#loc166)
      %167 = stablehlo.transpose %166, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,18432]{0,1}"} : (tensor<4608x3584xbf16>) -> tensor<3584x4608xbf16> loc(#loc167)
      %168 = "stablehlo.all_gather"(%164) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<16x3584xbf16>) -> tensor<64x3584xbf16> loc(#loc164)
      %169 = stablehlo.dot_general %168, %167, contracting_dims = [1] x [0] : (tensor<64x3584xbf16>, tensor<3584x4608xbf16>) -> tensor<64x4608xbf16> loc(#loc168)
      %170 = "stablehlo.all_reduce"(%169) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg40: tensor<bf16> loc("dot.403"), %arg41: tensor<bf16> loc("dot.403")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<bf16> loc(#loc168)
        stablehlo.return %196 : tensor<bf16> loc(#loc168)
      }) : (tensor<64x4608xbf16>) -> tensor<64x4608xbf16> loc(#loc168)
      %171 = stablehlo.reshape %170 : (tensor<64x4608xbf16>) -> tensor<64x1x4608xbf16> loc(#loc169)
      %172 = stablehlo.convert %171 : (tensor<64x1x4608xbf16>) -> tensor<64x1x4608xf32> loc(#loc170)
      %173 = stablehlo.logistic %172 : tensor<64x1x4608xf32> loc(#loc171)
      %174 = stablehlo.multiply %172, %173 : tensor<64x1x4608xf32> loc(#loc172)
      %175 = stablehlo.convert %174 : (tensor<64x1x4608xf32>) -> tensor<64x1x4608xbf16> loc(#loc173)
      %176 = stablehlo.reshape %arg30 : (tensor<4608x3584xbf16>) -> tensor<1x4608x3584xbf16> loc(#loc174)
      %177 = stablehlo.reshape %176 : (tensor<1x4608x3584xbf16>) -> tensor<4608x3584xbf16> loc(#loc175)
      %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,18432]{0,1}"} : (tensor<4608x3584xbf16>) -> tensor<3584x4608xbf16> loc(#loc176)
      %179 = "stablehlo.all_gather"(%164) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<16x3584xbf16>) -> tensor<64x3584xbf16> loc(#loc164)
      %180 = stablehlo.dot_general %179, %178, contracting_dims = [1] x [0] : (tensor<64x3584xbf16>, tensor<3584x4608xbf16>) -> tensor<64x4608xbf16> loc(#loc177)
      %181 = "stablehlo.all_reduce"(%180) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg40: tensor<bf16> loc("dot.395"), %arg41: tensor<bf16> loc("dot.395")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<bf16> loc(#loc177)
        stablehlo.return %196 : tensor<bf16> loc(#loc177)
      }) : (tensor<64x4608xbf16>) -> tensor<64x4608xbf16> loc(#loc177)
      %182 = stablehlo.reshape %181 : (tensor<64x4608xbf16>) -> tensor<64x1x4608xbf16> loc(#loc178)
      %183 = stablehlo.multiply %175, %182 : tensor<64x1x4608xbf16> loc(#loc179)
      %184 = stablehlo.reshape %183 : (tensor<64x1x4608xbf16>) -> tensor<64x4608xbf16> loc(#loc180)
      %185 = stablehlo.reshape %arg29 : (tensor<3584x4608xbf16>) -> tensor<1x3584x4608xbf16> loc(#loc181)
      %186 = stablehlo.reshape %185 : (tensor<1x3584x4608xbf16>) -> tensor<3584x4608xbf16> loc(#loc182)
      %187 = stablehlo.transpose %186, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[18432,7168]{0,1}"} : (tensor<3584x4608xbf16>) -> tensor<4608x3584xbf16> loc(#loc183)
      %188 = stablehlo.dot_general %184, %187, contracting_dims = [1] x [0] : (tensor<64x4608xbf16>, tensor<4608x3584xbf16>) -> tensor<64x3584xbf16> loc(#loc184)
      %189 = "stablehlo.all_reduce"(%188) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg40: tensor<bf16> loc("dot.411"), %arg41: tensor<bf16> loc("dot.411")):
        %196 = stablehlo.add %arg40, %arg41 : tensor<bf16> loc(#loc184)
        stablehlo.return %196 : tensor<bf16> loc(#loc184)
      }) : (tensor<64x3584xbf16>) -> tensor<64x3584xbf16> loc(#loc184)
      %190 = stablehlo.reshape %189 : (tensor<64x3584xbf16>) -> tensor<4x16x3584xbf16> loc(#loc184)
      %191 = "stablehlo.all_to_all"(%190) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16x3584xbf16>) -> tensor<4x16x3584xbf16> loc(#loc184)
      %192 = stablehlo.slice %191 [0:1, 0:16, 0:3584] : (tensor<4x16x3584xbf16>) -> tensor<1x16x3584xbf16> loc(#loc184)
      %193 = stablehlo.reshape %192 : (tensor<1x16x3584xbf16>) -> tensor<16x3584xbf16> loc(#loc184)
      %194 = stablehlo.reshape %193 : (tensor<16x3584xbf16>) -> tensor<16x1x3584xbf16> loc(#loc185)
      %195 = stablehlo.add %147, %194 : tensor<16x1x3584xbf16> loc(#loc186)
      sdy.return %41, %71, %195 : tensor<16x1x1024x512xbf16>, tensor<16x1x1024x64xbf16>, tensor<16x1x3584xbf16> loc(#loc)
    } : (tensor<576x7168xbf16>, tensor<64x1x7168xbf16>, tensor<7168xbf16>, tensor<64xi64>, tensor<64x1x1024x512xbf16>, tensor<1xi64>, tensor<1024x64xbf16>, tensor<1024x64xbf16>, tensor<64x1x1024x64xbf16>, tensor<7168x18432xbf16>, tensor<18432x7168xbf16>, tensor<7168x8192xbf16>, tensor<16384x512xbf16>, tensor<512xbf16>, tensor<64x1x1x1024xbf16>, tensor<12288x1536xbf16>, tensor<1536xbf16>, tensor<1536x7168xbf16>, tensor<7168xbf16>, tensor<18432x7168xbf16>) -> (tensor<64x1x1024x512xbf16>, tensor<64x1x1024x64xbf16>, tensor<64x1x7168xbf16>) loc(#loc)
    return %0#0, %0#1, %0#2 : tensor<64x1x1024x512xbf16>, tensor<64x1x1024x64xbf16>, tensor<64x1x7168xbf16> loc(#loc)
  } loc(#loc)
  func.func private @outlined_composite_tenstorrent.rms_norm.impl(%arg0: tensor<16x1024x512xbf16> loc(unknown), %arg1: tensor<512xbf16> loc(unknown)) -> tensor<16x1024x512xbf16> {
    %cst = stablehlo.constant {reoutline.comp_attrs = {epsilon = 9.99999997E-7 : f32, normalized_shape = dense<512> : tensor<1xi64>}, reoutline.orig_name = "tenstorrent.rms_norm"} dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %cst_1 = stablehlo.constant dense<0.001953125> : tensor<f32> loc(#loc)
    %cst_2 = stablehlo.constant dense<9.99999997E-7> : tensor<f32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x1024x1xf32> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<16x1024xf32> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<16x1024x512xf32> loc(#loc)
    %3 = stablehlo.convert %arg0 : (tensor<16x1024x512xbf16>) -> tensor<16x1024x512xf32> loc(#loc187)
    %4 = stablehlo.power %3, %2 : tensor<16x1024x512xf32> loc(#loc188)
    %5 = stablehlo.reduce(%4 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<16x1024x512xf32>, tensor<f32>) -> tensor<16x1024xf32> loc(#loc189)
    %6 = stablehlo.multiply %5, %1 : tensor<16x1024xf32> loc(#loc190)
    %7 = stablehlo.reshape %6 : (tensor<16x1024xf32>) -> tensor<16x1024x1xf32> loc(#loc191)
    %8 = stablehlo.add %7, %0 : tensor<16x1024x1xf32> loc(#loc192)
    %9 = stablehlo.rsqrt %8 : tensor<16x1024x1xf32> loc(#loc193)
    %10 = stablehlo.reshape %9 : (tensor<16x1024x1xf32>) -> tensor<16x1024xf32> loc(#loc194)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<16x1024xf32>) -> tensor<16x1024x512xf32> loc(#loc195)
    %12 = stablehlo.multiply %3, %11 : tensor<16x1024x512xf32> loc(#loc196)
    %13 = stablehlo.convert %arg1 : (tensor<512xbf16>) -> tensor<512xf32> loc(#loc197)
    %14 = stablehlo.broadcast_in_dim %13, dims = [2] : (tensor<512xf32>) -> tensor<16x1024x512xf32> loc(#loc198)
    %15 = stablehlo.multiply %12, %14 : tensor<16x1024x512xf32> loc(#loc199)
    %16 = stablehlo.convert %15 : (tensor<16x1024x512xf32>) -> tensor<16x1024x512xbf16> loc(#loc200)
    return %16 : tensor<16x1024x512xbf16> loc(#loc)
  } loc(#loc)
  func.func private @outlined_composite_tenstorrent.rms_norm.impl_0(%arg0: tensor<16x1x1536xbf16> loc(unknown), %arg1: tensor<1536xbf16> loc(unknown)) -> tensor<16x1x1536xbf16> {
    %cst = stablehlo.constant {reoutline.comp_attrs = {epsilon = 9.99999997E-7 : f32, normalized_shape = dense<1536> : tensor<1xi64>}, reoutline.orig_name = "tenstorrent.rms_norm"} dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %cst_1 = stablehlo.constant dense<6.51041686E-4> : tensor<f32> loc(#loc)
    %cst_2 = stablehlo.constant dense<9.99999997E-7> : tensor<f32> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<16x1x1xf32> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<16x1xf32> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<16x1x1536xf32> loc(#loc)
    %3 = stablehlo.convert %arg0 : (tensor<16x1x1536xbf16>) -> tensor<16x1x1536xf32> loc(#loc201)
    %4 = stablehlo.power %3, %2 : tensor<16x1x1536xf32> loc(#loc202)
    %5 = stablehlo.reduce(%4 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<16x1x1536xf32>, tensor<f32>) -> tensor<16x1xf32> loc(#loc203)
    %6 = stablehlo.multiply %5, %1 : tensor<16x1xf32> loc(#loc204)
    %7 = stablehlo.reshape %6 : (tensor<16x1xf32>) -> tensor<16x1x1xf32> loc(#loc205)
    %8 = stablehlo.add %7, %0 : tensor<16x1x1xf32> loc(#loc206)
    %9 = stablehlo.rsqrt %8 : tensor<16x1x1xf32> loc(#loc207)
    %10 = stablehlo.reshape %9 : (tensor<16x1x1xf32>) -> tensor<16x1xf32> loc(#loc208)
    %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1] : (tensor<16x1xf32>) -> tensor<16x1x1536xf32> loc(#loc209)
    %12 = stablehlo.multiply %3, %11 : tensor<16x1x1536xf32> loc(#loc210)
    %13 = stablehlo.convert %arg1 : (tensor<1536xbf16>) -> tensor<1536xf32> loc(#loc211)
    %14 = stablehlo.broadcast_in_dim %13, dims = [2] : (tensor<1536xf32>) -> tensor<16x1x1536xf32> loc(#loc212)
    %15 = stablehlo.multiply %12, %14 : tensor<16x1x1536xf32> loc(#loc213)
    %16 = stablehlo.convert %15 : (tensor<16x1x1536xf32>) -> tensor<16x1x1536xbf16> loc(#loc214)
    return %16 : tensor<16x1x1536xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc21 = loc("reshape.52")
#loc22 = loc("reshape.54")
#loc23 = loc("compare.63")
#loc24 = loc("add.60")
#loc25 = loc("select.64")
#loc26 = loc("reshape.65")
#loc27 = loc("reshape.41")
#loc28 = loc("reshape.43")
#loc29 = loc("broadcast.44")
#loc30 = loc("convert.11")
#loc31 = loc("power.13")
#loc33 = loc("multiply.29")
#loc34 = loc("reshape.30")
#loc35 = loc("add.34")
#loc36 = loc("rsqrt.35")
#loc37 = loc("reshape.36")
#loc38 = loc("broadcast.37")
#loc39 = loc("multiply.38")
#loc40 = loc("convert.39")
#loc41 = loc("multiply.45")
#loc42 = loc("reshape.46")
#loc43 = loc("reshape.2")
#loc44 = loc("reshape.4")
#loc45 = loc("transpose.5")
#loc47 = loc("reshape.48")
#loc48 = loc("slice.49")
#loc49 = loc("broadcast.71")
#loc51 = loc("slice.99")
#loc52 = loc("reshape.102")
#loc53 = loc("transpose.103")
#loc54 = loc("reshape.104")
#loc55 = loc("reshape.122")
#loc56 = loc("reshape.124")
#loc57 = loc("reshape.79")
#loc58 = loc("reshape.81")
#loc59 = loc("compare.117")
#loc60 = loc("add.114")
#loc61 = loc("select.118")
#loc62 = loc("reshape.119")
#loc63 = loc("gather.126")
#loc64 = loc("reshape.127")
#loc65 = loc("broadcast.128")
#loc66 = loc("multiply.129")
#loc67 = loc("slice.106")
#loc68 = loc("negate.107")
#loc69 = loc("slice.105")
#loc70 = loc("concatenate.108")
#loc71 = loc("reshape.93")
#loc72 = loc("reshape.95")
#loc73 = loc("gather.97")
#loc74 = loc("reshape.98")
#loc75 = loc("broadcast.109")
#loc76 = loc("multiply.110")
#loc77 = loc("add.132")
#loc78 = loc("reshape.148")
#loc79 = loc("broadcast.149")
#loc81 = loc("reshape.255")
#loc82 = loc("reshape.257")
#loc83 = loc("transpose.258")
#loc85 = loc("reshape.261")
#loc86 = loc("reshape.246")
#loc87 = loc("reshape.248")
#loc88 = loc("reshape.295")
#loc89 = loc("reshape.241")
#loc90 = loc("reshape.243")
#loc91 = loc("transpose.244")
#loc92 = loc("dot.296")
#loc93 = loc("transpose.299")
#loc94 = loc("slice.319")
#loc95 = loc("slice.300")
#loc96 = loc("reshape.301")
#loc97 = loc("transpose.302")
#loc98 = loc("reshape.303")
#loc99 = loc("reshape.313")
#loc100 = loc("broadcast.314")
#loc101 = loc("multiply.315")
#loc102 = loc("slice.305")
#loc103 = loc("negate.306")
#loc104 = loc("slice.304")
#loc105 = loc("concatenate.307")
#loc106 = loc("reshape.309")
#loc107 = loc("broadcast.310")
#loc108 = loc("multiply.311")
#loc109 = loc("add.318")
#loc110 = loc("concatenate.320")
#loc111 = loc("reshape.189")
#loc112 = loc("reshape.181")
#loc113 = loc("reshape.183")
#loc114 = loc("reshape.223")
#loc115 = loc("reshape.176")
#loc116 = loc("reshape.178")
#loc117 = loc("transpose.179")
#loc118 = loc("dot.224")
#loc119 = loc("reshape.226")
#loc120 = loc("transpose.227")
#loc121 = loc("slice.236")
#loc122 = loc("reshape.234")
#loc123 = loc("broadcast.235")
#loc124 = loc("concatenate.237")
#loc125 = loc("transpose.238")
#loc126 = loc("dot.321")
#loc127 = loc("multiply.324")
#loc128 = loc("reshape.327")
#loc129 = loc("broadcast.328")
#loc130 = loc("add.329")
#loc131 = loc("convert.330")
#loc132 = loc("reduce.336")
#loc133 = loc("broadcast.337")
#loc134 = loc("subtract.338")
#loc135 = loc("exponential.339")
#loc136 = loc("reduce.345")
#loc137 = loc("broadcast.346")
#loc138 = loc("divide.347")
#loc139 = loc("convert.348")
#loc140 = loc("slice.228")
#loc141 = loc("dot.349")
#loc142 = loc("reshape.353")
#loc143 = loc("reshape.171")
#loc144 = loc("reshape.173")
#loc145 = loc("transpose.174")
#loc147 = loc("reshape.355")
#loc148 = loc("add.358")
#loc149 = loc("reshape.389")
#loc150 = loc("reshape.391")
#loc151 = loc("broadcast.392")
#loc152 = loc("convert.359")
#loc153 = loc("power.361")
#loc155 = loc("multiply.377")
#loc156 = loc("reshape.378")
#loc157 = loc("add.382")
#loc158 = loc("rsqrt.383")
#loc159 = loc("reshape.384")
#loc160 = loc("broadcast.385")
#loc161 = loc("multiply.386")
#loc162 = loc("convert.387")
#loc163 = loc("multiply.393")
#loc164 = loc("reshape.402")
#loc165 = loc("reshape.398")
#loc166 = loc("reshape.400")
#loc167 = loc("transpose.401")
#loc169 = loc("reshape.404")
#loc170 = loc("convert.405")
#loc171 = loc("logistic.406")
#loc172 = loc("multiply.407")
#loc173 = loc("convert.408")
#loc174 = loc("reshape.162")
#loc175 = loc("reshape.164")
#loc176 = loc("transpose.165")
#loc178 = loc("reshape.396")
#loc179 = loc("multiply.409")
#loc180 = loc("reshape.410")
#loc181 = loc("reshape.157")
#loc182 = loc("reshape.159")
#loc183 = loc("transpose.160")
#loc185 = loc("reshape.412")
#loc186 = loc("add.415")
#loc187 = loc("convert.191")
#loc188 = loc("power.193")
#loc189 = loc("reduce.200")
#loc190 = loc("multiply.209")
#loc191 = loc("reshape.210")
#loc192 = loc("add.214")
#loc193 = loc("rsqrt.215")
#loc194 = loc("reshape.216")
#loc195 = loc("broadcast.217")
#loc196 = loc("multiply.218")
#loc197 = loc("convert.185")
#loc198 = loc("broadcast.219")
#loc199 = loc("multiply.220")
#loc200 = loc("convert.221")
#loc201 = loc("convert.263")
#loc202 = loc("power.265")
#loc203 = loc("reduce.272")
#loc204 = loc("multiply.281")
#loc205 = loc("reshape.282")
#loc206 = loc("add.286")
#loc207 = loc("rsqrt.287")
#loc208 = loc("reshape.288")
#loc209 = loc("broadcast.289")
#loc210 = loc("multiply.290")
#loc211 = loc("convert.250")
#loc212 = loc("broadcast.291")
#loc213 = loc("multiply.292")
#loc214 = loc("convert.293")

