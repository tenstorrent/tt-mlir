#loc1 = loc("p0.2")
#loc2 = loc("p1.6")
#loc3 = loc("p2.14")
#loc4 = loc("p3.19")
#loc5 = loc("p4.55")
#loc6 = loc("p5.75")
#loc7 = loc("p6.79")
#loc8 = loc("p7.121")
#loc9 = loc("p8.136")
#loc10 = loc("p9.175")
#loc11 = loc("p10.184")
#loc12 = loc("p11.189")
#loc13 = loc("p12.198")
#loc14 = loc("p13.254")
#loc15 = loc("p14.258")
#loc16 = loc("p15.300")
#loc17 = loc("p16.427")
#loc18 = loc("p17.436")
#loc19 = loc("p18.482")
#loc163 = loc("reduce.376")
module @SyncTensorsGraph.492 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_bias"} loc("p0.2"), %arg1: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"} loc("p1.6"), %arg2: tensor<1x32xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p2.14"), %arg3: tensor<151552x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p3.19"), %arg4: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"} loc("p4.55"), %arg5: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_bias"} loc("p5.75"), %arg6: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"} loc("p6.79"), %arg7: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_norm_weight"} loc("p7.121"), %arg8: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p8.136"), %arg9: tensor<151552x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___lm_head_weight"} loc("p9.175"), %arg10: tensor<5120x12288xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"} loc("p10.184"), %arg11: tensor<12288x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"} loc("p11.189"), %arg12: tensor<5120x12288xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"} loc("p12.198"), %arg13: tensor<12288xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_bias"} loc("p13.254"), %arg14: tensor<12288x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"} loc("p14.258"), %arg15: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_norm_weight"} loc("p15.300"), %arg16: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"} loc("p16.427"), %arg17: tensor<12288x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"} loc("p17.436"), %arg18: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_norm_weight"} loc("p18.482")) -> (tensor<1x8x32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x32x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x32x151552xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<1x96x32x32xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<0xFFF0000000000000> : tensor<1x96x32x32xf64> loc(#loc)
    %cst_1 = stablehlo.constant dense<0xFF80> : tensor<1x1x32x32xbf16> loc(#loc)
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<1x1x32x32xbf16> loc(#loc)
    %cst_3 = stablehlo.constant dense<0.297301769> : tensor<1x96x128x32xf32> loc(#loc)
    %cst_4 = stablehlo.constant dense<0.297301769> : tensor<1x96x32x128xf32> loc(#loc)
    %cst_5 = stablehlo.constant dense<9.99999974E-6> : tensor<1x32x96x1xf32> loc(#loc)
    %cst_6 = stablehlo.constant dense<7.812500e-03> : tensor<1x32x96xf32> loc(#loc)
    %cst_7 = stablehlo.constant dense<2.000000e+00> : tensor<1x32x96x128xf32> loc(#loc)
    %cst_8 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01, 1.300000e+01, 1.400000e+01, 1.500000e+01, 1.600000e+01, 1.700000e+01, 1.800000e+01, 1.900000e+01, 2.000000e+01, 2.100000e+01, 2.200000e+01, 2.300000e+01, 2.400000e+01, 2.500000e+01, 2.600000e+01, 2.700000e+01, 2.800000e+01, 2.900000e+01, 3.000000e+01, 3.100000e+01]]]> : tensor<1x1x32xf32> loc(#loc)
    %cst_9 = stablehlo.constant dense<9.99999974E-6> : tensor<1x32x8x1xf32> loc(#loc)
    %cst_10 = stablehlo.constant dense<7.812500e-03> : tensor<1x32x8xf32> loc(#loc)
    %cst_11 = stablehlo.constant dense<2.000000e+00> : tensor<1x32x8x128xf32> loc(#loc)
    %cst_12 = stablehlo.constant dense<9.99999974E-6> : tensor<1x32x1xf32> loc(#loc)
    %cst_13 = stablehlo.constant dense<1.95312503E-4> : tensor<1x32xf32> loc(#loc)
    %cst_14 = stablehlo.constant dense<2.000000e+00> : tensor<1x32x5120xf32> loc(#loc)
    %cst_15 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<true> : tensor<i1> loc(#loc)
    %c_16 = stablehlo.constant dense<false> : tensor<i1> loc(#loc)
    %c_17 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64> loc(#loc)
    %cst_18 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %0 = stablehlo.reshape %arg4 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc20)
    %1 = stablehlo.reshape %0 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc21)
    %2 = stablehlo.broadcast_in_dim %1, dims = [2] : (tensor<5120xbf16>) -> tensor<1x32x5120xbf16> loc(#loc22)
    %3 = stablehlo.reshape %arg3 : (tensor<151552x5120xbf16>) -> tensor<1x151552x5120xbf16> loc(#loc23)
    %4 = stablehlo.reshape %3 : (tensor<1x151552x5120xbf16>) -> tensor<151552x5120xbf16> loc(#loc24)
    %5 = stablehlo.reshape %arg2 : (tensor<1x32xi64>) -> tensor<1x1x32xi64> loc(#loc25)
    %6 = stablehlo.reshape %5 : (tensor<1x1x32xi64>) -> tensor<32xi64> loc(#loc26)
    %7 = stablehlo.convert %6 : (tensor<32xi64>) -> tensor<32xui32> loc(#loc27)
    %8 = "stablehlo.gather"(%4, %7) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 5120>}> : (tensor<151552x5120xbf16>, tensor<32xui32>) -> tensor<32x5120xbf16> loc(#loc28)
    %9 = stablehlo.reshape %8 : (tensor<32x5120xbf16>) -> tensor<1x32x5120xbf16> loc(#loc29)
    %10 = stablehlo.convert %9 : (tensor<1x32x5120xbf16>) -> tensor<1x32x5120xf32> loc(#loc30)
    %11 = stablehlo.power %10, %cst_14 : tensor<1x32x5120xf32> loc(#loc31)
    %12 = stablehlo.reduce(%11 init: %cst_18) applies stablehlo.add across dimensions = [2] : (tensor<1x32x5120xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc32)
    %13 = stablehlo.multiply %12, %cst_13 : tensor<1x32xf32> loc(#loc33)
    %14 = stablehlo.reshape %13 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc34)
    %15 = stablehlo.add %14, %cst_12 : tensor<1x32x1xf32> loc(#loc35)
    %16 = stablehlo.rsqrt %15 : tensor<1x32x1xf32> loc(#loc36)
    %17 = stablehlo.reshape %16 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc37)
    %18 = stablehlo.broadcast_in_dim %17, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x5120xf32> loc(#loc38)
    %19 = stablehlo.multiply %10, %18 : tensor<1x32x5120xf32> loc(#loc39)
    %20 = stablehlo.convert %19 : (tensor<1x32x5120xf32>) -> tensor<1x32x5120xbf16> loc(#loc40)
    %21 = stablehlo.multiply %2, %20 : tensor<1x32x5120xbf16> loc(#loc41)
    %22 = stablehlo.reshape %21 : (tensor<1x32x5120xbf16>) -> tensor<32x5120xbf16> loc(#loc42)
    %23 = stablehlo.reshape %arg1 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc43)
    %24 = stablehlo.reshape %23 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc44)
    %25 = stablehlo.transpose %24, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc45)
    %26 = stablehlo.dot_general %22, %25, contracting_dims = [1] x [0] : (tensor<32x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<32x1024xbf16> loc(#loc46)
    %27 = stablehlo.reshape %26 : (tensor<32x1024xbf16>) -> tensor<1x32x1024xbf16> loc(#loc47)
    %28 = stablehlo.reshape %arg0 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc48)
    %29 = stablehlo.reshape %28 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc49)
    %30 = stablehlo.broadcast_in_dim %29, dims = [2] : (tensor<1024xbf16>) -> tensor<1x32x1024xbf16> loc(#loc50)
    %31 = stablehlo.add %27, %30 : tensor<1x32x1024xbf16> loc(#loc51)
    %32 = stablehlo.reshape %31 : (tensor<1x32x1024xbf16>) -> tensor<1x32x8x128xbf16> loc(#loc52)
    %33 = stablehlo.transpose %32, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,32,128]{3,1,2,0}"} : (tensor<1x32x8x128xbf16>) -> tensor<1x8x32x128xbf16> loc(#loc53)
    %34 = stablehlo.reshape %arg7 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc54)
    %35 = stablehlo.reshape %34 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc55)
    %36 = stablehlo.broadcast_in_dim %35, dims = [3] : (tensor<128xbf16>) -> tensor<1x32x8x128xbf16> loc(#loc56)
    %37 = stablehlo.reshape %arg6 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc57)
    %38 = stablehlo.reshape %37 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc58)
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc59)
    %40 = stablehlo.dot_general %22, %39, contracting_dims = [1] x [0] : (tensor<32x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<32x1024xbf16> loc(#loc60)
    %41 = stablehlo.reshape %40 : (tensor<32x1024xbf16>) -> tensor<1x32x1024xbf16> loc(#loc61)
    %42 = stablehlo.reshape %arg5 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc62)
    %43 = stablehlo.reshape %42 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc63)
    %44 = stablehlo.broadcast_in_dim %43, dims = [2] : (tensor<1024xbf16>) -> tensor<1x32x1024xbf16> loc(#loc64)
    %45 = stablehlo.add %41, %44 : tensor<1x32x1024xbf16> loc(#loc65)
    %46 = stablehlo.reshape %45 : (tensor<1x32x1024xbf16>) -> tensor<1x32x8x128xbf16> loc(#loc66)
    %47 = stablehlo.convert %46 : (tensor<1x32x8x128xbf16>) -> tensor<1x32x8x128xf32> loc(#loc67)
    %48 = stablehlo.power %47, %cst_11 : tensor<1x32x8x128xf32> loc(#loc68)
    %49 = stablehlo.reduce(%48 init: %cst_18) applies stablehlo.add across dimensions = [3] : (tensor<1x32x8x128xf32>, tensor<f32>) -> tensor<1x32x8xf32> loc(#loc69)
    %50 = stablehlo.multiply %49, %cst_10 : tensor<1x32x8xf32> loc(#loc70)
    %51 = stablehlo.reshape %50 : (tensor<1x32x8xf32>) -> tensor<1x32x8x1xf32> loc(#loc71)
    %52 = stablehlo.add %51, %cst_9 : tensor<1x32x8x1xf32> loc(#loc72)
    %53 = stablehlo.rsqrt %52 : tensor<1x32x8x1xf32> loc(#loc73)
    %54 = stablehlo.reshape %53 : (tensor<1x32x8x1xf32>) -> tensor<1x32x8xf32> loc(#loc74)
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 1, 2] : (tensor<1x32x8xf32>) -> tensor<1x32x8x128xf32> loc(#loc75)
    %56 = stablehlo.multiply %47, %55 : tensor<1x32x8x128xf32> loc(#loc76)
    %57 = stablehlo.convert %56 : (tensor<1x32x8x128xf32>) -> tensor<1x32x8x128xbf16> loc(#loc77)
    %58 = stablehlo.multiply %36, %57 : tensor<1x32x8x128xbf16> loc(#loc78)
    %59 = stablehlo.transpose %58, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,32,128]{3,1,2,0}"} : (tensor<1x32x8x128xbf16>) -> tensor<1x8x32x128xbf16> loc(#loc79)
    %60 = stablehlo.slice %59 [0:1, 0:8, 0:32, 0:64] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x32x64xbf16> loc(#loc80)
    %61 = stablehlo.reshape %arg8 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc81)
    %62 = stablehlo.reshape %61 : (tensor<1x1x32xbf16>) -> tensor<1x32x1xbf16> loc(#loc82)
    %63 = stablehlo.convert %62 : (tensor<1x32x1xbf16>) -> tensor<1x32x1xf32> loc(#loc83)
    %64 = stablehlo.dot_general %63, %cst_8, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x32xf32>) -> tensor<1x32x32xf32> loc(#loc84)
    %65 = stablehlo.transpose %64, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,32,32]{1,2,0}"} : (tensor<1x32x32xf32>) -> tensor<1x32x32xf32> loc(#loc85)
    %66 = stablehlo.concatenate %65, %65, dim = 2 : (tensor<1x32x32xf32>, tensor<1x32x32xf32>) -> tensor<1x32x64xf32> loc(#loc86)
    %67 = stablehlo.cosine %66 : tensor<1x32x64xf32> loc(#loc87)
    %68 = stablehlo.convert %67 : (tensor<1x32x64xf32>) -> tensor<1x32x64xbf16> loc(#loc88)
    %69 = stablehlo.broadcast_in_dim %68, dims = [0, 2, 3] : (tensor<1x32x64xbf16>) -> tensor<1x8x32x64xbf16> loc(#loc89)
    %70 = stablehlo.multiply %60, %69 : tensor<1x8x32x64xbf16> loc(#loc90)
    %71 = stablehlo.slice %60 [0:1, 0:8, 0:32, 32:64] : (tensor<1x8x32x64xbf16>) -> tensor<1x8x32x32xbf16> loc(#loc91)
    %72 = stablehlo.negate %71 : tensor<1x8x32x32xbf16> loc(#loc92)
    %73 = stablehlo.slice %60 [0:1, 0:8, 0:32, 0:32] : (tensor<1x8x32x64xbf16>) -> tensor<1x8x32x32xbf16> loc(#loc93)
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x8x32x32xbf16>, tensor<1x8x32x32xbf16>) -> tensor<1x8x32x64xbf16> loc(#loc94)
    %75 = stablehlo.sine %66 : tensor<1x32x64xf32> loc(#loc95)
    %76 = stablehlo.convert %75 : (tensor<1x32x64xf32>) -> tensor<1x32x64xbf16> loc(#loc96)
    %77 = stablehlo.broadcast_in_dim %76, dims = [0, 2, 3] : (tensor<1x32x64xbf16>) -> tensor<1x8x32x64xbf16> loc(#loc97)
    %78 = stablehlo.multiply %74, %77 : tensor<1x8x32x64xbf16> loc(#loc98)
    %79 = stablehlo.add %70, %78 : tensor<1x8x32x64xbf16> loc(#loc99)
    %80 = stablehlo.slice %59 [0:1, 0:8, 0:32, 64:128] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x32x64xbf16> loc(#loc100)
    %81 = stablehlo.concatenate %79, %80, dim = 3 : (tensor<1x8x32x64xbf16>, tensor<1x8x32x64xbf16>) -> tensor<1x8x32x128xbf16> loc(#loc101)
    %82 = stablehlo.reshape %arg18 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc102)
    %83 = stablehlo.reshape %82 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc103)
    %84 = stablehlo.broadcast_in_dim %83, dims = [2] : (tensor<5120xbf16>) -> tensor<1x32x5120xbf16> loc(#loc104)
    %85 = stablehlo.reshape %arg15 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc105)
    %86 = stablehlo.reshape %85 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc106)
    %87 = stablehlo.broadcast_in_dim %86, dims = [3] : (tensor<128xbf16>) -> tensor<1x32x96x128xbf16> loc(#loc107)
    %88 = stablehlo.reshape %arg14 : (tensor<12288x5120xbf16>) -> tensor<1x12288x5120xbf16> loc(#loc108)
    %89 = stablehlo.reshape %88 : (tensor<1x12288x5120xbf16>) -> tensor<12288x5120xbf16> loc(#loc109)
    %90 = stablehlo.transpose %89, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,12288]{0,1}"} : (tensor<12288x5120xbf16>) -> tensor<5120x12288xbf16> loc(#loc110)
    %91 = stablehlo.dot_general %22, %90, contracting_dims = [1] x [0] : (tensor<32x5120xbf16>, tensor<5120x12288xbf16>) -> tensor<32x12288xbf16> loc(#loc111)
    %92 = stablehlo.reshape %91 : (tensor<32x12288xbf16>) -> tensor<1x32x12288xbf16> loc(#loc112)
    %93 = stablehlo.reshape %arg13 : (tensor<12288xbf16>) -> tensor<1x1x12288xbf16> loc(#loc113)
    %94 = stablehlo.reshape %93 : (tensor<1x1x12288xbf16>) -> tensor<12288xbf16> loc(#loc114)
    %95 = stablehlo.broadcast_in_dim %94, dims = [2] : (tensor<12288xbf16>) -> tensor<1x32x12288xbf16> loc(#loc115)
    %96 = stablehlo.add %92, %95 : tensor<1x32x12288xbf16> loc(#loc116)
    %97 = stablehlo.reshape %96 : (tensor<1x32x12288xbf16>) -> tensor<1x32x96x128xbf16> loc(#loc117)
    %98 = stablehlo.convert %97 : (tensor<1x32x96x128xbf16>) -> tensor<1x32x96x128xf32> loc(#loc118)
    %99 = stablehlo.power %98, %cst_7 : tensor<1x32x96x128xf32> loc(#loc119)
    %100 = stablehlo.reduce(%99 init: %cst_18) applies stablehlo.add across dimensions = [3] : (tensor<1x32x96x128xf32>, tensor<f32>) -> tensor<1x32x96xf32> loc(#loc120)
    %101 = stablehlo.multiply %100, %cst_6 : tensor<1x32x96xf32> loc(#loc121)
    %102 = stablehlo.reshape %101 : (tensor<1x32x96xf32>) -> tensor<1x32x96x1xf32> loc(#loc122)
    %103 = stablehlo.add %102, %cst_5 : tensor<1x32x96x1xf32> loc(#loc123)
    %104 = stablehlo.rsqrt %103 : tensor<1x32x96x1xf32> loc(#loc124)
    %105 = stablehlo.reshape %104 : (tensor<1x32x96x1xf32>) -> tensor<1x32x96xf32> loc(#loc125)
    %106 = stablehlo.broadcast_in_dim %105, dims = [0, 1, 2] : (tensor<1x32x96xf32>) -> tensor<1x32x96x128xf32> loc(#loc126)
    %107 = stablehlo.multiply %98, %106 : tensor<1x32x96x128xf32> loc(#loc127)
    %108 = stablehlo.convert %107 : (tensor<1x32x96x128xf32>) -> tensor<1x32x96x128xbf16> loc(#loc128)
    %109 = stablehlo.multiply %87, %108 : tensor<1x32x96x128xbf16> loc(#loc129)
    %110 = stablehlo.transpose %109, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,96,32,128]{3,1,2,0}"} : (tensor<1x32x96x128xbf16>) -> tensor<1x96x32x128xbf16> loc(#loc130)
    %111 = stablehlo.slice %110 [0:1, 0:96, 0:32, 0:64] : (tensor<1x96x32x128xbf16>) -> tensor<1x96x32x64xbf16> loc(#loc131)
    %112 = stablehlo.broadcast_in_dim %68, dims = [0, 2, 3] : (tensor<1x32x64xbf16>) -> tensor<1x96x32x64xbf16> loc(#loc132)
    %113 = stablehlo.multiply %111, %112 : tensor<1x96x32x64xbf16> loc(#loc133)
    %114 = stablehlo.slice %111 [0:1, 0:96, 0:32, 32:64] : (tensor<1x96x32x64xbf16>) -> tensor<1x96x32x32xbf16> loc(#loc134)
    %115 = stablehlo.negate %114 : tensor<1x96x32x32xbf16> loc(#loc135)
    %116 = stablehlo.slice %111 [0:1, 0:96, 0:32, 0:32] : (tensor<1x96x32x64xbf16>) -> tensor<1x96x32x32xbf16> loc(#loc136)
    %117 = stablehlo.concatenate %115, %116, dim = 3 : (tensor<1x96x32x32xbf16>, tensor<1x96x32x32xbf16>) -> tensor<1x96x32x64xbf16> loc(#loc137)
    %118 = stablehlo.broadcast_in_dim %76, dims = [0, 2, 3] : (tensor<1x32x64xbf16>) -> tensor<1x96x32x64xbf16> loc(#loc138)
    %119 = stablehlo.multiply %117, %118 : tensor<1x96x32x64xbf16> loc(#loc139)
    %120 = stablehlo.add %113, %119 : tensor<1x96x32x64xbf16> loc(#loc140)
    %121 = stablehlo.slice %110 [0:1, 0:96, 0:32, 64:128] : (tensor<1x96x32x128xbf16>) -> tensor<1x96x32x64xbf16> loc(#loc141)
    %122 = stablehlo.concatenate %120, %121, dim = 3 : (tensor<1x96x32x64xbf16>, tensor<1x96x32x64xbf16>) -> tensor<1x96x32x128xbf16> loc(#loc142)
    %123 = stablehlo.convert %122 : (tensor<1x96x32x128xbf16>) -> tensor<1x96x32x128xf32> loc(#loc143)
    %124 = stablehlo.multiply %123, %cst_4 : tensor<1x96x32x128xf32> loc(#loc144)
    %125 = stablehlo.broadcast_in_dim %81, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x12x32x128xbf16> loc(#loc145)
    %126 = stablehlo.reshape %125 : (tensor<1x8x12x32x128xbf16>) -> tensor<1x96x32x128xbf16> loc(#loc146)
    %127 = stablehlo.convert %126 : (tensor<1x96x32x128xbf16>) -> tensor<1x96x32x128xf32> loc(#loc147)
    %128 = stablehlo.transpose %127, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[1,96,128,32]{2,3,1,0}"} : (tensor<1x96x32x128xf32>) -> tensor<1x96x128x32xf32> loc(#loc148)
    %129 = stablehlo.multiply %128, %cst_3 : tensor<1x96x128x32xf32> loc(#loc149)
    %130 = stablehlo.dot_general %124, %129, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x96x32x128xf32>, tensor<1x96x128x32xf32>) -> tensor<1x96x32x32xf32> loc(#loc150)
    %131 = stablehlo.broadcast_in_dim %c_17, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc151)
    %132 = stablehlo.broadcast_in_dim %c_17, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc152)
    %133 = stablehlo.compare  LE, %131, %132 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1> loc(#loc153)
    %134 = stablehlo.reshape %133 : (tensor<32x32xi1>) -> tensor<1x1x32x32xi1> loc(#loc154)
    %135 = stablehlo.select %134, %cst_2, %cst_1 : tensor<1x1x32x32xi1>, tensor<1x1x32x32xbf16> loc(#loc155)
    %136 = stablehlo.convert %135 : (tensor<1x1x32x32xbf16>) -> tensor<1x1x32x32xf32> loc(#loc156)
    %137 = stablehlo.reshape %136 : (tensor<1x1x32x32xf32>) -> tensor<1x32x32xf32> loc(#loc157)
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 2, 3] : (tensor<1x32x32xf32>) -> tensor<1x96x32x32xf32> loc(#loc158)
    %139 = stablehlo.add %130, %138 : tensor<1x96x32x32xf32> loc(#loc159)
    %140 = stablehlo.convert %139 : (tensor<1x96x32x32xf32>) -> tensor<1x96x32x32xf64> loc(#loc160)
    %141 = stablehlo.compare  EQ, %140, %cst_0 : (tensor<1x96x32x32xf64>, tensor<1x96x32x32xf64>) -> tensor<1x96x32x32xi1> loc(#loc161)
    %142 = stablehlo.not %141 : tensor<1x96x32x32xi1> loc(#loc162)
    %143 = stablehlo.reduce(%142 init: %c_16) across dimensions = [3] : (tensor<1x96x32x32xi1>, tensor<i1>) -> tensor<1x96x32xi1>
     reducer(%arg19: tensor<i1> loc("reduce.376"), %arg20: tensor<i1> loc("reduce.376"))  {
      %223 = stablehlo.or %arg19, %arg20 : tensor<i1> loc(#loc164)
      %224 = stablehlo.select %223, %c, %c_16 : tensor<i1>, tensor<i1> loc(#loc165)
      stablehlo.return %224 : tensor<i1> loc(#loc)
    } loc(#loc163)
    %144 = stablehlo.reshape %143 : (tensor<1x96x32xi1>) -> tensor<1x96x32x1xi1> loc(#loc166)
    %145 = stablehlo.not %144 : tensor<1x96x32x1xi1> loc(#loc167)
    %146 = stablehlo.reshape %145 : (tensor<1x96x32x1xi1>) -> tensor<1x96x32xi1> loc(#loc168)
    %147 = stablehlo.broadcast_in_dim %146, dims = [0, 1, 2] : (tensor<1x96x32xi1>) -> tensor<1x96x32x32xi1> loc(#loc169)
    %148 = stablehlo.reduce(%139 init: %cst_15) applies stablehlo.maximum across dimensions = [3] : (tensor<1x96x32x32xf32>, tensor<f32>) -> tensor<1x96x32xf32> loc(#loc170)
    %149 = stablehlo.broadcast_in_dim %148, dims = [0, 1, 2] : (tensor<1x96x32xf32>) -> tensor<1x96x32x32xf32> loc(#loc171)
    %150 = stablehlo.subtract %139, %149 : tensor<1x96x32x32xf32> loc(#loc172)
    %151 = stablehlo.exponential %150 : tensor<1x96x32x32xf32> loc(#loc173)
    %152 = stablehlo.reduce(%151 init: %cst_18) applies stablehlo.add across dimensions = [3] : (tensor<1x96x32x32xf32>, tensor<f32>) -> tensor<1x96x32xf32> loc(#loc174)
    %153 = stablehlo.broadcast_in_dim %152, dims = [0, 1, 2] : (tensor<1x96x32xf32>) -> tensor<1x96x32x32xf32> loc(#loc175)
    %154 = stablehlo.divide %151, %153 : tensor<1x96x32x32xf32> loc(#loc176)
    %155 = stablehlo.select %147, %cst, %154 : tensor<1x96x32x32xi1>, tensor<1x96x32x32xf32> loc(#loc177)
    %156 = stablehlo.broadcast_in_dim %33, dims = [0, 1, 3, 4] : (tensor<1x8x32x128xbf16>) -> tensor<1x8x12x32x128xbf16> loc(#loc178)
    %157 = stablehlo.reshape %156 : (tensor<1x8x12x32x128xbf16>) -> tensor<1x96x32x128xbf16> loc(#loc179)
    %158 = stablehlo.convert %157 : (tensor<1x96x32x128xbf16>) -> tensor<1x96x32x128xf32> loc(#loc180)
    %159 = stablehlo.dot_general %155, %158, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x96x32x32xf32>, tensor<1x96x32x128xf32>) -> tensor<1x96x32x128xf32> loc(#loc181)
    %160 = stablehlo.convert %159 : (tensor<1x96x32x128xf32>) -> tensor<1x96x32x128xbf16> loc(#loc182)
    %161 = stablehlo.transpose %160, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,32,96,128]{3,1,2,0}"} : (tensor<1x96x32x128xbf16>) -> tensor<1x32x96x128xbf16> loc(#loc183)
    %162 = stablehlo.reshape %161 : (tensor<1x32x96x128xbf16>) -> tensor<32x12288xbf16> loc(#loc184)
    %163 = stablehlo.reshape %arg12 : (tensor<5120x12288xbf16>) -> tensor<1x5120x12288xbf16> loc(#loc185)
    %164 = stablehlo.reshape %163 : (tensor<1x5120x12288xbf16>) -> tensor<5120x12288xbf16> loc(#loc186)
    %165 = stablehlo.transpose %164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[12288,5120]{0,1}"} : (tensor<5120x12288xbf16>) -> tensor<12288x5120xbf16> loc(#loc187)
    %166 = stablehlo.dot_general %162, %165, contracting_dims = [1] x [0] : (tensor<32x12288xbf16>, tensor<12288x5120xbf16>) -> tensor<32x5120xbf16> loc(#loc188)
    %167 = stablehlo.reshape %166 : (tensor<32x5120xbf16>) -> tensor<1x32x5120xbf16> loc(#loc189)
    %168 = stablehlo.add %9, %167 : tensor<1x32x5120xbf16> loc(#loc190)
    %169 = stablehlo.reshape %arg16 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc191)
    %170 = stablehlo.reshape %169 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc192)
    %171 = stablehlo.broadcast_in_dim %170, dims = [2] : (tensor<5120xbf16>) -> tensor<1x32x5120xbf16> loc(#loc193)
    %172 = stablehlo.convert %168 : (tensor<1x32x5120xbf16>) -> tensor<1x32x5120xf32> loc(#loc194)
    %173 = stablehlo.power %172, %cst_14 : tensor<1x32x5120xf32> loc(#loc195)
    %174 = stablehlo.reduce(%173 init: %cst_18) applies stablehlo.add across dimensions = [2] : (tensor<1x32x5120xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc196)
    %175 = stablehlo.multiply %174, %cst_13 : tensor<1x32xf32> loc(#loc197)
    %176 = stablehlo.reshape %175 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc198)
    %177 = stablehlo.add %176, %cst_12 : tensor<1x32x1xf32> loc(#loc199)
    %178 = stablehlo.rsqrt %177 : tensor<1x32x1xf32> loc(#loc200)
    %179 = stablehlo.reshape %178 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc201)
    %180 = stablehlo.broadcast_in_dim %179, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x5120xf32> loc(#loc202)
    %181 = stablehlo.multiply %172, %180 : tensor<1x32x5120xf32> loc(#loc203)
    %182 = stablehlo.convert %181 : (tensor<1x32x5120xf32>) -> tensor<1x32x5120xbf16> loc(#loc204)
    %183 = stablehlo.multiply %171, %182 : tensor<1x32x5120xbf16> loc(#loc205)
    %184 = stablehlo.reshape %183 : (tensor<1x32x5120xbf16>) -> tensor<32x5120xbf16> loc(#loc206)
    %185 = stablehlo.reshape %arg17 : (tensor<12288x5120xbf16>) -> tensor<1x12288x5120xbf16> loc(#loc207)
    %186 = stablehlo.reshape %185 : (tensor<1x12288x5120xbf16>) -> tensor<12288x5120xbf16> loc(#loc208)
    %187 = stablehlo.transpose %186, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,12288]{0,1}"} : (tensor<12288x5120xbf16>) -> tensor<5120x12288xbf16> loc(#loc209)
    %188 = stablehlo.dot_general %184, %187, contracting_dims = [1] x [0] : (tensor<32x5120xbf16>, tensor<5120x12288xbf16>) -> tensor<32x12288xbf16> loc(#loc210)
    %189 = stablehlo.reshape %188 : (tensor<32x12288xbf16>) -> tensor<1x32x12288xbf16> loc(#loc211)
    %190 = stablehlo.logistic %189 : tensor<1x32x12288xbf16> loc(#loc212)
    %191 = stablehlo.multiply %189, %190 : tensor<1x32x12288xbf16> loc(#loc213)
    %192 = stablehlo.reshape %arg11 : (tensor<12288x5120xbf16>) -> tensor<1x12288x5120xbf16> loc(#loc214)
    %193 = stablehlo.reshape %192 : (tensor<1x12288x5120xbf16>) -> tensor<12288x5120xbf16> loc(#loc215)
    %194 = stablehlo.transpose %193, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,12288]{0,1}"} : (tensor<12288x5120xbf16>) -> tensor<5120x12288xbf16> loc(#loc216)
    %195 = stablehlo.dot_general %184, %194, contracting_dims = [1] x [0] : (tensor<32x5120xbf16>, tensor<5120x12288xbf16>) -> tensor<32x12288xbf16> loc(#loc217)
    %196 = stablehlo.reshape %195 : (tensor<32x12288xbf16>) -> tensor<1x32x12288xbf16> loc(#loc218)
    %197 = stablehlo.multiply %191, %196 : tensor<1x32x12288xbf16> loc(#loc219)
    %198 = stablehlo.reshape %197 : (tensor<1x32x12288xbf16>) -> tensor<32x12288xbf16> loc(#loc220)
    %199 = stablehlo.reshape %arg10 : (tensor<5120x12288xbf16>) -> tensor<1x5120x12288xbf16> loc(#loc221)
    %200 = stablehlo.reshape %199 : (tensor<1x5120x12288xbf16>) -> tensor<5120x12288xbf16> loc(#loc222)
    %201 = stablehlo.transpose %200, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[12288,5120]{0,1}"} : (tensor<5120x12288xbf16>) -> tensor<12288x5120xbf16> loc(#loc223)
    %202 = stablehlo.dot_general %198, %201, contracting_dims = [1] x [0] : (tensor<32x12288xbf16>, tensor<12288x5120xbf16>) -> tensor<32x5120xbf16> loc(#loc224)
    %203 = stablehlo.reshape %202 : (tensor<32x5120xbf16>) -> tensor<1x32x5120xbf16> loc(#loc225)
    %204 = stablehlo.add %168, %203 : tensor<1x32x5120xbf16> loc(#loc226)
    %205 = stablehlo.convert %204 : (tensor<1x32x5120xbf16>) -> tensor<1x32x5120xf32> loc(#loc227)
    %206 = stablehlo.power %205, %cst_14 : tensor<1x32x5120xf32> loc(#loc228)
    %207 = stablehlo.reduce(%206 init: %cst_18) applies stablehlo.add across dimensions = [2] : (tensor<1x32x5120xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc229)
    %208 = stablehlo.multiply %207, %cst_13 : tensor<1x32xf32> loc(#loc230)
    %209 = stablehlo.reshape %208 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc231)
    %210 = stablehlo.add %209, %cst_12 : tensor<1x32x1xf32> loc(#loc232)
    %211 = stablehlo.rsqrt %210 : tensor<1x32x1xf32> loc(#loc233)
    %212 = stablehlo.reshape %211 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc234)
    %213 = stablehlo.broadcast_in_dim %212, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x5120xf32> loc(#loc235)
    %214 = stablehlo.multiply %205, %213 : tensor<1x32x5120xf32> loc(#loc236)
    %215 = stablehlo.convert %214 : (tensor<1x32x5120xf32>) -> tensor<1x32x5120xbf16> loc(#loc237)
    %216 = stablehlo.multiply %84, %215 : tensor<1x32x5120xbf16> loc(#loc238)
    %217 = stablehlo.reshape %216 : (tensor<1x32x5120xbf16>) -> tensor<32x5120xbf16> loc(#loc239)
    %218 = stablehlo.reshape %arg9 : (tensor<151552x5120xbf16>) -> tensor<1x151552x5120xbf16> loc(#loc240)
    %219 = stablehlo.reshape %218 : (tensor<1x151552x5120xbf16>) -> tensor<151552x5120xbf16> loc(#loc241)
    %220 = stablehlo.transpose %219, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,151552]{0,1}"} : (tensor<151552x5120xbf16>) -> tensor<5120x151552xbf16> loc(#loc242)
    %221 = stablehlo.dot_general %217, %220, contracting_dims = [1] x [0] : (tensor<32x5120xbf16>, tensor<5120x151552xbf16>) -> tensor<32x151552xbf16> loc(#loc243)
    %222 = stablehlo.reshape %221 : (tensor<32x151552xbf16>) -> tensor<1x32x151552xbf16> loc(#loc244)
    return %33, %81, %222 : tensor<1x8x32x128xbf16>, tensor<1x8x32x128xbf16>, tensor<1x32x151552xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("reshape.56")
#loc21 = loc("reshape.58")
#loc22 = loc("broadcast.59")
#loc23 = loc("reshape.20")
#loc24 = loc("reshape.22")
#loc25 = loc("reshape.15")
#loc26 = loc("reshape.18")
#loc27 = loc("convert.23")
#loc28 = loc("gather.24")
#loc29 = loc("reshape.25")
#loc30 = loc("convert.26")
#loc31 = loc("power.28")
#loc32 = loc("reduce.35")
#loc33 = loc("multiply.44")
#loc34 = loc("reshape.45")
#loc35 = loc("add.49")
#loc36 = loc("rsqrt.50")
#loc37 = loc("reshape.51")
#loc38 = loc("broadcast.52")
#loc39 = loc("multiply.53")
#loc40 = loc("convert.54")
#loc41 = loc("multiply.60")
#loc42 = loc("reshape.61")
#loc43 = loc("reshape.7")
#loc44 = loc("reshape.9")
#loc45 = loc("transpose.10")
#loc46 = loc("dot.62")
#loc47 = loc("reshape.63")
#loc48 = loc("reshape.3")
#loc49 = loc("reshape.5")
#loc50 = loc("broadcast.66")
#loc51 = loc("add.67")
#loc52 = loc("reshape.68")
#loc53 = loc("transpose.69")
#loc54 = loc("reshape.122")
#loc55 = loc("reshape.124")
#loc56 = loc("broadcast.125")
#loc57 = loc("reshape.80")
#loc58 = loc("reshape.82")
#loc59 = loc("transpose.83")
#loc60 = loc("dot.85")
#loc61 = loc("reshape.86")
#loc62 = loc("reshape.76")
#loc63 = loc("reshape.78")
#loc64 = loc("broadcast.89")
#loc65 = loc("add.90")
#loc66 = loc("reshape.91")
#loc67 = loc("convert.92")
#loc68 = loc("power.94")
#loc69 = loc("reduce.101")
#loc70 = loc("multiply.110")
#loc71 = loc("reshape.111")
#loc72 = loc("add.115")
#loc73 = loc("rsqrt.116")
#loc74 = loc("reshape.117")
#loc75 = loc("broadcast.118")
#loc76 = loc("multiply.119")
#loc77 = loc("convert.120")
#loc78 = loc("multiply.126")
#loc79 = loc("transpose.127")
#loc80 = loc("slice.153")
#loc81 = loc("reshape.137")
#loc82 = loc("reshape.141")
#loc83 = loc("convert.142")
#loc84 = loc("dot.145")
#loc85 = loc("transpose.146")
#loc86 = loc("concatenate.147")
#loc87 = loc("cosine.162")
#loc88 = loc("convert.165")
#loc89 = loc("broadcast.168")
#loc90 = loc("multiply.169")
#loc91 = loc("slice.155")
#loc92 = loc("negate.156")
#loc93 = loc("slice.154")
#loc94 = loc("concatenate.157")
#loc95 = loc("sine.148")
#loc96 = loc("convert.151")
#loc97 = loc("broadcast.159")
#loc98 = loc("multiply.160")
#loc99 = loc("add.172")
#loc100 = loc("slice.128")
#loc101 = loc("concatenate.173")
#loc102 = loc("reshape.483")
#loc103 = loc("reshape.485")
#loc104 = loc("broadcast.486")
#loc105 = loc("reshape.301")
#loc106 = loc("reshape.303")
#loc107 = loc("broadcast.304")
#loc108 = loc("reshape.259")
#loc109 = loc("reshape.261")
#loc110 = loc("transpose.262")
#loc111 = loc("dot.264")
#loc112 = loc("reshape.265")
#loc113 = loc("reshape.255")
#loc114 = loc("reshape.257")
#loc115 = loc("broadcast.268")
#loc116 = loc("add.269")
#loc117 = loc("reshape.270")
#loc118 = loc("convert.271")
#loc119 = loc("power.273")
#loc120 = loc("reduce.280")
#loc121 = loc("multiply.289")
#loc122 = loc("reshape.290")
#loc123 = loc("add.294")
#loc124 = loc("rsqrt.295")
#loc125 = loc("reshape.296")
#loc126 = loc("broadcast.297")
#loc127 = loc("multiply.298")
#loc128 = loc("convert.299")
#loc129 = loc("multiply.305")
#loc130 = loc("transpose.306")
#loc131 = loc("slice.309")
#loc132 = loc("broadcast.318")
#loc133 = loc("multiply.319")
#loc134 = loc("slice.311")
#loc135 = loc("negate.312")
#loc136 = loc("slice.310")
#loc137 = loc("concatenate.313")
#loc138 = loc("broadcast.315")
#loc139 = loc("multiply.316")
#loc140 = loc("add.322")
#loc141 = loc("slice.307")
#loc142 = loc("concatenate.323")
#loc143 = loc("convert.324")
#loc144 = loc("multiply.326")
#loc145 = loc("broadcast.243")
#loc146 = loc("reshape.244")
#loc147 = loc("convert.245")
#loc148 = loc("transpose.246")
#loc149 = loc("multiply.248")
#loc150 = loc("dot.327")
#loc151 = loc("broadcast.229")
#loc152 = loc("broadcast.231")
#loc153 = loc("compare.232")
#loc154 = loc("reshape.235")
#loc155 = loc("select.238")
#loc156 = loc("convert.329")
#loc157 = loc("reshape.332")
#loc158 = loc("broadcast.333")
#loc159 = loc("add.334")
#loc160 = loc("convert.360")
#loc161 = loc("compare.362")
#loc162 = loc("not.364")
#loc164 = loc("or.374")
#loc165 = loc("select.375")
#loc166 = loc("reshape.380")
#loc167 = loc("not.382")
#loc168 = loc("reshape.384")
#loc169 = loc("broadcast.385")
#loc170 = loc("reduce.340")
#loc171 = loc("broadcast.341")
#loc172 = loc("subtract.342")
#loc173 = loc("exponential.343")
#loc174 = loc("reduce.349")
#loc175 = loc("broadcast.350")
#loc176 = loc("divide.351")
#loc177 = loc("select.386")
#loc178 = loc("broadcast.206")
#loc179 = loc("reshape.207")
#loc180 = loc("convert.208")
#loc181 = loc("dot.387")
#loc182 = loc("convert.389")
#loc183 = loc("transpose.390")
#loc184 = loc("reshape.392")
#loc185 = loc("reshape.199")
#loc186 = loc("reshape.201")
#loc187 = loc("transpose.202")
#loc188 = loc("dot.393")
#loc189 = loc("reshape.394")
#loc190 = loc("add.397")
#loc191 = loc("reshape.428")
#loc192 = loc("reshape.430")
#loc193 = loc("broadcast.431")
#loc194 = loc("convert.398")
#loc195 = loc("power.400")
#loc196 = loc("reduce.407")
#loc197 = loc("multiply.416")
#loc198 = loc("reshape.417")
#loc199 = loc("add.421")
#loc200 = loc("rsqrt.422")
#loc201 = loc("reshape.423")
#loc202 = loc("broadcast.424")
#loc203 = loc("multiply.425")
#loc204 = loc("convert.426")
#loc205 = loc("multiply.432")
#loc206 = loc("reshape.441")
#loc207 = loc("reshape.437")
#loc208 = loc("reshape.439")
#loc209 = loc("transpose.440")
#loc210 = loc("dot.442")
#loc211 = loc("reshape.443")
#loc212 = loc("logistic.444")
#loc213 = loc("multiply.445")
#loc214 = loc("reshape.190")
#loc215 = loc("reshape.192")
#loc216 = loc("transpose.193")
#loc217 = loc("dot.434")
#loc218 = loc("reshape.435")
#loc219 = loc("multiply.446")
#loc220 = loc("reshape.447")
#loc221 = loc("reshape.185")
#loc222 = loc("reshape.187")
#loc223 = loc("transpose.188")
#loc224 = loc("dot.448")
#loc225 = loc("reshape.449")
#loc226 = loc("add.452")
#loc227 = loc("convert.453")
#loc228 = loc("power.455")
#loc229 = loc("reduce.462")
#loc230 = loc("multiply.471")
#loc231 = loc("reshape.472")
#loc232 = loc("add.476")
#loc233 = loc("rsqrt.477")
#loc234 = loc("reshape.478")
#loc235 = loc("broadcast.479")
#loc236 = loc("multiply.480")
#loc237 = loc("convert.481")
#loc238 = loc("multiply.487")
#loc239 = loc("reshape.488")
#loc240 = loc("reshape.176")
#loc241 = loc("reshape.178")
#loc242 = loc("transpose.179")
#loc243 = loc("dot.489")
#loc244 = loc("reshape.490")
------------------ END OF MLIR MODULE ------------------
