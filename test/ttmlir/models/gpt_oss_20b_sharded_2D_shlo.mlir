#loc1 = loc("p0.3")
#loc2 = loc("p1.11")
#loc3 = loc("p2.27")
#loc4 = loc("p3.31")
#loc5 = loc("p4.39")
#loc6 = loc("p5.44")
#loc7 = loc("p6.79")
#loc8 = loc("p7.134")
#loc9 = loc("p8.143")
#loc10 = loc("p9.147")
#loc11 = loc("p10.171")
#loc12 = loc("p11.182")
#loc13 = loc("p12.186")
#loc14 = loc("p13.197")
#loc15 = loc("p14.205")
#loc16 = loc("p15.216")
#loc17 = loc("p16.220")
#loc18 = loc("p17.231")
#loc19 = loc("p18.244")
#loc20 = loc("p19.288")
#loc21 = loc("p20.310")
#loc22 = loc("p21.347")
#loc23 = loc("p22.351")
#loc24 = loc("p23.501")
#loc25 = loc("p24.571")
#loc26 = loc("p25.576")
#loc27 = loc("p26.582")
#loc28 = loc("p27.587")
#loc29 = loc("p28.666")
#loc30 = loc("p29.716")
#loc31 = loc("p30.725")
#loc32 = loc("p31.729")
#loc33 = loc("p32.753")
#loc34 = loc("p33.764")
#loc35 = loc("p34.768")
#loc36 = loc("p35.779")
#loc37 = loc("p36.787")
#loc38 = loc("p37.798")
#loc39 = loc("p38.802")
#loc40 = loc("p39.813")
#loc41 = loc("p40.826")
#loc42 = loc("p41.903")
#loc43 = loc("p42.907")
#loc44 = loc("p43.1057")
#loc45 = loc("p44.1127")
#loc46 = loc("p45.1132")
#loc47 = loc("p46.1138")
#loc48 = loc("p47.1143")
#loc49 = loc("p48.1222")
#loc50 = loc("p49.1272")
#loc51 = loc("p50.1281")
#loc52 = loc("p51.1285")
#loc53 = loc("p52.1309")
#loc54 = loc("p53.1320")
#loc55 = loc("p54.1324")
#loc56 = loc("p55.1335")
#loc57 = loc("p56.1343")
#loc58 = loc("p57.1354")
#loc59 = loc("p58.1358")
#loc60 = loc("p59.1369")
#loc61 = loc("p60.1385")
#loc62 = loc("p61.1389")
#loc63 = loc("p62.1539")
#loc64 = loc("p63.1609")
#loc65 = loc("p64.1614")
#loc66 = loc("p65.1620")
#loc67 = loc("p66.1625")
#loc68 = loc("p67.1704")
#loc69 = loc("p68.1754")
#loc70 = loc("p69.1763")
#loc71 = loc("p70.1767")
#loc72 = loc("p71.1791")
#loc73 = loc("p72.1802")
#loc74 = loc("p73.1806")
#loc75 = loc("p74.1817")
#loc76 = loc("p75.1825")
#loc77 = loc("p76.1836")
#loc78 = loc("p77.1840")
#loc79 = loc("p78.1851")
#loc80 = loc("p79.1867")
#loc81 = loc("p80.1871")
#loc82 = loc("p81.2021")
#loc83 = loc("p82.2091")
#loc84 = loc("p83.2096")
#loc85 = loc("p84.2102")
#loc86 = loc("p85.2107")
#loc87 = loc("p86.2186")
#loc88 = loc("p87.2236")
#loc89 = loc("p88.2245")
#loc90 = loc("p89.2249")
#loc91 = loc("p90.2273")
#loc92 = loc("p91.2284")
#loc93 = loc("p92.2288")
#loc94 = loc("p93.2299")
#loc95 = loc("p94.2307")
#loc96 = loc("p95.2318")
#loc97 = loc("p96.2322")
#loc98 = loc("p97.2333")
#loc99 = loc("p98.2349")
#loc100 = loc("p99.2353")
#loc101 = loc("p100.2503")
#loc102 = loc("p101.2573")
#loc103 = loc("p102.2578")
#loc104 = loc("p103.2584")
#loc105 = loc("p104.2589")
#loc106 = loc("p105.2668")
#loc107 = loc("p106.2718")
#loc108 = loc("p107.2727")
#loc109 = loc("p108.2731")
#loc110 = loc("p109.2755")
#loc111 = loc("p110.2766")
#loc112 = loc("p111.2770")
#loc113 = loc("p112.2781")
#loc114 = loc("p113.2789")
#loc115 = loc("p114.2800")
#loc116 = loc("p115.2804")
#loc117 = loc("p116.2815")
#loc118 = loc("p117.2831")
#loc119 = loc("p118.2835")
#loc120 = loc("p119.2985")
#loc121 = loc("p120.3055")
#loc122 = loc("p121.3060")
#loc123 = loc("p122.3066")
#loc124 = loc("p123.3071")
#loc125 = loc("p124.3150")
#loc126 = loc("p125.3200")
#loc127 = loc("p126.3209")
#loc128 = loc("p127.3213")
#loc129 = loc("p128.3237")
#loc130 = loc("p129.3248")
#loc131 = loc("p130.3252")
#loc132 = loc("p131.3263")
#loc133 = loc("p132.3271")
#loc134 = loc("p133.3282")
#loc135 = loc("p134.3286")
#loc136 = loc("p135.3297")
#loc137 = loc("p136.3313")
#loc138 = loc("p137.3317")
#loc139 = loc("p138.3467")
#loc140 = loc("p139.3537")
#loc141 = loc("p140.3542")
#loc142 = loc("p141.3548")
#loc143 = loc("p142.3553")
#loc144 = loc("p143.3632")
#loc145 = loc("p144.3682")
#loc146 = loc("p145.3691")
#loc147 = loc("p146.3695")
#loc148 = loc("p147.3719")
#loc149 = loc("p148.3730")
#loc150 = loc("p149.3734")
#loc151 = loc("p150.3745")
#loc152 = loc("p151.3753")
#loc153 = loc("p152.3764")
#loc154 = loc("p153.3768")
#loc155 = loc("p154.3779")
#loc156 = loc("p155.3795")
#loc157 = loc("p156.3799")
#loc158 = loc("p157.3949")
#loc159 = loc("p158.4019")
#loc160 = loc("p159.4024")
#loc161 = loc("p160.4030")
#loc162 = loc("p161.4035")
#loc163 = loc("p162.4114")
#loc164 = loc("p163.4164")
#loc165 = loc("p164.4173")
#loc166 = loc("p165.4177")
#loc167 = loc("p166.4201")
#loc168 = loc("p167.4212")
#loc169 = loc("p168.4216")
#loc170 = loc("p169.4227")
#loc171 = loc("p170.4235")
#loc172 = loc("p171.4246")
#loc173 = loc("p172.4250")
#loc174 = loc("p173.4261")
#loc175 = loc("p174.4277")
#loc176 = loc("p175.4281")
#loc177 = loc("p176.4431")
#loc178 = loc("p177.4501")
#loc179 = loc("p178.4506")
#loc180 = loc("p179.4512")
#loc181 = loc("p180.4517")
#loc182 = loc("p181.4596")
#loc183 = loc("p182.4646")
#loc184 = loc("p183.4655")
#loc185 = loc("p184.4659")
#loc186 = loc("p185.4683")
#loc187 = loc("p186.4694")
#loc188 = loc("p187.4698")
#loc189 = loc("p188.4709")
#loc190 = loc("p189.4717")
#loc191 = loc("p190.4728")
#loc192 = loc("p191.4732")
#loc193 = loc("p192.4743")
#loc194 = loc("p193.4759")
#loc195 = loc("p194.4763")
#loc196 = loc("p195.4913")
#loc197 = loc("p196.4983")
#loc198 = loc("p197.4988")
#loc199 = loc("p198.4994")
#loc200 = loc("p199.4999")
#loc201 = loc("p200.5078")
#loc202 = loc("p201.5128")
#loc203 = loc("p202.5137")
#loc204 = loc("p203.5141")
#loc205 = loc("p204.5165")
#loc206 = loc("p205.5176")
#loc207 = loc("p206.5180")
#loc208 = loc("p207.5191")
#loc209 = loc("p208.5199")
#loc210 = loc("p209.5210")
#loc211 = loc("p210.5214")
#loc212 = loc("p211.5225")
#loc213 = loc("p212.5241")
#loc214 = loc("p213.5245")
#loc215 = loc("p214.5395")
#loc216 = loc("p215.5465")
#loc217 = loc("p216.5470")
#loc218 = loc("p217.5476")
#loc219 = loc("p218.5481")
#loc220 = loc("p219.5560")
#loc221 = loc("p220.5610")
#loc222 = loc("p221.5619")
#loc223 = loc("p222.5623")
#loc224 = loc("p223.5647")
#loc225 = loc("p224.5658")
#loc226 = loc("p225.5662")
#loc227 = loc("p226.5673")
#loc228 = loc("p227.5681")
#loc229 = loc("p228.5692")
#loc230 = loc("p229.5696")
#loc231 = loc("p230.5707")
#loc232 = loc("p231.5723")
#loc233 = loc("p232.5727")
#loc234 = loc("p233.5877")
#loc235 = loc("p234.5947")
#loc236 = loc("p235.5952")
#loc237 = loc("p236.5958")
#loc238 = loc("p237.5963")
#loc239 = loc("p238.6042")
#loc240 = loc("p239.6092")
#loc241 = loc("p240.6101")
#loc242 = loc("p241.6105")
#loc243 = loc("p242.6129")
#loc244 = loc("p243.6140")
#loc245 = loc("p244.6144")
#loc246 = loc("p245.6155")
#loc247 = loc("p246.6163")
#loc248 = loc("p247.6174")
#loc249 = loc("p248.6178")
#loc250 = loc("p249.6189")
#loc251 = loc("p250.6205")
#loc252 = loc("p251.6209")
#loc253 = loc("p252.6359")
#loc254 = loc("p253.6429")
#loc255 = loc("p254.6434")
#loc256 = loc("p255.6440")
#loc257 = loc("p256.6445")
#loc258 = loc("p257.6524")
#loc259 = loc("p258.6574")
#loc260 = loc("p259.6583")
#loc261 = loc("p260.6587")
#loc262 = loc("p261.6611")
#loc263 = loc("p262.6622")
#loc264 = loc("p263.6626")
#loc265 = loc("p264.6637")
#loc266 = loc("p265.6645")
#loc267 = loc("p266.6656")
#loc268 = loc("p267.6660")
#loc269 = loc("p268.6671")
#loc270 = loc("p269.6687")
#loc271 = loc("p270.6691")
#loc272 = loc("p271.6841")
#loc273 = loc("p272.6911")
#loc274 = loc("p273.6916")
#loc275 = loc("p274.6922")
#loc276 = loc("p275.6927")
#loc277 = loc("p276.7006")
#loc278 = loc("p277.7056")
#loc279 = loc("p278.7065")
#loc280 = loc("p279.7069")
#loc281 = loc("p280.7093")
#loc282 = loc("p281.7104")
#loc283 = loc("p282.7108")
#loc284 = loc("p283.7119")
#loc285 = loc("p284.7127")
#loc286 = loc("p285.7138")
#loc287 = loc("p286.7142")
#loc288 = loc("p287.7153")
#loc289 = loc("p288.7169")
#loc290 = loc("p289.7173")
#loc291 = loc("p290.7323")
#loc292 = loc("p291.7393")
#loc293 = loc("p292.7398")
#loc294 = loc("p293.7404")
#loc295 = loc("p294.7409")
#loc296 = loc("p295.7488")
#loc297 = loc("p296.7538")
#loc298 = loc("p297.7547")
#loc299 = loc("p298.7551")
#loc300 = loc("p299.7575")
#loc301 = loc("p300.7586")
#loc302 = loc("p301.7590")
#loc303 = loc("p302.7601")
#loc304 = loc("p303.7609")
#loc305 = loc("p304.7620")
#loc306 = loc("p305.7624")
#loc307 = loc("p306.7635")
#loc308 = loc("p307.7651")
#loc309 = loc("p308.7655")
#loc310 = loc("p309.7805")
#loc311 = loc("p310.7875")
#loc312 = loc("p311.7880")
#loc313 = loc("p312.7886")
#loc314 = loc("p313.7891")
#loc315 = loc("p314.7970")
#loc316 = loc("p315.8020")
#loc317 = loc("p316.8029")
#loc318 = loc("p317.8033")
#loc319 = loc("p318.8057")
#loc320 = loc("p319.8068")
#loc321 = loc("p320.8072")
#loc322 = loc("p321.8083")
#loc323 = loc("p322.8091")
#loc324 = loc("p323.8102")
#loc325 = loc("p324.8106")
#loc326 = loc("p325.8117")
#loc327 = loc("p326.8133")
#loc328 = loc("p327.8137")
#loc329 = loc("p328.8287")
#loc330 = loc("p329.8357")
#loc331 = loc("p330.8362")
#loc332 = loc("p331.8368")
#loc333 = loc("p332.8373")
#loc334 = loc("p333.8452")
#loc335 = loc("p334.8502")
#loc336 = loc("p335.8511")
#loc337 = loc("p336.8515")
#loc338 = loc("p337.8539")
#loc339 = loc("p338.8550")
#loc340 = loc("p339.8554")
#loc341 = loc("p340.8565")
#loc342 = loc("p341.8573")
#loc343 = loc("p342.8584")
#loc344 = loc("p343.8588")
#loc345 = loc("p344.8599")
#loc346 = loc("p345.8615")
#loc347 = loc("p346.8619")
#loc348 = loc("p347.8769")
#loc349 = loc("p348.8839")
#loc350 = loc("p349.8844")
#loc351 = loc("p350.8850")
#loc352 = loc("p351.8855")
#loc353 = loc("p352.8934")
#loc354 = loc("p353.8984")
#loc355 = loc("p354.8993")
#loc356 = loc("p355.8997")
#loc357 = loc("p356.9021")
#loc358 = loc("p357.9032")
#loc359 = loc("p358.9036")
#loc360 = loc("p359.9047")
#loc361 = loc("p360.9055")
#loc362 = loc("p361.9066")
#loc363 = loc("p362.9070")
#loc364 = loc("p363.9081")
#loc365 = loc("p364.9097")
#loc366 = loc("p365.9101")
#loc367 = loc("p366.9251")
#loc368 = loc("p367.9321")
#loc369 = loc("p368.9326")
#loc370 = loc("p369.9332")
#loc371 = loc("p370.9337")
#loc372 = loc("p371.9416")
#loc373 = loc("p372.9466")
#loc374 = loc("p373.9475")
#loc375 = loc("p374.9479")
#loc376 = loc("p375.9503")
#loc377 = loc("p376.9514")
#loc378 = loc("p377.9518")
#loc379 = loc("p378.9529")
#loc380 = loc("p379.9537")
#loc381 = loc("p380.9548")
#loc382 = loc("p381.9552")
#loc383 = loc("p382.9563")
#loc384 = loc("p383.9579")
#loc385 = loc("p384.9583")
#loc386 = loc("p385.9733")
#loc387 = loc("p386.9803")
#loc388 = loc("p387.9808")
#loc389 = loc("p388.9814")
#loc390 = loc("p389.9819")
#loc391 = loc("p390.9898")
#loc392 = loc("p391.9948")
#loc393 = loc("p392.9957")
#loc394 = loc("p393.9961")
#loc395 = loc("p394.9985")
#loc396 = loc("p395.9996")
#loc397 = loc("p396.10000")
#loc398 = loc("p397.10011")
#loc399 = loc("p398.10019")
#loc400 = loc("p399.10030")
#loc401 = loc("p400.10034")
#loc402 = loc("p401.10045")
#loc403 = loc("p402.10061")
#loc404 = loc("p403.10065")
#loc405 = loc("p404.10215")
#loc406 = loc("p405.10285")
#loc407 = loc("p406.10290")
#loc408 = loc("p407.10296")
#loc409 = loc("p408.10301")
#loc410 = loc("p409.10380")
#loc411 = loc("p410.10430")
#loc412 = loc("p411.10439")
#loc413 = loc("p412.10443")
#loc414 = loc("p413.10467")
#loc415 = loc("p414.10478")
#loc416 = loc("p415.10482")
#loc417 = loc("p416.10493")
#loc418 = loc("p417.10501")
#loc419 = loc("p418.10512")
#loc420 = loc("p419.10516")
#loc421 = loc("p420.10527")
#loc422 = loc("p421.10543")
#loc423 = loc("p422.10547")
#loc424 = loc("p423.10697")
#loc425 = loc("p424.10767")
#loc426 = loc("p425.10772")
#loc427 = loc("p426.10778")
#loc428 = loc("p427.10783")
#loc429 = loc("p428.10862")
#loc430 = loc("p429.10912")
#loc431 = loc("p430.10921")
#loc432 = loc("p431.10925")
#loc433 = loc("p432.10949")
#loc434 = loc("p433.10960")
#loc435 = loc("p434.10964")
#loc436 = loc("p435.10975")
#loc437 = loc("p436.10983")
#loc438 = loc("p437.10994")
#loc439 = loc("p438.10998")
#loc440 = loc("p439.11009")
#loc441 = loc("p440.11025")
#loc442 = loc("p441.11029")
#loc443 = loc("p442.11179")
#loc444 = loc("p443.11249")
#loc445 = loc("p444.11254")
#loc446 = loc("p445.11260")
#loc447 = loc("p446.11265")
#loc448 = loc("p447.11344")
#loc449 = loc("p448.11394")
#loc450 = loc("p449.11403")
#loc451 = loc("p450.11407")
#loc452 = loc("p451.11431")
#loc453 = loc("p452.11439")
#loc454 = loc("p453.11450")
#loc455 = loc("p454.11458")
#loc456 = loc("p455.11469")
#loc457 = loc("p456.11473")
#loc458 = loc("p457.11484")
#loc459 = loc("p458.11500")
#loc460 = loc("p459.11504")
#loc461 = loc("p460.11654")
#loc462 = loc("p461.11724")
#loc463 = loc("p462.11729")
#loc464 = loc("p463.11735")
#loc465 = loc("p464.11740")
#loc466 = loc("p465.11819")
#loc480 = loc("reduce.60")
#loc494 = loc("dot.357")
#loc530 = loc("dot.88")
#loc546 = loc("scatter.140")
#loc597 = loc("dot.153")
#loc602 = loc("scatter.177")
#loc610 = loc("dot.464")
#loc621 = loc("reduce.482")
#loc633 = loc("dot.596")
#loc656 = loc("dot.511")
#loc664 = loc("sort.532")
#loc678 = loc("scatter.566")
#loc682 = loc("reduce.634")
#loc686 = loc("reduce.647")
#loc700 = loc("dot.913")
#loc717 = loc("dot.675")
#loc731 = loc("scatter.722")
#loc768 = loc("dot.735")
#loc773 = loc("scatter.759")
#loc781 = loc("dot.1020")
#loc792 = loc("reduce.1038")
#loc804 = loc("dot.1152")
#loc827 = loc("dot.1067")
#loc834 = loc("sort.1088")
#loc848 = loc("scatter.1122")
#loc852 = loc("reduce.1190")
#loc856 = loc("reduce.1203")
#loc870 = loc("dot.1395")
#loc887 = loc("dot.1231")
#loc901 = loc("scatter.1278")
#loc929 = loc("dot.1291")
#loc934 = loc("scatter.1315")
#loc942 = loc("dot.1502")
#loc953 = loc("reduce.1520")
#loc965 = loc("dot.1634")
#loc988 = loc("dot.1549")
#loc995 = loc("sort.1570")
#loc1009 = loc("scatter.1604")
#loc1013 = loc("reduce.1672")
#loc1017 = loc("reduce.1685")
#loc1031 = loc("dot.1877")
#loc1048 = loc("dot.1713")
#loc1062 = loc("scatter.1760")
#loc1090 = loc("dot.1773")
#loc1095 = loc("scatter.1797")
#loc1103 = loc("dot.1984")
#loc1114 = loc("reduce.2002")
#loc1126 = loc("dot.2116")
#loc1149 = loc("dot.2031")
#loc1156 = loc("sort.2052")
#loc1170 = loc("scatter.2086")
#loc1174 = loc("reduce.2154")
#loc1178 = loc("reduce.2167")
#loc1192 = loc("dot.2359")
#loc1209 = loc("dot.2195")
#loc1223 = loc("scatter.2242")
#loc1251 = loc("dot.2255")
#loc1256 = loc("scatter.2279")
#loc1264 = loc("dot.2466")
#loc1275 = loc("reduce.2484")
#loc1287 = loc("dot.2598")
#loc1310 = loc("dot.2513")
#loc1317 = loc("sort.2534")
#loc1331 = loc("scatter.2568")
#loc1335 = loc("reduce.2636")
#loc1339 = loc("reduce.2649")
#loc1353 = loc("dot.2841")
#loc1370 = loc("dot.2677")
#loc1384 = loc("scatter.2724")
#loc1412 = loc("dot.2737")
#loc1417 = loc("scatter.2761")
#loc1425 = loc("dot.2948")
#loc1436 = loc("reduce.2966")
#loc1448 = loc("dot.3080")
#loc1471 = loc("dot.2995")
#loc1478 = loc("sort.3016")
#loc1492 = loc("scatter.3050")
#loc1496 = loc("reduce.3118")
#loc1500 = loc("reduce.3131")
#loc1514 = loc("dot.3323")
#loc1531 = loc("dot.3159")
#loc1545 = loc("scatter.3206")
#loc1573 = loc("dot.3219")
#loc1578 = loc("scatter.3243")
#loc1586 = loc("dot.3430")
#loc1597 = loc("reduce.3448")
#loc1609 = loc("dot.3562")
#loc1632 = loc("dot.3477")
#loc1639 = loc("sort.3498")
#loc1653 = loc("scatter.3532")
#loc1657 = loc("reduce.3600")
#loc1661 = loc("reduce.3613")
#loc1675 = loc("dot.3805")
#loc1692 = loc("dot.3641")
#loc1706 = loc("scatter.3688")
#loc1734 = loc("dot.3701")
#loc1739 = loc("scatter.3725")
#loc1747 = loc("dot.3912")
#loc1758 = loc("reduce.3930")
#loc1770 = loc("dot.4044")
#loc1793 = loc("dot.3959")
#loc1800 = loc("sort.3980")
#loc1814 = loc("scatter.4014")
#loc1818 = loc("reduce.4082")
#loc1822 = loc("reduce.4095")
#loc1836 = loc("dot.4287")
#loc1853 = loc("dot.4123")
#loc1867 = loc("scatter.4170")
#loc1895 = loc("dot.4183")
#loc1900 = loc("scatter.4207")
#loc1908 = loc("dot.4394")
#loc1919 = loc("reduce.4412")
#loc1931 = loc("dot.4526")
#loc1954 = loc("dot.4441")
#loc1961 = loc("sort.4462")
#loc1975 = loc("scatter.4496")
#loc1979 = loc("reduce.4564")
#loc1983 = loc("reduce.4577")
#loc1997 = loc("dot.4769")
#loc2014 = loc("dot.4605")
#loc2028 = loc("scatter.4652")
#loc2056 = loc("dot.4665")
#loc2061 = loc("scatter.4689")
#loc2069 = loc("dot.4876")
#loc2080 = loc("reduce.4894")
#loc2092 = loc("dot.5008")
#loc2115 = loc("dot.4923")
#loc2122 = loc("sort.4944")
#loc2136 = loc("scatter.4978")
#loc2140 = loc("reduce.5046")
#loc2144 = loc("reduce.5059")
#loc2158 = loc("dot.5251")
#loc2175 = loc("dot.5087")
#loc2189 = loc("scatter.5134")
#loc2217 = loc("dot.5147")
#loc2222 = loc("scatter.5171")
#loc2230 = loc("dot.5358")
#loc2241 = loc("reduce.5376")
#loc2253 = loc("dot.5490")
#loc2276 = loc("dot.5405")
#loc2283 = loc("sort.5426")
#loc2297 = loc("scatter.5460")
#loc2301 = loc("reduce.5528")
#loc2305 = loc("reduce.5541")
#loc2319 = loc("dot.5733")
#loc2336 = loc("dot.5569")
#loc2350 = loc("scatter.5616")
#loc2378 = loc("dot.5629")
#loc2383 = loc("scatter.5653")
#loc2391 = loc("dot.5840")
#loc2402 = loc("reduce.5858")
#loc2414 = loc("dot.5972")
#loc2437 = loc("dot.5887")
#loc2444 = loc("sort.5908")
#loc2458 = loc("scatter.5942")
#loc2462 = loc("reduce.6010")
#loc2466 = loc("reduce.6023")
#loc2480 = loc("dot.6215")
#loc2497 = loc("dot.6051")
#loc2511 = loc("scatter.6098")
#loc2539 = loc("dot.6111")
#loc2544 = loc("scatter.6135")
#loc2552 = loc("dot.6322")
#loc2563 = loc("reduce.6340")
#loc2575 = loc("dot.6454")
#loc2598 = loc("dot.6369")
#loc2605 = loc("sort.6390")
#loc2619 = loc("scatter.6424")
#loc2623 = loc("reduce.6492")
#loc2627 = loc("reduce.6505")
#loc2641 = loc("dot.6697")
#loc2658 = loc("dot.6533")
#loc2672 = loc("scatter.6580")
#loc2700 = loc("dot.6593")
#loc2705 = loc("scatter.6617")
#loc2713 = loc("dot.6804")
#loc2724 = loc("reduce.6822")
#loc2736 = loc("dot.6936")
#loc2759 = loc("dot.6851")
#loc2766 = loc("sort.6872")
#loc2780 = loc("scatter.6906")
#loc2784 = loc("reduce.6974")
#loc2788 = loc("reduce.6987")
#loc2802 = loc("dot.7179")
#loc2819 = loc("dot.7015")
#loc2833 = loc("scatter.7062")
#loc2861 = loc("dot.7075")
#loc2866 = loc("scatter.7099")
#loc2874 = loc("dot.7286")
#loc2885 = loc("reduce.7304")
#loc2897 = loc("dot.7418")
#loc2920 = loc("dot.7333")
#loc2927 = loc("sort.7354")
#loc2941 = loc("scatter.7388")
#loc2945 = loc("reduce.7456")
#loc2949 = loc("reduce.7469")
#loc2963 = loc("dot.7661")
#loc2980 = loc("dot.7497")
#loc2994 = loc("scatter.7544")
#loc3022 = loc("dot.7557")
#loc3027 = loc("scatter.7581")
#loc3035 = loc("dot.7768")
#loc3046 = loc("reduce.7786")
#loc3058 = loc("dot.7900")
#loc3081 = loc("dot.7815")
#loc3088 = loc("sort.7836")
#loc3102 = loc("scatter.7870")
#loc3106 = loc("reduce.7938")
#loc3110 = loc("reduce.7951")
#loc3124 = loc("dot.8143")
#loc3141 = loc("dot.7979")
#loc3155 = loc("scatter.8026")
#loc3183 = loc("dot.8039")
#loc3188 = loc("scatter.8063")
#loc3196 = loc("dot.8250")
#loc3207 = loc("reduce.8268")
#loc3219 = loc("dot.8382")
#loc3242 = loc("dot.8297")
#loc3249 = loc("sort.8318")
#loc3263 = loc("scatter.8352")
#loc3267 = loc("reduce.8420")
#loc3271 = loc("reduce.8433")
#loc3285 = loc("dot.8625")
#loc3302 = loc("dot.8461")
#loc3316 = loc("scatter.8508")
#loc3344 = loc("dot.8521")
#loc3349 = loc("scatter.8545")
#loc3357 = loc("dot.8732")
#loc3368 = loc("reduce.8750")
#loc3380 = loc("dot.8864")
#loc3403 = loc("dot.8779")
#loc3410 = loc("sort.8800")
#loc3424 = loc("scatter.8834")
#loc3428 = loc("reduce.8902")
#loc3432 = loc("reduce.8915")
#loc3446 = loc("dot.9107")
#loc3463 = loc("dot.8943")
#loc3477 = loc("scatter.8990")
#loc3505 = loc("dot.9003")
#loc3510 = loc("scatter.9027")
#loc3518 = loc("dot.9214")
#loc3529 = loc("reduce.9232")
#loc3541 = loc("dot.9346")
#loc3564 = loc("dot.9261")
#loc3571 = loc("sort.9282")
#loc3585 = loc("scatter.9316")
#loc3589 = loc("reduce.9384")
#loc3593 = loc("reduce.9397")
#loc3607 = loc("dot.9589")
#loc3624 = loc("dot.9425")
#loc3638 = loc("scatter.9472")
#loc3666 = loc("dot.9485")
#loc3671 = loc("scatter.9509")
#loc3679 = loc("dot.9696")
#loc3690 = loc("reduce.9714")
#loc3702 = loc("dot.9828")
#loc3725 = loc("dot.9743")
#loc3732 = loc("sort.9764")
#loc3746 = loc("scatter.9798")
#loc3750 = loc("reduce.9866")
#loc3754 = loc("reduce.9879")
#loc3768 = loc("dot.10071")
#loc3785 = loc("dot.9907")
#loc3799 = loc("scatter.9954")
#loc3827 = loc("dot.9967")
#loc3832 = loc("scatter.9991")
#loc3840 = loc("dot.10178")
#loc3851 = loc("reduce.10196")
#loc3863 = loc("dot.10310")
#loc3886 = loc("dot.10225")
#loc3893 = loc("sort.10246")
#loc3907 = loc("scatter.10280")
#loc3911 = loc("reduce.10348")
#loc3915 = loc("reduce.10361")
#loc3929 = loc("dot.10553")
#loc3946 = loc("dot.10389")
#loc3960 = loc("scatter.10436")
#loc3988 = loc("dot.10449")
#loc3993 = loc("scatter.10473")
#loc4001 = loc("dot.10660")
#loc4012 = loc("reduce.10678")
#loc4024 = loc("dot.10792")
#loc4047 = loc("dot.10707")
#loc4054 = loc("sort.10728")
#loc4068 = loc("scatter.10762")
#loc4072 = loc("reduce.10830")
#loc4076 = loc("reduce.10843")
#loc4090 = loc("dot.11035")
#loc4107 = loc("dot.10871")
#loc4121 = loc("scatter.10918")
#loc4149 = loc("dot.10931")
#loc4154 = loc("scatter.10955")
#loc4162 = loc("dot.11142")
#loc4173 = loc("reduce.11160")
#loc4185 = loc("dot.11274")
#loc4208 = loc("dot.11189")
#loc4215 = loc("sort.11210")
#loc4229 = loc("scatter.11244")
#loc4233 = loc("reduce.11312")
#loc4237 = loc("reduce.11325")
#loc4251 = loc("dot.11510")
#loc4268 = loc("dot.11353")
#loc4282 = loc("scatter.11400")
#loc4295 = loc("dot.11413")
#loc4300 = loc("scatter.11437")
#loc4323 = loc("dot.11617")
#loc4334 = loc("reduce.11635")
#loc4346 = loc("dot.11749")
#loc4369 = loc("dot.11664")
#loc4376 = loc("sort.11685")
#loc4390 = loc("scatter.11719")
#loc4394 = loc("reduce.11787")
#loc4398 = loc("reduce.11800")
#loc4412 = loc("dot.11828")
module @SyncTensorsGraph.11831 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2, "_axis_1"=4]> loc(#loc)
  func.func @main(%arg0: tensor<1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1xi64>>, ttir.name = "args_1"} loc("p0.3"), %arg1: tensor<32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xf32>>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p1.11"), %arg2: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_k_proj_bias"} loc("p2.27"), %arg3: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_k_proj_weight"} loc("p3.31"), %arg4: tensor<1x1xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x1xi64>>, ttir.name = "args_0"} loc("p4.39"), %arg5: tensor<201088x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<201088x1440xbf16>>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p5.44"), %arg6: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_0_input_layernorm_weight"} loc("p6.79"), %arg7: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_3"} loc("p7.134"), %arg8: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_v_proj_bias"} loc("p8.143"), %arg9: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_v_proj_weight"} loc("p9.147"), %arg10: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_4"} loc("p10.171"), %arg11: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_k_proj_bias"} loc("p11.182"), %arg12: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_k_proj_weight"} loc("p12.186"), %arg13: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_0_mlp_router_bias"} loc("p13.197"), %arg14: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_0_mlp_router_weight"} loc("p14.205"), %arg15: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_o_proj_bias"} loc("p15.216"), %arg16: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_o_proj_weight"} loc("p16.220"), %arg17: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_sinks"} loc("p17.231"), %arg18: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<bf16>>, ttir.name = "l_self_model_lifted_tensor_1"} loc("p18.244"), %arg19: tensor<1x256xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x256xi64>>, ttir.name = "args_2"} loc("p19.288"), %arg20: tensor<i1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<i1>>} loc("p20.310"), %arg21: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_q_proj_bias"} loc("p21.347"), %arg22: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_0_self_attn_q_proj_weight"} loc("p22.351"), %arg23: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_0_post_attention_layernorm_weight"} loc("p23.501"), %arg24: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_0_mlp_experts_down_proj_bias"} loc("p24.571"), %arg25: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_0_mlp_experts_down_proj"} loc("p25.576"), %arg26: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_0_mlp_experts_gate_up_proj_bias"} loc("p26.582"), %arg27: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_0_mlp_experts_gate_up_proj"} loc("p27.587"), %arg28: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_1_input_layernorm_weight"} loc("p28.666"), %arg29: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_5"} loc("p29.716"), %arg30: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_v_proj_bias"} loc("p30.725"), %arg31: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_v_proj_weight"} loc("p31.729"), %arg32: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_6"} loc("p32.753"), %arg33: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_k_proj_bias"} loc("p33.764"), %arg34: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_k_proj_weight"} loc("p34.768"), %arg35: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_1_mlp_router_bias"} loc("p35.779"), %arg36: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_1_mlp_router_weight"} loc("p36.787"), %arg37: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_o_proj_bias"} loc("p37.798"), %arg38: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_o_proj_weight"} loc("p38.802"), %arg39: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_sinks"} loc("p39.813"), %arg40: tensor<bf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<bf16>>, ttir.name = "l_self_model_lifted_tensor_0"} loc("p40.826"), %arg41: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_q_proj_bias"} loc("p41.903"), %arg42: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_1_self_attn_q_proj_weight"} loc("p42.907"), %arg43: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_1_post_attention_layernorm_weight"} loc("p43.1057"), %arg44: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_1_mlp_experts_down_proj_bias"} loc("p44.1127"), %arg45: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_1_mlp_experts_down_proj"} loc("p45.1132"), %arg46: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_1_mlp_experts_gate_up_proj_bias"} loc("p46.1138"), %arg47: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_1_mlp_experts_gate_up_proj"} loc("p47.1143"), %arg48: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_2_input_layernorm_weight"} loc("p48.1222"), %arg49: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_7"} loc("p49.1272"), %arg50: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_v_proj_bias"} loc("p50.1281"), %arg51: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_v_proj_weight"} loc("p51.1285"), %arg52: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_8"} loc("p52.1309"), %arg53: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_k_proj_bias"} loc("p53.1320"), %arg54: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_k_proj_weight"} loc("p54.1324"), %arg55: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_2_mlp_router_bias"} loc("p55.1335"), %arg56: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_2_mlp_router_weight"} loc("p56.1343"), %arg57: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_o_proj_bias"} loc("p57.1354"), %arg58: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_o_proj_weight"} loc("p58.1358"), %arg59: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_sinks"} loc("p59.1369"), %arg60: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_q_proj_bias"} loc("p60.1385"), %arg61: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_2_self_attn_q_proj_weight"} loc("p61.1389"), %arg62: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_2_post_attention_layernorm_weight"} loc("p62.1539"), %arg63: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_2_mlp_experts_down_proj_bias"} loc("p63.1609"), %arg64: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_2_mlp_experts_down_proj"} loc("p64.1614"), %arg65: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_2_mlp_experts_gate_up_proj_bias"} loc("p65.1620"), %arg66: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_2_mlp_experts_gate_up_proj"} loc("p66.1625"), %arg67: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_3_input_layernorm_weight"} loc("p67.1704"), %arg68: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_9"} loc("p68.1754"), %arg69: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_v_proj_bias"} loc("p69.1763"), %arg70: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_v_proj_weight"} loc("p70.1767"), %arg71: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_10"} loc("p71.1791"), %arg72: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_k_proj_bias"} loc("p72.1802"), %arg73: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_k_proj_weight"} loc("p73.1806"), %arg74: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_3_mlp_router_bias"} loc("p74.1817"), %arg75: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_3_mlp_router_weight"} loc("p75.1825"), %arg76: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_o_proj_bias"} loc("p76.1836"), %arg77: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_o_proj_weight"} loc("p77.1840"), %arg78: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_sinks"} loc("p78.1851"), %arg79: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_q_proj_bias"} loc("p79.1867"), %arg80: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_3_self_attn_q_proj_weight"} loc("p80.1871"), %arg81: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_3_post_attention_layernorm_weight"} loc("p81.2021"), %arg82: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_3_mlp_experts_down_proj_bias"} loc("p82.2091"), %arg83: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_3_mlp_experts_down_proj"} loc("p83.2096"), %arg84: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_3_mlp_experts_gate_up_proj_bias"} loc("p84.2102"), %arg85: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_3_mlp_experts_gate_up_proj"} loc("p85.2107"), %arg86: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_4_input_layernorm_weight"} loc("p86.2186"), %arg87: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_11"} loc("p87.2236"), %arg88: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_v_proj_bias"} loc("p88.2245"), %arg89: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_v_proj_weight"} loc("p89.2249"), %arg90: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_12"} loc("p90.2273"), %arg91: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_k_proj_bias"} loc("p91.2284"), %arg92: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_k_proj_weight"} loc("p92.2288"), %arg93: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_4_mlp_router_bias"} loc("p93.2299"), %arg94: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_4_mlp_router_weight"} loc("p94.2307"), %arg95: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_o_proj_bias"} loc("p95.2318"), %arg96: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_o_proj_weight"} loc("p96.2322"), %arg97: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_sinks"} loc("p97.2333"), %arg98: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_q_proj_bias"} loc("p98.2349"), %arg99: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_4_self_attn_q_proj_weight"} loc("p99.2353"), %arg100: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_4_post_attention_layernorm_weight"} loc("p100.2503"), %arg101: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_4_mlp_experts_down_proj_bias"} loc("p101.2573"), %arg102: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_4_mlp_experts_down_proj"} loc("p102.2578"), %arg103: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_4_mlp_experts_gate_up_proj_bias"} loc("p103.2584"), %arg104: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_4_mlp_experts_gate_up_proj"} loc("p104.2589"), %arg105: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_5_input_layernorm_weight"} loc("p105.2668"), %arg106: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_13"} loc("p106.2718"), %arg107: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_v_proj_bias"} loc("p107.2727"), %arg108: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_v_proj_weight"} loc("p108.2731"), %arg109: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_14"} loc("p109.2755"), %arg110: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_k_proj_bias"} loc("p110.2766"), %arg111: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_k_proj_weight"} loc("p111.2770"), %arg112: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_5_mlp_router_bias"} loc("p112.2781"), %arg113: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_5_mlp_router_weight"} loc("p113.2789"), %arg114: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_o_proj_bias"} loc("p114.2800"), %arg115: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_o_proj_weight"} loc("p115.2804"), %arg116: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_sinks"} loc("p116.2815"), %arg117: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_q_proj_bias"} loc("p117.2831"), %arg118: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_5_self_attn_q_proj_weight"} loc("p118.2835"), %arg119: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_5_post_attention_layernorm_weight"} loc("p119.2985"), %arg120: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_5_mlp_experts_down_proj_bias"} loc("p120.3055"), %arg121: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_5_mlp_experts_down_proj"} loc("p121.3060"), %arg122: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_5_mlp_experts_gate_up_proj_bias"} loc("p122.3066"), %arg123: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_5_mlp_experts_gate_up_proj"} loc("p123.3071"), %arg124: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_6_input_layernorm_weight"} loc("p124.3150"), %arg125: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_15"} loc("p125.3200"), %arg126: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_v_proj_bias"} loc("p126.3209"), %arg127: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_v_proj_weight"} loc("p127.3213"), %arg128: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_16"} loc("p128.3237"), %arg129: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_k_proj_bias"} loc("p129.3248"), %arg130: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_k_proj_weight"} loc("p130.3252"), %arg131: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_6_mlp_router_bias"} loc("p131.3263"), %arg132: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_6_mlp_router_weight"} loc("p132.3271"), %arg133: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_o_proj_bias"} loc("p133.3282"), %arg134: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_o_proj_weight"} loc("p134.3286"), %arg135: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_sinks"} loc("p135.3297"), %arg136: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_q_proj_bias"} loc("p136.3313"), %arg137: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_6_self_attn_q_proj_weight"} loc("p137.3317"), %arg138: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_6_post_attention_layernorm_weight"} loc("p138.3467"), %arg139: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_6_mlp_experts_down_proj_bias"} loc("p139.3537"), %arg140: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_6_mlp_experts_down_proj"} loc("p140.3542"), %arg141: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_6_mlp_experts_gate_up_proj_bias"} loc("p141.3548"), %arg142: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_6_mlp_experts_gate_up_proj"} loc("p142.3553"), %arg143: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_7_input_layernorm_weight"} loc("p143.3632"), %arg144: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_17"} loc("p144.3682"), %arg145: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_v_proj_bias"} loc("p145.3691"), %arg146: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_v_proj_weight"} loc("p146.3695"), %arg147: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_18"} loc("p147.3719"), %arg148: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_k_proj_bias"} loc("p148.3730"), %arg149: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_k_proj_weight"} loc("p149.3734"), %arg150: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_7_mlp_router_bias"} loc("p150.3745"), %arg151: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_7_mlp_router_weight"} loc("p151.3753"), %arg152: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_o_proj_bias"} loc("p152.3764"), %arg153: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_o_proj_weight"} loc("p153.3768"), %arg154: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_sinks"} loc("p154.3779"), %arg155: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_q_proj_bias"} loc("p155.3795"), %arg156: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_7_self_attn_q_proj_weight"} loc("p156.3799"), %arg157: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_7_post_attention_layernorm_weight"} loc("p157.3949"), %arg158: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_7_mlp_experts_down_proj_bias"} loc("p158.4019"), %arg159: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_7_mlp_experts_down_proj"} loc("p159.4024"), %arg160: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_7_mlp_experts_gate_up_proj_bias"} loc("p160.4030"), %arg161: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_7_mlp_experts_gate_up_proj"} loc("p161.4035"), %arg162: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_8_input_layernorm_weight"} loc("p162.4114"), %arg163: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_19"} loc("p163.4164"), %arg164: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_v_proj_bias"} loc("p164.4173"), %arg165: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_v_proj_weight"} loc("p165.4177"), %arg166: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_20"} loc("p166.4201"), %arg167: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_k_proj_bias"} loc("p167.4212"), %arg168: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_k_proj_weight"} loc("p168.4216"), %arg169: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_8_mlp_router_bias"} loc("p169.4227"), %arg170: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_8_mlp_router_weight"} loc("p170.4235"), %arg171: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_o_proj_bias"} loc("p171.4246"), %arg172: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_o_proj_weight"} loc("p172.4250"), %arg173: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_sinks"} loc("p173.4261"), %arg174: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_q_proj_bias"} loc("p174.4277"), %arg175: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_8_self_attn_q_proj_weight"} loc("p175.4281"), %arg176: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_8_post_attention_layernorm_weight"} loc("p176.4431"), %arg177: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_8_mlp_experts_down_proj_bias"} loc("p177.4501"), %arg178: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_8_mlp_experts_down_proj"} loc("p178.4506"), %arg179: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_8_mlp_experts_gate_up_proj_bias"} loc("p179.4512"), %arg180: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_8_mlp_experts_gate_up_proj"} loc("p180.4517"), %arg181: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_9_input_layernorm_weight"} loc("p181.4596"), %arg182: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_21"} loc("p182.4646"), %arg183: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_v_proj_bias"} loc("p183.4655"), %arg184: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_v_proj_weight"} loc("p184.4659"), %arg185: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_22"} loc("p185.4683"), %arg186: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_k_proj_bias"} loc("p186.4694"), %arg187: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_k_proj_weight"} loc("p187.4698"), %arg188: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_9_mlp_router_bias"} loc("p188.4709"), %arg189: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_9_mlp_router_weight"} loc("p189.4717"), %arg190: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_o_proj_bias"} loc("p190.4728"), %arg191: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_o_proj_weight"} loc("p191.4732"), %arg192: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_sinks"} loc("p192.4743"), %arg193: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_q_proj_bias"} loc("p193.4759"), %arg194: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_9_self_attn_q_proj_weight"} loc("p194.4763"), %arg195: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_9_post_attention_layernorm_weight"} loc("p195.4913"), %arg196: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_9_mlp_experts_down_proj_bias"} loc("p196.4983"), %arg197: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_9_mlp_experts_down_proj"} loc("p197.4988"), %arg198: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_9_mlp_experts_gate_up_proj_bias"} loc("p198.4994"), %arg199: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_9_mlp_experts_gate_up_proj"} loc("p199.4999"), %arg200: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_10_input_layernorm_weight"} loc("p200.5078"), %arg201: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_23"} loc("p201.5128"), %arg202: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_v_proj_bias"} loc("p202.5137"), %arg203: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_v_proj_weight"} loc("p203.5141"), %arg204: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_24"} loc("p204.5165"), %arg205: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_k_proj_bias"} loc("p205.5176"), %arg206: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_k_proj_weight"} loc("p206.5180"), %arg207: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_10_mlp_router_bias"} loc("p207.5191"), %arg208: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_10_mlp_router_weight"} loc("p208.5199"), %arg209: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_o_proj_bias"} loc("p209.5210"), %arg210: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_o_proj_weight"} loc("p210.5214"), %arg211: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_sinks"} loc("p211.5225"), %arg212: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_q_proj_bias"} loc("p212.5241"), %arg213: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_10_self_attn_q_proj_weight"} loc("p213.5245"), %arg214: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_10_post_attention_layernorm_weight"} loc("p214.5395"), %arg215: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_10_mlp_experts_down_proj_bias"} loc("p215.5465"), %arg216: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_10_mlp_experts_down_proj"} loc("p216.5470"), %arg217: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_10_mlp_experts_gate_up_proj_bias"} loc("p217.5476"), %arg218: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_10_mlp_experts_gate_up_proj"} loc("p218.5481"), %arg219: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_11_input_layernorm_weight"} loc("p219.5560"), %arg220: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_25"} loc("p220.5610"), %arg221: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_v_proj_bias"} loc("p221.5619"), %arg222: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_v_proj_weight"} loc("p222.5623"), %arg223: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_26"} loc("p223.5647"), %arg224: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_k_proj_bias"} loc("p224.5658"), %arg225: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_k_proj_weight"} loc("p225.5662"), %arg226: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_11_mlp_router_bias"} loc("p226.5673"), %arg227: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_11_mlp_router_weight"} loc("p227.5681"), %arg228: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_o_proj_bias"} loc("p228.5692"), %arg229: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_o_proj_weight"} loc("p229.5696"), %arg230: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_sinks"} loc("p230.5707"), %arg231: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_q_proj_bias"} loc("p231.5723"), %arg232: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_11_self_attn_q_proj_weight"} loc("p232.5727"), %arg233: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_11_post_attention_layernorm_weight"} loc("p233.5877"), %arg234: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_11_mlp_experts_down_proj_bias"} loc("p234.5947"), %arg235: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_11_mlp_experts_down_proj"} loc("p235.5952"), %arg236: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_11_mlp_experts_gate_up_proj_bias"} loc("p236.5958"), %arg237: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_11_mlp_experts_gate_up_proj"} loc("p237.5963"), %arg238: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_12_input_layernorm_weight"} loc("p238.6042"), %arg239: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_27"} loc("p239.6092"), %arg240: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_v_proj_bias"} loc("p240.6101"), %arg241: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_v_proj_weight"} loc("p241.6105"), %arg242: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_28"} loc("p242.6129"), %arg243: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_k_proj_bias"} loc("p243.6140"), %arg244: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_k_proj_weight"} loc("p244.6144"), %arg245: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_12_mlp_router_bias"} loc("p245.6155"), %arg246: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_12_mlp_router_weight"} loc("p246.6163"), %arg247: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_o_proj_bias"} loc("p247.6174"), %arg248: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_o_proj_weight"} loc("p248.6178"), %arg249: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_sinks"} loc("p249.6189"), %arg250: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_q_proj_bias"} loc("p250.6205"), %arg251: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_12_self_attn_q_proj_weight"} loc("p251.6209"), %arg252: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_12_post_attention_layernorm_weight"} loc("p252.6359"), %arg253: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_12_mlp_experts_down_proj_bias"} loc("p253.6429"), %arg254: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_12_mlp_experts_down_proj"} loc("p254.6434"), %arg255: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_12_mlp_experts_gate_up_proj_bias"} loc("p255.6440"), %arg256: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_12_mlp_experts_gate_up_proj"} loc("p256.6445"), %arg257: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_13_input_layernorm_weight"} loc("p257.6524"), %arg258: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_29"} loc("p258.6574"), %arg259: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_v_proj_bias"} loc("p259.6583"), %arg260: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_v_proj_weight"} loc("p260.6587"), %arg261: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_30"} loc("p261.6611"), %arg262: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_k_proj_bias"} loc("p262.6622"), %arg263: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_k_proj_weight"} loc("p263.6626"), %arg264: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_13_mlp_router_bias"} loc("p264.6637"), %arg265: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_13_mlp_router_weight"} loc("p265.6645"), %arg266: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_o_proj_bias"} loc("p266.6656"), %arg267: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_o_proj_weight"} loc("p267.6660"), %arg268: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_sinks"} loc("p268.6671"), %arg269: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_q_proj_bias"} loc("p269.6687"), %arg270: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_13_self_attn_q_proj_weight"} loc("p270.6691"), %arg271: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_13_post_attention_layernorm_weight"} loc("p271.6841"), %arg272: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_13_mlp_experts_down_proj_bias"} loc("p272.6911"), %arg273: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_13_mlp_experts_down_proj"} loc("p273.6916"), %arg274: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_13_mlp_experts_gate_up_proj_bias"} loc("p274.6922"), %arg275: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_13_mlp_experts_gate_up_proj"} loc("p275.6927"), %arg276: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_14_input_layernorm_weight"} loc("p276.7006"), %arg277: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_31"} loc("p277.7056"), %arg278: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_v_proj_bias"} loc("p278.7065"), %arg279: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_v_proj_weight"} loc("p279.7069"), %arg280: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_32"} loc("p280.7093"), %arg281: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_k_proj_bias"} loc("p281.7104"), %arg282: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_k_proj_weight"} loc("p282.7108"), %arg283: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_14_mlp_router_bias"} loc("p283.7119"), %arg284: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_14_mlp_router_weight"} loc("p284.7127"), %arg285: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_o_proj_bias"} loc("p285.7138"), %arg286: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_o_proj_weight"} loc("p286.7142"), %arg287: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_sinks"} loc("p287.7153"), %arg288: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_q_proj_bias"} loc("p288.7169"), %arg289: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_14_self_attn_q_proj_weight"} loc("p289.7173"), %arg290: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_14_post_attention_layernorm_weight"} loc("p290.7323"), %arg291: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_14_mlp_experts_down_proj_bias"} loc("p291.7393"), %arg292: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_14_mlp_experts_down_proj"} loc("p292.7398"), %arg293: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_14_mlp_experts_gate_up_proj_bias"} loc("p293.7404"), %arg294: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_14_mlp_experts_gate_up_proj"} loc("p294.7409"), %arg295: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_15_input_layernorm_weight"} loc("p295.7488"), %arg296: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_33"} loc("p296.7538"), %arg297: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_v_proj_bias"} loc("p297.7547"), %arg298: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_v_proj_weight"} loc("p298.7551"), %arg299: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_34"} loc("p299.7575"), %arg300: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_k_proj_bias"} loc("p300.7586"), %arg301: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_k_proj_weight"} loc("p301.7590"), %arg302: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_15_mlp_router_bias"} loc("p302.7601"), %arg303: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_15_mlp_router_weight"} loc("p303.7609"), %arg304: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_o_proj_bias"} loc("p304.7620"), %arg305: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_o_proj_weight"} loc("p305.7624"), %arg306: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_sinks"} loc("p306.7635"), %arg307: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_q_proj_bias"} loc("p307.7651"), %arg308: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_15_self_attn_q_proj_weight"} loc("p308.7655"), %arg309: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_15_post_attention_layernorm_weight"} loc("p309.7805"), %arg310: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_15_mlp_experts_down_proj_bias"} loc("p310.7875"), %arg311: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_15_mlp_experts_down_proj"} loc("p311.7880"), %arg312: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_15_mlp_experts_gate_up_proj_bias"} loc("p312.7886"), %arg313: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_15_mlp_experts_gate_up_proj"} loc("p313.7891"), %arg314: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_16_input_layernorm_weight"} loc("p314.7970"), %arg315: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_35"} loc("p315.8020"), %arg316: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_v_proj_bias"} loc("p316.8029"), %arg317: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_v_proj_weight"} loc("p317.8033"), %arg318: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_36"} loc("p318.8057"), %arg319: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_k_proj_bias"} loc("p319.8068"), %arg320: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_k_proj_weight"} loc("p320.8072"), %arg321: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_16_mlp_router_bias"} loc("p321.8083"), %arg322: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_16_mlp_router_weight"} loc("p322.8091"), %arg323: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_o_proj_bias"} loc("p323.8102"), %arg324: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_o_proj_weight"} loc("p324.8106"), %arg325: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_sinks"} loc("p325.8117"), %arg326: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_q_proj_bias"} loc("p326.8133"), %arg327: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_16_self_attn_q_proj_weight"} loc("p327.8137"), %arg328: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_16_post_attention_layernorm_weight"} loc("p328.8287"), %arg329: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_16_mlp_experts_down_proj_bias"} loc("p329.8357"), %arg330: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_16_mlp_experts_down_proj"} loc("p330.8362"), %arg331: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_16_mlp_experts_gate_up_proj_bias"} loc("p331.8368"), %arg332: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_16_mlp_experts_gate_up_proj"} loc("p332.8373"), %arg333: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_17_input_layernorm_weight"} loc("p333.8452"), %arg334: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_37"} loc("p334.8502"), %arg335: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_v_proj_bias"} loc("p335.8511"), %arg336: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_v_proj_weight"} loc("p336.8515"), %arg337: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_38"} loc("p337.8539"), %arg338: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_k_proj_bias"} loc("p338.8550"), %arg339: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_k_proj_weight"} loc("p339.8554"), %arg340: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_17_mlp_router_bias"} loc("p340.8565"), %arg341: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_17_mlp_router_weight"} loc("p341.8573"), %arg342: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_o_proj_bias"} loc("p342.8584"), %arg343: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_o_proj_weight"} loc("p343.8588"), %arg344: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_sinks"} loc("p344.8599"), %arg345: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_q_proj_bias"} loc("p345.8615"), %arg346: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_17_self_attn_q_proj_weight"} loc("p346.8619"), %arg347: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_17_post_attention_layernorm_weight"} loc("p347.8769"), %arg348: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_17_mlp_experts_down_proj_bias"} loc("p348.8839"), %arg349: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_17_mlp_experts_down_proj"} loc("p349.8844"), %arg350: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_17_mlp_experts_gate_up_proj_bias"} loc("p350.8850"), %arg351: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_17_mlp_experts_gate_up_proj"} loc("p351.8855"), %arg352: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_18_input_layernorm_weight"} loc("p352.8934"), %arg353: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_39"} loc("p353.8984"), %arg354: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_v_proj_bias"} loc("p354.8993"), %arg355: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_v_proj_weight"} loc("p355.8997"), %arg356: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_40"} loc("p356.9021"), %arg357: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_k_proj_bias"} loc("p357.9032"), %arg358: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_k_proj_weight"} loc("p358.9036"), %arg359: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_18_mlp_router_bias"} loc("p359.9047"), %arg360: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_18_mlp_router_weight"} loc("p360.9055"), %arg361: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_o_proj_bias"} loc("p361.9066"), %arg362: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_o_proj_weight"} loc("p362.9070"), %arg363: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_sinks"} loc("p363.9081"), %arg364: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_q_proj_bias"} loc("p364.9097"), %arg365: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_18_self_attn_q_proj_weight"} loc("p365.9101"), %arg366: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_18_post_attention_layernorm_weight"} loc("p366.9251"), %arg367: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_18_mlp_experts_down_proj_bias"} loc("p367.9321"), %arg368: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_18_mlp_experts_down_proj"} loc("p368.9326"), %arg369: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_18_mlp_experts_gate_up_proj_bias"} loc("p369.9332"), %arg370: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_18_mlp_experts_gate_up_proj"} loc("p370.9337"), %arg371: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_19_input_layernorm_weight"} loc("p371.9416"), %arg372: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_41"} loc("p372.9466"), %arg373: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_v_proj_bias"} loc("p373.9475"), %arg374: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_v_proj_weight"} loc("p374.9479"), %arg375: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_42"} loc("p375.9503"), %arg376: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_k_proj_bias"} loc("p376.9514"), %arg377: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_k_proj_weight"} loc("p377.9518"), %arg378: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_19_mlp_router_bias"} loc("p378.9529"), %arg379: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_19_mlp_router_weight"} loc("p379.9537"), %arg380: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_o_proj_bias"} loc("p380.9548"), %arg381: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_o_proj_weight"} loc("p381.9552"), %arg382: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_sinks"} loc("p382.9563"), %arg383: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_q_proj_bias"} loc("p383.9579"), %arg384: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_19_self_attn_q_proj_weight"} loc("p384.9583"), %arg385: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_19_post_attention_layernorm_weight"} loc("p385.9733"), %arg386: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_19_mlp_experts_down_proj_bias"} loc("p386.9803"), %arg387: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_19_mlp_experts_down_proj"} loc("p387.9808"), %arg388: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_19_mlp_experts_gate_up_proj_bias"} loc("p388.9814"), %arg389: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_19_mlp_experts_gate_up_proj"} loc("p389.9819"), %arg390: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_20_input_layernorm_weight"} loc("p390.9898"), %arg391: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_43"} loc("p391.9948"), %arg392: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_v_proj_bias"} loc("p392.9957"), %arg393: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_v_proj_weight"} loc("p393.9961"), %arg394: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_44"} loc("p394.9985"), %arg395: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_k_proj_bias"} loc("p395.9996"), %arg396: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_k_proj_weight"} loc("p396.10000"), %arg397: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_20_mlp_router_bias"} loc("p397.10011"), %arg398: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_20_mlp_router_weight"} loc("p398.10019"), %arg399: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_o_proj_bias"} loc("p399.10030"), %arg400: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_o_proj_weight"} loc("p400.10034"), %arg401: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_sinks"} loc("p401.10045"), %arg402: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_q_proj_bias"} loc("p402.10061"), %arg403: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_20_self_attn_q_proj_weight"} loc("p403.10065"), %arg404: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_20_post_attention_layernorm_weight"} loc("p404.10215"), %arg405: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_20_mlp_experts_down_proj_bias"} loc("p405.10285"), %arg406: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_20_mlp_experts_down_proj"} loc("p406.10290"), %arg407: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_20_mlp_experts_gate_up_proj_bias"} loc("p407.10296"), %arg408: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_20_mlp_experts_gate_up_proj"} loc("p408.10301"), %arg409: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_21_input_layernorm_weight"} loc("p409.10380"), %arg410: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_45"} loc("p410.10430"), %arg411: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_v_proj_bias"} loc("p411.10439"), %arg412: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_v_proj_weight"} loc("p412.10443"), %arg413: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_46"} loc("p413.10467"), %arg414: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_k_proj_bias"} loc("p414.10478"), %arg415: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_k_proj_weight"} loc("p415.10482"), %arg416: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_21_mlp_router_bias"} loc("p416.10493"), %arg417: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_21_mlp_router_weight"} loc("p417.10501"), %arg418: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_o_proj_bias"} loc("p418.10512"), %arg419: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_o_proj_weight"} loc("p419.10516"), %arg420: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_sinks"} loc("p420.10527"), %arg421: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_q_proj_bias"} loc("p421.10543"), %arg422: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_21_self_attn_q_proj_weight"} loc("p422.10547"), %arg423: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_21_post_attention_layernorm_weight"} loc("p423.10697"), %arg424: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_21_mlp_experts_down_proj_bias"} loc("p424.10767"), %arg425: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_21_mlp_experts_down_proj"} loc("p425.10772"), %arg426: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_21_mlp_experts_gate_up_proj_bias"} loc("p426.10778"), %arg427: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_21_mlp_experts_gate_up_proj"} loc("p427.10783"), %arg428: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_22_input_layernorm_weight"} loc("p428.10862"), %arg429: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_47"} loc("p429.10912"), %arg430: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_v_proj_bias"} loc("p430.10921"), %arg431: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_v_proj_weight"} loc("p431.10925"), %arg432: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_48"} loc("p432.10949"), %arg433: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_k_proj_bias"} loc("p433.10960"), %arg434: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_k_proj_weight"} loc("p434.10964"), %arg435: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_22_mlp_router_bias"} loc("p435.10975"), %arg436: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_22_mlp_router_weight"} loc("p436.10983"), %arg437: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_o_proj_bias"} loc("p437.10994"), %arg438: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_o_proj_weight"} loc("p438.10998"), %arg439: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_sinks"} loc("p439.11009"), %arg440: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_q_proj_bias"} loc("p440.11025"), %arg441: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_22_self_attn_q_proj_weight"} loc("p441.11029"), %arg442: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_22_post_attention_layernorm_weight"} loc("p442.11179"), %arg443: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_22_mlp_experts_down_proj_bias"} loc("p443.11249"), %arg444: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_22_mlp_experts_down_proj"} loc("p444.11254"), %arg445: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_22_mlp_experts_gate_up_proj_bias"} loc("p445.11260"), %arg446: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_22_mlp_experts_gate_up_proj"} loc("p446.11265"), %arg447: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_23_input_layernorm_weight"} loc("p447.11344"), %arg448: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_49"} loc("p448.11394"), %arg449: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_v_proj_bias"} loc("p449.11403"), %arg450: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<128x1440xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_v_proj_weight"} loc("p450.11407"), %arg451: tensor<1x8x256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1x2x256x64xbf16>>, ttir.name = "args_50"} loc("p451.11431"), %arg452: tensor<201088x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<50272x1440xbf16>>, ttir.name = "l__self___lm_head_weight"} loc("p452.11439"), %arg453: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32xbf16>>, ttir.name = "l__self___model_layers_23_mlp_router_bias"} loc("p453.11450"), %arg454: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<32x1440xbf16>>, ttir.name = "l__self___model_layers_23_mlp_router_weight"} loc("p454.11458"), %arg455: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_o_proj_bias"} loc("p455.11469"), %arg456: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440x1024xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_o_proj_weight"} loc("p456.11473"), %arg457: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<64xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_sinks"} loc("p457.11484"), %arg458: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_q_proj_bias"} loc("p458.11500"), %arg459: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1024x1440xbf16>>, ttir.name = "l__self___model_layers_23_self_attn_q_proj_weight"} loc("p459.11504"), %arg460: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_layers_23_post_attention_layernorm_weight"} loc("p460.11654"), %arg461: tensor<32x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440xbf16>>, ttir.name = "l__self___model_layers_23_mlp_experts_down_proj_bias"} loc("p461.11724"), %arg462: tensor<32x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x2880x1440xbf16>>, ttir.name = "l__self___model_layers_23_mlp_experts_down_proj"} loc("p462.11729"), %arg463: tensor<32x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x5760xbf16>>, ttir.name = "l__self___model_layers_23_mlp_experts_gate_up_proj_bias"} loc("p463.11735"), %arg464: tensor<32x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<8x1440x5760xbf16>>, ttir.name = "l__self___model_layers_23_mlp_experts_gate_up_proj"} loc("p464.11740"), %arg465: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <presharded>, local_shape = tensor<1440xbf16>>, ttir.name = "l__self___model_norm_weight"} loc("p465.11819")) -> (tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x8x256x64xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x8x256x64xbf16>>}, tensor<1x1x201088xbf16> {ttcore.runtime_tensor_sharding = #ttcore<runtime_tensor_sharding shard_status = <unsharded>, local_shape = tensor<1x1x201088xbf16>>}) {
    %0:49 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73, %arg74, %arg75, %arg76, %arg77, %arg78, %arg79, %arg80, %arg81, %arg82, %arg83, %arg84, %arg85, %arg86, %arg87, %arg88, %arg89, %arg90, %arg91, %arg92, %arg93, %arg94, %arg95, %arg96, %arg97, %arg98, %arg99, %arg100, %arg101, %arg102, %arg103, %arg104, %arg105, %arg106, %arg107, %arg108, %arg109, %arg110, %arg111, %arg112, %arg113, %arg114, %arg115, %arg116, %arg117, %arg118, %arg119, %arg120, %arg121, %arg122, %arg123, %arg124, %arg125, %arg126, %arg127, %arg128, %arg129, %arg130, %arg131, %arg132, %arg133, %arg134, %arg135, %arg136, %arg137, %arg138, %arg139, %arg140, %arg141, %arg142, %arg143, %arg144, %arg145, %arg146, %arg147, %arg148, %arg149, %arg150, %arg151, %arg152, %arg153, %arg154, %arg155, %arg156, %arg157, %arg158, %arg159, %arg160, %arg161, %arg162, %arg163, %arg164, %arg165, %arg166, %arg167, %arg168, %arg169, %arg170, %arg171, %arg172, %arg173, %arg174, %arg175, %arg176, %arg177, %arg178, %arg179, %arg180, %arg181, %arg182, %arg183, %arg184, %arg185, %arg186, %arg187, %arg188, %arg189, %arg190, %arg191, %arg192, %arg193, %arg194, %arg195, %arg196, %arg197, %arg198, %arg199, %arg200, %arg201, %arg202, %arg203, %arg204, %arg205, %arg206, %arg207, %arg208, %arg209, %arg210, %arg211, %arg212, %arg213, %arg214, %arg215, %arg216, %arg217, %arg218, %arg219, %arg220, %arg221, %arg222, %arg223, %arg224, %arg225, %arg226, %arg227, %arg228, %arg229, %arg230, %arg231, %arg232, %arg233, %arg234, %arg235, %arg236, %arg237, %arg238, %arg239, %arg240, %arg241, %arg242, %arg243, %arg244, %arg245, %arg246, %arg247, %arg248, %arg249, %arg250, %arg251, %arg252, %arg253, %arg254, %arg255, %arg256, %arg257, %arg258, %arg259, %arg260, %arg261, %arg262, %arg263, %arg264, %arg265, %arg266, %arg267, %arg268, %arg269, %arg270, %arg271, %arg272, %arg273, %arg274, %arg275, %arg276, %arg277, %arg278, %arg279, %arg280, %arg281, %arg282, %arg283, %arg284, %arg285, %arg286, %arg287, %arg288, %arg289, %arg290, %arg291, %arg292, %arg293, %arg294, %arg295, %arg296, %arg297, %arg298, %arg299, %arg300, %arg301, %arg302, %arg303, %arg304, %arg305, %arg306, %arg307, %arg308, %arg309, %arg310, %arg311, %arg312, %arg313, %arg314, %arg315, %arg316, %arg317, %arg318, %arg319, %arg320, %arg321, %arg322, %arg323, %arg324, %arg325, %arg326, %arg327, %arg328, %arg329, %arg330, %arg331, %arg332, %arg333, %arg334, %arg335, %arg336, %arg337, %arg338, %arg339, %arg340, %arg341, %arg342, %arg343, %arg344, %arg345, %arg346, %arg347, %arg348, %arg349, %arg350, %arg351, %arg352, %arg353, %arg354, %arg355, %arg356, %arg357, %arg358, %arg359, %arg360, %arg361, %arg362, %arg363, %arg364, %arg365, %arg366, %arg367, %arg368, %arg369, %arg370, %arg371, %arg372, %arg373, %arg374, %arg375, %arg376, %arg377, %arg378, %arg379, %arg380, %arg381, %arg382, %arg383, %arg384, %arg385, %arg386, %arg387, %arg388, %arg389, %arg390, %arg391, %arg392, %arg393, %arg394, %arg395, %arg396, %arg397, %arg398, %arg399, %arg400, %arg401, %arg402, %arg403, %arg404, %arg405, %arg406, %arg407, %arg408, %arg409, %arg410, %arg411, %arg412, %arg413, %arg414, %arg415, %arg416, %arg417, %arg418, %arg419, %arg420, %arg421, %arg422, %arg423, %arg424, %arg425, %arg426, %arg427, %arg428, %arg429, %arg430, %arg431, %arg432, %arg433, %arg434, %arg435, %arg436, %arg437, %arg438, %arg439, %arg440, %arg441, %arg442, %arg443, %arg444, %arg445, %arg446, %arg447, %arg448, %arg449, %arg450, %arg451, %arg452, %arg453, %arg454, %arg455, %arg456, %arg457, %arg458, %arg459, %arg460, %arg461, %arg462, %arg463, %arg464, %arg465) in_shardings=[<@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>] out_shardings=[<@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {"_axis_1"}, {}, {}]>, <@mesh, [{}, {}, {"_axis_1"}]>] manual_axes={"_axis_0", "_axis_1"} (%arg466: tensor<1xi64> loc("p0.3"), %arg467: tensor<32xf32> loc("p1.11"), %arg468: tensor<128xbf16> loc("p2.27"), %arg469: tensor<128x1440xbf16> loc("p3.31"), %arg470: tensor<1x1xi64> loc("p4.39"), %arg471: tensor<201088x1440xbf16> loc("p5.44"), %arg472: tensor<1440xbf16> loc("p6.79"), %arg473: tensor<1x2x256x64xbf16> loc("p7.134"), %arg474: tensor<128xbf16> loc("p8.143"), %arg475: tensor<128x1440xbf16> loc("p9.147"), %arg476: tensor<1x2x256x64xbf16> loc("p10.171"), %arg477: tensor<128xbf16> loc("p11.182"), %arg478: tensor<128x1440xbf16> loc("p12.186"), %arg479: tensor<32xbf16> loc("p13.197"), %arg480: tensor<32x1440xbf16> loc("p14.205"), %arg481: tensor<1440xbf16> loc("p15.216"), %arg482: tensor<1440x1024xbf16> loc("p16.220"), %arg483: tensor<64xbf16> loc("p17.231"), %arg484: tensor<bf16> loc("p18.244"), %arg485: tensor<1x256xi64> loc("p19.288"), %arg486: tensor<i1> loc("p20.310"), %arg487: tensor<1024xbf16> loc("p21.347"), %arg488: tensor<1024x1440xbf16> loc("p22.351"), %arg489: tensor<1440xbf16> loc("p23.501"), %arg490: tensor<8x1440xbf16> loc("p24.571"), %arg491: tensor<8x2880x1440xbf16> loc("p25.576"), %arg492: tensor<8x5760xbf16> loc("p26.582"), %arg493: tensor<8x1440x5760xbf16> loc("p27.587"), %arg494: tensor<1440xbf16> loc("p28.666"), %arg495: tensor<1x2x256x64xbf16> loc("p29.716"), %arg496: tensor<128xbf16> loc("p30.725"), %arg497: tensor<128x1440xbf16> loc("p31.729"), %arg498: tensor<1x2x256x64xbf16> loc("p32.753"), %arg499: tensor<128xbf16> loc("p33.764"), %arg500: tensor<128x1440xbf16> loc("p34.768"), %arg501: tensor<32xbf16> loc("p35.779"), %arg502: tensor<32x1440xbf16> loc("p36.787"), %arg503: tensor<1440xbf16> loc("p37.798"), %arg504: tensor<1440x1024xbf16> loc("p38.802"), %arg505: tensor<64xbf16> loc("p39.813"), %arg506: tensor<bf16> loc("p40.826"), %arg507: tensor<1024xbf16> loc("p41.903"), %arg508: tensor<1024x1440xbf16> loc("p42.907"), %arg509: tensor<1440xbf16> loc("p43.1057"), %arg510: tensor<8x1440xbf16> loc("p44.1127"), %arg511: tensor<8x2880x1440xbf16> loc("p45.1132"), %arg512: tensor<8x5760xbf16> loc("p46.1138"), %arg513: tensor<8x1440x5760xbf16> loc("p47.1143"), %arg514: tensor<1440xbf16> loc("p48.1222"), %arg515: tensor<1x2x256x64xbf16> loc("p49.1272"), %arg516: tensor<128xbf16> loc("p50.1281"), %arg517: tensor<128x1440xbf16> loc("p51.1285"), %arg518: tensor<1x2x256x64xbf16> loc("p52.1309"), %arg519: tensor<128xbf16> loc("p53.1320"), %arg520: tensor<128x1440xbf16> loc("p54.1324"), %arg521: tensor<32xbf16> loc("p55.1335"), %arg522: tensor<32x1440xbf16> loc("p56.1343"), %arg523: tensor<1440xbf16> loc("p57.1354"), %arg524: tensor<1440x1024xbf16> loc("p58.1358"), %arg525: tensor<64xbf16> loc("p59.1369"), %arg526: tensor<1024xbf16> loc("p60.1385"), %arg527: tensor<1024x1440xbf16> loc("p61.1389"), %arg528: tensor<1440xbf16> loc("p62.1539"), %arg529: tensor<8x1440xbf16> loc("p63.1609"), %arg530: tensor<8x2880x1440xbf16> loc("p64.1614"), %arg531: tensor<8x5760xbf16> loc("p65.1620"), %arg532: tensor<8x1440x5760xbf16> loc("p66.1625"), %arg533: tensor<1440xbf16> loc("p67.1704"), %arg534: tensor<1x2x256x64xbf16> loc("p68.1754"), %arg535: tensor<128xbf16> loc("p69.1763"), %arg536: tensor<128x1440xbf16> loc("p70.1767"), %arg537: tensor<1x2x256x64xbf16> loc("p71.1791"), %arg538: tensor<128xbf16> loc("p72.1802"), %arg539: tensor<128x1440xbf16> loc("p73.1806"), %arg540: tensor<32xbf16> loc("p74.1817"), %arg541: tensor<32x1440xbf16> loc("p75.1825"), %arg542: tensor<1440xbf16> loc("p76.1836"), %arg543: tensor<1440x1024xbf16> loc("p77.1840"), %arg544: tensor<64xbf16> loc("p78.1851"), %arg545: tensor<1024xbf16> loc("p79.1867"), %arg546: tensor<1024x1440xbf16> loc("p80.1871"), %arg547: tensor<1440xbf16> loc("p81.2021"), %arg548: tensor<8x1440xbf16> loc("p82.2091"), %arg549: tensor<8x2880x1440xbf16> loc("p83.2096"), %arg550: tensor<8x5760xbf16> loc("p84.2102"), %arg551: tensor<8x1440x5760xbf16> loc("p85.2107"), %arg552: tensor<1440xbf16> loc("p86.2186"), %arg553: tensor<1x2x256x64xbf16> loc("p87.2236"), %arg554: tensor<128xbf16> loc("p88.2245"), %arg555: tensor<128x1440xbf16> loc("p89.2249"), %arg556: tensor<1x2x256x64xbf16> loc("p90.2273"), %arg557: tensor<128xbf16> loc("p91.2284"), %arg558: tensor<128x1440xbf16> loc("p92.2288"), %arg559: tensor<32xbf16> loc("p93.2299"), %arg560: tensor<32x1440xbf16> loc("p94.2307"), %arg561: tensor<1440xbf16> loc("p95.2318"), %arg562: tensor<1440x1024xbf16> loc("p96.2322"), %arg563: tensor<64xbf16> loc("p97.2333"), %arg564: tensor<1024xbf16> loc("p98.2349"), %arg565: tensor<1024x1440xbf16> loc("p99.2353"), %arg566: tensor<1440xbf16> loc("p100.2503"), %arg567: tensor<8x1440xbf16> loc("p101.2573"), %arg568: tensor<8x2880x1440xbf16> loc("p102.2578"), %arg569: tensor<8x5760xbf16> loc("p103.2584"), %arg570: tensor<8x1440x5760xbf16> loc("p104.2589"), %arg571: tensor<1440xbf16> loc("p105.2668"), %arg572: tensor<1x2x256x64xbf16> loc("p106.2718"), %arg573: tensor<128xbf16> loc("p107.2727"), %arg574: tensor<128x1440xbf16> loc("p108.2731"), %arg575: tensor<1x2x256x64xbf16> loc("p109.2755"), %arg576: tensor<128xbf16> loc("p110.2766"), %arg577: tensor<128x1440xbf16> loc("p111.2770"), %arg578: tensor<32xbf16> loc("p112.2781"), %arg579: tensor<32x1440xbf16> loc("p113.2789"), %arg580: tensor<1440xbf16> loc("p114.2800"), %arg581: tensor<1440x1024xbf16> loc("p115.2804"), %arg582: tensor<64xbf16> loc("p116.2815"), %arg583: tensor<1024xbf16> loc("p117.2831"), %arg584: tensor<1024x1440xbf16> loc("p118.2835"), %arg585: tensor<1440xbf16> loc("p119.2985"), %arg586: tensor<8x1440xbf16> loc("p120.3055"), %arg587: tensor<8x2880x1440xbf16> loc("p121.3060"), %arg588: tensor<8x5760xbf16> loc("p122.3066"), %arg589: tensor<8x1440x5760xbf16> loc("p123.3071"), %arg590: tensor<1440xbf16> loc("p124.3150"), %arg591: tensor<1x2x256x64xbf16> loc("p125.3200"), %arg592: tensor<128xbf16> loc("p126.3209"), %arg593: tensor<128x1440xbf16> loc("p127.3213"), %arg594: tensor<1x2x256x64xbf16> loc("p128.3237"), %arg595: tensor<128xbf16> loc("p129.3248"), %arg596: tensor<128x1440xbf16> loc("p130.3252"), %arg597: tensor<32xbf16> loc("p131.3263"), %arg598: tensor<32x1440xbf16> loc("p132.3271"), %arg599: tensor<1440xbf16> loc("p133.3282"), %arg600: tensor<1440x1024xbf16> loc("p134.3286"), %arg601: tensor<64xbf16> loc("p135.3297"), %arg602: tensor<1024xbf16> loc("p136.3313"), %arg603: tensor<1024x1440xbf16> loc("p137.3317"), %arg604: tensor<1440xbf16> loc("p138.3467"), %arg605: tensor<8x1440xbf16> loc("p139.3537"), %arg606: tensor<8x2880x1440xbf16> loc("p140.3542"), %arg607: tensor<8x5760xbf16> loc("p141.3548"), %arg608: tensor<8x1440x5760xbf16> loc("p142.3553"), %arg609: tensor<1440xbf16> loc("p143.3632"), %arg610: tensor<1x2x256x64xbf16> loc("p144.3682"), %arg611: tensor<128xbf16> loc("p145.3691"), %arg612: tensor<128x1440xbf16> loc("p146.3695"), %arg613: tensor<1x2x256x64xbf16> loc("p147.3719"), %arg614: tensor<128xbf16> loc("p148.3730"), %arg615: tensor<128x1440xbf16> loc("p149.3734"), %arg616: tensor<32xbf16> loc("p150.3745"), %arg617: tensor<32x1440xbf16> loc("p151.3753"), %arg618: tensor<1440xbf16> loc("p152.3764"), %arg619: tensor<1440x1024xbf16> loc("p153.3768"), %arg620: tensor<64xbf16> loc("p154.3779"), %arg621: tensor<1024xbf16> loc("p155.3795"), %arg622: tensor<1024x1440xbf16> loc("p156.3799"), %arg623: tensor<1440xbf16> loc("p157.3949"), %arg624: tensor<8x1440xbf16> loc("p158.4019"), %arg625: tensor<8x2880x1440xbf16> loc("p159.4024"), %arg626: tensor<8x5760xbf16> loc("p160.4030"), %arg627: tensor<8x1440x5760xbf16> loc("p161.4035"), %arg628: tensor<1440xbf16> loc("p162.4114"), %arg629: tensor<1x2x256x64xbf16> loc("p163.4164"), %arg630: tensor<128xbf16> loc("p164.4173"), %arg631: tensor<128x1440xbf16> loc("p165.4177"), %arg632: tensor<1x2x256x64xbf16> loc("p166.4201"), %arg633: tensor<128xbf16> loc("p167.4212"), %arg634: tensor<128x1440xbf16> loc("p168.4216"), %arg635: tensor<32xbf16> loc("p169.4227"), %arg636: tensor<32x1440xbf16> loc("p170.4235"), %arg637: tensor<1440xbf16> loc("p171.4246"), %arg638: tensor<1440x1024xbf16> loc("p172.4250"), %arg639: tensor<64xbf16> loc("p173.4261"), %arg640: tensor<1024xbf16> loc("p174.4277"), %arg641: tensor<1024x1440xbf16> loc("p175.4281"), %arg642: tensor<1440xbf16> loc("p176.4431"), %arg643: tensor<8x1440xbf16> loc("p177.4501"), %arg644: tensor<8x2880x1440xbf16> loc("p178.4506"), %arg645: tensor<8x5760xbf16> loc("p179.4512"), %arg646: tensor<8x1440x5760xbf16> loc("p180.4517"), %arg647: tensor<1440xbf16> loc("p181.4596"), %arg648: tensor<1x2x256x64xbf16> loc("p182.4646"), %arg649: tensor<128xbf16> loc("p183.4655"), %arg650: tensor<128x1440xbf16> loc("p184.4659"), %arg651: tensor<1x2x256x64xbf16> loc("p185.4683"), %arg652: tensor<128xbf16> loc("p186.4694"), %arg653: tensor<128x1440xbf16> loc("p187.4698"), %arg654: tensor<32xbf16> loc("p188.4709"), %arg655: tensor<32x1440xbf16> loc("p189.4717"), %arg656: tensor<1440xbf16> loc("p190.4728"), %arg657: tensor<1440x1024xbf16> loc("p191.4732"), %arg658: tensor<64xbf16> loc("p192.4743"), %arg659: tensor<1024xbf16> loc("p193.4759"), %arg660: tensor<1024x1440xbf16> loc("p194.4763"), %arg661: tensor<1440xbf16> loc("p195.4913"), %arg662: tensor<8x1440xbf16> loc("p196.4983"), %arg663: tensor<8x2880x1440xbf16> loc("p197.4988"), %arg664: tensor<8x5760xbf16> loc("p198.4994"), %arg665: tensor<8x1440x5760xbf16> loc("p199.4999"), %arg666: tensor<1440xbf16> loc("p200.5078"), %arg667: tensor<1x2x256x64xbf16> loc("p201.5128"), %arg668: tensor<128xbf16> loc("p202.5137"), %arg669: tensor<128x1440xbf16> loc("p203.5141"), %arg670: tensor<1x2x256x64xbf16> loc("p204.5165"), %arg671: tensor<128xbf16> loc("p205.5176"), %arg672: tensor<128x1440xbf16> loc("p206.5180"), %arg673: tensor<32xbf16> loc("p207.5191"), %arg674: tensor<32x1440xbf16> loc("p208.5199"), %arg675: tensor<1440xbf16> loc("p209.5210"), %arg676: tensor<1440x1024xbf16> loc("p210.5214"), %arg677: tensor<64xbf16> loc("p211.5225"), %arg678: tensor<1024xbf16> loc("p212.5241"), %arg679: tensor<1024x1440xbf16> loc("p213.5245"), %arg680: tensor<1440xbf16> loc("p214.5395"), %arg681: tensor<8x1440xbf16> loc("p215.5465"), %arg682: tensor<8x2880x1440xbf16> loc("p216.5470"), %arg683: tensor<8x5760xbf16> loc("p217.5476"), %arg684: tensor<8x1440x5760xbf16> loc("p218.5481"), %arg685: tensor<1440xbf16> loc("p219.5560"), %arg686: tensor<1x2x256x64xbf16> loc("p220.5610"), %arg687: tensor<128xbf16> loc("p221.5619"), %arg688: tensor<128x1440xbf16> loc("p222.5623"), %arg689: tensor<1x2x256x64xbf16> loc("p223.5647"), %arg690: tensor<128xbf16> loc("p224.5658"), %arg691: tensor<128x1440xbf16> loc("p225.5662"), %arg692: tensor<32xbf16> loc("p226.5673"), %arg693: tensor<32x1440xbf16> loc("p227.5681"), %arg694: tensor<1440xbf16> loc("p228.5692"), %arg695: tensor<1440x1024xbf16> loc("p229.5696"), %arg696: tensor<64xbf16> loc("p230.5707"), %arg697: tensor<1024xbf16> loc("p231.5723"), %arg698: tensor<1024x1440xbf16> loc("p232.5727"), %arg699: tensor<1440xbf16> loc("p233.5877"), %arg700: tensor<8x1440xbf16> loc("p234.5947"), %arg701: tensor<8x2880x1440xbf16> loc("p235.5952"), %arg702: tensor<8x5760xbf16> loc("p236.5958"), %arg703: tensor<8x1440x5760xbf16> loc("p237.5963"), %arg704: tensor<1440xbf16> loc("p238.6042"), %arg705: tensor<1x2x256x64xbf16> loc("p239.6092"), %arg706: tensor<128xbf16> loc("p240.6101"), %arg707: tensor<128x1440xbf16> loc("p241.6105"), %arg708: tensor<1x2x256x64xbf16> loc("p242.6129"), %arg709: tensor<128xbf16> loc("p243.6140"), %arg710: tensor<128x1440xbf16> loc("p244.6144"), %arg711: tensor<32xbf16> loc("p245.6155"), %arg712: tensor<32x1440xbf16> loc("p246.6163"), %arg713: tensor<1440xbf16> loc("p247.6174"), %arg714: tensor<1440x1024xbf16> loc("p248.6178"), %arg715: tensor<64xbf16> loc("p249.6189"), %arg716: tensor<1024xbf16> loc("p250.6205"), %arg717: tensor<1024x1440xbf16> loc("p251.6209"), %arg718: tensor<1440xbf16> loc("p252.6359"), %arg719: tensor<8x1440xbf16> loc("p253.6429"), %arg720: tensor<8x2880x1440xbf16> loc("p254.6434"), %arg721: tensor<8x5760xbf16> loc("p255.6440"), %arg722: tensor<8x1440x5760xbf16> loc("p256.6445"), %arg723: tensor<1440xbf16> loc("p257.6524"), %arg724: tensor<1x2x256x64xbf16> loc("p258.6574"), %arg725: tensor<128xbf16> loc("p259.6583"), %arg726: tensor<128x1440xbf16> loc("p260.6587"), %arg727: tensor<1x2x256x64xbf16> loc("p261.6611"), %arg728: tensor<128xbf16> loc("p262.6622"), %arg729: tensor<128x1440xbf16> loc("p263.6626"), %arg730: tensor<32xbf16> loc("p264.6637"), %arg731: tensor<32x1440xbf16> loc("p265.6645"), %arg732: tensor<1440xbf16> loc("p266.6656"), %arg733: tensor<1440x1024xbf16> loc("p267.6660"), %arg734: tensor<64xbf16> loc("p268.6671"), %arg735: tensor<1024xbf16> loc("p269.6687"), %arg736: tensor<1024x1440xbf16> loc("p270.6691"), %arg737: tensor<1440xbf16> loc("p271.6841"), %arg738: tensor<8x1440xbf16> loc("p272.6911"), %arg739: tensor<8x2880x1440xbf16> loc("p273.6916"), %arg740: tensor<8x5760xbf16> loc("p274.6922"), %arg741: tensor<8x1440x5760xbf16> loc("p275.6927"), %arg742: tensor<1440xbf16> loc("p276.7006"), %arg743: tensor<1x2x256x64xbf16> loc("p277.7056"), %arg744: tensor<128xbf16> loc("p278.7065"), %arg745: tensor<128x1440xbf16> loc("p279.7069"), %arg746: tensor<1x2x256x64xbf16> loc("p280.7093"), %arg747: tensor<128xbf16> loc("p281.7104"), %arg748: tensor<128x1440xbf16> loc("p282.7108"), %arg749: tensor<32xbf16> loc("p283.7119"), %arg750: tensor<32x1440xbf16> loc("p284.7127"), %arg751: tensor<1440xbf16> loc("p285.7138"), %arg752: tensor<1440x1024xbf16> loc("p286.7142"), %arg753: tensor<64xbf16> loc("p287.7153"), %arg754: tensor<1024xbf16> loc("p288.7169"), %arg755: tensor<1024x1440xbf16> loc("p289.7173"), %arg756: tensor<1440xbf16> loc("p290.7323"), %arg757: tensor<8x1440xbf16> loc("p291.7393"), %arg758: tensor<8x2880x1440xbf16> loc("p292.7398"), %arg759: tensor<8x5760xbf16> loc("p293.7404"), %arg760: tensor<8x1440x5760xbf16> loc("p294.7409"), %arg761: tensor<1440xbf16> loc("p295.7488"), %arg762: tensor<1x2x256x64xbf16> loc("p296.7538"), %arg763: tensor<128xbf16> loc("p297.7547"), %arg764: tensor<128x1440xbf16> loc("p298.7551"), %arg765: tensor<1x2x256x64xbf16> loc("p299.7575"), %arg766: tensor<128xbf16> loc("p300.7586"), %arg767: tensor<128x1440xbf16> loc("p301.7590"), %arg768: tensor<32xbf16> loc("p302.7601"), %arg769: tensor<32x1440xbf16> loc("p303.7609"), %arg770: tensor<1440xbf16> loc("p304.7620"), %arg771: tensor<1440x1024xbf16> loc("p305.7624"), %arg772: tensor<64xbf16> loc("p306.7635"), %arg773: tensor<1024xbf16> loc("p307.7651"), %arg774: tensor<1024x1440xbf16> loc("p308.7655"), %arg775: tensor<1440xbf16> loc("p309.7805"), %arg776: tensor<8x1440xbf16> loc("p310.7875"), %arg777: tensor<8x2880x1440xbf16> loc("p311.7880"), %arg778: tensor<8x5760xbf16> loc("p312.7886"), %arg779: tensor<8x1440x5760xbf16> loc("p313.7891"), %arg780: tensor<1440xbf16> loc("p314.7970"), %arg781: tensor<1x2x256x64xbf16> loc("p315.8020"), %arg782: tensor<128xbf16> loc("p316.8029"), %arg783: tensor<128x1440xbf16> loc("p317.8033"), %arg784: tensor<1x2x256x64xbf16> loc("p318.8057"), %arg785: tensor<128xbf16> loc("p319.8068"), %arg786: tensor<128x1440xbf16> loc("p320.8072"), %arg787: tensor<32xbf16> loc("p321.8083"), %arg788: tensor<32x1440xbf16> loc("p322.8091"), %arg789: tensor<1440xbf16> loc("p323.8102"), %arg790: tensor<1440x1024xbf16> loc("p324.8106"), %arg791: tensor<64xbf16> loc("p325.8117"), %arg792: tensor<1024xbf16> loc("p326.8133"), %arg793: tensor<1024x1440xbf16> loc("p327.8137"), %arg794: tensor<1440xbf16> loc("p328.8287"), %arg795: tensor<8x1440xbf16> loc("p329.8357"), %arg796: tensor<8x2880x1440xbf16> loc("p330.8362"), %arg797: tensor<8x5760xbf16> loc("p331.8368"), %arg798: tensor<8x1440x5760xbf16> loc("p332.8373"), %arg799: tensor<1440xbf16> loc("p333.8452"), %arg800: tensor<1x2x256x64xbf16> loc("p334.8502"), %arg801: tensor<128xbf16> loc("p335.8511"), %arg802: tensor<128x1440xbf16> loc("p336.8515"), %arg803: tensor<1x2x256x64xbf16> loc("p337.8539"), %arg804: tensor<128xbf16> loc("p338.8550"), %arg805: tensor<128x1440xbf16> loc("p339.8554"), %arg806: tensor<32xbf16> loc("p340.8565"), %arg807: tensor<32x1440xbf16> loc("p341.8573"), %arg808: tensor<1440xbf16> loc("p342.8584"), %arg809: tensor<1440x1024xbf16> loc("p343.8588"), %arg810: tensor<64xbf16> loc("p344.8599"), %arg811: tensor<1024xbf16> loc("p345.8615"), %arg812: tensor<1024x1440xbf16> loc("p346.8619"), %arg813: tensor<1440xbf16> loc("p347.8769"), %arg814: tensor<8x1440xbf16> loc("p348.8839"), %arg815: tensor<8x2880x1440xbf16> loc("p349.8844"), %arg816: tensor<8x5760xbf16> loc("p350.8850"), %arg817: tensor<8x1440x5760xbf16> loc("p351.8855"), %arg818: tensor<1440xbf16> loc("p352.8934"), %arg819: tensor<1x2x256x64xbf16> loc("p353.8984"), %arg820: tensor<128xbf16> loc("p354.8993"), %arg821: tensor<128x1440xbf16> loc("p355.8997"), %arg822: tensor<1x2x256x64xbf16> loc("p356.9021"), %arg823: tensor<128xbf16> loc("p357.9032"), %arg824: tensor<128x1440xbf16> loc("p358.9036"), %arg825: tensor<32xbf16> loc("p359.9047"), %arg826: tensor<32x1440xbf16> loc("p360.9055"), %arg827: tensor<1440xbf16> loc("p361.9066"), %arg828: tensor<1440x1024xbf16> loc("p362.9070"), %arg829: tensor<64xbf16> loc("p363.9081"), %arg830: tensor<1024xbf16> loc("p364.9097"), %arg831: tensor<1024x1440xbf16> loc("p365.9101"), %arg832: tensor<1440xbf16> loc("p366.9251"), %arg833: tensor<8x1440xbf16> loc("p367.9321"), %arg834: tensor<8x2880x1440xbf16> loc("p368.9326"), %arg835: tensor<8x5760xbf16> loc("p369.9332"), %arg836: tensor<8x1440x5760xbf16> loc("p370.9337"), %arg837: tensor<1440xbf16> loc("p371.9416"), %arg838: tensor<1x2x256x64xbf16> loc("p372.9466"), %arg839: tensor<128xbf16> loc("p373.9475"), %arg840: tensor<128x1440xbf16> loc("p374.9479"), %arg841: tensor<1x2x256x64xbf16> loc("p375.9503"), %arg842: tensor<128xbf16> loc("p376.9514"), %arg843: tensor<128x1440xbf16> loc("p377.9518"), %arg844: tensor<32xbf16> loc("p378.9529"), %arg845: tensor<32x1440xbf16> loc("p379.9537"), %arg846: tensor<1440xbf16> loc("p380.9548"), %arg847: tensor<1440x1024xbf16> loc("p381.9552"), %arg848: tensor<64xbf16> loc("p382.9563"), %arg849: tensor<1024xbf16> loc("p383.9579"), %arg850: tensor<1024x1440xbf16> loc("p384.9583"), %arg851: tensor<1440xbf16> loc("p385.9733"), %arg852: tensor<8x1440xbf16> loc("p386.9803"), %arg853: tensor<8x2880x1440xbf16> loc("p387.9808"), %arg854: tensor<8x5760xbf16> loc("p388.9814"), %arg855: tensor<8x1440x5760xbf16> loc("p389.9819"), %arg856: tensor<1440xbf16> loc("p390.9898"), %arg857: tensor<1x2x256x64xbf16> loc("p391.9948"), %arg858: tensor<128xbf16> loc("p392.9957"), %arg859: tensor<128x1440xbf16> loc("p393.9961"), %arg860: tensor<1x2x256x64xbf16> loc("p394.9985"), %arg861: tensor<128xbf16> loc("p395.9996"), %arg862: tensor<128x1440xbf16> loc("p396.10000"), %arg863: tensor<32xbf16> loc("p397.10011"), %arg864: tensor<32x1440xbf16> loc("p398.10019"), %arg865: tensor<1440xbf16> loc("p399.10030"), %arg866: tensor<1440x1024xbf16> loc("p400.10034"), %arg867: tensor<64xbf16> loc("p401.10045"), %arg868: tensor<1024xbf16> loc("p402.10061"), %arg869: tensor<1024x1440xbf16> loc("p403.10065"), %arg870: tensor<1440xbf16> loc("p404.10215"), %arg871: tensor<8x1440xbf16> loc("p405.10285"), %arg872: tensor<8x2880x1440xbf16> loc("p406.10290"), %arg873: tensor<8x5760xbf16> loc("p407.10296"), %arg874: tensor<8x1440x5760xbf16> loc("p408.10301"), %arg875: tensor<1440xbf16> loc("p409.10380"), %arg876: tensor<1x2x256x64xbf16> loc("p410.10430"), %arg877: tensor<128xbf16> loc("p411.10439"), %arg878: tensor<128x1440xbf16> loc("p412.10443"), %arg879: tensor<1x2x256x64xbf16> loc("p413.10467"), %arg880: tensor<128xbf16> loc("p414.10478"), %arg881: tensor<128x1440xbf16> loc("p415.10482"), %arg882: tensor<32xbf16> loc("p416.10493"), %arg883: tensor<32x1440xbf16> loc("p417.10501"), %arg884: tensor<1440xbf16> loc("p418.10512"), %arg885: tensor<1440x1024xbf16> loc("p419.10516"), %arg886: tensor<64xbf16> loc("p420.10527"), %arg887: tensor<1024xbf16> loc("p421.10543"), %arg888: tensor<1024x1440xbf16> loc("p422.10547"), %arg889: tensor<1440xbf16> loc("p423.10697"), %arg890: tensor<8x1440xbf16> loc("p424.10767"), %arg891: tensor<8x2880x1440xbf16> loc("p425.10772"), %arg892: tensor<8x5760xbf16> loc("p426.10778"), %arg893: tensor<8x1440x5760xbf16> loc("p427.10783"), %arg894: tensor<1440xbf16> loc("p428.10862"), %arg895: tensor<1x2x256x64xbf16> loc("p429.10912"), %arg896: tensor<128xbf16> loc("p430.10921"), %arg897: tensor<128x1440xbf16> loc("p431.10925"), %arg898: tensor<1x2x256x64xbf16> loc("p432.10949"), %arg899: tensor<128xbf16> loc("p433.10960"), %arg900: tensor<128x1440xbf16> loc("p434.10964"), %arg901: tensor<32xbf16> loc("p435.10975"), %arg902: tensor<32x1440xbf16> loc("p436.10983"), %arg903: tensor<1440xbf16> loc("p437.10994"), %arg904: tensor<1440x1024xbf16> loc("p438.10998"), %arg905: tensor<64xbf16> loc("p439.11009"), %arg906: tensor<1024xbf16> loc("p440.11025"), %arg907: tensor<1024x1440xbf16> loc("p441.11029"), %arg908: tensor<1440xbf16> loc("p442.11179"), %arg909: tensor<8x1440xbf16> loc("p443.11249"), %arg910: tensor<8x2880x1440xbf16> loc("p444.11254"), %arg911: tensor<8x5760xbf16> loc("p445.11260"), %arg912: tensor<8x1440x5760xbf16> loc("p446.11265"), %arg913: tensor<1440xbf16> loc("p447.11344"), %arg914: tensor<1x2x256x64xbf16> loc("p448.11394"), %arg915: tensor<128xbf16> loc("p449.11403"), %arg916: tensor<128x1440xbf16> loc("p450.11407"), %arg917: tensor<1x2x256x64xbf16> loc("p451.11431"), %arg918: tensor<50272x1440xbf16> loc("p452.11439"), %arg919: tensor<32xbf16> loc("p453.11450"), %arg920: tensor<32x1440xbf16> loc("p454.11458"), %arg921: tensor<1440xbf16> loc("p455.11469"), %arg922: tensor<1440x1024xbf16> loc("p456.11473"), %arg923: tensor<64xbf16> loc("p457.11484"), %arg924: tensor<1024xbf16> loc("p458.11500"), %arg925: tensor<1024x1440xbf16> loc("p459.11504"), %arg926: tensor<1440xbf16> loc("p460.11654"), %arg927: tensor<8x1440xbf16> loc("p461.11724"), %arg928: tensor<8x2880x1440xbf16> loc("p462.11729"), %arg929: tensor<8x5760xbf16> loc("p463.11735"), %arg930: tensor<8x1440x5760xbf16> loc("p464.11740"), %arg931: tensor<1440xbf16> loc("p465.11819")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
      %c = stablehlo.constant dense<0> : tensor<i64> loc(#loc)
      %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
      %cst_1 = stablehlo.constant dense<3.47222231E-4> : tensor<1x1xf32> loc(#loc)
      %cst_2 = stablehlo.constant dense<9.99999974E-6> : tensor<1x1x1xf32> loc(#loc)
      %cst_3 = stablehlo.constant dense<1.34657359> : tensor<f32> loc(#loc)
      %c_4 = stablehlo.constant dense<0> : tensor<1xi64> loc(#loc)
      %c_5 = stablehlo.constant dense<256> : tensor<1xi64> loc(#loc)
      %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16> loc(#loc)
      %c_7 = stablehlo.constant dense<128> : tensor<1xi64> loc(#loc)
      %c_8 = stablehlo.constant dense<"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"> : tensor<1x256x2xi64> loc(#loc)
      %cst_9 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16> loc(#loc)
      %cst_10 = stablehlo.constant dense<-7.000000e+00> : tensor<bf16> loc(#loc)
      %cst_11 = stablehlo.constant dense<7.000000e+00> : tensor<bf16> loc(#loc)
      %cst_12 = stablehlo.constant dense<1.000000e+00> : tensor<bf16> loc(#loc)
      %cst_13 = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
      %cst_14 = stablehlo.constant dense<1.703130e+00> : tensor<bf16> loc(#loc)
      %c_15 = stablehlo.constant dense<"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tensor<1x256xi64> loc(#loc)
      %c_16 = stablehlo.constant dense<0> : tensor<1x4x1xi64> loc(#loc)
      %cst_17 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %2 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %5 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %6 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %7 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<1x1x1x256xbf16> loc(#loc)
      %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x32xf32> loc(#loc)
      %10 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %11 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i64>) -> tensor<1x256xi64> loc(#loc)
      %12 = stablehlo.reshape %arg472 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc467)
      %13 = stablehlo.reshape %12 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc468)
      %14 = stablehlo.convert %13 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc469)
      %15 = stablehlo.reshape %14 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc470)
      %16 = stablehlo.reshape %arg471 : (tensor<201088x1440xbf16>) -> tensor<1x201088x1440xbf16> loc(#loc471)
      %17 = stablehlo.reshape %16 : (tensor<1x201088x1440xbf16>) -> tensor<201088x1440xbf16> loc(#loc472)
      %18 = stablehlo.reshape %arg470 : (tensor<1x1xi64>) -> tensor<1x1x1xi64> loc(#loc473)
      %19 = stablehlo.reshape %18 : (tensor<1x1x1xi64>) -> tensor<1xi64> loc(#loc474)
      %20 = stablehlo.convert %19 : (tensor<1xi64>) -> tensor<1xui32> loc(#loc475)
      %21 = "stablehlo.gather"(%17, %20) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1440>}> : (tensor<201088x1440xbf16>, tensor<1xui32>) -> tensor<1x1440xbf16> loc(#loc476)
      %22 = stablehlo.reshape %21 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc477)
      %23 = stablehlo.convert %22 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc478)
      %24 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %25 = stablehlo.power %23, %24 : tensor<1x1x1440xf32> loc(#loc479)
      %26 = stablehlo.reduce(%25 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc480)
      %27 = "stablehlo.all_reduce"(%26) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.60"), %arg933: tensor<f32> loc("reduce.60")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc480)
        stablehlo.return %4605 : tensor<f32> loc(#loc480)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc480)
      %28 = stablehlo.multiply %27, %cst_1 : tensor<1x1xf32> loc(#loc481)
      %29 = stablehlo.reshape %28 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc482)
      %30 = stablehlo.add %29, %cst_2 : tensor<1x1x1xf32> loc(#loc483)
      %31 = stablehlo.rsqrt %30 : tensor<1x1x1xf32> loc(#loc484)
      %32 = stablehlo.reshape %31 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc485)
      %33 = stablehlo.broadcast_in_dim %32, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc486)
      %34 = stablehlo.multiply %23, %33 : tensor<1x1x1440xf32> loc(#loc487)
      %35 = stablehlo.multiply %15, %34 : tensor<1x1x1440xf32> loc(#loc488)
      %36 = stablehlo.convert %35 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc489)
      %37 = stablehlo.reshape %36 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc490)
      %38 = stablehlo.reshape %arg488 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc491)
      %39 = stablehlo.reshape %38 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc492)
      %40 = stablehlo.transpose %39, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc493)
      %41 = stablehlo.dot_general %37, %40, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc494)
      %42 = "stablehlo.all_reduce"(%41) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.357"), %arg933: tensor<bf16> loc("dot.357")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc494)
        stablehlo.return %4605 : tensor<bf16> loc(#loc494)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc494)
      %43 = stablehlo.reshape %42 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc495)
      %44 = stablehlo.reshape %arg487 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc496)
      %45 = stablehlo.add %43, %44 : tensor<1x1x1024xbf16> loc(#loc497)
      %46 = stablehlo.reshape %45 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc498)
      %47 = stablehlo.slice %46 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc499)
      %48 = stablehlo.reshape %arg467 : (tensor<32xf32>) -> tensor<1x1x32xf32> loc(#loc500)
      %49 = stablehlo.reshape %48 : (tensor<1x1x32xf32>) -> tensor<1x32x1xf32> loc(#loc501)
      %50 = stablehlo.reshape %arg466 : (tensor<1xi64>) -> tensor<1x1x1xi64> loc(#loc502)
      %51 = stablehlo.reshape %50 : (tensor<1x1x1xi64>) -> tensor<1xi64> loc(#loc503)
      %52 = stablehlo.convert %50 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32> loc(#loc504)
      %53 = stablehlo.dot_general %49, %52, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x1xf32>) -> tensor<1x32x1xf32> loc(#loc505)
      %54 = stablehlo.reshape %53 : (tensor<1x32x1xf32>) -> tensor<1x1x32xf32> loc(#loc506)
      %55 = stablehlo.cosine %54 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,1,32]{1,2,0}"} : tensor<1x1x32xf32> loc(#loc507)
      %56 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x1x32xf32> loc(#loc)
      %57 = stablehlo.multiply %55, %56 : tensor<1x1x32xf32> loc(#loc508)
      %58 = stablehlo.convert %57 : (tensor<1x1x32xf32>) -> tensor<1x1x32xbf16> loc(#loc509)
      %59 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] : (tensor<1x1x32xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc510)
      %60 = stablehlo.multiply %47, %59 : tensor<1x16x1x32xbf16> loc(#loc511)
      %61 = stablehlo.slice %46 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc512)
      %62 = stablehlo.sine %54 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,1,32]{1,2,0}"} : tensor<1x1x32xf32> loc(#loc513)
      %63 = stablehlo.multiply %62, %9 : tensor<1x1x32xf32> loc(#loc514)
      %64 = stablehlo.convert %63 : (tensor<1x1x32xf32>) -> tensor<1x1x32xbf16> loc(#loc515)
      %65 = stablehlo.broadcast_in_dim %64, dims = [0, 2, 3] : (tensor<1x1x32xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc516)
      %66 = stablehlo.multiply %61, %65 : tensor<1x16x1x32xbf16> loc(#loc517)
      %67 = stablehlo.subtract %60, %66 : tensor<1x16x1x32xbf16> loc(#loc518)
      %68 = stablehlo.multiply %61, %59 : tensor<1x16x1x32xbf16> loc(#loc519)
      %69 = stablehlo.multiply %47, %65 : tensor<1x16x1x32xbf16> loc(#loc520)
      %70 = stablehlo.add %68, %69 : tensor<1x16x1x32xbf16> loc(#loc521)
      %71 = stablehlo.concatenate %67, %70, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc522)
      %72 = stablehlo.compare  LT, %51, %c_4 : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1> loc(#loc523)
      %73 = stablehlo.add %51, %c_5 : tensor<1xi64> loc(#loc524)
      %74 = stablehlo.select %72, %73, %51 : tensor<1xi1>, tensor<1xi64> loc(#loc525)
      %75 = stablehlo.reshape %74 : (tensor<1xi64>) -> tensor<1x1xi64> loc(#loc526)
      %76 = stablehlo.reshape %arg469 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc527)
      %77 = stablehlo.reshape %76 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc528)
      %78 = stablehlo.transpose %77, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc529)
      %79 = stablehlo.dot_general %37, %78, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc530)
      %80 = "stablehlo.all_reduce"(%79) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.88"), %arg933: tensor<bf16> loc("dot.88")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc530)
        stablehlo.return %4605 : tensor<bf16> loc(#loc530)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc530)
      %81 = stablehlo.reshape %80 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc531)
      %82 = stablehlo.reshape %arg468 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc532)
      %83 = stablehlo.add %81, %82 : tensor<1x1x128xbf16> loc(#loc533)
      %84 = stablehlo.reshape %83 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc534)
      %85 = stablehlo.slice %84 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc535)
      %86 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] : (tensor<1x1x32xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc536)
      %87 = stablehlo.multiply %85, %86 : tensor<1x2x1x32xbf16> loc(#loc537)
      %88 = stablehlo.slice %84 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc538)
      %89 = stablehlo.broadcast_in_dim %64, dims = [0, 2, 3] : (tensor<1x1x32xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc539)
      %90 = stablehlo.multiply %88, %89 : tensor<1x2x1x32xbf16> loc(#loc540)
      %91 = stablehlo.subtract %87, %90 : tensor<1x2x1x32xbf16> loc(#loc541)
      %92 = stablehlo.multiply %88, %86 : tensor<1x2x1x32xbf16> loc(#loc542)
      %93 = stablehlo.multiply %85, %89 : tensor<1x2x1x32xbf16> loc(#loc543)
      %94 = stablehlo.add %92, %93 : tensor<1x2x1x32xbf16> loc(#loc544)
      %95 = stablehlo.concatenate %91, %94, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc545)
      %96 = "stablehlo.scatter"(%arg473, %75, %95) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.140"), %arg933: tensor<bf16> loc("scatter.140")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc546)
      %97 = stablehlo.broadcast_in_dim %96, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc547)
      %98 = stablehlo.reshape %97 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc548)
      %99 = stablehlo.transpose %98, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc549)
      %100 = stablehlo.dot_general %71, %99, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc550)
      %101 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %102 = stablehlo.multiply %100, %101 : tensor<1x16x1x256xbf16> loc(#loc551)
      %103 = stablehlo.reshape %arg486 : (tensor<i1>) -> tensor<1xi1> loc(#loc552)
      %104 = stablehlo.broadcast_in_dim %103, dims = [0] : (tensor<1xi1>) -> tensor<1x256xi1> loc(#loc553)
      %105 = stablehlo.subtract %51, %c_7 : tensor<1xi64> loc(#loc554)
      %106 = stablehlo.broadcast_in_dim %105, dims = [0] : (tensor<1xi64>) -> tensor<1x256xi64> loc(#loc555)
      %107 = stablehlo.compare  GT, %c_15, %106 : (tensor<1x256xi64>, tensor<1x256xi64>) -> tensor<1x256xi1> loc(#loc556)
      %108 = stablehlo.and %104, %107 : tensor<1x256xi1> loc(#loc557)
      %109 = stablehlo.broadcast_in_dim %51, dims = [0] : (tensor<1xi64>) -> tensor<1x256xi64> loc(#loc558)
      %110 = stablehlo.compare  LE, %c_15, %109 : (tensor<1x256xi64>, tensor<1x256xi64>) -> tensor<1x256xi1> loc(#loc559)
      %111 = stablehlo.and %108, %110 : tensor<1x256xi1> loc(#loc560)
      %112 = stablehlo.and %104, %111 : tensor<1x256xi1> loc(#loc561)
      %113 = stablehlo.reshape %112 : (tensor<1x256xi1>) -> tensor<1x1x256xi1> loc(#loc562)
      %114 = stablehlo.reshape %arg485 : (tensor<1x256xi64>) -> tensor<1x1x256xi64> loc(#loc563)
      %115 = stablehlo.reshape %114 : (tensor<1x1x256xi64>) -> tensor<1x256xi64> loc(#loc564)
      %116 = stablehlo.compare  NE, %115, %11 : (tensor<1x256xi64>, tensor<1x256xi64>) -> tensor<1x256xi1> loc(#loc565)
      %117 = "stablehlo.gather"(%116, %c_8) <{dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1>}> : (tensor<1x256xi1>, tensor<1x256x2xi64>) -> tensor<1x256xi1> loc(#loc566)
      %118 = stablehlo.reshape %117 : (tensor<1x256xi1>) -> tensor<1x1x256xi1> loc(#loc567)
      %119 = stablehlo.and %113, %118 : tensor<1x1x256xi1> loc(#loc568)
      %120 = stablehlo.reshape %119 : (tensor<1x1x256xi1>) -> tensor<1x1x1x256xi1> loc(#loc569)
      %121 = stablehlo.reshape %arg484 : (tensor<bf16>) -> tensor<1x1x1xbf16> loc(#loc570)
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 2] : (tensor<1x1x1xbf16>) -> tensor<1x1x1x256xbf16> loc(#loc571)
      %123 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<1x1x1x256xbf16> loc(#loc)
      %124 = stablehlo.select %120, %122, %123 : tensor<1x1x1x256xi1>, tensor<1x1x1x256xbf16> loc(#loc572)
      %125 = stablehlo.reshape %124 : (tensor<1x1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc573)
      %126 = stablehlo.broadcast_in_dim %125, dims = [0, 2, 3] : (tensor<1x1x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc574)
      %127 = stablehlo.add %102, %126 : tensor<1x16x1x256xbf16> loc(#loc575)
      %128 = stablehlo.reshape %arg483 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc18)
      %129 = "stablehlo.all_to_all"(%128) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc18)
      %130 = stablehlo.slice %129 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc18)
      %131 = stablehlo.reshape %130 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc18)
      %132 = stablehlo.reshape %131 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc576)
      %133 = stablehlo.reshape %132 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc577)
      %134 = stablehlo.concatenate %127, %133, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc578)
      %135 = stablehlo.reshape %arg494 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc579)
      %136 = stablehlo.reshape %135 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc580)
      %137 = stablehlo.convert %136 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc581)
      %138 = stablehlo.reshape %137 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc582)
      %139 = stablehlo.reduce(%134 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc583)
      %140 = stablehlo.broadcast_in_dim %139, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc584)
      %141 = stablehlo.subtract %134, %140 : tensor<1x16x1x257xbf16> loc(#loc585)
      %142 = stablehlo.reduce(%141 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc586)
      %143 = stablehlo.broadcast_in_dim %142, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc587)
      %144 = stablehlo.subtract %141, %143 : tensor<1x16x1x257xbf16> loc(#loc588)
      %145 = stablehlo.exponential %144 : tensor<1x16x1x257xbf16> loc(#loc589)
      %146 = stablehlo.reduce(%145 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc590)
      %147 = stablehlo.broadcast_in_dim %146, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc591)
      %148 = stablehlo.divide %145, %147 : tensor<1x16x1x257xbf16> loc(#loc592)
      %149 = stablehlo.slice %148 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc593)
      %150 = stablehlo.reshape %arg475 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc594)
      %151 = stablehlo.reshape %150 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc595)
      %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc596)
      %153 = stablehlo.dot_general %37, %152, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc597)
      %154 = "stablehlo.all_reduce"(%153) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.153"), %arg933: tensor<bf16> loc("dot.153")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc597)
        stablehlo.return %4605 : tensor<bf16> loc(#loc597)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc597)
      %155 = stablehlo.reshape %154 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc598)
      %156 = stablehlo.reshape %arg474 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc599)
      %157 = stablehlo.add %155, %156 : tensor<1x1x128xbf16> loc(#loc600)
      %158 = stablehlo.reshape %157 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc601)
      %159 = "stablehlo.scatter"(%arg476, %75, %158) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.177"), %arg933: tensor<bf16> loc("scatter.177")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc602)
      %160 = stablehlo.broadcast_in_dim %159, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc603)
      %161 = stablehlo.reshape %160 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc604)
      %162 = stablehlo.dot_general %149, %161, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc605)
      %163 = stablehlo.reshape %162 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc606)
      %164 = stablehlo.reshape %arg482 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc607)
      %165 = stablehlo.reshape %164 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc608)
      %166 = stablehlo.transpose %165, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc609)
      %167 = stablehlo.dot_general %163, %166, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc610)
      %168 = "stablehlo.all_reduce"(%167) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.464"), %arg933: tensor<bf16> loc("dot.464")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc610)
        stablehlo.return %4605 : tensor<bf16> loc(#loc610)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc610)
      %169 = stablehlo.reshape %168 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc611)
      %170 = stablehlo.reshape %arg481 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc612)
      %171 = stablehlo.add %169, %170 : tensor<1x1x1440xbf16> loc(#loc613)
      %172 = stablehlo.add %22, %171 : tensor<1x1x1440xbf16> loc(#loc614)
      %173 = stablehlo.reshape %arg489 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc615)
      %174 = stablehlo.reshape %173 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc616)
      %175 = stablehlo.convert %174 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc617)
      %176 = stablehlo.reshape %175 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc618)
      %177 = stablehlo.convert %172 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc619)
      %178 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %179 = stablehlo.power %177, %178 : tensor<1x1x1440xf32> loc(#loc620)
      %180 = stablehlo.reduce(%179 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc621)
      %181 = "stablehlo.all_reduce"(%180) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.482"), %arg933: tensor<f32> loc("reduce.482")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc621)
        stablehlo.return %4605 : tensor<f32> loc(#loc621)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc621)
      %182 = stablehlo.multiply %181, %cst_1 : tensor<1x1xf32> loc(#loc622)
      %183 = stablehlo.reshape %182 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc623)
      %184 = stablehlo.add %183, %cst_2 : tensor<1x1x1xf32> loc(#loc624)
      %185 = stablehlo.rsqrt %184 : tensor<1x1x1xf32> loc(#loc625)
      %186 = stablehlo.reshape %185 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc626)
      %187 = stablehlo.broadcast_in_dim %186, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc627)
      %188 = stablehlo.multiply %177, %187 : tensor<1x1x1440xf32> loc(#loc628)
      %189 = stablehlo.multiply %176, %188 : tensor<1x1x1440xf32> loc(#loc629)
      %190 = stablehlo.convert %189 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc630)
      %191 = stablehlo.reshape %190 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc631)
      %192 = stablehlo.broadcast_in_dim %191, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc632)
      %193 = stablehlo.dot_general %192, %arg493, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc633)
      %194 = "stablehlo.all_reduce"(%193) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.596"), %arg933: tensor<bf16> loc("dot.596")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc633)
        stablehlo.return %4605 : tensor<bf16> loc(#loc633)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc633)
      %195 = stablehlo.reshape %arg492 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc634)
      %196 = stablehlo.reshape %195 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc635)
      %197 = stablehlo.add %194, %196 : tensor<8x1x5760xbf16> loc(#loc636)
      %198 = stablehlo.slice %197 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc637)
      %199 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %200 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %201 = stablehlo.clamp %200, %198, %199 : tensor<8x1x2880xbf16> loc(#loc638)
      %202 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %203 = stablehlo.add %201, %202 : tensor<8x1x2880xbf16> loc(#loc639)
      %204 = stablehlo.slice %197 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc640)
      %205 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %206 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %207 = stablehlo.clamp %205, %204, %206 : tensor<8x1x2880xbf16> loc(#loc641)
      %208 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %209 = stablehlo.multiply %207, %208 : tensor<8x1x2880xbf16> loc(#loc642)
      %210 = stablehlo.logistic %209 : tensor<8x1x2880xbf16> loc(#loc643)
      %211 = stablehlo.multiply %207, %210 : tensor<8x1x2880xbf16> loc(#loc644)
      %212 = stablehlo.multiply %203, %211 : tensor<8x1x2880xbf16> loc(#loc645)
      %213 = stablehlo.dot_general %212, %arg491, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc646)
      %214 = stablehlo.reshape %arg490 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc647)
      %215 = stablehlo.reshape %214 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc648)
      %216 = stablehlo.add %213, %215 : tensor<8x1x1440xbf16> loc(#loc649)
      %217 = stablehlo.reshape %216 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc650)
      %218 = stablehlo.convert %191 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc651)
      %219 = stablehlo.reshape %arg480 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc652)
      %220 = stablehlo.reshape %219 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc653)
      %221 = stablehlo.transpose %220, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc654)
      %222 = stablehlo.convert %221 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc655)
      %223 = stablehlo.dot_general %218, %222, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc656)
      %224 = "stablehlo.all_reduce"(%223) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.511"), %arg933: tensor<f32> loc("dot.511")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc656)
        stablehlo.return %4605 : tensor<f32> loc(#loc656)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc656)
      %225 = stablehlo.reshape %arg479 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc657)
      %226 = stablehlo.reshape %225 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc658)
      %227 = stablehlo.convert %226 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc659)
      %228 = stablehlo.reshape %227 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc660)
      %229 = stablehlo.add %224, %228 : tensor<1x32xf32> loc(#loc661)
      %230 = stablehlo.convert %229 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc662)
      %231 = stablehlo.iota dim = 0 : tensor<32xi32> loc(#loc663)
      %232 = stablehlo.reshape %231 : (tensor<32xi32>) -> tensor<1x32xi32> loc(#loc663)
      %233:2 = "stablehlo.sort"(%230, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.532"), %arg933: tensor<bf16> loc("sort.532"), %arg934: tensor<i32> loc("sort.532"), %arg935: tensor<i32> loc("sort.532")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc665)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc664)
      %234 = stablehlo.slice %233#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc666)
      %235 = stablehlo.convert %234 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc667)
      %236 = stablehlo.reshape %235 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc668)
      %237 = stablehlo.concatenate %c_16, %236, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc669)
      %238 = stablehlo.slice %233#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc670)
      %239 = stablehlo.reduce(%238 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc671)
      %240 = stablehlo.broadcast_in_dim %239, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc672)
      %241 = stablehlo.subtract %238, %240 : tensor<1x4xbf16> loc(#loc673)
      %242 = stablehlo.exponential %241 : tensor<1x4xbf16> loc(#loc674)
      %243 = stablehlo.reduce(%242 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc675)
      %244 = stablehlo.broadcast_in_dim %243, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc676)
      %245 = stablehlo.divide %242, %244 : tensor<1x4xbf16> loc(#loc677)
      %246 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %247 = "stablehlo.all_gather"(%246) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %248 = "stablehlo.scatter"(%247, %237, %245) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.566"), %arg933: tensor<bf16> loc("scatter.566")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc678)
      %249 = stablehlo.reshape %248 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc678)
      %250 = "stablehlo.all_to_all"(%249) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc678)
      %251 = stablehlo.slice %250 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc678)
      %252 = stablehlo.reshape %251 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc678)
      %253 = stablehlo.reshape %252 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc679)
      %254 = stablehlo.broadcast_in_dim %253, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc680)
      %255 = stablehlo.multiply %217, %254 : tensor<8x1x1x1440xbf16> loc(#loc681)
      %256 = stablehlo.reduce(%255 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc682)
      %257 = "stablehlo.all_reduce"(%256) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.634"), %arg933: tensor<bf16> loc("reduce.634")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc682)
        stablehlo.return %4605 : tensor<bf16> loc(#loc682)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc682)
      %258 = stablehlo.add %172, %257 : tensor<1x1x1440xbf16> loc(#loc683)
      %259 = stablehlo.convert %258 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc684)
      %260 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %261 = stablehlo.power %259, %260 : tensor<1x1x1440xf32> loc(#loc685)
      %262 = stablehlo.reduce(%261 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc686)
      %263 = "stablehlo.all_reduce"(%262) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.647"), %arg933: tensor<f32> loc("reduce.647")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc686)
        stablehlo.return %4605 : tensor<f32> loc(#loc686)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc686)
      %264 = stablehlo.multiply %263, %cst_1 : tensor<1x1xf32> loc(#loc687)
      %265 = stablehlo.reshape %264 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc688)
      %266 = stablehlo.add %265, %cst_2 : tensor<1x1x1xf32> loc(#loc689)
      %267 = stablehlo.rsqrt %266 : tensor<1x1x1xf32> loc(#loc690)
      %268 = stablehlo.reshape %267 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc691)
      %269 = stablehlo.broadcast_in_dim %268, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc692)
      %270 = stablehlo.multiply %259, %269 : tensor<1x1x1440xf32> loc(#loc693)
      %271 = stablehlo.multiply %138, %270 : tensor<1x1x1440xf32> loc(#loc694)
      %272 = stablehlo.convert %271 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc695)
      %273 = stablehlo.reshape %272 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc696)
      %274 = stablehlo.reshape %arg508 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc697)
      %275 = stablehlo.reshape %274 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc698)
      %276 = stablehlo.transpose %275, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc699)
      %277 = stablehlo.dot_general %273, %276, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc700)
      %278 = "stablehlo.all_reduce"(%277) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.913"), %arg933: tensor<bf16> loc("dot.913")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc700)
        stablehlo.return %4605 : tensor<bf16> loc(#loc700)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc700)
      %279 = stablehlo.reshape %278 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc701)
      %280 = stablehlo.reshape %arg507 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc702)
      %281 = stablehlo.add %279, %280 : tensor<1x1x1024xbf16> loc(#loc703)
      %282 = stablehlo.reshape %281 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc704)
      %283 = stablehlo.slice %282 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc705)
      %284 = stablehlo.multiply %283, %59 : tensor<1x16x1x32xbf16> loc(#loc706)
      %285 = stablehlo.slice %282 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc707)
      %286 = stablehlo.multiply %285, %65 : tensor<1x16x1x32xbf16> loc(#loc708)
      %287 = stablehlo.subtract %284, %286 : tensor<1x16x1x32xbf16> loc(#loc709)
      %288 = stablehlo.multiply %285, %59 : tensor<1x16x1x32xbf16> loc(#loc710)
      %289 = stablehlo.multiply %283, %65 : tensor<1x16x1x32xbf16> loc(#loc711)
      %290 = stablehlo.add %288, %289 : tensor<1x16x1x32xbf16> loc(#loc712)
      %291 = stablehlo.concatenate %287, %290, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc713)
      %292 = stablehlo.reshape %arg478 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc714)
      %293 = stablehlo.reshape %292 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc715)
      %294 = stablehlo.transpose %293, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc716)
      %295 = stablehlo.dot_general %273, %294, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc717)
      %296 = "stablehlo.all_reduce"(%295) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.675"), %arg933: tensor<bf16> loc("dot.675")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc717)
        stablehlo.return %4605 : tensor<bf16> loc(#loc717)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc717)
      %297 = stablehlo.reshape %296 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc718)
      %298 = stablehlo.reshape %arg477 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc719)
      %299 = stablehlo.add %297, %298 : tensor<1x1x128xbf16> loc(#loc720)
      %300 = stablehlo.reshape %299 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc721)
      %301 = stablehlo.slice %300 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc722)
      %302 = stablehlo.multiply %301, %86 : tensor<1x2x1x32xbf16> loc(#loc723)
      %303 = stablehlo.slice %300 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc724)
      %304 = stablehlo.multiply %303, %89 : tensor<1x2x1x32xbf16> loc(#loc725)
      %305 = stablehlo.subtract %302, %304 : tensor<1x2x1x32xbf16> loc(#loc726)
      %306 = stablehlo.multiply %303, %86 : tensor<1x2x1x32xbf16> loc(#loc727)
      %307 = stablehlo.multiply %301, %89 : tensor<1x2x1x32xbf16> loc(#loc728)
      %308 = stablehlo.add %306, %307 : tensor<1x2x1x32xbf16> loc(#loc729)
      %309 = stablehlo.concatenate %305, %308, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc730)
      %310 = "stablehlo.scatter"(%arg495, %75, %309) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.722"), %arg933: tensor<bf16> loc("scatter.722")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc731)
      %311 = stablehlo.broadcast_in_dim %310, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc732)
      %312 = stablehlo.reshape %311 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc733)
      %313 = stablehlo.transpose %312, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc734)
      %314 = stablehlo.dot_general %291, %313, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc735)
      %315 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %316 = stablehlo.multiply %314, %315 : tensor<1x16x1x256xbf16> loc(#loc736)
      %317 = stablehlo.and %104, %110 : tensor<1x256xi1> loc(#loc737)
      %318 = stablehlo.reshape %317 : (tensor<1x256xi1>) -> tensor<1x1x256xi1> loc(#loc738)
      %319 = stablehlo.and %318, %118 : tensor<1x1x256xi1> loc(#loc739)
      %320 = stablehlo.reshape %319 : (tensor<1x1x256xi1>) -> tensor<1x1x1x256xi1> loc(#loc740)
      %321 = stablehlo.reshape %arg506 : (tensor<bf16>) -> tensor<1x1x1xbf16> loc(#loc741)
      %322 = stablehlo.broadcast_in_dim %321, dims = [0, 1, 2] : (tensor<1x1x1xbf16>) -> tensor<1x1x1x256xbf16> loc(#loc742)
      %323 = stablehlo.select %320, %322, %7 : tensor<1x1x1x256xi1>, tensor<1x1x1x256xbf16> loc(#loc743)
      %324 = stablehlo.reshape %323 : (tensor<1x1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc744)
      %325 = stablehlo.broadcast_in_dim %324, dims = [0, 2, 3] : (tensor<1x1x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc745)
      %326 = stablehlo.add %316, %325 : tensor<1x16x1x256xbf16> loc(#loc746)
      %327 = stablehlo.reshape %arg505 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc40)
      %328 = "stablehlo.all_to_all"(%327) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc40)
      %329 = stablehlo.slice %328 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc40)
      %330 = stablehlo.reshape %329 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc40)
      %331 = stablehlo.reshape %330 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc747)
      %332 = stablehlo.reshape %331 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc748)
      %333 = stablehlo.concatenate %326, %332, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc749)
      %334 = stablehlo.reshape %arg514 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc750)
      %335 = stablehlo.reshape %334 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc751)
      %336 = stablehlo.convert %335 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc752)
      %337 = stablehlo.reshape %336 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc753)
      %338 = stablehlo.reduce(%333 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc754)
      %339 = stablehlo.broadcast_in_dim %338, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc755)
      %340 = stablehlo.subtract %333, %339 : tensor<1x16x1x257xbf16> loc(#loc756)
      %341 = stablehlo.reduce(%340 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc757)
      %342 = stablehlo.broadcast_in_dim %341, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc758)
      %343 = stablehlo.subtract %340, %342 : tensor<1x16x1x257xbf16> loc(#loc759)
      %344 = stablehlo.exponential %343 : tensor<1x16x1x257xbf16> loc(#loc760)
      %345 = stablehlo.reduce(%344 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc761)
      %346 = stablehlo.broadcast_in_dim %345, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc762)
      %347 = stablehlo.divide %344, %346 : tensor<1x16x1x257xbf16> loc(#loc763)
      %348 = stablehlo.slice %347 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc764)
      %349 = stablehlo.reshape %arg497 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc765)
      %350 = stablehlo.reshape %349 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc766)
      %351 = stablehlo.transpose %350, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc767)
      %352 = stablehlo.dot_general %273, %351, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc768)
      %353 = "stablehlo.all_reduce"(%352) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.735"), %arg933: tensor<bf16> loc("dot.735")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc768)
        stablehlo.return %4605 : tensor<bf16> loc(#loc768)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc768)
      %354 = stablehlo.reshape %353 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc769)
      %355 = stablehlo.reshape %arg496 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc770)
      %356 = stablehlo.add %354, %355 : tensor<1x1x128xbf16> loc(#loc771)
      %357 = stablehlo.reshape %356 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc772)
      %358 = "stablehlo.scatter"(%arg498, %75, %357) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.759"), %arg933: tensor<bf16> loc("scatter.759")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc773)
      %359 = stablehlo.broadcast_in_dim %358, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc774)
      %360 = stablehlo.reshape %359 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc775)
      %361 = stablehlo.dot_general %348, %360, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc776)
      %362 = stablehlo.reshape %361 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc777)
      %363 = stablehlo.reshape %arg504 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc778)
      %364 = stablehlo.reshape %363 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc779)
      %365 = stablehlo.transpose %364, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc780)
      %366 = stablehlo.dot_general %362, %365, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc781)
      %367 = "stablehlo.all_reduce"(%366) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1020"), %arg933: tensor<bf16> loc("dot.1020")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc781)
        stablehlo.return %4605 : tensor<bf16> loc(#loc781)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc781)
      %368 = stablehlo.reshape %367 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc782)
      %369 = stablehlo.reshape %arg503 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc783)
      %370 = stablehlo.add %368, %369 : tensor<1x1x1440xbf16> loc(#loc784)
      %371 = stablehlo.add %258, %370 : tensor<1x1x1440xbf16> loc(#loc785)
      %372 = stablehlo.reshape %arg509 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc786)
      %373 = stablehlo.reshape %372 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc787)
      %374 = stablehlo.convert %373 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc788)
      %375 = stablehlo.reshape %374 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc789)
      %376 = stablehlo.convert %371 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc790)
      %377 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %378 = stablehlo.power %376, %377 : tensor<1x1x1440xf32> loc(#loc791)
      %379 = stablehlo.reduce(%378 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc792)
      %380 = "stablehlo.all_reduce"(%379) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.1038"), %arg933: tensor<f32> loc("reduce.1038")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc792)
        stablehlo.return %4605 : tensor<f32> loc(#loc792)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc792)
      %381 = stablehlo.multiply %380, %cst_1 : tensor<1x1xf32> loc(#loc793)
      %382 = stablehlo.reshape %381 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc794)
      %383 = stablehlo.add %382, %cst_2 : tensor<1x1x1xf32> loc(#loc795)
      %384 = stablehlo.rsqrt %383 : tensor<1x1x1xf32> loc(#loc796)
      %385 = stablehlo.reshape %384 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc797)
      %386 = stablehlo.broadcast_in_dim %385, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc798)
      %387 = stablehlo.multiply %376, %386 : tensor<1x1x1440xf32> loc(#loc799)
      %388 = stablehlo.multiply %375, %387 : tensor<1x1x1440xf32> loc(#loc800)
      %389 = stablehlo.convert %388 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc801)
      %390 = stablehlo.reshape %389 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc802)
      %391 = stablehlo.broadcast_in_dim %390, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc803)
      %392 = stablehlo.dot_general %391, %arg513, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc804)
      %393 = "stablehlo.all_reduce"(%392) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1152"), %arg933: tensor<bf16> loc("dot.1152")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc804)
        stablehlo.return %4605 : tensor<bf16> loc(#loc804)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc804)
      %394 = stablehlo.reshape %arg512 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc805)
      %395 = stablehlo.reshape %394 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc806)
      %396 = stablehlo.add %393, %395 : tensor<8x1x5760xbf16> loc(#loc807)
      %397 = stablehlo.slice %396 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc808)
      %398 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %399 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %400 = stablehlo.clamp %399, %397, %398 : tensor<8x1x2880xbf16> loc(#loc809)
      %401 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %402 = stablehlo.add %400, %401 : tensor<8x1x2880xbf16> loc(#loc810)
      %403 = stablehlo.slice %396 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc811)
      %404 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %405 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %406 = stablehlo.clamp %404, %403, %405 : tensor<8x1x2880xbf16> loc(#loc812)
      %407 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %408 = stablehlo.multiply %406, %407 : tensor<8x1x2880xbf16> loc(#loc813)
      %409 = stablehlo.logistic %408 : tensor<8x1x2880xbf16> loc(#loc814)
      %410 = stablehlo.multiply %406, %409 : tensor<8x1x2880xbf16> loc(#loc815)
      %411 = stablehlo.multiply %402, %410 : tensor<8x1x2880xbf16> loc(#loc816)
      %412 = stablehlo.dot_general %411, %arg511, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc817)
      %413 = stablehlo.reshape %arg510 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc818)
      %414 = stablehlo.reshape %413 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc819)
      %415 = stablehlo.add %412, %414 : tensor<8x1x1440xbf16> loc(#loc820)
      %416 = stablehlo.reshape %415 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc821)
      %417 = stablehlo.convert %390 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc822)
      %418 = stablehlo.reshape %arg502 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc823)
      %419 = stablehlo.reshape %418 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc824)
      %420 = stablehlo.transpose %419, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc825)
      %421 = stablehlo.convert %420 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc826)
      %422 = stablehlo.dot_general %417, %421, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc827)
      %423 = "stablehlo.all_reduce"(%422) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.1067"), %arg933: tensor<f32> loc("dot.1067")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc827)
        stablehlo.return %4605 : tensor<f32> loc(#loc827)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc827)
      %424 = stablehlo.reshape %arg501 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc828)
      %425 = stablehlo.reshape %424 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc829)
      %426 = stablehlo.convert %425 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc830)
      %427 = stablehlo.reshape %426 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc831)
      %428 = stablehlo.add %423, %427 : tensor<1x32xf32> loc(#loc832)
      %429 = stablehlo.convert %428 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc833)
      %430:2 = "stablehlo.sort"(%429, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.1088"), %arg933: tensor<bf16> loc("sort.1088"), %arg934: tensor<i32> loc("sort.1088"), %arg935: tensor<i32> loc("sort.1088")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc835)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc834)
      %431 = stablehlo.slice %430#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc836)
      %432 = stablehlo.convert %431 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc837)
      %433 = stablehlo.reshape %432 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc838)
      %434 = stablehlo.concatenate %c_16, %433, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc839)
      %435 = stablehlo.slice %430#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc840)
      %436 = stablehlo.reduce(%435 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc841)
      %437 = stablehlo.broadcast_in_dim %436, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc842)
      %438 = stablehlo.subtract %435, %437 : tensor<1x4xbf16> loc(#loc843)
      %439 = stablehlo.exponential %438 : tensor<1x4xbf16> loc(#loc844)
      %440 = stablehlo.reduce(%439 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc845)
      %441 = stablehlo.broadcast_in_dim %440, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc846)
      %442 = stablehlo.divide %439, %441 : tensor<1x4xbf16> loc(#loc847)
      %443 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %444 = "stablehlo.all_gather"(%443) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %445 = "stablehlo.scatter"(%444, %434, %442) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.1122"), %arg933: tensor<bf16> loc("scatter.1122")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc848)
      %446 = stablehlo.reshape %445 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc848)
      %447 = "stablehlo.all_to_all"(%446) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc848)
      %448 = stablehlo.slice %447 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc848)
      %449 = stablehlo.reshape %448 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc848)
      %450 = stablehlo.reshape %449 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc849)
      %451 = stablehlo.broadcast_in_dim %450, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc850)
      %452 = stablehlo.multiply %416, %451 : tensor<8x1x1x1440xbf16> loc(#loc851)
      %453 = stablehlo.reduce(%452 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc852)
      %454 = "stablehlo.all_reduce"(%453) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.1190"), %arg933: tensor<bf16> loc("reduce.1190")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc852)
        stablehlo.return %4605 : tensor<bf16> loc(#loc852)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc852)
      %455 = stablehlo.add %371, %454 : tensor<1x1x1440xbf16> loc(#loc853)
      %456 = stablehlo.convert %455 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc854)
      %457 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %458 = stablehlo.power %456, %457 : tensor<1x1x1440xf32> loc(#loc855)
      %459 = stablehlo.reduce(%458 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc856)
      %460 = "stablehlo.all_reduce"(%459) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.1203"), %arg933: tensor<f32> loc("reduce.1203")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc856)
        stablehlo.return %4605 : tensor<f32> loc(#loc856)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc856)
      %461 = stablehlo.multiply %460, %cst_1 : tensor<1x1xf32> loc(#loc857)
      %462 = stablehlo.reshape %461 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc858)
      %463 = stablehlo.add %462, %cst_2 : tensor<1x1x1xf32> loc(#loc859)
      %464 = stablehlo.rsqrt %463 : tensor<1x1x1xf32> loc(#loc860)
      %465 = stablehlo.reshape %464 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc861)
      %466 = stablehlo.broadcast_in_dim %465, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc862)
      %467 = stablehlo.multiply %456, %466 : tensor<1x1x1440xf32> loc(#loc863)
      %468 = stablehlo.multiply %337, %467 : tensor<1x1x1440xf32> loc(#loc864)
      %469 = stablehlo.convert %468 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc865)
      %470 = stablehlo.reshape %469 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc866)
      %471 = stablehlo.reshape %arg527 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc867)
      %472 = stablehlo.reshape %471 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc868)
      %473 = stablehlo.transpose %472, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc869)
      %474 = stablehlo.dot_general %470, %473, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc870)
      %475 = "stablehlo.all_reduce"(%474) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1395"), %arg933: tensor<bf16> loc("dot.1395")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc870)
        stablehlo.return %4605 : tensor<bf16> loc(#loc870)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc870)
      %476 = stablehlo.reshape %475 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc871)
      %477 = stablehlo.reshape %arg526 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc872)
      %478 = stablehlo.add %476, %477 : tensor<1x1x1024xbf16> loc(#loc873)
      %479 = stablehlo.reshape %478 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc874)
      %480 = stablehlo.slice %479 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc875)
      %481 = stablehlo.multiply %480, %59 : tensor<1x16x1x32xbf16> loc(#loc876)
      %482 = stablehlo.slice %479 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc877)
      %483 = stablehlo.multiply %482, %65 : tensor<1x16x1x32xbf16> loc(#loc878)
      %484 = stablehlo.subtract %481, %483 : tensor<1x16x1x32xbf16> loc(#loc879)
      %485 = stablehlo.multiply %482, %59 : tensor<1x16x1x32xbf16> loc(#loc880)
      %486 = stablehlo.multiply %480, %65 : tensor<1x16x1x32xbf16> loc(#loc881)
      %487 = stablehlo.add %485, %486 : tensor<1x16x1x32xbf16> loc(#loc882)
      %488 = stablehlo.concatenate %484, %487, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc883)
      %489 = stablehlo.reshape %arg500 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc884)
      %490 = stablehlo.reshape %489 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc885)
      %491 = stablehlo.transpose %490, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc886)
      %492 = stablehlo.dot_general %470, %491, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc887)
      %493 = "stablehlo.all_reduce"(%492) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1231"), %arg933: tensor<bf16> loc("dot.1231")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc887)
        stablehlo.return %4605 : tensor<bf16> loc(#loc887)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc887)
      %494 = stablehlo.reshape %493 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc888)
      %495 = stablehlo.reshape %arg499 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc889)
      %496 = stablehlo.add %494, %495 : tensor<1x1x128xbf16> loc(#loc890)
      %497 = stablehlo.reshape %496 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc891)
      %498 = stablehlo.slice %497 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc892)
      %499 = stablehlo.multiply %498, %86 : tensor<1x2x1x32xbf16> loc(#loc893)
      %500 = stablehlo.slice %497 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc894)
      %501 = stablehlo.multiply %500, %89 : tensor<1x2x1x32xbf16> loc(#loc895)
      %502 = stablehlo.subtract %499, %501 : tensor<1x2x1x32xbf16> loc(#loc896)
      %503 = stablehlo.multiply %500, %86 : tensor<1x2x1x32xbf16> loc(#loc897)
      %504 = stablehlo.multiply %498, %89 : tensor<1x2x1x32xbf16> loc(#loc898)
      %505 = stablehlo.add %503, %504 : tensor<1x2x1x32xbf16> loc(#loc899)
      %506 = stablehlo.concatenate %502, %505, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc900)
      %507 = "stablehlo.scatter"(%arg515, %75, %506) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.1278"), %arg933: tensor<bf16> loc("scatter.1278")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc901)
      %508 = stablehlo.broadcast_in_dim %507, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc902)
      %509 = stablehlo.reshape %508 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc903)
      %510 = stablehlo.transpose %509, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc904)
      %511 = stablehlo.dot_general %488, %510, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc905)
      %512 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %513 = stablehlo.multiply %511, %512 : tensor<1x16x1x256xbf16> loc(#loc906)
      %514 = stablehlo.add %513, %126 : tensor<1x16x1x256xbf16> loc(#loc907)
      %515 = stablehlo.reshape %arg525 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc60)
      %516 = "stablehlo.all_to_all"(%515) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc60)
      %517 = stablehlo.slice %516 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc60)
      %518 = stablehlo.reshape %517 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc60)
      %519 = stablehlo.reshape %518 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc908)
      %520 = stablehlo.reshape %519 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc909)
      %521 = stablehlo.concatenate %514, %520, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc910)
      %522 = stablehlo.reshape %arg533 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc911)
      %523 = stablehlo.reshape %522 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc912)
      %524 = stablehlo.convert %523 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc913)
      %525 = stablehlo.reshape %524 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc914)
      %526 = stablehlo.reduce(%521 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc915)
      %527 = stablehlo.broadcast_in_dim %526, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc916)
      %528 = stablehlo.subtract %521, %527 : tensor<1x16x1x257xbf16> loc(#loc917)
      %529 = stablehlo.reduce(%528 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc918)
      %530 = stablehlo.broadcast_in_dim %529, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc919)
      %531 = stablehlo.subtract %528, %530 : tensor<1x16x1x257xbf16> loc(#loc920)
      %532 = stablehlo.exponential %531 : tensor<1x16x1x257xbf16> loc(#loc921)
      %533 = stablehlo.reduce(%532 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc922)
      %534 = stablehlo.broadcast_in_dim %533, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc923)
      %535 = stablehlo.divide %532, %534 : tensor<1x16x1x257xbf16> loc(#loc924)
      %536 = stablehlo.slice %535 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc925)
      %537 = stablehlo.reshape %arg517 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc926)
      %538 = stablehlo.reshape %537 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc927)
      %539 = stablehlo.transpose %538, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc928)
      %540 = stablehlo.dot_general %470, %539, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc929)
      %541 = "stablehlo.all_reduce"(%540) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1291"), %arg933: tensor<bf16> loc("dot.1291")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc929)
        stablehlo.return %4605 : tensor<bf16> loc(#loc929)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc929)
      %542 = stablehlo.reshape %541 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc930)
      %543 = stablehlo.reshape %arg516 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc931)
      %544 = stablehlo.add %542, %543 : tensor<1x1x128xbf16> loc(#loc932)
      %545 = stablehlo.reshape %544 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc933)
      %546 = "stablehlo.scatter"(%arg518, %75, %545) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.1315"), %arg933: tensor<bf16> loc("scatter.1315")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc934)
      %547 = stablehlo.broadcast_in_dim %546, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc935)
      %548 = stablehlo.reshape %547 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc936)
      %549 = stablehlo.dot_general %536, %548, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc937)
      %550 = stablehlo.reshape %549 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc938)
      %551 = stablehlo.reshape %arg524 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc939)
      %552 = stablehlo.reshape %551 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc940)
      %553 = stablehlo.transpose %552, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc941)
      %554 = stablehlo.dot_general %550, %553, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc942)
      %555 = "stablehlo.all_reduce"(%554) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1502"), %arg933: tensor<bf16> loc("dot.1502")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc942)
        stablehlo.return %4605 : tensor<bf16> loc(#loc942)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc942)
      %556 = stablehlo.reshape %555 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc943)
      %557 = stablehlo.reshape %arg523 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc944)
      %558 = stablehlo.add %556, %557 : tensor<1x1x1440xbf16> loc(#loc945)
      %559 = stablehlo.add %455, %558 : tensor<1x1x1440xbf16> loc(#loc946)
      %560 = stablehlo.reshape %arg528 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc947)
      %561 = stablehlo.reshape %560 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc948)
      %562 = stablehlo.convert %561 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc949)
      %563 = stablehlo.reshape %562 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc950)
      %564 = stablehlo.convert %559 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc951)
      %565 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %566 = stablehlo.power %564, %565 : tensor<1x1x1440xf32> loc(#loc952)
      %567 = stablehlo.reduce(%566 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc953)
      %568 = "stablehlo.all_reduce"(%567) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.1520"), %arg933: tensor<f32> loc("reduce.1520")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc953)
        stablehlo.return %4605 : tensor<f32> loc(#loc953)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc953)
      %569 = stablehlo.multiply %568, %cst_1 : tensor<1x1xf32> loc(#loc954)
      %570 = stablehlo.reshape %569 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc955)
      %571 = stablehlo.add %570, %cst_2 : tensor<1x1x1xf32> loc(#loc956)
      %572 = stablehlo.rsqrt %571 : tensor<1x1x1xf32> loc(#loc957)
      %573 = stablehlo.reshape %572 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc958)
      %574 = stablehlo.broadcast_in_dim %573, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc959)
      %575 = stablehlo.multiply %564, %574 : tensor<1x1x1440xf32> loc(#loc960)
      %576 = stablehlo.multiply %563, %575 : tensor<1x1x1440xf32> loc(#loc961)
      %577 = stablehlo.convert %576 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc962)
      %578 = stablehlo.reshape %577 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc963)
      %579 = stablehlo.broadcast_in_dim %578, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc964)
      %580 = stablehlo.dot_general %579, %arg532, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc965)
      %581 = "stablehlo.all_reduce"(%580) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1634"), %arg933: tensor<bf16> loc("dot.1634")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc965)
        stablehlo.return %4605 : tensor<bf16> loc(#loc965)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc965)
      %582 = stablehlo.reshape %arg531 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc966)
      %583 = stablehlo.reshape %582 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc967)
      %584 = stablehlo.add %581, %583 : tensor<8x1x5760xbf16> loc(#loc968)
      %585 = stablehlo.slice %584 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc969)
      %586 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %587 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %588 = stablehlo.clamp %587, %585, %586 : tensor<8x1x2880xbf16> loc(#loc970)
      %589 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %590 = stablehlo.add %588, %589 : tensor<8x1x2880xbf16> loc(#loc971)
      %591 = stablehlo.slice %584 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc972)
      %592 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %593 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %594 = stablehlo.clamp %592, %591, %593 : tensor<8x1x2880xbf16> loc(#loc973)
      %595 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %596 = stablehlo.multiply %594, %595 : tensor<8x1x2880xbf16> loc(#loc974)
      %597 = stablehlo.logistic %596 : tensor<8x1x2880xbf16> loc(#loc975)
      %598 = stablehlo.multiply %594, %597 : tensor<8x1x2880xbf16> loc(#loc976)
      %599 = stablehlo.multiply %590, %598 : tensor<8x1x2880xbf16> loc(#loc977)
      %600 = stablehlo.dot_general %599, %arg530, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc978)
      %601 = stablehlo.reshape %arg529 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc979)
      %602 = stablehlo.reshape %601 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc980)
      %603 = stablehlo.add %600, %602 : tensor<8x1x1440xbf16> loc(#loc981)
      %604 = stablehlo.reshape %603 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc982)
      %605 = stablehlo.convert %578 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc983)
      %606 = stablehlo.reshape %arg522 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc984)
      %607 = stablehlo.reshape %606 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc985)
      %608 = stablehlo.transpose %607, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc986)
      %609 = stablehlo.convert %608 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc987)
      %610 = stablehlo.dot_general %605, %609, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc988)
      %611 = "stablehlo.all_reduce"(%610) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.1549"), %arg933: tensor<f32> loc("dot.1549")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc988)
        stablehlo.return %4605 : tensor<f32> loc(#loc988)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc988)
      %612 = stablehlo.reshape %arg521 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc989)
      %613 = stablehlo.reshape %612 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc990)
      %614 = stablehlo.convert %613 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc991)
      %615 = stablehlo.reshape %614 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc992)
      %616 = stablehlo.add %611, %615 : tensor<1x32xf32> loc(#loc993)
      %617 = stablehlo.convert %616 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc994)
      %618:2 = "stablehlo.sort"(%617, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.1570"), %arg933: tensor<bf16> loc("sort.1570"), %arg934: tensor<i32> loc("sort.1570"), %arg935: tensor<i32> loc("sort.1570")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc996)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc995)
      %619 = stablehlo.slice %618#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc997)
      %620 = stablehlo.convert %619 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc998)
      %621 = stablehlo.reshape %620 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc999)
      %622 = stablehlo.concatenate %c_16, %621, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc1000)
      %623 = stablehlo.slice %618#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc1001)
      %624 = stablehlo.reduce(%623 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1002)
      %625 = stablehlo.broadcast_in_dim %624, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1003)
      %626 = stablehlo.subtract %623, %625 : tensor<1x4xbf16> loc(#loc1004)
      %627 = stablehlo.exponential %626 : tensor<1x4xbf16> loc(#loc1005)
      %628 = stablehlo.reduce(%627 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1006)
      %629 = stablehlo.broadcast_in_dim %628, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1007)
      %630 = stablehlo.divide %627, %629 : tensor<1x4xbf16> loc(#loc1008)
      %631 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %632 = "stablehlo.all_gather"(%631) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %633 = "stablehlo.scatter"(%632, %622, %630) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.1604"), %arg933: tensor<bf16> loc("scatter.1604")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc1009)
      %634 = stablehlo.reshape %633 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc1009)
      %635 = "stablehlo.all_to_all"(%634) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc1009)
      %636 = stablehlo.slice %635 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc1009)
      %637 = stablehlo.reshape %636 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc1009)
      %638 = stablehlo.reshape %637 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc1010)
      %639 = stablehlo.broadcast_in_dim %638, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1011)
      %640 = stablehlo.multiply %604, %639 : tensor<8x1x1x1440xbf16> loc(#loc1012)
      %641 = stablehlo.reduce(%640 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc1013)
      %642 = "stablehlo.all_reduce"(%641) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.1672"), %arg933: tensor<bf16> loc("reduce.1672")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1013)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1013)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1013)
      %643 = stablehlo.add %559, %642 : tensor<1x1x1440xbf16> loc(#loc1014)
      %644 = stablehlo.convert %643 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1015)
      %645 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %646 = stablehlo.power %644, %645 : tensor<1x1x1440xf32> loc(#loc1016)
      %647 = stablehlo.reduce(%646 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1017)
      %648 = "stablehlo.all_reduce"(%647) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.1685"), %arg933: tensor<f32> loc("reduce.1685")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1017)
        stablehlo.return %4605 : tensor<f32> loc(#loc1017)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1017)
      %649 = stablehlo.multiply %648, %cst_1 : tensor<1x1xf32> loc(#loc1018)
      %650 = stablehlo.reshape %649 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1019)
      %651 = stablehlo.add %650, %cst_2 : tensor<1x1x1xf32> loc(#loc1020)
      %652 = stablehlo.rsqrt %651 : tensor<1x1x1xf32> loc(#loc1021)
      %653 = stablehlo.reshape %652 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1022)
      %654 = stablehlo.broadcast_in_dim %653, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1023)
      %655 = stablehlo.multiply %644, %654 : tensor<1x1x1440xf32> loc(#loc1024)
      %656 = stablehlo.multiply %525, %655 : tensor<1x1x1440xf32> loc(#loc1025)
      %657 = stablehlo.convert %656 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1026)
      %658 = stablehlo.reshape %657 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1027)
      %659 = stablehlo.reshape %arg546 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc1028)
      %660 = stablehlo.reshape %659 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc1029)
      %661 = stablehlo.transpose %660, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc1030)
      %662 = stablehlo.dot_general %658, %661, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1031)
      %663 = "stablehlo.all_reduce"(%662) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1877"), %arg933: tensor<bf16> loc("dot.1877")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1031)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1031)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1031)
      %664 = stablehlo.reshape %663 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1032)
      %665 = stablehlo.reshape %arg545 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1033)
      %666 = stablehlo.add %664, %665 : tensor<1x1x1024xbf16> loc(#loc1034)
      %667 = stablehlo.reshape %666 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1035)
      %668 = stablehlo.slice %667 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1036)
      %669 = stablehlo.multiply %668, %59 : tensor<1x16x1x32xbf16> loc(#loc1037)
      %670 = stablehlo.slice %667 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1038)
      %671 = stablehlo.multiply %670, %65 : tensor<1x16x1x32xbf16> loc(#loc1039)
      %672 = stablehlo.subtract %669, %671 : tensor<1x16x1x32xbf16> loc(#loc1040)
      %673 = stablehlo.multiply %670, %59 : tensor<1x16x1x32xbf16> loc(#loc1041)
      %674 = stablehlo.multiply %668, %65 : tensor<1x16x1x32xbf16> loc(#loc1042)
      %675 = stablehlo.add %673, %674 : tensor<1x16x1x32xbf16> loc(#loc1043)
      %676 = stablehlo.concatenate %672, %675, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1044)
      %677 = stablehlo.reshape %arg520 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1045)
      %678 = stablehlo.reshape %677 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1046)
      %679 = stablehlo.transpose %678, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1047)
      %680 = stablehlo.dot_general %658, %679, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1048)
      %681 = "stablehlo.all_reduce"(%680) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1713"), %arg933: tensor<bf16> loc("dot.1713")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1048)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1048)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1048)
      %682 = stablehlo.reshape %681 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1049)
      %683 = stablehlo.reshape %arg519 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1050)
      %684 = stablehlo.add %682, %683 : tensor<1x1x128xbf16> loc(#loc1051)
      %685 = stablehlo.reshape %684 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1052)
      %686 = stablehlo.slice %685 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1053)
      %687 = stablehlo.multiply %686, %86 : tensor<1x2x1x32xbf16> loc(#loc1054)
      %688 = stablehlo.slice %685 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1055)
      %689 = stablehlo.multiply %688, %89 : tensor<1x2x1x32xbf16> loc(#loc1056)
      %690 = stablehlo.subtract %687, %689 : tensor<1x2x1x32xbf16> loc(#loc1057)
      %691 = stablehlo.multiply %688, %86 : tensor<1x2x1x32xbf16> loc(#loc1058)
      %692 = stablehlo.multiply %686, %89 : tensor<1x2x1x32xbf16> loc(#loc1059)
      %693 = stablehlo.add %691, %692 : tensor<1x2x1x32xbf16> loc(#loc1060)
      %694 = stablehlo.concatenate %690, %693, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1061)
      %695 = "stablehlo.scatter"(%arg534, %75, %694) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.1760"), %arg933: tensor<bf16> loc("scatter.1760")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1062)
      %696 = stablehlo.broadcast_in_dim %695, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1063)
      %697 = stablehlo.reshape %696 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1064)
      %698 = stablehlo.transpose %697, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc1065)
      %699 = stablehlo.dot_general %676, %698, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1066)
      %700 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %701 = stablehlo.multiply %699, %700 : tensor<1x16x1x256xbf16> loc(#loc1067)
      %702 = stablehlo.add %701, %325 : tensor<1x16x1x256xbf16> loc(#loc1068)
      %703 = stablehlo.reshape %arg544 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc79)
      %704 = "stablehlo.all_to_all"(%703) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc79)
      %705 = stablehlo.slice %704 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc79)
      %706 = stablehlo.reshape %705 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc79)
      %707 = stablehlo.reshape %706 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1069)
      %708 = stablehlo.reshape %707 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc1070)
      %709 = stablehlo.concatenate %702, %708, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1071)
      %710 = stablehlo.reshape %arg552 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1072)
      %711 = stablehlo.reshape %710 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1073)
      %712 = stablehlo.convert %711 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1074)
      %713 = stablehlo.reshape %712 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1075)
      %714 = stablehlo.reduce(%709 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1076)
      %715 = stablehlo.broadcast_in_dim %714, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1077)
      %716 = stablehlo.subtract %709, %715 : tensor<1x16x1x257xbf16> loc(#loc1078)
      %717 = stablehlo.reduce(%716 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1079)
      %718 = stablehlo.broadcast_in_dim %717, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1080)
      %719 = stablehlo.subtract %716, %718 : tensor<1x16x1x257xbf16> loc(#loc1081)
      %720 = stablehlo.exponential %719 : tensor<1x16x1x257xbf16> loc(#loc1082)
      %721 = stablehlo.reduce(%720 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1083)
      %722 = stablehlo.broadcast_in_dim %721, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1084)
      %723 = stablehlo.divide %720, %722 : tensor<1x16x1x257xbf16> loc(#loc1085)
      %724 = stablehlo.slice %723 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1086)
      %725 = stablehlo.reshape %arg536 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1087)
      %726 = stablehlo.reshape %725 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1088)
      %727 = stablehlo.transpose %726, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1089)
      %728 = stablehlo.dot_general %658, %727, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1090)
      %729 = "stablehlo.all_reduce"(%728) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1773"), %arg933: tensor<bf16> loc("dot.1773")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1090)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1090)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1090)
      %730 = stablehlo.reshape %729 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1091)
      %731 = stablehlo.reshape %arg535 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1092)
      %732 = stablehlo.add %730, %731 : tensor<1x1x128xbf16> loc(#loc1093)
      %733 = stablehlo.reshape %732 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1094)
      %734 = "stablehlo.scatter"(%arg537, %75, %733) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.1797"), %arg933: tensor<bf16> loc("scatter.1797")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1095)
      %735 = stablehlo.broadcast_in_dim %734, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1096)
      %736 = stablehlo.reshape %735 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1097)
      %737 = stablehlo.dot_general %724, %736, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1098)
      %738 = stablehlo.reshape %737 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc1099)
      %739 = stablehlo.reshape %arg543 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc1100)
      %740 = stablehlo.reshape %739 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc1101)
      %741 = stablehlo.transpose %740, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc1102)
      %742 = stablehlo.dot_general %738, %741, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1103)
      %743 = "stablehlo.all_reduce"(%742) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.1984"), %arg933: tensor<bf16> loc("dot.1984")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1103)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1103)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1103)
      %744 = stablehlo.reshape %743 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1104)
      %745 = stablehlo.reshape %arg542 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1105)
      %746 = stablehlo.add %744, %745 : tensor<1x1x1440xbf16> loc(#loc1106)
      %747 = stablehlo.add %643, %746 : tensor<1x1x1440xbf16> loc(#loc1107)
      %748 = stablehlo.reshape %arg547 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1108)
      %749 = stablehlo.reshape %748 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1109)
      %750 = stablehlo.convert %749 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1110)
      %751 = stablehlo.reshape %750 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1111)
      %752 = stablehlo.convert %747 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1112)
      %753 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %754 = stablehlo.power %752, %753 : tensor<1x1x1440xf32> loc(#loc1113)
      %755 = stablehlo.reduce(%754 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1114)
      %756 = "stablehlo.all_reduce"(%755) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.2002"), %arg933: tensor<f32> loc("reduce.2002")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1114)
        stablehlo.return %4605 : tensor<f32> loc(#loc1114)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1114)
      %757 = stablehlo.multiply %756, %cst_1 : tensor<1x1xf32> loc(#loc1115)
      %758 = stablehlo.reshape %757 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1116)
      %759 = stablehlo.add %758, %cst_2 : tensor<1x1x1xf32> loc(#loc1117)
      %760 = stablehlo.rsqrt %759 : tensor<1x1x1xf32> loc(#loc1118)
      %761 = stablehlo.reshape %760 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1119)
      %762 = stablehlo.broadcast_in_dim %761, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1120)
      %763 = stablehlo.multiply %752, %762 : tensor<1x1x1440xf32> loc(#loc1121)
      %764 = stablehlo.multiply %751, %763 : tensor<1x1x1440xf32> loc(#loc1122)
      %765 = stablehlo.convert %764 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1123)
      %766 = stablehlo.reshape %765 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1124)
      %767 = stablehlo.broadcast_in_dim %766, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1125)
      %768 = stablehlo.dot_general %767, %arg551, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1126)
      %769 = "stablehlo.all_reduce"(%768) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2116"), %arg933: tensor<bf16> loc("dot.2116")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1126)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1126)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1126)
      %770 = stablehlo.reshape %arg550 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc1127)
      %771 = stablehlo.reshape %770 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1128)
      %772 = stablehlo.add %769, %771 : tensor<8x1x5760xbf16> loc(#loc1129)
      %773 = stablehlo.slice %772 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1130)
      %774 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %775 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %776 = stablehlo.clamp %775, %773, %774 : tensor<8x1x2880xbf16> loc(#loc1131)
      %777 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %778 = stablehlo.add %776, %777 : tensor<8x1x2880xbf16> loc(#loc1132)
      %779 = stablehlo.slice %772 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1133)
      %780 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %781 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %782 = stablehlo.clamp %780, %779, %781 : tensor<8x1x2880xbf16> loc(#loc1134)
      %783 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %784 = stablehlo.multiply %782, %783 : tensor<8x1x2880xbf16> loc(#loc1135)
      %785 = stablehlo.logistic %784 : tensor<8x1x2880xbf16> loc(#loc1136)
      %786 = stablehlo.multiply %782, %785 : tensor<8x1x2880xbf16> loc(#loc1137)
      %787 = stablehlo.multiply %778, %786 : tensor<8x1x2880xbf16> loc(#loc1138)
      %788 = stablehlo.dot_general %787, %arg549, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1139)
      %789 = stablehlo.reshape %arg548 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc1140)
      %790 = stablehlo.reshape %789 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1141)
      %791 = stablehlo.add %788, %790 : tensor<8x1x1440xbf16> loc(#loc1142)
      %792 = stablehlo.reshape %791 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1143)
      %793 = stablehlo.convert %766 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc1144)
      %794 = stablehlo.reshape %arg541 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc1145)
      %795 = stablehlo.reshape %794 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc1146)
      %796 = stablehlo.transpose %795, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc1147)
      %797 = stablehlo.convert %796 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc1148)
      %798 = stablehlo.dot_general %793, %797, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc1149)
      %799 = "stablehlo.all_reduce"(%798) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.2031"), %arg933: tensor<f32> loc("dot.2031")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1149)
        stablehlo.return %4605 : tensor<f32> loc(#loc1149)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc1149)
      %800 = stablehlo.reshape %arg540 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc1150)
      %801 = stablehlo.reshape %800 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc1151)
      %802 = stablehlo.convert %801 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc1152)
      %803 = stablehlo.reshape %802 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc1153)
      %804 = stablehlo.add %799, %803 : tensor<1x32xf32> loc(#loc1154)
      %805 = stablehlo.convert %804 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc1155)
      %806:2 = "stablehlo.sort"(%805, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.2052"), %arg933: tensor<bf16> loc("sort.2052"), %arg934: tensor<i32> loc("sort.2052"), %arg935: tensor<i32> loc("sort.2052")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1157)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc1156)
      %807 = stablehlo.slice %806#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc1158)
      %808 = stablehlo.convert %807 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc1159)
      %809 = stablehlo.reshape %808 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc1160)
      %810 = stablehlo.concatenate %c_16, %809, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc1161)
      %811 = stablehlo.slice %806#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc1162)
      %812 = stablehlo.reduce(%811 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1163)
      %813 = stablehlo.broadcast_in_dim %812, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1164)
      %814 = stablehlo.subtract %811, %813 : tensor<1x4xbf16> loc(#loc1165)
      %815 = stablehlo.exponential %814 : tensor<1x4xbf16> loc(#loc1166)
      %816 = stablehlo.reduce(%815 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1167)
      %817 = stablehlo.broadcast_in_dim %816, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1168)
      %818 = stablehlo.divide %815, %817 : tensor<1x4xbf16> loc(#loc1169)
      %819 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %820 = "stablehlo.all_gather"(%819) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %821 = "stablehlo.scatter"(%820, %810, %818) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.2086"), %arg933: tensor<bf16> loc("scatter.2086")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc1170)
      %822 = stablehlo.reshape %821 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc1170)
      %823 = "stablehlo.all_to_all"(%822) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc1170)
      %824 = stablehlo.slice %823 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc1170)
      %825 = stablehlo.reshape %824 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc1170)
      %826 = stablehlo.reshape %825 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc1171)
      %827 = stablehlo.broadcast_in_dim %826, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1172)
      %828 = stablehlo.multiply %792, %827 : tensor<8x1x1x1440xbf16> loc(#loc1173)
      %829 = stablehlo.reduce(%828 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc1174)
      %830 = "stablehlo.all_reduce"(%829) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.2154"), %arg933: tensor<bf16> loc("reduce.2154")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1174)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1174)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1174)
      %831 = stablehlo.add %747, %830 : tensor<1x1x1440xbf16> loc(#loc1175)
      %832 = stablehlo.convert %831 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1176)
      %833 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %834 = stablehlo.power %832, %833 : tensor<1x1x1440xf32> loc(#loc1177)
      %835 = stablehlo.reduce(%834 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1178)
      %836 = "stablehlo.all_reduce"(%835) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.2167"), %arg933: tensor<f32> loc("reduce.2167")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1178)
        stablehlo.return %4605 : tensor<f32> loc(#loc1178)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1178)
      %837 = stablehlo.multiply %836, %cst_1 : tensor<1x1xf32> loc(#loc1179)
      %838 = stablehlo.reshape %837 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1180)
      %839 = stablehlo.add %838, %cst_2 : tensor<1x1x1xf32> loc(#loc1181)
      %840 = stablehlo.rsqrt %839 : tensor<1x1x1xf32> loc(#loc1182)
      %841 = stablehlo.reshape %840 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1183)
      %842 = stablehlo.broadcast_in_dim %841, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1184)
      %843 = stablehlo.multiply %832, %842 : tensor<1x1x1440xf32> loc(#loc1185)
      %844 = stablehlo.multiply %713, %843 : tensor<1x1x1440xf32> loc(#loc1186)
      %845 = stablehlo.convert %844 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1187)
      %846 = stablehlo.reshape %845 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1188)
      %847 = stablehlo.reshape %arg565 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc1189)
      %848 = stablehlo.reshape %847 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc1190)
      %849 = stablehlo.transpose %848, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc1191)
      %850 = stablehlo.dot_general %846, %849, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1192)
      %851 = "stablehlo.all_reduce"(%850) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2359"), %arg933: tensor<bf16> loc("dot.2359")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1192)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1192)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1192)
      %852 = stablehlo.reshape %851 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1193)
      %853 = stablehlo.reshape %arg564 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1194)
      %854 = stablehlo.add %852, %853 : tensor<1x1x1024xbf16> loc(#loc1195)
      %855 = stablehlo.reshape %854 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1196)
      %856 = stablehlo.slice %855 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1197)
      %857 = stablehlo.multiply %856, %59 : tensor<1x16x1x32xbf16> loc(#loc1198)
      %858 = stablehlo.slice %855 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1199)
      %859 = stablehlo.multiply %858, %65 : tensor<1x16x1x32xbf16> loc(#loc1200)
      %860 = stablehlo.subtract %857, %859 : tensor<1x16x1x32xbf16> loc(#loc1201)
      %861 = stablehlo.multiply %858, %59 : tensor<1x16x1x32xbf16> loc(#loc1202)
      %862 = stablehlo.multiply %856, %65 : tensor<1x16x1x32xbf16> loc(#loc1203)
      %863 = stablehlo.add %861, %862 : tensor<1x16x1x32xbf16> loc(#loc1204)
      %864 = stablehlo.concatenate %860, %863, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1205)
      %865 = stablehlo.reshape %arg539 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1206)
      %866 = stablehlo.reshape %865 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1207)
      %867 = stablehlo.transpose %866, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1208)
      %868 = stablehlo.dot_general %846, %867, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1209)
      %869 = "stablehlo.all_reduce"(%868) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2195"), %arg933: tensor<bf16> loc("dot.2195")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1209)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1209)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1209)
      %870 = stablehlo.reshape %869 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1210)
      %871 = stablehlo.reshape %arg538 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1211)
      %872 = stablehlo.add %870, %871 : tensor<1x1x128xbf16> loc(#loc1212)
      %873 = stablehlo.reshape %872 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1213)
      %874 = stablehlo.slice %873 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1214)
      %875 = stablehlo.multiply %874, %86 : tensor<1x2x1x32xbf16> loc(#loc1215)
      %876 = stablehlo.slice %873 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1216)
      %877 = stablehlo.multiply %876, %89 : tensor<1x2x1x32xbf16> loc(#loc1217)
      %878 = stablehlo.subtract %875, %877 : tensor<1x2x1x32xbf16> loc(#loc1218)
      %879 = stablehlo.multiply %876, %86 : tensor<1x2x1x32xbf16> loc(#loc1219)
      %880 = stablehlo.multiply %874, %89 : tensor<1x2x1x32xbf16> loc(#loc1220)
      %881 = stablehlo.add %879, %880 : tensor<1x2x1x32xbf16> loc(#loc1221)
      %882 = stablehlo.concatenate %878, %881, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1222)
      %883 = "stablehlo.scatter"(%arg553, %75, %882) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.2242"), %arg933: tensor<bf16> loc("scatter.2242")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1223)
      %884 = stablehlo.broadcast_in_dim %883, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1224)
      %885 = stablehlo.reshape %884 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1225)
      %886 = stablehlo.transpose %885, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc1226)
      %887 = stablehlo.dot_general %864, %886, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1227)
      %888 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %889 = stablehlo.multiply %887, %888 : tensor<1x16x1x256xbf16> loc(#loc1228)
      %890 = stablehlo.add %889, %126 : tensor<1x16x1x256xbf16> loc(#loc1229)
      %891 = stablehlo.reshape %arg563 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc98)
      %892 = "stablehlo.all_to_all"(%891) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc98)
      %893 = stablehlo.slice %892 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc98)
      %894 = stablehlo.reshape %893 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc98)
      %895 = stablehlo.reshape %894 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1230)
      %896 = stablehlo.reshape %895 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc1231)
      %897 = stablehlo.concatenate %890, %896, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1232)
      %898 = stablehlo.reshape %arg571 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1233)
      %899 = stablehlo.reshape %898 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1234)
      %900 = stablehlo.convert %899 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1235)
      %901 = stablehlo.reshape %900 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1236)
      %902 = stablehlo.reduce(%897 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1237)
      %903 = stablehlo.broadcast_in_dim %902, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1238)
      %904 = stablehlo.subtract %897, %903 : tensor<1x16x1x257xbf16> loc(#loc1239)
      %905 = stablehlo.reduce(%904 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1240)
      %906 = stablehlo.broadcast_in_dim %905, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1241)
      %907 = stablehlo.subtract %904, %906 : tensor<1x16x1x257xbf16> loc(#loc1242)
      %908 = stablehlo.exponential %907 : tensor<1x16x1x257xbf16> loc(#loc1243)
      %909 = stablehlo.reduce(%908 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1244)
      %910 = stablehlo.broadcast_in_dim %909, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1245)
      %911 = stablehlo.divide %908, %910 : tensor<1x16x1x257xbf16> loc(#loc1246)
      %912 = stablehlo.slice %911 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1247)
      %913 = stablehlo.reshape %arg555 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1248)
      %914 = stablehlo.reshape %913 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1249)
      %915 = stablehlo.transpose %914, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1250)
      %916 = stablehlo.dot_general %846, %915, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1251)
      %917 = "stablehlo.all_reduce"(%916) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2255"), %arg933: tensor<bf16> loc("dot.2255")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1251)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1251)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1251)
      %918 = stablehlo.reshape %917 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1252)
      %919 = stablehlo.reshape %arg554 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1253)
      %920 = stablehlo.add %918, %919 : tensor<1x1x128xbf16> loc(#loc1254)
      %921 = stablehlo.reshape %920 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1255)
      %922 = "stablehlo.scatter"(%arg556, %75, %921) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.2279"), %arg933: tensor<bf16> loc("scatter.2279")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1256)
      %923 = stablehlo.broadcast_in_dim %922, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1257)
      %924 = stablehlo.reshape %923 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1258)
      %925 = stablehlo.dot_general %912, %924, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1259)
      %926 = stablehlo.reshape %925 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc1260)
      %927 = stablehlo.reshape %arg562 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc1261)
      %928 = stablehlo.reshape %927 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc1262)
      %929 = stablehlo.transpose %928, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc1263)
      %930 = stablehlo.dot_general %926, %929, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1264)
      %931 = "stablehlo.all_reduce"(%930) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2466"), %arg933: tensor<bf16> loc("dot.2466")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1264)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1264)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1264)
      %932 = stablehlo.reshape %931 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1265)
      %933 = stablehlo.reshape %arg561 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1266)
      %934 = stablehlo.add %932, %933 : tensor<1x1x1440xbf16> loc(#loc1267)
      %935 = stablehlo.add %831, %934 : tensor<1x1x1440xbf16> loc(#loc1268)
      %936 = stablehlo.reshape %arg566 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1269)
      %937 = stablehlo.reshape %936 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1270)
      %938 = stablehlo.convert %937 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1271)
      %939 = stablehlo.reshape %938 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1272)
      %940 = stablehlo.convert %935 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1273)
      %941 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %942 = stablehlo.power %940, %941 : tensor<1x1x1440xf32> loc(#loc1274)
      %943 = stablehlo.reduce(%942 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1275)
      %944 = "stablehlo.all_reduce"(%943) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.2484"), %arg933: tensor<f32> loc("reduce.2484")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1275)
        stablehlo.return %4605 : tensor<f32> loc(#loc1275)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1275)
      %945 = stablehlo.multiply %944, %cst_1 : tensor<1x1xf32> loc(#loc1276)
      %946 = stablehlo.reshape %945 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1277)
      %947 = stablehlo.add %946, %cst_2 : tensor<1x1x1xf32> loc(#loc1278)
      %948 = stablehlo.rsqrt %947 : tensor<1x1x1xf32> loc(#loc1279)
      %949 = stablehlo.reshape %948 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1280)
      %950 = stablehlo.broadcast_in_dim %949, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1281)
      %951 = stablehlo.multiply %940, %950 : tensor<1x1x1440xf32> loc(#loc1282)
      %952 = stablehlo.multiply %939, %951 : tensor<1x1x1440xf32> loc(#loc1283)
      %953 = stablehlo.convert %952 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1284)
      %954 = stablehlo.reshape %953 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1285)
      %955 = stablehlo.broadcast_in_dim %954, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1286)
      %956 = stablehlo.dot_general %955, %arg570, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1287)
      %957 = "stablehlo.all_reduce"(%956) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2598"), %arg933: tensor<bf16> loc("dot.2598")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1287)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1287)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1287)
      %958 = stablehlo.reshape %arg569 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc1288)
      %959 = stablehlo.reshape %958 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1289)
      %960 = stablehlo.add %957, %959 : tensor<8x1x5760xbf16> loc(#loc1290)
      %961 = stablehlo.slice %960 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1291)
      %962 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %963 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %964 = stablehlo.clamp %963, %961, %962 : tensor<8x1x2880xbf16> loc(#loc1292)
      %965 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %966 = stablehlo.add %964, %965 : tensor<8x1x2880xbf16> loc(#loc1293)
      %967 = stablehlo.slice %960 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1294)
      %968 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %969 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %970 = stablehlo.clamp %968, %967, %969 : tensor<8x1x2880xbf16> loc(#loc1295)
      %971 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %972 = stablehlo.multiply %970, %971 : tensor<8x1x2880xbf16> loc(#loc1296)
      %973 = stablehlo.logistic %972 : tensor<8x1x2880xbf16> loc(#loc1297)
      %974 = stablehlo.multiply %970, %973 : tensor<8x1x2880xbf16> loc(#loc1298)
      %975 = stablehlo.multiply %966, %974 : tensor<8x1x2880xbf16> loc(#loc1299)
      %976 = stablehlo.dot_general %975, %arg568, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1300)
      %977 = stablehlo.reshape %arg567 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc1301)
      %978 = stablehlo.reshape %977 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1302)
      %979 = stablehlo.add %976, %978 : tensor<8x1x1440xbf16> loc(#loc1303)
      %980 = stablehlo.reshape %979 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1304)
      %981 = stablehlo.convert %954 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc1305)
      %982 = stablehlo.reshape %arg560 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc1306)
      %983 = stablehlo.reshape %982 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc1307)
      %984 = stablehlo.transpose %983, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc1308)
      %985 = stablehlo.convert %984 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc1309)
      %986 = stablehlo.dot_general %981, %985, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc1310)
      %987 = "stablehlo.all_reduce"(%986) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.2513"), %arg933: tensor<f32> loc("dot.2513")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1310)
        stablehlo.return %4605 : tensor<f32> loc(#loc1310)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc1310)
      %988 = stablehlo.reshape %arg559 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc1311)
      %989 = stablehlo.reshape %988 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc1312)
      %990 = stablehlo.convert %989 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc1313)
      %991 = stablehlo.reshape %990 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc1314)
      %992 = stablehlo.add %987, %991 : tensor<1x32xf32> loc(#loc1315)
      %993 = stablehlo.convert %992 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc1316)
      %994:2 = "stablehlo.sort"(%993, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.2534"), %arg933: tensor<bf16> loc("sort.2534"), %arg934: tensor<i32> loc("sort.2534"), %arg935: tensor<i32> loc("sort.2534")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1318)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc1317)
      %995 = stablehlo.slice %994#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc1319)
      %996 = stablehlo.convert %995 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc1320)
      %997 = stablehlo.reshape %996 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc1321)
      %998 = stablehlo.concatenate %c_16, %997, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc1322)
      %999 = stablehlo.slice %994#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc1323)
      %1000 = stablehlo.reduce(%999 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1324)
      %1001 = stablehlo.broadcast_in_dim %1000, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1325)
      %1002 = stablehlo.subtract %999, %1001 : tensor<1x4xbf16> loc(#loc1326)
      %1003 = stablehlo.exponential %1002 : tensor<1x4xbf16> loc(#loc1327)
      %1004 = stablehlo.reduce(%1003 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1328)
      %1005 = stablehlo.broadcast_in_dim %1004, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1329)
      %1006 = stablehlo.divide %1003, %1005 : tensor<1x4xbf16> loc(#loc1330)
      %1007 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %1008 = "stablehlo.all_gather"(%1007) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %1009 = "stablehlo.scatter"(%1008, %998, %1006) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.2568"), %arg933: tensor<bf16> loc("scatter.2568")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc1331)
      %1010 = stablehlo.reshape %1009 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc1331)
      %1011 = "stablehlo.all_to_all"(%1010) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc1331)
      %1012 = stablehlo.slice %1011 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc1331)
      %1013 = stablehlo.reshape %1012 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc1331)
      %1014 = stablehlo.reshape %1013 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc1332)
      %1015 = stablehlo.broadcast_in_dim %1014, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1333)
      %1016 = stablehlo.multiply %980, %1015 : tensor<8x1x1x1440xbf16> loc(#loc1334)
      %1017 = stablehlo.reduce(%1016 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc1335)
      %1018 = "stablehlo.all_reduce"(%1017) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.2636"), %arg933: tensor<bf16> loc("reduce.2636")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1335)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1335)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1335)
      %1019 = stablehlo.add %935, %1018 : tensor<1x1x1440xbf16> loc(#loc1336)
      %1020 = stablehlo.convert %1019 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1337)
      %1021 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1022 = stablehlo.power %1020, %1021 : tensor<1x1x1440xf32> loc(#loc1338)
      %1023 = stablehlo.reduce(%1022 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1339)
      %1024 = "stablehlo.all_reduce"(%1023) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.2649"), %arg933: tensor<f32> loc("reduce.2649")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1339)
        stablehlo.return %4605 : tensor<f32> loc(#loc1339)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1339)
      %1025 = stablehlo.multiply %1024, %cst_1 : tensor<1x1xf32> loc(#loc1340)
      %1026 = stablehlo.reshape %1025 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1341)
      %1027 = stablehlo.add %1026, %cst_2 : tensor<1x1x1xf32> loc(#loc1342)
      %1028 = stablehlo.rsqrt %1027 : tensor<1x1x1xf32> loc(#loc1343)
      %1029 = stablehlo.reshape %1028 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1344)
      %1030 = stablehlo.broadcast_in_dim %1029, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1345)
      %1031 = stablehlo.multiply %1020, %1030 : tensor<1x1x1440xf32> loc(#loc1346)
      %1032 = stablehlo.multiply %901, %1031 : tensor<1x1x1440xf32> loc(#loc1347)
      %1033 = stablehlo.convert %1032 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1348)
      %1034 = stablehlo.reshape %1033 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1349)
      %1035 = stablehlo.reshape %arg584 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc1350)
      %1036 = stablehlo.reshape %1035 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc1351)
      %1037 = stablehlo.transpose %1036, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc1352)
      %1038 = stablehlo.dot_general %1034, %1037, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1353)
      %1039 = "stablehlo.all_reduce"(%1038) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2841"), %arg933: tensor<bf16> loc("dot.2841")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1353)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1353)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1353)
      %1040 = stablehlo.reshape %1039 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1354)
      %1041 = stablehlo.reshape %arg583 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1355)
      %1042 = stablehlo.add %1040, %1041 : tensor<1x1x1024xbf16> loc(#loc1356)
      %1043 = stablehlo.reshape %1042 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1357)
      %1044 = stablehlo.slice %1043 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1358)
      %1045 = stablehlo.multiply %1044, %59 : tensor<1x16x1x32xbf16> loc(#loc1359)
      %1046 = stablehlo.slice %1043 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1360)
      %1047 = stablehlo.multiply %1046, %65 : tensor<1x16x1x32xbf16> loc(#loc1361)
      %1048 = stablehlo.subtract %1045, %1047 : tensor<1x16x1x32xbf16> loc(#loc1362)
      %1049 = stablehlo.multiply %1046, %59 : tensor<1x16x1x32xbf16> loc(#loc1363)
      %1050 = stablehlo.multiply %1044, %65 : tensor<1x16x1x32xbf16> loc(#loc1364)
      %1051 = stablehlo.add %1049, %1050 : tensor<1x16x1x32xbf16> loc(#loc1365)
      %1052 = stablehlo.concatenate %1048, %1051, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1366)
      %1053 = stablehlo.reshape %arg558 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1367)
      %1054 = stablehlo.reshape %1053 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1368)
      %1055 = stablehlo.transpose %1054, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1369)
      %1056 = stablehlo.dot_general %1034, %1055, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1370)
      %1057 = "stablehlo.all_reduce"(%1056) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2677"), %arg933: tensor<bf16> loc("dot.2677")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1370)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1370)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1370)
      %1058 = stablehlo.reshape %1057 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1371)
      %1059 = stablehlo.reshape %arg557 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1372)
      %1060 = stablehlo.add %1058, %1059 : tensor<1x1x128xbf16> loc(#loc1373)
      %1061 = stablehlo.reshape %1060 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1374)
      %1062 = stablehlo.slice %1061 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1375)
      %1063 = stablehlo.multiply %1062, %86 : tensor<1x2x1x32xbf16> loc(#loc1376)
      %1064 = stablehlo.slice %1061 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1377)
      %1065 = stablehlo.multiply %1064, %89 : tensor<1x2x1x32xbf16> loc(#loc1378)
      %1066 = stablehlo.subtract %1063, %1065 : tensor<1x2x1x32xbf16> loc(#loc1379)
      %1067 = stablehlo.multiply %1064, %86 : tensor<1x2x1x32xbf16> loc(#loc1380)
      %1068 = stablehlo.multiply %1062, %89 : tensor<1x2x1x32xbf16> loc(#loc1381)
      %1069 = stablehlo.add %1067, %1068 : tensor<1x2x1x32xbf16> loc(#loc1382)
      %1070 = stablehlo.concatenate %1066, %1069, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1383)
      %1071 = "stablehlo.scatter"(%arg572, %75, %1070) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.2724"), %arg933: tensor<bf16> loc("scatter.2724")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1384)
      %1072 = stablehlo.broadcast_in_dim %1071, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1385)
      %1073 = stablehlo.reshape %1072 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1386)
      %1074 = stablehlo.transpose %1073, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc1387)
      %1075 = stablehlo.dot_general %1052, %1074, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1388)
      %1076 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %1077 = stablehlo.multiply %1075, %1076 : tensor<1x16x1x256xbf16> loc(#loc1389)
      %1078 = stablehlo.add %1077, %325 : tensor<1x16x1x256xbf16> loc(#loc1390)
      %1079 = stablehlo.reshape %arg582 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc117)
      %1080 = "stablehlo.all_to_all"(%1079) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc117)
      %1081 = stablehlo.slice %1080 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc117)
      %1082 = stablehlo.reshape %1081 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc117)
      %1083 = stablehlo.reshape %1082 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1391)
      %1084 = stablehlo.reshape %1083 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc1392)
      %1085 = stablehlo.concatenate %1078, %1084, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1393)
      %1086 = stablehlo.reshape %arg590 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1394)
      %1087 = stablehlo.reshape %1086 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1395)
      %1088 = stablehlo.convert %1087 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1396)
      %1089 = stablehlo.reshape %1088 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1397)
      %1090 = stablehlo.reduce(%1085 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1398)
      %1091 = stablehlo.broadcast_in_dim %1090, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1399)
      %1092 = stablehlo.subtract %1085, %1091 : tensor<1x16x1x257xbf16> loc(#loc1400)
      %1093 = stablehlo.reduce(%1092 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1401)
      %1094 = stablehlo.broadcast_in_dim %1093, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1402)
      %1095 = stablehlo.subtract %1092, %1094 : tensor<1x16x1x257xbf16> loc(#loc1403)
      %1096 = stablehlo.exponential %1095 : tensor<1x16x1x257xbf16> loc(#loc1404)
      %1097 = stablehlo.reduce(%1096 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1405)
      %1098 = stablehlo.broadcast_in_dim %1097, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1406)
      %1099 = stablehlo.divide %1096, %1098 : tensor<1x16x1x257xbf16> loc(#loc1407)
      %1100 = stablehlo.slice %1099 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1408)
      %1101 = stablehlo.reshape %arg574 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1409)
      %1102 = stablehlo.reshape %1101 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1410)
      %1103 = stablehlo.transpose %1102, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1411)
      %1104 = stablehlo.dot_general %1034, %1103, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1412)
      %1105 = "stablehlo.all_reduce"(%1104) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2737"), %arg933: tensor<bf16> loc("dot.2737")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1412)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1412)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1412)
      %1106 = stablehlo.reshape %1105 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1413)
      %1107 = stablehlo.reshape %arg573 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1414)
      %1108 = stablehlo.add %1106, %1107 : tensor<1x1x128xbf16> loc(#loc1415)
      %1109 = stablehlo.reshape %1108 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1416)
      %1110 = "stablehlo.scatter"(%arg575, %75, %1109) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.2761"), %arg933: tensor<bf16> loc("scatter.2761")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1417)
      %1111 = stablehlo.broadcast_in_dim %1110, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1418)
      %1112 = stablehlo.reshape %1111 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1419)
      %1113 = stablehlo.dot_general %1100, %1112, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1420)
      %1114 = stablehlo.reshape %1113 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc1421)
      %1115 = stablehlo.reshape %arg581 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc1422)
      %1116 = stablehlo.reshape %1115 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc1423)
      %1117 = stablehlo.transpose %1116, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc1424)
      %1118 = stablehlo.dot_general %1114, %1117, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1425)
      %1119 = "stablehlo.all_reduce"(%1118) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.2948"), %arg933: tensor<bf16> loc("dot.2948")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1425)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1425)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1425)
      %1120 = stablehlo.reshape %1119 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1426)
      %1121 = stablehlo.reshape %arg580 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1427)
      %1122 = stablehlo.add %1120, %1121 : tensor<1x1x1440xbf16> loc(#loc1428)
      %1123 = stablehlo.add %1019, %1122 : tensor<1x1x1440xbf16> loc(#loc1429)
      %1124 = stablehlo.reshape %arg585 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1430)
      %1125 = stablehlo.reshape %1124 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1431)
      %1126 = stablehlo.convert %1125 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1432)
      %1127 = stablehlo.reshape %1126 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1433)
      %1128 = stablehlo.convert %1123 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1434)
      %1129 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1130 = stablehlo.power %1128, %1129 : tensor<1x1x1440xf32> loc(#loc1435)
      %1131 = stablehlo.reduce(%1130 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1436)
      %1132 = "stablehlo.all_reduce"(%1131) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.2966"), %arg933: tensor<f32> loc("reduce.2966")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1436)
        stablehlo.return %4605 : tensor<f32> loc(#loc1436)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1436)
      %1133 = stablehlo.multiply %1132, %cst_1 : tensor<1x1xf32> loc(#loc1437)
      %1134 = stablehlo.reshape %1133 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1438)
      %1135 = stablehlo.add %1134, %cst_2 : tensor<1x1x1xf32> loc(#loc1439)
      %1136 = stablehlo.rsqrt %1135 : tensor<1x1x1xf32> loc(#loc1440)
      %1137 = stablehlo.reshape %1136 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1441)
      %1138 = stablehlo.broadcast_in_dim %1137, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1442)
      %1139 = stablehlo.multiply %1128, %1138 : tensor<1x1x1440xf32> loc(#loc1443)
      %1140 = stablehlo.multiply %1127, %1139 : tensor<1x1x1440xf32> loc(#loc1444)
      %1141 = stablehlo.convert %1140 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1445)
      %1142 = stablehlo.reshape %1141 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1446)
      %1143 = stablehlo.broadcast_in_dim %1142, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1447)
      %1144 = stablehlo.dot_general %1143, %arg589, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1448)
      %1145 = "stablehlo.all_reduce"(%1144) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3080"), %arg933: tensor<bf16> loc("dot.3080")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1448)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1448)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1448)
      %1146 = stablehlo.reshape %arg588 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc1449)
      %1147 = stablehlo.reshape %1146 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1450)
      %1148 = stablehlo.add %1145, %1147 : tensor<8x1x5760xbf16> loc(#loc1451)
      %1149 = stablehlo.slice %1148 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1452)
      %1150 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1151 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1152 = stablehlo.clamp %1151, %1149, %1150 : tensor<8x1x2880xbf16> loc(#loc1453)
      %1153 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1154 = stablehlo.add %1152, %1153 : tensor<8x1x2880xbf16> loc(#loc1454)
      %1155 = stablehlo.slice %1148 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1455)
      %1156 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1157 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1158 = stablehlo.clamp %1156, %1155, %1157 : tensor<8x1x2880xbf16> loc(#loc1456)
      %1159 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1160 = stablehlo.multiply %1158, %1159 : tensor<8x1x2880xbf16> loc(#loc1457)
      %1161 = stablehlo.logistic %1160 : tensor<8x1x2880xbf16> loc(#loc1458)
      %1162 = stablehlo.multiply %1158, %1161 : tensor<8x1x2880xbf16> loc(#loc1459)
      %1163 = stablehlo.multiply %1154, %1162 : tensor<8x1x2880xbf16> loc(#loc1460)
      %1164 = stablehlo.dot_general %1163, %arg587, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1461)
      %1165 = stablehlo.reshape %arg586 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc1462)
      %1166 = stablehlo.reshape %1165 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1463)
      %1167 = stablehlo.add %1164, %1166 : tensor<8x1x1440xbf16> loc(#loc1464)
      %1168 = stablehlo.reshape %1167 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1465)
      %1169 = stablehlo.convert %1142 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc1466)
      %1170 = stablehlo.reshape %arg579 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc1467)
      %1171 = stablehlo.reshape %1170 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc1468)
      %1172 = stablehlo.transpose %1171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc1469)
      %1173 = stablehlo.convert %1172 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc1470)
      %1174 = stablehlo.dot_general %1169, %1173, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc1471)
      %1175 = "stablehlo.all_reduce"(%1174) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.2995"), %arg933: tensor<f32> loc("dot.2995")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1471)
        stablehlo.return %4605 : tensor<f32> loc(#loc1471)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc1471)
      %1176 = stablehlo.reshape %arg578 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc1472)
      %1177 = stablehlo.reshape %1176 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc1473)
      %1178 = stablehlo.convert %1177 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc1474)
      %1179 = stablehlo.reshape %1178 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc1475)
      %1180 = stablehlo.add %1175, %1179 : tensor<1x32xf32> loc(#loc1476)
      %1181 = stablehlo.convert %1180 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc1477)
      %1182:2 = "stablehlo.sort"(%1181, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.3016"), %arg933: tensor<bf16> loc("sort.3016"), %arg934: tensor<i32> loc("sort.3016"), %arg935: tensor<i32> loc("sort.3016")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1479)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc1478)
      %1183 = stablehlo.slice %1182#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc1480)
      %1184 = stablehlo.convert %1183 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc1481)
      %1185 = stablehlo.reshape %1184 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc1482)
      %1186 = stablehlo.concatenate %c_16, %1185, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc1483)
      %1187 = stablehlo.slice %1182#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc1484)
      %1188 = stablehlo.reduce(%1187 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1485)
      %1189 = stablehlo.broadcast_in_dim %1188, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1486)
      %1190 = stablehlo.subtract %1187, %1189 : tensor<1x4xbf16> loc(#loc1487)
      %1191 = stablehlo.exponential %1190 : tensor<1x4xbf16> loc(#loc1488)
      %1192 = stablehlo.reduce(%1191 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1489)
      %1193 = stablehlo.broadcast_in_dim %1192, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1490)
      %1194 = stablehlo.divide %1191, %1193 : tensor<1x4xbf16> loc(#loc1491)
      %1195 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %1196 = "stablehlo.all_gather"(%1195) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %1197 = "stablehlo.scatter"(%1196, %1186, %1194) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.3050"), %arg933: tensor<bf16> loc("scatter.3050")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc1492)
      %1198 = stablehlo.reshape %1197 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc1492)
      %1199 = "stablehlo.all_to_all"(%1198) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc1492)
      %1200 = stablehlo.slice %1199 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc1492)
      %1201 = stablehlo.reshape %1200 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc1492)
      %1202 = stablehlo.reshape %1201 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc1493)
      %1203 = stablehlo.broadcast_in_dim %1202, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1494)
      %1204 = stablehlo.multiply %1168, %1203 : tensor<8x1x1x1440xbf16> loc(#loc1495)
      %1205 = stablehlo.reduce(%1204 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc1496)
      %1206 = "stablehlo.all_reduce"(%1205) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.3118"), %arg933: tensor<bf16> loc("reduce.3118")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1496)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1496)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1496)
      %1207 = stablehlo.add %1123, %1206 : tensor<1x1x1440xbf16> loc(#loc1497)
      %1208 = stablehlo.convert %1207 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1498)
      %1209 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1210 = stablehlo.power %1208, %1209 : tensor<1x1x1440xf32> loc(#loc1499)
      %1211 = stablehlo.reduce(%1210 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1500)
      %1212 = "stablehlo.all_reduce"(%1211) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.3131"), %arg933: tensor<f32> loc("reduce.3131")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1500)
        stablehlo.return %4605 : tensor<f32> loc(#loc1500)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1500)
      %1213 = stablehlo.multiply %1212, %cst_1 : tensor<1x1xf32> loc(#loc1501)
      %1214 = stablehlo.reshape %1213 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1502)
      %1215 = stablehlo.add %1214, %cst_2 : tensor<1x1x1xf32> loc(#loc1503)
      %1216 = stablehlo.rsqrt %1215 : tensor<1x1x1xf32> loc(#loc1504)
      %1217 = stablehlo.reshape %1216 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1505)
      %1218 = stablehlo.broadcast_in_dim %1217, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1506)
      %1219 = stablehlo.multiply %1208, %1218 : tensor<1x1x1440xf32> loc(#loc1507)
      %1220 = stablehlo.multiply %1089, %1219 : tensor<1x1x1440xf32> loc(#loc1508)
      %1221 = stablehlo.convert %1220 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1509)
      %1222 = stablehlo.reshape %1221 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1510)
      %1223 = stablehlo.reshape %arg603 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc1511)
      %1224 = stablehlo.reshape %1223 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc1512)
      %1225 = stablehlo.transpose %1224, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc1513)
      %1226 = stablehlo.dot_general %1222, %1225, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1514)
      %1227 = "stablehlo.all_reduce"(%1226) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3323"), %arg933: tensor<bf16> loc("dot.3323")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1514)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1514)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1514)
      %1228 = stablehlo.reshape %1227 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1515)
      %1229 = stablehlo.reshape %arg602 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1516)
      %1230 = stablehlo.add %1228, %1229 : tensor<1x1x1024xbf16> loc(#loc1517)
      %1231 = stablehlo.reshape %1230 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1518)
      %1232 = stablehlo.slice %1231 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1519)
      %1233 = stablehlo.multiply %1232, %59 : tensor<1x16x1x32xbf16> loc(#loc1520)
      %1234 = stablehlo.slice %1231 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1521)
      %1235 = stablehlo.multiply %1234, %65 : tensor<1x16x1x32xbf16> loc(#loc1522)
      %1236 = stablehlo.subtract %1233, %1235 : tensor<1x16x1x32xbf16> loc(#loc1523)
      %1237 = stablehlo.multiply %1234, %59 : tensor<1x16x1x32xbf16> loc(#loc1524)
      %1238 = stablehlo.multiply %1232, %65 : tensor<1x16x1x32xbf16> loc(#loc1525)
      %1239 = stablehlo.add %1237, %1238 : tensor<1x16x1x32xbf16> loc(#loc1526)
      %1240 = stablehlo.concatenate %1236, %1239, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1527)
      %1241 = stablehlo.reshape %arg577 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1528)
      %1242 = stablehlo.reshape %1241 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1529)
      %1243 = stablehlo.transpose %1242, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1530)
      %1244 = stablehlo.dot_general %1222, %1243, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1531)
      %1245 = "stablehlo.all_reduce"(%1244) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3159"), %arg933: tensor<bf16> loc("dot.3159")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1531)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1531)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1531)
      %1246 = stablehlo.reshape %1245 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1532)
      %1247 = stablehlo.reshape %arg576 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1533)
      %1248 = stablehlo.add %1246, %1247 : tensor<1x1x128xbf16> loc(#loc1534)
      %1249 = stablehlo.reshape %1248 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1535)
      %1250 = stablehlo.slice %1249 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1536)
      %1251 = stablehlo.multiply %1250, %86 : tensor<1x2x1x32xbf16> loc(#loc1537)
      %1252 = stablehlo.slice %1249 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1538)
      %1253 = stablehlo.multiply %1252, %89 : tensor<1x2x1x32xbf16> loc(#loc1539)
      %1254 = stablehlo.subtract %1251, %1253 : tensor<1x2x1x32xbf16> loc(#loc1540)
      %1255 = stablehlo.multiply %1252, %86 : tensor<1x2x1x32xbf16> loc(#loc1541)
      %1256 = stablehlo.multiply %1250, %89 : tensor<1x2x1x32xbf16> loc(#loc1542)
      %1257 = stablehlo.add %1255, %1256 : tensor<1x2x1x32xbf16> loc(#loc1543)
      %1258 = stablehlo.concatenate %1254, %1257, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1544)
      %1259 = "stablehlo.scatter"(%arg591, %75, %1258) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.3206"), %arg933: tensor<bf16> loc("scatter.3206")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1545)
      %1260 = stablehlo.broadcast_in_dim %1259, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1546)
      %1261 = stablehlo.reshape %1260 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1547)
      %1262 = stablehlo.transpose %1261, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc1548)
      %1263 = stablehlo.dot_general %1240, %1262, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1549)
      %1264 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %1265 = stablehlo.multiply %1263, %1264 : tensor<1x16x1x256xbf16> loc(#loc1550)
      %1266 = stablehlo.add %1265, %126 : tensor<1x16x1x256xbf16> loc(#loc1551)
      %1267 = stablehlo.reshape %arg601 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc136)
      %1268 = "stablehlo.all_to_all"(%1267) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc136)
      %1269 = stablehlo.slice %1268 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc136)
      %1270 = stablehlo.reshape %1269 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc136)
      %1271 = stablehlo.reshape %1270 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1552)
      %1272 = stablehlo.reshape %1271 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc1553)
      %1273 = stablehlo.concatenate %1266, %1272, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1554)
      %1274 = stablehlo.reshape %arg609 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1555)
      %1275 = stablehlo.reshape %1274 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1556)
      %1276 = stablehlo.convert %1275 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1557)
      %1277 = stablehlo.reshape %1276 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1558)
      %1278 = stablehlo.reduce(%1273 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1559)
      %1279 = stablehlo.broadcast_in_dim %1278, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1560)
      %1280 = stablehlo.subtract %1273, %1279 : tensor<1x16x1x257xbf16> loc(#loc1561)
      %1281 = stablehlo.reduce(%1280 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1562)
      %1282 = stablehlo.broadcast_in_dim %1281, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1563)
      %1283 = stablehlo.subtract %1280, %1282 : tensor<1x16x1x257xbf16> loc(#loc1564)
      %1284 = stablehlo.exponential %1283 : tensor<1x16x1x257xbf16> loc(#loc1565)
      %1285 = stablehlo.reduce(%1284 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1566)
      %1286 = stablehlo.broadcast_in_dim %1285, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1567)
      %1287 = stablehlo.divide %1284, %1286 : tensor<1x16x1x257xbf16> loc(#loc1568)
      %1288 = stablehlo.slice %1287 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1569)
      %1289 = stablehlo.reshape %arg593 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1570)
      %1290 = stablehlo.reshape %1289 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1571)
      %1291 = stablehlo.transpose %1290, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1572)
      %1292 = stablehlo.dot_general %1222, %1291, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1573)
      %1293 = "stablehlo.all_reduce"(%1292) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3219"), %arg933: tensor<bf16> loc("dot.3219")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1573)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1573)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1573)
      %1294 = stablehlo.reshape %1293 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1574)
      %1295 = stablehlo.reshape %arg592 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1575)
      %1296 = stablehlo.add %1294, %1295 : tensor<1x1x128xbf16> loc(#loc1576)
      %1297 = stablehlo.reshape %1296 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1577)
      %1298 = "stablehlo.scatter"(%arg594, %75, %1297) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.3243"), %arg933: tensor<bf16> loc("scatter.3243")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1578)
      %1299 = stablehlo.broadcast_in_dim %1298, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1579)
      %1300 = stablehlo.reshape %1299 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1580)
      %1301 = stablehlo.dot_general %1288, %1300, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1581)
      %1302 = stablehlo.reshape %1301 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc1582)
      %1303 = stablehlo.reshape %arg600 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc1583)
      %1304 = stablehlo.reshape %1303 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc1584)
      %1305 = stablehlo.transpose %1304, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc1585)
      %1306 = stablehlo.dot_general %1302, %1305, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1586)
      %1307 = "stablehlo.all_reduce"(%1306) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3430"), %arg933: tensor<bf16> loc("dot.3430")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1586)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1586)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1586)
      %1308 = stablehlo.reshape %1307 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1587)
      %1309 = stablehlo.reshape %arg599 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1588)
      %1310 = stablehlo.add %1308, %1309 : tensor<1x1x1440xbf16> loc(#loc1589)
      %1311 = stablehlo.add %1207, %1310 : tensor<1x1x1440xbf16> loc(#loc1590)
      %1312 = stablehlo.reshape %arg604 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1591)
      %1313 = stablehlo.reshape %1312 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1592)
      %1314 = stablehlo.convert %1313 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1593)
      %1315 = stablehlo.reshape %1314 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1594)
      %1316 = stablehlo.convert %1311 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1595)
      %1317 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1318 = stablehlo.power %1316, %1317 : tensor<1x1x1440xf32> loc(#loc1596)
      %1319 = stablehlo.reduce(%1318 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1597)
      %1320 = "stablehlo.all_reduce"(%1319) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.3448"), %arg933: tensor<f32> loc("reduce.3448")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1597)
        stablehlo.return %4605 : tensor<f32> loc(#loc1597)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1597)
      %1321 = stablehlo.multiply %1320, %cst_1 : tensor<1x1xf32> loc(#loc1598)
      %1322 = stablehlo.reshape %1321 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1599)
      %1323 = stablehlo.add %1322, %cst_2 : tensor<1x1x1xf32> loc(#loc1600)
      %1324 = stablehlo.rsqrt %1323 : tensor<1x1x1xf32> loc(#loc1601)
      %1325 = stablehlo.reshape %1324 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1602)
      %1326 = stablehlo.broadcast_in_dim %1325, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1603)
      %1327 = stablehlo.multiply %1316, %1326 : tensor<1x1x1440xf32> loc(#loc1604)
      %1328 = stablehlo.multiply %1315, %1327 : tensor<1x1x1440xf32> loc(#loc1605)
      %1329 = stablehlo.convert %1328 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1606)
      %1330 = stablehlo.reshape %1329 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1607)
      %1331 = stablehlo.broadcast_in_dim %1330, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1608)
      %1332 = stablehlo.dot_general %1331, %arg608, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1609)
      %1333 = "stablehlo.all_reduce"(%1332) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3562"), %arg933: tensor<bf16> loc("dot.3562")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1609)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1609)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1609)
      %1334 = stablehlo.reshape %arg607 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc1610)
      %1335 = stablehlo.reshape %1334 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1611)
      %1336 = stablehlo.add %1333, %1335 : tensor<8x1x5760xbf16> loc(#loc1612)
      %1337 = stablehlo.slice %1336 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1613)
      %1338 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1339 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1340 = stablehlo.clamp %1339, %1337, %1338 : tensor<8x1x2880xbf16> loc(#loc1614)
      %1341 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1342 = stablehlo.add %1340, %1341 : tensor<8x1x2880xbf16> loc(#loc1615)
      %1343 = stablehlo.slice %1336 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1616)
      %1344 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1345 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1346 = stablehlo.clamp %1344, %1343, %1345 : tensor<8x1x2880xbf16> loc(#loc1617)
      %1347 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1348 = stablehlo.multiply %1346, %1347 : tensor<8x1x2880xbf16> loc(#loc1618)
      %1349 = stablehlo.logistic %1348 : tensor<8x1x2880xbf16> loc(#loc1619)
      %1350 = stablehlo.multiply %1346, %1349 : tensor<8x1x2880xbf16> loc(#loc1620)
      %1351 = stablehlo.multiply %1342, %1350 : tensor<8x1x2880xbf16> loc(#loc1621)
      %1352 = stablehlo.dot_general %1351, %arg606, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1622)
      %1353 = stablehlo.reshape %arg605 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc1623)
      %1354 = stablehlo.reshape %1353 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1624)
      %1355 = stablehlo.add %1352, %1354 : tensor<8x1x1440xbf16> loc(#loc1625)
      %1356 = stablehlo.reshape %1355 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1626)
      %1357 = stablehlo.convert %1330 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc1627)
      %1358 = stablehlo.reshape %arg598 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc1628)
      %1359 = stablehlo.reshape %1358 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc1629)
      %1360 = stablehlo.transpose %1359, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc1630)
      %1361 = stablehlo.convert %1360 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc1631)
      %1362 = stablehlo.dot_general %1357, %1361, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc1632)
      %1363 = "stablehlo.all_reduce"(%1362) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.3477"), %arg933: tensor<f32> loc("dot.3477")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1632)
        stablehlo.return %4605 : tensor<f32> loc(#loc1632)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc1632)
      %1364 = stablehlo.reshape %arg597 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc1633)
      %1365 = stablehlo.reshape %1364 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc1634)
      %1366 = stablehlo.convert %1365 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc1635)
      %1367 = stablehlo.reshape %1366 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc1636)
      %1368 = stablehlo.add %1363, %1367 : tensor<1x32xf32> loc(#loc1637)
      %1369 = stablehlo.convert %1368 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc1638)
      %1370:2 = "stablehlo.sort"(%1369, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.3498"), %arg933: tensor<bf16> loc("sort.3498"), %arg934: tensor<i32> loc("sort.3498"), %arg935: tensor<i32> loc("sort.3498")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1640)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc1639)
      %1371 = stablehlo.slice %1370#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc1641)
      %1372 = stablehlo.convert %1371 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc1642)
      %1373 = stablehlo.reshape %1372 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc1643)
      %1374 = stablehlo.concatenate %c_16, %1373, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc1644)
      %1375 = stablehlo.slice %1370#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc1645)
      %1376 = stablehlo.reduce(%1375 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1646)
      %1377 = stablehlo.broadcast_in_dim %1376, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1647)
      %1378 = stablehlo.subtract %1375, %1377 : tensor<1x4xbf16> loc(#loc1648)
      %1379 = stablehlo.exponential %1378 : tensor<1x4xbf16> loc(#loc1649)
      %1380 = stablehlo.reduce(%1379 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1650)
      %1381 = stablehlo.broadcast_in_dim %1380, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1651)
      %1382 = stablehlo.divide %1379, %1381 : tensor<1x4xbf16> loc(#loc1652)
      %1383 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %1384 = "stablehlo.all_gather"(%1383) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %1385 = "stablehlo.scatter"(%1384, %1374, %1382) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.3532"), %arg933: tensor<bf16> loc("scatter.3532")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc1653)
      %1386 = stablehlo.reshape %1385 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc1653)
      %1387 = "stablehlo.all_to_all"(%1386) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc1653)
      %1388 = stablehlo.slice %1387 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc1653)
      %1389 = stablehlo.reshape %1388 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc1653)
      %1390 = stablehlo.reshape %1389 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc1654)
      %1391 = stablehlo.broadcast_in_dim %1390, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1655)
      %1392 = stablehlo.multiply %1356, %1391 : tensor<8x1x1x1440xbf16> loc(#loc1656)
      %1393 = stablehlo.reduce(%1392 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc1657)
      %1394 = "stablehlo.all_reduce"(%1393) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.3600"), %arg933: tensor<bf16> loc("reduce.3600")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1657)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1657)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1657)
      %1395 = stablehlo.add %1311, %1394 : tensor<1x1x1440xbf16> loc(#loc1658)
      %1396 = stablehlo.convert %1395 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1659)
      %1397 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1398 = stablehlo.power %1396, %1397 : tensor<1x1x1440xf32> loc(#loc1660)
      %1399 = stablehlo.reduce(%1398 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1661)
      %1400 = "stablehlo.all_reduce"(%1399) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.3613"), %arg933: tensor<f32> loc("reduce.3613")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1661)
        stablehlo.return %4605 : tensor<f32> loc(#loc1661)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1661)
      %1401 = stablehlo.multiply %1400, %cst_1 : tensor<1x1xf32> loc(#loc1662)
      %1402 = stablehlo.reshape %1401 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1663)
      %1403 = stablehlo.add %1402, %cst_2 : tensor<1x1x1xf32> loc(#loc1664)
      %1404 = stablehlo.rsqrt %1403 : tensor<1x1x1xf32> loc(#loc1665)
      %1405 = stablehlo.reshape %1404 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1666)
      %1406 = stablehlo.broadcast_in_dim %1405, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1667)
      %1407 = stablehlo.multiply %1396, %1406 : tensor<1x1x1440xf32> loc(#loc1668)
      %1408 = stablehlo.multiply %1277, %1407 : tensor<1x1x1440xf32> loc(#loc1669)
      %1409 = stablehlo.convert %1408 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1670)
      %1410 = stablehlo.reshape %1409 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1671)
      %1411 = stablehlo.reshape %arg622 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc1672)
      %1412 = stablehlo.reshape %1411 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc1673)
      %1413 = stablehlo.transpose %1412, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc1674)
      %1414 = stablehlo.dot_general %1410, %1413, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1675)
      %1415 = "stablehlo.all_reduce"(%1414) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3805"), %arg933: tensor<bf16> loc("dot.3805")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1675)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1675)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1675)
      %1416 = stablehlo.reshape %1415 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1676)
      %1417 = stablehlo.reshape %arg621 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1677)
      %1418 = stablehlo.add %1416, %1417 : tensor<1x1x1024xbf16> loc(#loc1678)
      %1419 = stablehlo.reshape %1418 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1679)
      %1420 = stablehlo.slice %1419 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1680)
      %1421 = stablehlo.multiply %1420, %59 : tensor<1x16x1x32xbf16> loc(#loc1681)
      %1422 = stablehlo.slice %1419 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1682)
      %1423 = stablehlo.multiply %1422, %65 : tensor<1x16x1x32xbf16> loc(#loc1683)
      %1424 = stablehlo.subtract %1421, %1423 : tensor<1x16x1x32xbf16> loc(#loc1684)
      %1425 = stablehlo.multiply %1422, %59 : tensor<1x16x1x32xbf16> loc(#loc1685)
      %1426 = stablehlo.multiply %1420, %65 : tensor<1x16x1x32xbf16> loc(#loc1686)
      %1427 = stablehlo.add %1425, %1426 : tensor<1x16x1x32xbf16> loc(#loc1687)
      %1428 = stablehlo.concatenate %1424, %1427, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1688)
      %1429 = stablehlo.reshape %arg596 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1689)
      %1430 = stablehlo.reshape %1429 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1690)
      %1431 = stablehlo.transpose %1430, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1691)
      %1432 = stablehlo.dot_general %1410, %1431, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1692)
      %1433 = "stablehlo.all_reduce"(%1432) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3641"), %arg933: tensor<bf16> loc("dot.3641")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1692)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1692)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1692)
      %1434 = stablehlo.reshape %1433 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1693)
      %1435 = stablehlo.reshape %arg595 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1694)
      %1436 = stablehlo.add %1434, %1435 : tensor<1x1x128xbf16> loc(#loc1695)
      %1437 = stablehlo.reshape %1436 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1696)
      %1438 = stablehlo.slice %1437 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1697)
      %1439 = stablehlo.multiply %1438, %86 : tensor<1x2x1x32xbf16> loc(#loc1698)
      %1440 = stablehlo.slice %1437 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1699)
      %1441 = stablehlo.multiply %1440, %89 : tensor<1x2x1x32xbf16> loc(#loc1700)
      %1442 = stablehlo.subtract %1439, %1441 : tensor<1x2x1x32xbf16> loc(#loc1701)
      %1443 = stablehlo.multiply %1440, %86 : tensor<1x2x1x32xbf16> loc(#loc1702)
      %1444 = stablehlo.multiply %1438, %89 : tensor<1x2x1x32xbf16> loc(#loc1703)
      %1445 = stablehlo.add %1443, %1444 : tensor<1x2x1x32xbf16> loc(#loc1704)
      %1446 = stablehlo.concatenate %1442, %1445, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1705)
      %1447 = "stablehlo.scatter"(%arg610, %75, %1446) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.3688"), %arg933: tensor<bf16> loc("scatter.3688")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1706)
      %1448 = stablehlo.broadcast_in_dim %1447, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1707)
      %1449 = stablehlo.reshape %1448 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1708)
      %1450 = stablehlo.transpose %1449, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc1709)
      %1451 = stablehlo.dot_general %1428, %1450, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1710)
      %1452 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %1453 = stablehlo.multiply %1451, %1452 : tensor<1x16x1x256xbf16> loc(#loc1711)
      %1454 = stablehlo.add %1453, %325 : tensor<1x16x1x256xbf16> loc(#loc1712)
      %1455 = stablehlo.reshape %arg620 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc155)
      %1456 = "stablehlo.all_to_all"(%1455) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc155)
      %1457 = stablehlo.slice %1456 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc155)
      %1458 = stablehlo.reshape %1457 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc155)
      %1459 = stablehlo.reshape %1458 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1713)
      %1460 = stablehlo.reshape %1459 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc1714)
      %1461 = stablehlo.concatenate %1454, %1460, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1715)
      %1462 = stablehlo.reshape %arg628 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1716)
      %1463 = stablehlo.reshape %1462 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1717)
      %1464 = stablehlo.convert %1463 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1718)
      %1465 = stablehlo.reshape %1464 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1719)
      %1466 = stablehlo.reduce(%1461 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1720)
      %1467 = stablehlo.broadcast_in_dim %1466, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1721)
      %1468 = stablehlo.subtract %1461, %1467 : tensor<1x16x1x257xbf16> loc(#loc1722)
      %1469 = stablehlo.reduce(%1468 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1723)
      %1470 = stablehlo.broadcast_in_dim %1469, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1724)
      %1471 = stablehlo.subtract %1468, %1470 : tensor<1x16x1x257xbf16> loc(#loc1725)
      %1472 = stablehlo.exponential %1471 : tensor<1x16x1x257xbf16> loc(#loc1726)
      %1473 = stablehlo.reduce(%1472 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1727)
      %1474 = stablehlo.broadcast_in_dim %1473, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1728)
      %1475 = stablehlo.divide %1472, %1474 : tensor<1x16x1x257xbf16> loc(#loc1729)
      %1476 = stablehlo.slice %1475 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1730)
      %1477 = stablehlo.reshape %arg612 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1731)
      %1478 = stablehlo.reshape %1477 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1732)
      %1479 = stablehlo.transpose %1478, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1733)
      %1480 = stablehlo.dot_general %1410, %1479, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1734)
      %1481 = "stablehlo.all_reduce"(%1480) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3701"), %arg933: tensor<bf16> loc("dot.3701")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1734)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1734)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1734)
      %1482 = stablehlo.reshape %1481 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1735)
      %1483 = stablehlo.reshape %arg611 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1736)
      %1484 = stablehlo.add %1482, %1483 : tensor<1x1x128xbf16> loc(#loc1737)
      %1485 = stablehlo.reshape %1484 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1738)
      %1486 = "stablehlo.scatter"(%arg613, %75, %1485) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.3725"), %arg933: tensor<bf16> loc("scatter.3725")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1739)
      %1487 = stablehlo.broadcast_in_dim %1486, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1740)
      %1488 = stablehlo.reshape %1487 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1741)
      %1489 = stablehlo.dot_general %1476, %1488, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1742)
      %1490 = stablehlo.reshape %1489 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc1743)
      %1491 = stablehlo.reshape %arg619 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc1744)
      %1492 = stablehlo.reshape %1491 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc1745)
      %1493 = stablehlo.transpose %1492, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc1746)
      %1494 = stablehlo.dot_general %1490, %1493, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1747)
      %1495 = "stablehlo.all_reduce"(%1494) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.3912"), %arg933: tensor<bf16> loc("dot.3912")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1747)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1747)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1747)
      %1496 = stablehlo.reshape %1495 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1748)
      %1497 = stablehlo.reshape %arg618 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1749)
      %1498 = stablehlo.add %1496, %1497 : tensor<1x1x1440xbf16> loc(#loc1750)
      %1499 = stablehlo.add %1395, %1498 : tensor<1x1x1440xbf16> loc(#loc1751)
      %1500 = stablehlo.reshape %arg623 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1752)
      %1501 = stablehlo.reshape %1500 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1753)
      %1502 = stablehlo.convert %1501 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1754)
      %1503 = stablehlo.reshape %1502 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1755)
      %1504 = stablehlo.convert %1499 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1756)
      %1505 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1506 = stablehlo.power %1504, %1505 : tensor<1x1x1440xf32> loc(#loc1757)
      %1507 = stablehlo.reduce(%1506 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1758)
      %1508 = "stablehlo.all_reduce"(%1507) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.3930"), %arg933: tensor<f32> loc("reduce.3930")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1758)
        stablehlo.return %4605 : tensor<f32> loc(#loc1758)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1758)
      %1509 = stablehlo.multiply %1508, %cst_1 : tensor<1x1xf32> loc(#loc1759)
      %1510 = stablehlo.reshape %1509 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1760)
      %1511 = stablehlo.add %1510, %cst_2 : tensor<1x1x1xf32> loc(#loc1761)
      %1512 = stablehlo.rsqrt %1511 : tensor<1x1x1xf32> loc(#loc1762)
      %1513 = stablehlo.reshape %1512 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1763)
      %1514 = stablehlo.broadcast_in_dim %1513, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1764)
      %1515 = stablehlo.multiply %1504, %1514 : tensor<1x1x1440xf32> loc(#loc1765)
      %1516 = stablehlo.multiply %1503, %1515 : tensor<1x1x1440xf32> loc(#loc1766)
      %1517 = stablehlo.convert %1516 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1767)
      %1518 = stablehlo.reshape %1517 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1768)
      %1519 = stablehlo.broadcast_in_dim %1518, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1769)
      %1520 = stablehlo.dot_general %1519, %arg627, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1770)
      %1521 = "stablehlo.all_reduce"(%1520) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4044"), %arg933: tensor<bf16> loc("dot.4044")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1770)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1770)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1770)
      %1522 = stablehlo.reshape %arg626 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc1771)
      %1523 = stablehlo.reshape %1522 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1772)
      %1524 = stablehlo.add %1521, %1523 : tensor<8x1x5760xbf16> loc(#loc1773)
      %1525 = stablehlo.slice %1524 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1774)
      %1526 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1527 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1528 = stablehlo.clamp %1527, %1525, %1526 : tensor<8x1x2880xbf16> loc(#loc1775)
      %1529 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1530 = stablehlo.add %1528, %1529 : tensor<8x1x2880xbf16> loc(#loc1776)
      %1531 = stablehlo.slice %1524 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1777)
      %1532 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1533 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1534 = stablehlo.clamp %1532, %1531, %1533 : tensor<8x1x2880xbf16> loc(#loc1778)
      %1535 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1536 = stablehlo.multiply %1534, %1535 : tensor<8x1x2880xbf16> loc(#loc1779)
      %1537 = stablehlo.logistic %1536 : tensor<8x1x2880xbf16> loc(#loc1780)
      %1538 = stablehlo.multiply %1534, %1537 : tensor<8x1x2880xbf16> loc(#loc1781)
      %1539 = stablehlo.multiply %1530, %1538 : tensor<8x1x2880xbf16> loc(#loc1782)
      %1540 = stablehlo.dot_general %1539, %arg625, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1783)
      %1541 = stablehlo.reshape %arg624 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc1784)
      %1542 = stablehlo.reshape %1541 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1785)
      %1543 = stablehlo.add %1540, %1542 : tensor<8x1x1440xbf16> loc(#loc1786)
      %1544 = stablehlo.reshape %1543 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1787)
      %1545 = stablehlo.convert %1518 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc1788)
      %1546 = stablehlo.reshape %arg617 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc1789)
      %1547 = stablehlo.reshape %1546 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc1790)
      %1548 = stablehlo.transpose %1547, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc1791)
      %1549 = stablehlo.convert %1548 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc1792)
      %1550 = stablehlo.dot_general %1545, %1549, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc1793)
      %1551 = "stablehlo.all_reduce"(%1550) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.3959"), %arg933: tensor<f32> loc("dot.3959")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1793)
        stablehlo.return %4605 : tensor<f32> loc(#loc1793)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc1793)
      %1552 = stablehlo.reshape %arg616 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc1794)
      %1553 = stablehlo.reshape %1552 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc1795)
      %1554 = stablehlo.convert %1553 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc1796)
      %1555 = stablehlo.reshape %1554 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc1797)
      %1556 = stablehlo.add %1551, %1555 : tensor<1x32xf32> loc(#loc1798)
      %1557 = stablehlo.convert %1556 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc1799)
      %1558:2 = "stablehlo.sort"(%1557, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.3980"), %arg933: tensor<bf16> loc("sort.3980"), %arg934: tensor<i32> loc("sort.3980"), %arg935: tensor<i32> loc("sort.3980")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1801)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc1800)
      %1559 = stablehlo.slice %1558#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc1802)
      %1560 = stablehlo.convert %1559 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc1803)
      %1561 = stablehlo.reshape %1560 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc1804)
      %1562 = stablehlo.concatenate %c_16, %1561, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc1805)
      %1563 = stablehlo.slice %1558#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc1806)
      %1564 = stablehlo.reduce(%1563 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1807)
      %1565 = stablehlo.broadcast_in_dim %1564, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1808)
      %1566 = stablehlo.subtract %1563, %1565 : tensor<1x4xbf16> loc(#loc1809)
      %1567 = stablehlo.exponential %1566 : tensor<1x4xbf16> loc(#loc1810)
      %1568 = stablehlo.reduce(%1567 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1811)
      %1569 = stablehlo.broadcast_in_dim %1568, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1812)
      %1570 = stablehlo.divide %1567, %1569 : tensor<1x4xbf16> loc(#loc1813)
      %1571 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %1572 = "stablehlo.all_gather"(%1571) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %1573 = "stablehlo.scatter"(%1572, %1562, %1570) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.4014"), %arg933: tensor<bf16> loc("scatter.4014")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc1814)
      %1574 = stablehlo.reshape %1573 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc1814)
      %1575 = "stablehlo.all_to_all"(%1574) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc1814)
      %1576 = stablehlo.slice %1575 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc1814)
      %1577 = stablehlo.reshape %1576 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc1814)
      %1578 = stablehlo.reshape %1577 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc1815)
      %1579 = stablehlo.broadcast_in_dim %1578, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1816)
      %1580 = stablehlo.multiply %1544, %1579 : tensor<8x1x1x1440xbf16> loc(#loc1817)
      %1581 = stablehlo.reduce(%1580 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc1818)
      %1582 = "stablehlo.all_reduce"(%1581) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.4082"), %arg933: tensor<bf16> loc("reduce.4082")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1818)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1818)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1818)
      %1583 = stablehlo.add %1499, %1582 : tensor<1x1x1440xbf16> loc(#loc1819)
      %1584 = stablehlo.convert %1583 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1820)
      %1585 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1586 = stablehlo.power %1584, %1585 : tensor<1x1x1440xf32> loc(#loc1821)
      %1587 = stablehlo.reduce(%1586 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1822)
      %1588 = "stablehlo.all_reduce"(%1587) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.4095"), %arg933: tensor<f32> loc("reduce.4095")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1822)
        stablehlo.return %4605 : tensor<f32> loc(#loc1822)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1822)
      %1589 = stablehlo.multiply %1588, %cst_1 : tensor<1x1xf32> loc(#loc1823)
      %1590 = stablehlo.reshape %1589 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1824)
      %1591 = stablehlo.add %1590, %cst_2 : tensor<1x1x1xf32> loc(#loc1825)
      %1592 = stablehlo.rsqrt %1591 : tensor<1x1x1xf32> loc(#loc1826)
      %1593 = stablehlo.reshape %1592 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1827)
      %1594 = stablehlo.broadcast_in_dim %1593, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1828)
      %1595 = stablehlo.multiply %1584, %1594 : tensor<1x1x1440xf32> loc(#loc1829)
      %1596 = stablehlo.multiply %1465, %1595 : tensor<1x1x1440xf32> loc(#loc1830)
      %1597 = stablehlo.convert %1596 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1831)
      %1598 = stablehlo.reshape %1597 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1832)
      %1599 = stablehlo.reshape %arg641 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc1833)
      %1600 = stablehlo.reshape %1599 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc1834)
      %1601 = stablehlo.transpose %1600, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc1835)
      %1602 = stablehlo.dot_general %1598, %1601, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1836)
      %1603 = "stablehlo.all_reduce"(%1602) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4287"), %arg933: tensor<bf16> loc("dot.4287")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1836)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1836)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1836)
      %1604 = stablehlo.reshape %1603 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1837)
      %1605 = stablehlo.reshape %arg640 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1838)
      %1606 = stablehlo.add %1604, %1605 : tensor<1x1x1024xbf16> loc(#loc1839)
      %1607 = stablehlo.reshape %1606 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1840)
      %1608 = stablehlo.slice %1607 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1841)
      %1609 = stablehlo.multiply %1608, %59 : tensor<1x16x1x32xbf16> loc(#loc1842)
      %1610 = stablehlo.slice %1607 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc1843)
      %1611 = stablehlo.multiply %1610, %65 : tensor<1x16x1x32xbf16> loc(#loc1844)
      %1612 = stablehlo.subtract %1609, %1611 : tensor<1x16x1x32xbf16> loc(#loc1845)
      %1613 = stablehlo.multiply %1610, %59 : tensor<1x16x1x32xbf16> loc(#loc1846)
      %1614 = stablehlo.multiply %1608, %65 : tensor<1x16x1x32xbf16> loc(#loc1847)
      %1615 = stablehlo.add %1613, %1614 : tensor<1x16x1x32xbf16> loc(#loc1848)
      %1616 = stablehlo.concatenate %1612, %1615, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1849)
      %1617 = stablehlo.reshape %arg615 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1850)
      %1618 = stablehlo.reshape %1617 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1851)
      %1619 = stablehlo.transpose %1618, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1852)
      %1620 = stablehlo.dot_general %1598, %1619, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1853)
      %1621 = "stablehlo.all_reduce"(%1620) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4123"), %arg933: tensor<bf16> loc("dot.4123")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1853)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1853)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1853)
      %1622 = stablehlo.reshape %1621 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1854)
      %1623 = stablehlo.reshape %arg614 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1855)
      %1624 = stablehlo.add %1622, %1623 : tensor<1x1x128xbf16> loc(#loc1856)
      %1625 = stablehlo.reshape %1624 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1857)
      %1626 = stablehlo.slice %1625 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1858)
      %1627 = stablehlo.multiply %1626, %86 : tensor<1x2x1x32xbf16> loc(#loc1859)
      %1628 = stablehlo.slice %1625 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc1860)
      %1629 = stablehlo.multiply %1628, %89 : tensor<1x2x1x32xbf16> loc(#loc1861)
      %1630 = stablehlo.subtract %1627, %1629 : tensor<1x2x1x32xbf16> loc(#loc1862)
      %1631 = stablehlo.multiply %1628, %86 : tensor<1x2x1x32xbf16> loc(#loc1863)
      %1632 = stablehlo.multiply %1626, %89 : tensor<1x2x1x32xbf16> loc(#loc1864)
      %1633 = stablehlo.add %1631, %1632 : tensor<1x2x1x32xbf16> loc(#loc1865)
      %1634 = stablehlo.concatenate %1630, %1633, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1866)
      %1635 = "stablehlo.scatter"(%arg629, %75, %1634) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.4170"), %arg933: tensor<bf16> loc("scatter.4170")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1867)
      %1636 = stablehlo.broadcast_in_dim %1635, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1868)
      %1637 = stablehlo.reshape %1636 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1869)
      %1638 = stablehlo.transpose %1637, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc1870)
      %1639 = stablehlo.dot_general %1616, %1638, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1871)
      %1640 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %1641 = stablehlo.multiply %1639, %1640 : tensor<1x16x1x256xbf16> loc(#loc1872)
      %1642 = stablehlo.add %1641, %126 : tensor<1x16x1x256xbf16> loc(#loc1873)
      %1643 = stablehlo.reshape %arg639 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc174)
      %1644 = "stablehlo.all_to_all"(%1643) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc174)
      %1645 = stablehlo.slice %1644 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc174)
      %1646 = stablehlo.reshape %1645 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc174)
      %1647 = stablehlo.reshape %1646 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1874)
      %1648 = stablehlo.reshape %1647 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc1875)
      %1649 = stablehlo.concatenate %1642, %1648, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1876)
      %1650 = stablehlo.reshape %arg647 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1877)
      %1651 = stablehlo.reshape %1650 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1878)
      %1652 = stablehlo.convert %1651 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1879)
      %1653 = stablehlo.reshape %1652 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1880)
      %1654 = stablehlo.reduce(%1649 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1881)
      %1655 = stablehlo.broadcast_in_dim %1654, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1882)
      %1656 = stablehlo.subtract %1649, %1655 : tensor<1x16x1x257xbf16> loc(#loc1883)
      %1657 = stablehlo.reduce(%1656 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1884)
      %1658 = stablehlo.broadcast_in_dim %1657, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1885)
      %1659 = stablehlo.subtract %1656, %1658 : tensor<1x16x1x257xbf16> loc(#loc1886)
      %1660 = stablehlo.exponential %1659 : tensor<1x16x1x257xbf16> loc(#loc1887)
      %1661 = stablehlo.reduce(%1660 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc1888)
      %1662 = stablehlo.broadcast_in_dim %1661, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc1889)
      %1663 = stablehlo.divide %1660, %1662 : tensor<1x16x1x257xbf16> loc(#loc1890)
      %1664 = stablehlo.slice %1663 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc1891)
      %1665 = stablehlo.reshape %arg631 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc1892)
      %1666 = stablehlo.reshape %1665 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc1893)
      %1667 = stablehlo.transpose %1666, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc1894)
      %1668 = stablehlo.dot_general %1598, %1667, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc1895)
      %1669 = "stablehlo.all_reduce"(%1668) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4183"), %arg933: tensor<bf16> loc("dot.4183")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1895)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1895)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc1895)
      %1670 = stablehlo.reshape %1669 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1896)
      %1671 = stablehlo.reshape %arg630 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1897)
      %1672 = stablehlo.add %1670, %1671 : tensor<1x1x128xbf16> loc(#loc1898)
      %1673 = stablehlo.reshape %1672 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc1899)
      %1674 = "stablehlo.scatter"(%arg632, %75, %1673) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.4207"), %arg933: tensor<bf16> loc("scatter.4207")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc1900)
      %1675 = stablehlo.broadcast_in_dim %1674, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc1901)
      %1676 = stablehlo.reshape %1675 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc1902)
      %1677 = stablehlo.dot_general %1664, %1676, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc1903)
      %1678 = stablehlo.reshape %1677 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc1904)
      %1679 = stablehlo.reshape %arg638 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc1905)
      %1680 = stablehlo.reshape %1679 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc1906)
      %1681 = stablehlo.transpose %1680, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc1907)
      %1682 = stablehlo.dot_general %1678, %1681, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1908)
      %1683 = "stablehlo.all_reduce"(%1682) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4394"), %arg933: tensor<bf16> loc("dot.4394")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1908)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1908)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1908)
      %1684 = stablehlo.reshape %1683 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1909)
      %1685 = stablehlo.reshape %arg637 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1910)
      %1686 = stablehlo.add %1684, %1685 : tensor<1x1x1440xbf16> loc(#loc1911)
      %1687 = stablehlo.add %1583, %1686 : tensor<1x1x1440xbf16> loc(#loc1912)
      %1688 = stablehlo.reshape %arg642 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1913)
      %1689 = stablehlo.reshape %1688 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc1914)
      %1690 = stablehlo.convert %1689 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc1915)
      %1691 = stablehlo.reshape %1690 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc1916)
      %1692 = stablehlo.convert %1687 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1917)
      %1693 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1694 = stablehlo.power %1692, %1693 : tensor<1x1x1440xf32> loc(#loc1918)
      %1695 = stablehlo.reduce(%1694 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1919)
      %1696 = "stablehlo.all_reduce"(%1695) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.4412"), %arg933: tensor<f32> loc("reduce.4412")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1919)
        stablehlo.return %4605 : tensor<f32> loc(#loc1919)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1919)
      %1697 = stablehlo.multiply %1696, %cst_1 : tensor<1x1xf32> loc(#loc1920)
      %1698 = stablehlo.reshape %1697 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1921)
      %1699 = stablehlo.add %1698, %cst_2 : tensor<1x1x1xf32> loc(#loc1922)
      %1700 = stablehlo.rsqrt %1699 : tensor<1x1x1xf32> loc(#loc1923)
      %1701 = stablehlo.reshape %1700 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1924)
      %1702 = stablehlo.broadcast_in_dim %1701, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1925)
      %1703 = stablehlo.multiply %1692, %1702 : tensor<1x1x1440xf32> loc(#loc1926)
      %1704 = stablehlo.multiply %1691, %1703 : tensor<1x1x1440xf32> loc(#loc1927)
      %1705 = stablehlo.convert %1704 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1928)
      %1706 = stablehlo.reshape %1705 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1929)
      %1707 = stablehlo.broadcast_in_dim %1706, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1930)
      %1708 = stablehlo.dot_general %1707, %arg646, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1931)
      %1709 = "stablehlo.all_reduce"(%1708) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4526"), %arg933: tensor<bf16> loc("dot.4526")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1931)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1931)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1931)
      %1710 = stablehlo.reshape %arg645 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc1932)
      %1711 = stablehlo.reshape %1710 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc1933)
      %1712 = stablehlo.add %1709, %1711 : tensor<8x1x5760xbf16> loc(#loc1934)
      %1713 = stablehlo.slice %1712 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1935)
      %1714 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1715 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1716 = stablehlo.clamp %1715, %1713, %1714 : tensor<8x1x2880xbf16> loc(#loc1936)
      %1717 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1718 = stablehlo.add %1716, %1717 : tensor<8x1x2880xbf16> loc(#loc1937)
      %1719 = stablehlo.slice %1712 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc1938)
      %1720 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1721 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1722 = stablehlo.clamp %1720, %1719, %1721 : tensor<8x1x2880xbf16> loc(#loc1939)
      %1723 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1724 = stablehlo.multiply %1722, %1723 : tensor<8x1x2880xbf16> loc(#loc1940)
      %1725 = stablehlo.logistic %1724 : tensor<8x1x2880xbf16> loc(#loc1941)
      %1726 = stablehlo.multiply %1722, %1725 : tensor<8x1x2880xbf16> loc(#loc1942)
      %1727 = stablehlo.multiply %1718, %1726 : tensor<8x1x2880xbf16> loc(#loc1943)
      %1728 = stablehlo.dot_general %1727, %arg644, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1944)
      %1729 = stablehlo.reshape %arg643 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc1945)
      %1730 = stablehlo.reshape %1729 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc1946)
      %1731 = stablehlo.add %1728, %1730 : tensor<8x1x1440xbf16> loc(#loc1947)
      %1732 = stablehlo.reshape %1731 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1948)
      %1733 = stablehlo.convert %1706 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc1949)
      %1734 = stablehlo.reshape %arg636 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc1950)
      %1735 = stablehlo.reshape %1734 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc1951)
      %1736 = stablehlo.transpose %1735, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc1952)
      %1737 = stablehlo.convert %1736 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc1953)
      %1738 = stablehlo.dot_general %1733, %1737, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc1954)
      %1739 = "stablehlo.all_reduce"(%1738) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.4441"), %arg933: tensor<f32> loc("dot.4441")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1954)
        stablehlo.return %4605 : tensor<f32> loc(#loc1954)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc1954)
      %1740 = stablehlo.reshape %arg635 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc1955)
      %1741 = stablehlo.reshape %1740 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc1956)
      %1742 = stablehlo.convert %1741 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc1957)
      %1743 = stablehlo.reshape %1742 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc1958)
      %1744 = stablehlo.add %1739, %1743 : tensor<1x32xf32> loc(#loc1959)
      %1745 = stablehlo.convert %1744 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc1960)
      %1746:2 = "stablehlo.sort"(%1745, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.4462"), %arg933: tensor<bf16> loc("sort.4462"), %arg934: tensor<i32> loc("sort.4462"), %arg935: tensor<i32> loc("sort.4462")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1962)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc1961)
      %1747 = stablehlo.slice %1746#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc1963)
      %1748 = stablehlo.convert %1747 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc1964)
      %1749 = stablehlo.reshape %1748 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc1965)
      %1750 = stablehlo.concatenate %c_16, %1749, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc1966)
      %1751 = stablehlo.slice %1746#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc1967)
      %1752 = stablehlo.reduce(%1751 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1968)
      %1753 = stablehlo.broadcast_in_dim %1752, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1969)
      %1754 = stablehlo.subtract %1751, %1753 : tensor<1x4xbf16> loc(#loc1970)
      %1755 = stablehlo.exponential %1754 : tensor<1x4xbf16> loc(#loc1971)
      %1756 = stablehlo.reduce(%1755 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc1972)
      %1757 = stablehlo.broadcast_in_dim %1756, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc1973)
      %1758 = stablehlo.divide %1755, %1757 : tensor<1x4xbf16> loc(#loc1974)
      %1759 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %1760 = "stablehlo.all_gather"(%1759) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %1761 = "stablehlo.scatter"(%1760, %1750, %1758) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.4496"), %arg933: tensor<bf16> loc("scatter.4496")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc1975)
      %1762 = stablehlo.reshape %1761 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc1975)
      %1763 = "stablehlo.all_to_all"(%1762) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc1975)
      %1764 = stablehlo.slice %1763 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc1975)
      %1765 = stablehlo.reshape %1764 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc1975)
      %1766 = stablehlo.reshape %1765 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc1976)
      %1767 = stablehlo.broadcast_in_dim %1766, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc1977)
      %1768 = stablehlo.multiply %1732, %1767 : tensor<8x1x1x1440xbf16> loc(#loc1978)
      %1769 = stablehlo.reduce(%1768 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc1979)
      %1770 = "stablehlo.all_reduce"(%1769) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.4564"), %arg933: tensor<bf16> loc("reduce.4564")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1979)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1979)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc1979)
      %1771 = stablehlo.add %1687, %1770 : tensor<1x1x1440xbf16> loc(#loc1980)
      %1772 = stablehlo.convert %1771 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc1981)
      %1773 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1774 = stablehlo.power %1772, %1773 : tensor<1x1x1440xf32> loc(#loc1982)
      %1775 = stablehlo.reduce(%1774 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc1983)
      %1776 = "stablehlo.all_reduce"(%1775) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.4577"), %arg933: tensor<f32> loc("reduce.4577")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc1983)
        stablehlo.return %4605 : tensor<f32> loc(#loc1983)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc1983)
      %1777 = stablehlo.multiply %1776, %cst_1 : tensor<1x1xf32> loc(#loc1984)
      %1778 = stablehlo.reshape %1777 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc1985)
      %1779 = stablehlo.add %1778, %cst_2 : tensor<1x1x1xf32> loc(#loc1986)
      %1780 = stablehlo.rsqrt %1779 : tensor<1x1x1xf32> loc(#loc1987)
      %1781 = stablehlo.reshape %1780 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc1988)
      %1782 = stablehlo.broadcast_in_dim %1781, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc1989)
      %1783 = stablehlo.multiply %1772, %1782 : tensor<1x1x1440xf32> loc(#loc1990)
      %1784 = stablehlo.multiply %1653, %1783 : tensor<1x1x1440xf32> loc(#loc1991)
      %1785 = stablehlo.convert %1784 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc1992)
      %1786 = stablehlo.reshape %1785 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc1993)
      %1787 = stablehlo.reshape %arg660 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc1994)
      %1788 = stablehlo.reshape %1787 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc1995)
      %1789 = stablehlo.transpose %1788, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc1996)
      %1790 = stablehlo.dot_general %1786, %1789, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1997)
      %1791 = "stablehlo.all_reduce"(%1790) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4769"), %arg933: tensor<bf16> loc("dot.4769")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc1997)
        stablehlo.return %4605 : tensor<bf16> loc(#loc1997)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc1997)
      %1792 = stablehlo.reshape %1791 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1998)
      %1793 = stablehlo.reshape %arg659 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1999)
      %1794 = stablehlo.add %1792, %1793 : tensor<1x1x1024xbf16> loc(#loc2000)
      %1795 = stablehlo.reshape %1794 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2001)
      %1796 = stablehlo.slice %1795 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2002)
      %1797 = stablehlo.multiply %1796, %59 : tensor<1x16x1x32xbf16> loc(#loc2003)
      %1798 = stablehlo.slice %1795 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2004)
      %1799 = stablehlo.multiply %1798, %65 : tensor<1x16x1x32xbf16> loc(#loc2005)
      %1800 = stablehlo.subtract %1797, %1799 : tensor<1x16x1x32xbf16> loc(#loc2006)
      %1801 = stablehlo.multiply %1798, %59 : tensor<1x16x1x32xbf16> loc(#loc2007)
      %1802 = stablehlo.multiply %1796, %65 : tensor<1x16x1x32xbf16> loc(#loc2008)
      %1803 = stablehlo.add %1801, %1802 : tensor<1x16x1x32xbf16> loc(#loc2009)
      %1804 = stablehlo.concatenate %1800, %1803, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2010)
      %1805 = stablehlo.reshape %arg634 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2011)
      %1806 = stablehlo.reshape %1805 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2012)
      %1807 = stablehlo.transpose %1806, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2013)
      %1808 = stablehlo.dot_general %1786, %1807, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2014)
      %1809 = "stablehlo.all_reduce"(%1808) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4605"), %arg933: tensor<bf16> loc("dot.4605")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2014)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2014)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2014)
      %1810 = stablehlo.reshape %1809 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2015)
      %1811 = stablehlo.reshape %arg633 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2016)
      %1812 = stablehlo.add %1810, %1811 : tensor<1x1x128xbf16> loc(#loc2017)
      %1813 = stablehlo.reshape %1812 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2018)
      %1814 = stablehlo.slice %1813 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2019)
      %1815 = stablehlo.multiply %1814, %86 : tensor<1x2x1x32xbf16> loc(#loc2020)
      %1816 = stablehlo.slice %1813 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2021)
      %1817 = stablehlo.multiply %1816, %89 : tensor<1x2x1x32xbf16> loc(#loc2022)
      %1818 = stablehlo.subtract %1815, %1817 : tensor<1x2x1x32xbf16> loc(#loc2023)
      %1819 = stablehlo.multiply %1816, %86 : tensor<1x2x1x32xbf16> loc(#loc2024)
      %1820 = stablehlo.multiply %1814, %89 : tensor<1x2x1x32xbf16> loc(#loc2025)
      %1821 = stablehlo.add %1819, %1820 : tensor<1x2x1x32xbf16> loc(#loc2026)
      %1822 = stablehlo.concatenate %1818, %1821, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2027)
      %1823 = "stablehlo.scatter"(%arg648, %75, %1822) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.4652"), %arg933: tensor<bf16> loc("scatter.4652")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2028)
      %1824 = stablehlo.broadcast_in_dim %1823, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2029)
      %1825 = stablehlo.reshape %1824 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2030)
      %1826 = stablehlo.transpose %1825, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc2031)
      %1827 = stablehlo.dot_general %1804, %1826, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2032)
      %1828 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %1829 = stablehlo.multiply %1827, %1828 : tensor<1x16x1x256xbf16> loc(#loc2033)
      %1830 = stablehlo.add %1829, %325 : tensor<1x16x1x256xbf16> loc(#loc2034)
      %1831 = stablehlo.reshape %arg658 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc193)
      %1832 = "stablehlo.all_to_all"(%1831) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc193)
      %1833 = stablehlo.slice %1832 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc193)
      %1834 = stablehlo.reshape %1833 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc193)
      %1835 = stablehlo.reshape %1834 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2035)
      %1836 = stablehlo.reshape %1835 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc2036)
      %1837 = stablehlo.concatenate %1830, %1836, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2037)
      %1838 = stablehlo.reshape %arg666 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2038)
      %1839 = stablehlo.reshape %1838 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2039)
      %1840 = stablehlo.convert %1839 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2040)
      %1841 = stablehlo.reshape %1840 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2041)
      %1842 = stablehlo.reduce(%1837 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2042)
      %1843 = stablehlo.broadcast_in_dim %1842, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2043)
      %1844 = stablehlo.subtract %1837, %1843 : tensor<1x16x1x257xbf16> loc(#loc2044)
      %1845 = stablehlo.reduce(%1844 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2045)
      %1846 = stablehlo.broadcast_in_dim %1845, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2046)
      %1847 = stablehlo.subtract %1844, %1846 : tensor<1x16x1x257xbf16> loc(#loc2047)
      %1848 = stablehlo.exponential %1847 : tensor<1x16x1x257xbf16> loc(#loc2048)
      %1849 = stablehlo.reduce(%1848 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2049)
      %1850 = stablehlo.broadcast_in_dim %1849, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2050)
      %1851 = stablehlo.divide %1848, %1850 : tensor<1x16x1x257xbf16> loc(#loc2051)
      %1852 = stablehlo.slice %1851 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2052)
      %1853 = stablehlo.reshape %arg650 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2053)
      %1854 = stablehlo.reshape %1853 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2054)
      %1855 = stablehlo.transpose %1854, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2055)
      %1856 = stablehlo.dot_general %1786, %1855, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2056)
      %1857 = "stablehlo.all_reduce"(%1856) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4665"), %arg933: tensor<bf16> loc("dot.4665")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2056)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2056)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2056)
      %1858 = stablehlo.reshape %1857 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2057)
      %1859 = stablehlo.reshape %arg649 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2058)
      %1860 = stablehlo.add %1858, %1859 : tensor<1x1x128xbf16> loc(#loc2059)
      %1861 = stablehlo.reshape %1860 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2060)
      %1862 = "stablehlo.scatter"(%arg651, %75, %1861) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.4689"), %arg933: tensor<bf16> loc("scatter.4689")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2061)
      %1863 = stablehlo.broadcast_in_dim %1862, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2062)
      %1864 = stablehlo.reshape %1863 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2063)
      %1865 = stablehlo.dot_general %1852, %1864, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2064)
      %1866 = stablehlo.reshape %1865 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc2065)
      %1867 = stablehlo.reshape %arg657 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc2066)
      %1868 = stablehlo.reshape %1867 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc2067)
      %1869 = stablehlo.transpose %1868, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc2068)
      %1870 = stablehlo.dot_general %1866, %1869, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2069)
      %1871 = "stablehlo.all_reduce"(%1870) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.4876"), %arg933: tensor<bf16> loc("dot.4876")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2069)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2069)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2069)
      %1872 = stablehlo.reshape %1871 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2070)
      %1873 = stablehlo.reshape %arg656 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2071)
      %1874 = stablehlo.add %1872, %1873 : tensor<1x1x1440xbf16> loc(#loc2072)
      %1875 = stablehlo.add %1771, %1874 : tensor<1x1x1440xbf16> loc(#loc2073)
      %1876 = stablehlo.reshape %arg661 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2074)
      %1877 = stablehlo.reshape %1876 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2075)
      %1878 = stablehlo.convert %1877 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2076)
      %1879 = stablehlo.reshape %1878 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2077)
      %1880 = stablehlo.convert %1875 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2078)
      %1881 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1882 = stablehlo.power %1880, %1881 : tensor<1x1x1440xf32> loc(#loc2079)
      %1883 = stablehlo.reduce(%1882 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2080)
      %1884 = "stablehlo.all_reduce"(%1883) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.4894"), %arg933: tensor<f32> loc("reduce.4894")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2080)
        stablehlo.return %4605 : tensor<f32> loc(#loc2080)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2080)
      %1885 = stablehlo.multiply %1884, %cst_1 : tensor<1x1xf32> loc(#loc2081)
      %1886 = stablehlo.reshape %1885 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2082)
      %1887 = stablehlo.add %1886, %cst_2 : tensor<1x1x1xf32> loc(#loc2083)
      %1888 = stablehlo.rsqrt %1887 : tensor<1x1x1xf32> loc(#loc2084)
      %1889 = stablehlo.reshape %1888 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2085)
      %1890 = stablehlo.broadcast_in_dim %1889, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2086)
      %1891 = stablehlo.multiply %1880, %1890 : tensor<1x1x1440xf32> loc(#loc2087)
      %1892 = stablehlo.multiply %1879, %1891 : tensor<1x1x1440xf32> loc(#loc2088)
      %1893 = stablehlo.convert %1892 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2089)
      %1894 = stablehlo.reshape %1893 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2090)
      %1895 = stablehlo.broadcast_in_dim %1894, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2091)
      %1896 = stablehlo.dot_general %1895, %arg665, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2092)
      %1897 = "stablehlo.all_reduce"(%1896) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5008"), %arg933: tensor<bf16> loc("dot.5008")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2092)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2092)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2092)
      %1898 = stablehlo.reshape %arg664 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc2093)
      %1899 = stablehlo.reshape %1898 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2094)
      %1900 = stablehlo.add %1897, %1899 : tensor<8x1x5760xbf16> loc(#loc2095)
      %1901 = stablehlo.slice %1900 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2096)
      %1902 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1903 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1904 = stablehlo.clamp %1903, %1901, %1902 : tensor<8x1x2880xbf16> loc(#loc2097)
      %1905 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1906 = stablehlo.add %1904, %1905 : tensor<8x1x2880xbf16> loc(#loc2098)
      %1907 = stablehlo.slice %1900 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2099)
      %1908 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1909 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1910 = stablehlo.clamp %1908, %1907, %1909 : tensor<8x1x2880xbf16> loc(#loc2100)
      %1911 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %1912 = stablehlo.multiply %1910, %1911 : tensor<8x1x2880xbf16> loc(#loc2101)
      %1913 = stablehlo.logistic %1912 : tensor<8x1x2880xbf16> loc(#loc2102)
      %1914 = stablehlo.multiply %1910, %1913 : tensor<8x1x2880xbf16> loc(#loc2103)
      %1915 = stablehlo.multiply %1906, %1914 : tensor<8x1x2880xbf16> loc(#loc2104)
      %1916 = stablehlo.dot_general %1915, %arg663, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2105)
      %1917 = stablehlo.reshape %arg662 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc2106)
      %1918 = stablehlo.reshape %1917 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2107)
      %1919 = stablehlo.add %1916, %1918 : tensor<8x1x1440xbf16> loc(#loc2108)
      %1920 = stablehlo.reshape %1919 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2109)
      %1921 = stablehlo.convert %1894 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc2110)
      %1922 = stablehlo.reshape %arg655 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc2111)
      %1923 = stablehlo.reshape %1922 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc2112)
      %1924 = stablehlo.transpose %1923, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc2113)
      %1925 = stablehlo.convert %1924 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc2114)
      %1926 = stablehlo.dot_general %1921, %1925, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc2115)
      %1927 = "stablehlo.all_reduce"(%1926) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.4923"), %arg933: tensor<f32> loc("dot.4923")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2115)
        stablehlo.return %4605 : tensor<f32> loc(#loc2115)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc2115)
      %1928 = stablehlo.reshape %arg654 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc2116)
      %1929 = stablehlo.reshape %1928 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc2117)
      %1930 = stablehlo.convert %1929 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc2118)
      %1931 = stablehlo.reshape %1930 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc2119)
      %1932 = stablehlo.add %1927, %1931 : tensor<1x32xf32> loc(#loc2120)
      %1933 = stablehlo.convert %1932 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc2121)
      %1934:2 = "stablehlo.sort"(%1933, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.4944"), %arg933: tensor<bf16> loc("sort.4944"), %arg934: tensor<i32> loc("sort.4944"), %arg935: tensor<i32> loc("sort.4944")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2123)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc2122)
      %1935 = stablehlo.slice %1934#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc2124)
      %1936 = stablehlo.convert %1935 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc2125)
      %1937 = stablehlo.reshape %1936 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc2126)
      %1938 = stablehlo.concatenate %c_16, %1937, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc2127)
      %1939 = stablehlo.slice %1934#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc2128)
      %1940 = stablehlo.reduce(%1939 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2129)
      %1941 = stablehlo.broadcast_in_dim %1940, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2130)
      %1942 = stablehlo.subtract %1939, %1941 : tensor<1x4xbf16> loc(#loc2131)
      %1943 = stablehlo.exponential %1942 : tensor<1x4xbf16> loc(#loc2132)
      %1944 = stablehlo.reduce(%1943 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2133)
      %1945 = stablehlo.broadcast_in_dim %1944, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2134)
      %1946 = stablehlo.divide %1943, %1945 : tensor<1x4xbf16> loc(#loc2135)
      %1947 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %1948 = "stablehlo.all_gather"(%1947) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %1949 = "stablehlo.scatter"(%1948, %1938, %1946) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.4978"), %arg933: tensor<bf16> loc("scatter.4978")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc2136)
      %1950 = stablehlo.reshape %1949 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc2136)
      %1951 = "stablehlo.all_to_all"(%1950) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc2136)
      %1952 = stablehlo.slice %1951 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc2136)
      %1953 = stablehlo.reshape %1952 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc2136)
      %1954 = stablehlo.reshape %1953 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc2137)
      %1955 = stablehlo.broadcast_in_dim %1954, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2138)
      %1956 = stablehlo.multiply %1920, %1955 : tensor<8x1x1x1440xbf16> loc(#loc2139)
      %1957 = stablehlo.reduce(%1956 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc2140)
      %1958 = "stablehlo.all_reduce"(%1957) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.5046"), %arg933: tensor<bf16> loc("reduce.5046")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2140)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2140)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2140)
      %1959 = stablehlo.add %1875, %1958 : tensor<1x1x1440xbf16> loc(#loc2141)
      %1960 = stablehlo.convert %1959 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2142)
      %1961 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %1962 = stablehlo.power %1960, %1961 : tensor<1x1x1440xf32> loc(#loc2143)
      %1963 = stablehlo.reduce(%1962 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2144)
      %1964 = "stablehlo.all_reduce"(%1963) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.5059"), %arg933: tensor<f32> loc("reduce.5059")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2144)
        stablehlo.return %4605 : tensor<f32> loc(#loc2144)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2144)
      %1965 = stablehlo.multiply %1964, %cst_1 : tensor<1x1xf32> loc(#loc2145)
      %1966 = stablehlo.reshape %1965 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2146)
      %1967 = stablehlo.add %1966, %cst_2 : tensor<1x1x1xf32> loc(#loc2147)
      %1968 = stablehlo.rsqrt %1967 : tensor<1x1x1xf32> loc(#loc2148)
      %1969 = stablehlo.reshape %1968 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2149)
      %1970 = stablehlo.broadcast_in_dim %1969, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2150)
      %1971 = stablehlo.multiply %1960, %1970 : tensor<1x1x1440xf32> loc(#loc2151)
      %1972 = stablehlo.multiply %1841, %1971 : tensor<1x1x1440xf32> loc(#loc2152)
      %1973 = stablehlo.convert %1972 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2153)
      %1974 = stablehlo.reshape %1973 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2154)
      %1975 = stablehlo.reshape %arg679 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc2155)
      %1976 = stablehlo.reshape %1975 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc2156)
      %1977 = stablehlo.transpose %1976, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc2157)
      %1978 = stablehlo.dot_general %1974, %1977, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2158)
      %1979 = "stablehlo.all_reduce"(%1978) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5251"), %arg933: tensor<bf16> loc("dot.5251")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2158)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2158)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2158)
      %1980 = stablehlo.reshape %1979 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2159)
      %1981 = stablehlo.reshape %arg678 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2160)
      %1982 = stablehlo.add %1980, %1981 : tensor<1x1x1024xbf16> loc(#loc2161)
      %1983 = stablehlo.reshape %1982 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2162)
      %1984 = stablehlo.slice %1983 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2163)
      %1985 = stablehlo.multiply %1984, %59 : tensor<1x16x1x32xbf16> loc(#loc2164)
      %1986 = stablehlo.slice %1983 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2165)
      %1987 = stablehlo.multiply %1986, %65 : tensor<1x16x1x32xbf16> loc(#loc2166)
      %1988 = stablehlo.subtract %1985, %1987 : tensor<1x16x1x32xbf16> loc(#loc2167)
      %1989 = stablehlo.multiply %1986, %59 : tensor<1x16x1x32xbf16> loc(#loc2168)
      %1990 = stablehlo.multiply %1984, %65 : tensor<1x16x1x32xbf16> loc(#loc2169)
      %1991 = stablehlo.add %1989, %1990 : tensor<1x16x1x32xbf16> loc(#loc2170)
      %1992 = stablehlo.concatenate %1988, %1991, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2171)
      %1993 = stablehlo.reshape %arg653 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2172)
      %1994 = stablehlo.reshape %1993 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2173)
      %1995 = stablehlo.transpose %1994, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2174)
      %1996 = stablehlo.dot_general %1974, %1995, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2175)
      %1997 = "stablehlo.all_reduce"(%1996) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5087"), %arg933: tensor<bf16> loc("dot.5087")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2175)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2175)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2175)
      %1998 = stablehlo.reshape %1997 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2176)
      %1999 = stablehlo.reshape %arg652 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2177)
      %2000 = stablehlo.add %1998, %1999 : tensor<1x1x128xbf16> loc(#loc2178)
      %2001 = stablehlo.reshape %2000 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2179)
      %2002 = stablehlo.slice %2001 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2180)
      %2003 = stablehlo.multiply %2002, %86 : tensor<1x2x1x32xbf16> loc(#loc2181)
      %2004 = stablehlo.slice %2001 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2182)
      %2005 = stablehlo.multiply %2004, %89 : tensor<1x2x1x32xbf16> loc(#loc2183)
      %2006 = stablehlo.subtract %2003, %2005 : tensor<1x2x1x32xbf16> loc(#loc2184)
      %2007 = stablehlo.multiply %2004, %86 : tensor<1x2x1x32xbf16> loc(#loc2185)
      %2008 = stablehlo.multiply %2002, %89 : tensor<1x2x1x32xbf16> loc(#loc2186)
      %2009 = stablehlo.add %2007, %2008 : tensor<1x2x1x32xbf16> loc(#loc2187)
      %2010 = stablehlo.concatenate %2006, %2009, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2188)
      %2011 = "stablehlo.scatter"(%arg667, %75, %2010) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.5134"), %arg933: tensor<bf16> loc("scatter.5134")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2189)
      %2012 = stablehlo.broadcast_in_dim %2011, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2190)
      %2013 = stablehlo.reshape %2012 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2191)
      %2014 = stablehlo.transpose %2013, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc2192)
      %2015 = stablehlo.dot_general %1992, %2014, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2193)
      %2016 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %2017 = stablehlo.multiply %2015, %2016 : tensor<1x16x1x256xbf16> loc(#loc2194)
      %2018 = stablehlo.add %2017, %126 : tensor<1x16x1x256xbf16> loc(#loc2195)
      %2019 = stablehlo.reshape %arg677 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc212)
      %2020 = "stablehlo.all_to_all"(%2019) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc212)
      %2021 = stablehlo.slice %2020 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc212)
      %2022 = stablehlo.reshape %2021 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc212)
      %2023 = stablehlo.reshape %2022 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2196)
      %2024 = stablehlo.reshape %2023 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc2197)
      %2025 = stablehlo.concatenate %2018, %2024, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2198)
      %2026 = stablehlo.reshape %arg685 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2199)
      %2027 = stablehlo.reshape %2026 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2200)
      %2028 = stablehlo.convert %2027 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2201)
      %2029 = stablehlo.reshape %2028 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2202)
      %2030 = stablehlo.reduce(%2025 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2203)
      %2031 = stablehlo.broadcast_in_dim %2030, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2204)
      %2032 = stablehlo.subtract %2025, %2031 : tensor<1x16x1x257xbf16> loc(#loc2205)
      %2033 = stablehlo.reduce(%2032 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2206)
      %2034 = stablehlo.broadcast_in_dim %2033, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2207)
      %2035 = stablehlo.subtract %2032, %2034 : tensor<1x16x1x257xbf16> loc(#loc2208)
      %2036 = stablehlo.exponential %2035 : tensor<1x16x1x257xbf16> loc(#loc2209)
      %2037 = stablehlo.reduce(%2036 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2210)
      %2038 = stablehlo.broadcast_in_dim %2037, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2211)
      %2039 = stablehlo.divide %2036, %2038 : tensor<1x16x1x257xbf16> loc(#loc2212)
      %2040 = stablehlo.slice %2039 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2213)
      %2041 = stablehlo.reshape %arg669 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2214)
      %2042 = stablehlo.reshape %2041 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2215)
      %2043 = stablehlo.transpose %2042, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2216)
      %2044 = stablehlo.dot_general %1974, %2043, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2217)
      %2045 = "stablehlo.all_reduce"(%2044) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5147"), %arg933: tensor<bf16> loc("dot.5147")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2217)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2217)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2217)
      %2046 = stablehlo.reshape %2045 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2218)
      %2047 = stablehlo.reshape %arg668 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2219)
      %2048 = stablehlo.add %2046, %2047 : tensor<1x1x128xbf16> loc(#loc2220)
      %2049 = stablehlo.reshape %2048 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2221)
      %2050 = "stablehlo.scatter"(%arg670, %75, %2049) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.5171"), %arg933: tensor<bf16> loc("scatter.5171")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2222)
      %2051 = stablehlo.broadcast_in_dim %2050, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2223)
      %2052 = stablehlo.reshape %2051 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2224)
      %2053 = stablehlo.dot_general %2040, %2052, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2225)
      %2054 = stablehlo.reshape %2053 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc2226)
      %2055 = stablehlo.reshape %arg676 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc2227)
      %2056 = stablehlo.reshape %2055 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc2228)
      %2057 = stablehlo.transpose %2056, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc2229)
      %2058 = stablehlo.dot_general %2054, %2057, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2230)
      %2059 = "stablehlo.all_reduce"(%2058) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5358"), %arg933: tensor<bf16> loc("dot.5358")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2230)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2230)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2230)
      %2060 = stablehlo.reshape %2059 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2231)
      %2061 = stablehlo.reshape %arg675 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2232)
      %2062 = stablehlo.add %2060, %2061 : tensor<1x1x1440xbf16> loc(#loc2233)
      %2063 = stablehlo.add %1959, %2062 : tensor<1x1x1440xbf16> loc(#loc2234)
      %2064 = stablehlo.reshape %arg680 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2235)
      %2065 = stablehlo.reshape %2064 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2236)
      %2066 = stablehlo.convert %2065 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2237)
      %2067 = stablehlo.reshape %2066 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2238)
      %2068 = stablehlo.convert %2063 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2239)
      %2069 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2070 = stablehlo.power %2068, %2069 : tensor<1x1x1440xf32> loc(#loc2240)
      %2071 = stablehlo.reduce(%2070 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2241)
      %2072 = "stablehlo.all_reduce"(%2071) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.5376"), %arg933: tensor<f32> loc("reduce.5376")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2241)
        stablehlo.return %4605 : tensor<f32> loc(#loc2241)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2241)
      %2073 = stablehlo.multiply %2072, %cst_1 : tensor<1x1xf32> loc(#loc2242)
      %2074 = stablehlo.reshape %2073 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2243)
      %2075 = stablehlo.add %2074, %cst_2 : tensor<1x1x1xf32> loc(#loc2244)
      %2076 = stablehlo.rsqrt %2075 : tensor<1x1x1xf32> loc(#loc2245)
      %2077 = stablehlo.reshape %2076 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2246)
      %2078 = stablehlo.broadcast_in_dim %2077, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2247)
      %2079 = stablehlo.multiply %2068, %2078 : tensor<1x1x1440xf32> loc(#loc2248)
      %2080 = stablehlo.multiply %2067, %2079 : tensor<1x1x1440xf32> loc(#loc2249)
      %2081 = stablehlo.convert %2080 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2250)
      %2082 = stablehlo.reshape %2081 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2251)
      %2083 = stablehlo.broadcast_in_dim %2082, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2252)
      %2084 = stablehlo.dot_general %2083, %arg684, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2253)
      %2085 = "stablehlo.all_reduce"(%2084) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5490"), %arg933: tensor<bf16> loc("dot.5490")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2253)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2253)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2253)
      %2086 = stablehlo.reshape %arg683 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc2254)
      %2087 = stablehlo.reshape %2086 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2255)
      %2088 = stablehlo.add %2085, %2087 : tensor<8x1x5760xbf16> loc(#loc2256)
      %2089 = stablehlo.slice %2088 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2257)
      %2090 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2091 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2092 = stablehlo.clamp %2091, %2089, %2090 : tensor<8x1x2880xbf16> loc(#loc2258)
      %2093 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2094 = stablehlo.add %2092, %2093 : tensor<8x1x2880xbf16> loc(#loc2259)
      %2095 = stablehlo.slice %2088 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2260)
      %2096 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2097 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2098 = stablehlo.clamp %2096, %2095, %2097 : tensor<8x1x2880xbf16> loc(#loc2261)
      %2099 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2100 = stablehlo.multiply %2098, %2099 : tensor<8x1x2880xbf16> loc(#loc2262)
      %2101 = stablehlo.logistic %2100 : tensor<8x1x2880xbf16> loc(#loc2263)
      %2102 = stablehlo.multiply %2098, %2101 : tensor<8x1x2880xbf16> loc(#loc2264)
      %2103 = stablehlo.multiply %2094, %2102 : tensor<8x1x2880xbf16> loc(#loc2265)
      %2104 = stablehlo.dot_general %2103, %arg682, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2266)
      %2105 = stablehlo.reshape %arg681 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc2267)
      %2106 = stablehlo.reshape %2105 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2268)
      %2107 = stablehlo.add %2104, %2106 : tensor<8x1x1440xbf16> loc(#loc2269)
      %2108 = stablehlo.reshape %2107 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2270)
      %2109 = stablehlo.convert %2082 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc2271)
      %2110 = stablehlo.reshape %arg674 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc2272)
      %2111 = stablehlo.reshape %2110 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc2273)
      %2112 = stablehlo.transpose %2111, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc2274)
      %2113 = stablehlo.convert %2112 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc2275)
      %2114 = stablehlo.dot_general %2109, %2113, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc2276)
      %2115 = "stablehlo.all_reduce"(%2114) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.5405"), %arg933: tensor<f32> loc("dot.5405")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2276)
        stablehlo.return %4605 : tensor<f32> loc(#loc2276)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc2276)
      %2116 = stablehlo.reshape %arg673 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc2277)
      %2117 = stablehlo.reshape %2116 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc2278)
      %2118 = stablehlo.convert %2117 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc2279)
      %2119 = stablehlo.reshape %2118 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc2280)
      %2120 = stablehlo.add %2115, %2119 : tensor<1x32xf32> loc(#loc2281)
      %2121 = stablehlo.convert %2120 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc2282)
      %2122:2 = "stablehlo.sort"(%2121, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.5426"), %arg933: tensor<bf16> loc("sort.5426"), %arg934: tensor<i32> loc("sort.5426"), %arg935: tensor<i32> loc("sort.5426")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2284)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc2283)
      %2123 = stablehlo.slice %2122#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc2285)
      %2124 = stablehlo.convert %2123 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc2286)
      %2125 = stablehlo.reshape %2124 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc2287)
      %2126 = stablehlo.concatenate %c_16, %2125, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc2288)
      %2127 = stablehlo.slice %2122#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc2289)
      %2128 = stablehlo.reduce(%2127 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2290)
      %2129 = stablehlo.broadcast_in_dim %2128, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2291)
      %2130 = stablehlo.subtract %2127, %2129 : tensor<1x4xbf16> loc(#loc2292)
      %2131 = stablehlo.exponential %2130 : tensor<1x4xbf16> loc(#loc2293)
      %2132 = stablehlo.reduce(%2131 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2294)
      %2133 = stablehlo.broadcast_in_dim %2132, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2295)
      %2134 = stablehlo.divide %2131, %2133 : tensor<1x4xbf16> loc(#loc2296)
      %2135 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %2136 = "stablehlo.all_gather"(%2135) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %2137 = "stablehlo.scatter"(%2136, %2126, %2134) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.5460"), %arg933: tensor<bf16> loc("scatter.5460")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc2297)
      %2138 = stablehlo.reshape %2137 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc2297)
      %2139 = "stablehlo.all_to_all"(%2138) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc2297)
      %2140 = stablehlo.slice %2139 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc2297)
      %2141 = stablehlo.reshape %2140 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc2297)
      %2142 = stablehlo.reshape %2141 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc2298)
      %2143 = stablehlo.broadcast_in_dim %2142, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2299)
      %2144 = stablehlo.multiply %2108, %2143 : tensor<8x1x1x1440xbf16> loc(#loc2300)
      %2145 = stablehlo.reduce(%2144 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc2301)
      %2146 = "stablehlo.all_reduce"(%2145) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.5528"), %arg933: tensor<bf16> loc("reduce.5528")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2301)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2301)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2301)
      %2147 = stablehlo.add %2063, %2146 : tensor<1x1x1440xbf16> loc(#loc2302)
      %2148 = stablehlo.convert %2147 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2303)
      %2149 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2150 = stablehlo.power %2148, %2149 : tensor<1x1x1440xf32> loc(#loc2304)
      %2151 = stablehlo.reduce(%2150 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2305)
      %2152 = "stablehlo.all_reduce"(%2151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.5541"), %arg933: tensor<f32> loc("reduce.5541")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2305)
        stablehlo.return %4605 : tensor<f32> loc(#loc2305)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2305)
      %2153 = stablehlo.multiply %2152, %cst_1 : tensor<1x1xf32> loc(#loc2306)
      %2154 = stablehlo.reshape %2153 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2307)
      %2155 = stablehlo.add %2154, %cst_2 : tensor<1x1x1xf32> loc(#loc2308)
      %2156 = stablehlo.rsqrt %2155 : tensor<1x1x1xf32> loc(#loc2309)
      %2157 = stablehlo.reshape %2156 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2310)
      %2158 = stablehlo.broadcast_in_dim %2157, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2311)
      %2159 = stablehlo.multiply %2148, %2158 : tensor<1x1x1440xf32> loc(#loc2312)
      %2160 = stablehlo.multiply %2029, %2159 : tensor<1x1x1440xf32> loc(#loc2313)
      %2161 = stablehlo.convert %2160 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2314)
      %2162 = stablehlo.reshape %2161 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2315)
      %2163 = stablehlo.reshape %arg698 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc2316)
      %2164 = stablehlo.reshape %2163 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc2317)
      %2165 = stablehlo.transpose %2164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc2318)
      %2166 = stablehlo.dot_general %2162, %2165, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2319)
      %2167 = "stablehlo.all_reduce"(%2166) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5733"), %arg933: tensor<bf16> loc("dot.5733")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2319)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2319)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2319)
      %2168 = stablehlo.reshape %2167 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2320)
      %2169 = stablehlo.reshape %arg697 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2321)
      %2170 = stablehlo.add %2168, %2169 : tensor<1x1x1024xbf16> loc(#loc2322)
      %2171 = stablehlo.reshape %2170 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2323)
      %2172 = stablehlo.slice %2171 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2324)
      %2173 = stablehlo.multiply %2172, %59 : tensor<1x16x1x32xbf16> loc(#loc2325)
      %2174 = stablehlo.slice %2171 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2326)
      %2175 = stablehlo.multiply %2174, %65 : tensor<1x16x1x32xbf16> loc(#loc2327)
      %2176 = stablehlo.subtract %2173, %2175 : tensor<1x16x1x32xbf16> loc(#loc2328)
      %2177 = stablehlo.multiply %2174, %59 : tensor<1x16x1x32xbf16> loc(#loc2329)
      %2178 = stablehlo.multiply %2172, %65 : tensor<1x16x1x32xbf16> loc(#loc2330)
      %2179 = stablehlo.add %2177, %2178 : tensor<1x16x1x32xbf16> loc(#loc2331)
      %2180 = stablehlo.concatenate %2176, %2179, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2332)
      %2181 = stablehlo.reshape %arg672 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2333)
      %2182 = stablehlo.reshape %2181 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2334)
      %2183 = stablehlo.transpose %2182, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2335)
      %2184 = stablehlo.dot_general %2162, %2183, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2336)
      %2185 = "stablehlo.all_reduce"(%2184) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5569"), %arg933: tensor<bf16> loc("dot.5569")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2336)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2336)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2336)
      %2186 = stablehlo.reshape %2185 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2337)
      %2187 = stablehlo.reshape %arg671 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2338)
      %2188 = stablehlo.add %2186, %2187 : tensor<1x1x128xbf16> loc(#loc2339)
      %2189 = stablehlo.reshape %2188 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2340)
      %2190 = stablehlo.slice %2189 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2341)
      %2191 = stablehlo.multiply %2190, %86 : tensor<1x2x1x32xbf16> loc(#loc2342)
      %2192 = stablehlo.slice %2189 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2343)
      %2193 = stablehlo.multiply %2192, %89 : tensor<1x2x1x32xbf16> loc(#loc2344)
      %2194 = stablehlo.subtract %2191, %2193 : tensor<1x2x1x32xbf16> loc(#loc2345)
      %2195 = stablehlo.multiply %2192, %86 : tensor<1x2x1x32xbf16> loc(#loc2346)
      %2196 = stablehlo.multiply %2190, %89 : tensor<1x2x1x32xbf16> loc(#loc2347)
      %2197 = stablehlo.add %2195, %2196 : tensor<1x2x1x32xbf16> loc(#loc2348)
      %2198 = stablehlo.concatenate %2194, %2197, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2349)
      %2199 = "stablehlo.scatter"(%arg686, %75, %2198) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.5616"), %arg933: tensor<bf16> loc("scatter.5616")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2350)
      %2200 = stablehlo.broadcast_in_dim %2199, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2351)
      %2201 = stablehlo.reshape %2200 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2352)
      %2202 = stablehlo.transpose %2201, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc2353)
      %2203 = stablehlo.dot_general %2180, %2202, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2354)
      %2204 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %2205 = stablehlo.multiply %2203, %2204 : tensor<1x16x1x256xbf16> loc(#loc2355)
      %2206 = stablehlo.add %2205, %325 : tensor<1x16x1x256xbf16> loc(#loc2356)
      %2207 = stablehlo.reshape %arg696 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc231)
      %2208 = "stablehlo.all_to_all"(%2207) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc231)
      %2209 = stablehlo.slice %2208 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc231)
      %2210 = stablehlo.reshape %2209 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc231)
      %2211 = stablehlo.reshape %2210 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2357)
      %2212 = stablehlo.reshape %2211 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc2358)
      %2213 = stablehlo.concatenate %2206, %2212, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2359)
      %2214 = stablehlo.reshape %arg704 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2360)
      %2215 = stablehlo.reshape %2214 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2361)
      %2216 = stablehlo.convert %2215 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2362)
      %2217 = stablehlo.reshape %2216 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2363)
      %2218 = stablehlo.reduce(%2213 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2364)
      %2219 = stablehlo.broadcast_in_dim %2218, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2365)
      %2220 = stablehlo.subtract %2213, %2219 : tensor<1x16x1x257xbf16> loc(#loc2366)
      %2221 = stablehlo.reduce(%2220 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2367)
      %2222 = stablehlo.broadcast_in_dim %2221, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2368)
      %2223 = stablehlo.subtract %2220, %2222 : tensor<1x16x1x257xbf16> loc(#loc2369)
      %2224 = stablehlo.exponential %2223 : tensor<1x16x1x257xbf16> loc(#loc2370)
      %2225 = stablehlo.reduce(%2224 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2371)
      %2226 = stablehlo.broadcast_in_dim %2225, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2372)
      %2227 = stablehlo.divide %2224, %2226 : tensor<1x16x1x257xbf16> loc(#loc2373)
      %2228 = stablehlo.slice %2227 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2374)
      %2229 = stablehlo.reshape %arg688 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2375)
      %2230 = stablehlo.reshape %2229 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2376)
      %2231 = stablehlo.transpose %2230, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2377)
      %2232 = stablehlo.dot_general %2162, %2231, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2378)
      %2233 = "stablehlo.all_reduce"(%2232) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5629"), %arg933: tensor<bf16> loc("dot.5629")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2378)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2378)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2378)
      %2234 = stablehlo.reshape %2233 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2379)
      %2235 = stablehlo.reshape %arg687 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2380)
      %2236 = stablehlo.add %2234, %2235 : tensor<1x1x128xbf16> loc(#loc2381)
      %2237 = stablehlo.reshape %2236 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2382)
      %2238 = "stablehlo.scatter"(%arg689, %75, %2237) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.5653"), %arg933: tensor<bf16> loc("scatter.5653")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2383)
      %2239 = stablehlo.broadcast_in_dim %2238, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2384)
      %2240 = stablehlo.reshape %2239 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2385)
      %2241 = stablehlo.dot_general %2228, %2240, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2386)
      %2242 = stablehlo.reshape %2241 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc2387)
      %2243 = stablehlo.reshape %arg695 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc2388)
      %2244 = stablehlo.reshape %2243 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc2389)
      %2245 = stablehlo.transpose %2244, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc2390)
      %2246 = stablehlo.dot_general %2242, %2245, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2391)
      %2247 = "stablehlo.all_reduce"(%2246) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5840"), %arg933: tensor<bf16> loc("dot.5840")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2391)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2391)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2391)
      %2248 = stablehlo.reshape %2247 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2392)
      %2249 = stablehlo.reshape %arg694 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2393)
      %2250 = stablehlo.add %2248, %2249 : tensor<1x1x1440xbf16> loc(#loc2394)
      %2251 = stablehlo.add %2147, %2250 : tensor<1x1x1440xbf16> loc(#loc2395)
      %2252 = stablehlo.reshape %arg699 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2396)
      %2253 = stablehlo.reshape %2252 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2397)
      %2254 = stablehlo.convert %2253 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2398)
      %2255 = stablehlo.reshape %2254 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2399)
      %2256 = stablehlo.convert %2251 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2400)
      %2257 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2258 = stablehlo.power %2256, %2257 : tensor<1x1x1440xf32> loc(#loc2401)
      %2259 = stablehlo.reduce(%2258 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2402)
      %2260 = "stablehlo.all_reduce"(%2259) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.5858"), %arg933: tensor<f32> loc("reduce.5858")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2402)
        stablehlo.return %4605 : tensor<f32> loc(#loc2402)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2402)
      %2261 = stablehlo.multiply %2260, %cst_1 : tensor<1x1xf32> loc(#loc2403)
      %2262 = stablehlo.reshape %2261 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2404)
      %2263 = stablehlo.add %2262, %cst_2 : tensor<1x1x1xf32> loc(#loc2405)
      %2264 = stablehlo.rsqrt %2263 : tensor<1x1x1xf32> loc(#loc2406)
      %2265 = stablehlo.reshape %2264 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2407)
      %2266 = stablehlo.broadcast_in_dim %2265, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2408)
      %2267 = stablehlo.multiply %2256, %2266 : tensor<1x1x1440xf32> loc(#loc2409)
      %2268 = stablehlo.multiply %2255, %2267 : tensor<1x1x1440xf32> loc(#loc2410)
      %2269 = stablehlo.convert %2268 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2411)
      %2270 = stablehlo.reshape %2269 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2412)
      %2271 = stablehlo.broadcast_in_dim %2270, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2413)
      %2272 = stablehlo.dot_general %2271, %arg703, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2414)
      %2273 = "stablehlo.all_reduce"(%2272) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.5972"), %arg933: tensor<bf16> loc("dot.5972")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2414)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2414)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2414)
      %2274 = stablehlo.reshape %arg702 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc2415)
      %2275 = stablehlo.reshape %2274 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2416)
      %2276 = stablehlo.add %2273, %2275 : tensor<8x1x5760xbf16> loc(#loc2417)
      %2277 = stablehlo.slice %2276 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2418)
      %2278 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2279 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2280 = stablehlo.clamp %2279, %2277, %2278 : tensor<8x1x2880xbf16> loc(#loc2419)
      %2281 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2282 = stablehlo.add %2280, %2281 : tensor<8x1x2880xbf16> loc(#loc2420)
      %2283 = stablehlo.slice %2276 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2421)
      %2284 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2285 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2286 = stablehlo.clamp %2284, %2283, %2285 : tensor<8x1x2880xbf16> loc(#loc2422)
      %2287 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2288 = stablehlo.multiply %2286, %2287 : tensor<8x1x2880xbf16> loc(#loc2423)
      %2289 = stablehlo.logistic %2288 : tensor<8x1x2880xbf16> loc(#loc2424)
      %2290 = stablehlo.multiply %2286, %2289 : tensor<8x1x2880xbf16> loc(#loc2425)
      %2291 = stablehlo.multiply %2282, %2290 : tensor<8x1x2880xbf16> loc(#loc2426)
      %2292 = stablehlo.dot_general %2291, %arg701, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2427)
      %2293 = stablehlo.reshape %arg700 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc2428)
      %2294 = stablehlo.reshape %2293 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2429)
      %2295 = stablehlo.add %2292, %2294 : tensor<8x1x1440xbf16> loc(#loc2430)
      %2296 = stablehlo.reshape %2295 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2431)
      %2297 = stablehlo.convert %2270 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc2432)
      %2298 = stablehlo.reshape %arg693 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc2433)
      %2299 = stablehlo.reshape %2298 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc2434)
      %2300 = stablehlo.transpose %2299, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc2435)
      %2301 = stablehlo.convert %2300 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc2436)
      %2302 = stablehlo.dot_general %2297, %2301, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc2437)
      %2303 = "stablehlo.all_reduce"(%2302) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.5887"), %arg933: tensor<f32> loc("dot.5887")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2437)
        stablehlo.return %4605 : tensor<f32> loc(#loc2437)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc2437)
      %2304 = stablehlo.reshape %arg692 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc2438)
      %2305 = stablehlo.reshape %2304 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc2439)
      %2306 = stablehlo.convert %2305 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc2440)
      %2307 = stablehlo.reshape %2306 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc2441)
      %2308 = stablehlo.add %2303, %2307 : tensor<1x32xf32> loc(#loc2442)
      %2309 = stablehlo.convert %2308 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc2443)
      %2310:2 = "stablehlo.sort"(%2309, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.5908"), %arg933: tensor<bf16> loc("sort.5908"), %arg934: tensor<i32> loc("sort.5908"), %arg935: tensor<i32> loc("sort.5908")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2445)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc2444)
      %2311 = stablehlo.slice %2310#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc2446)
      %2312 = stablehlo.convert %2311 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc2447)
      %2313 = stablehlo.reshape %2312 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc2448)
      %2314 = stablehlo.concatenate %c_16, %2313, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc2449)
      %2315 = stablehlo.slice %2310#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc2450)
      %2316 = stablehlo.reduce(%2315 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2451)
      %2317 = stablehlo.broadcast_in_dim %2316, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2452)
      %2318 = stablehlo.subtract %2315, %2317 : tensor<1x4xbf16> loc(#loc2453)
      %2319 = stablehlo.exponential %2318 : tensor<1x4xbf16> loc(#loc2454)
      %2320 = stablehlo.reduce(%2319 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2455)
      %2321 = stablehlo.broadcast_in_dim %2320, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2456)
      %2322 = stablehlo.divide %2319, %2321 : tensor<1x4xbf16> loc(#loc2457)
      %2323 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %2324 = "stablehlo.all_gather"(%2323) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %2325 = "stablehlo.scatter"(%2324, %2314, %2322) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.5942"), %arg933: tensor<bf16> loc("scatter.5942")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc2458)
      %2326 = stablehlo.reshape %2325 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc2458)
      %2327 = "stablehlo.all_to_all"(%2326) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc2458)
      %2328 = stablehlo.slice %2327 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc2458)
      %2329 = stablehlo.reshape %2328 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc2458)
      %2330 = stablehlo.reshape %2329 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc2459)
      %2331 = stablehlo.broadcast_in_dim %2330, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2460)
      %2332 = stablehlo.multiply %2296, %2331 : tensor<8x1x1x1440xbf16> loc(#loc2461)
      %2333 = stablehlo.reduce(%2332 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc2462)
      %2334 = "stablehlo.all_reduce"(%2333) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.6010"), %arg933: tensor<bf16> loc("reduce.6010")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2462)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2462)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2462)
      %2335 = stablehlo.add %2251, %2334 : tensor<1x1x1440xbf16> loc(#loc2463)
      %2336 = stablehlo.convert %2335 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2464)
      %2337 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2338 = stablehlo.power %2336, %2337 : tensor<1x1x1440xf32> loc(#loc2465)
      %2339 = stablehlo.reduce(%2338 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2466)
      %2340 = "stablehlo.all_reduce"(%2339) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.6023"), %arg933: tensor<f32> loc("reduce.6023")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2466)
        stablehlo.return %4605 : tensor<f32> loc(#loc2466)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2466)
      %2341 = stablehlo.multiply %2340, %cst_1 : tensor<1x1xf32> loc(#loc2467)
      %2342 = stablehlo.reshape %2341 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2468)
      %2343 = stablehlo.add %2342, %cst_2 : tensor<1x1x1xf32> loc(#loc2469)
      %2344 = stablehlo.rsqrt %2343 : tensor<1x1x1xf32> loc(#loc2470)
      %2345 = stablehlo.reshape %2344 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2471)
      %2346 = stablehlo.broadcast_in_dim %2345, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2472)
      %2347 = stablehlo.multiply %2336, %2346 : tensor<1x1x1440xf32> loc(#loc2473)
      %2348 = stablehlo.multiply %2217, %2347 : tensor<1x1x1440xf32> loc(#loc2474)
      %2349 = stablehlo.convert %2348 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2475)
      %2350 = stablehlo.reshape %2349 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2476)
      %2351 = stablehlo.reshape %arg717 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc2477)
      %2352 = stablehlo.reshape %2351 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc2478)
      %2353 = stablehlo.transpose %2352, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc2479)
      %2354 = stablehlo.dot_general %2350, %2353, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2480)
      %2355 = "stablehlo.all_reduce"(%2354) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6215"), %arg933: tensor<bf16> loc("dot.6215")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2480)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2480)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2480)
      %2356 = stablehlo.reshape %2355 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2481)
      %2357 = stablehlo.reshape %arg716 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2482)
      %2358 = stablehlo.add %2356, %2357 : tensor<1x1x1024xbf16> loc(#loc2483)
      %2359 = stablehlo.reshape %2358 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2484)
      %2360 = stablehlo.slice %2359 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2485)
      %2361 = stablehlo.multiply %2360, %59 : tensor<1x16x1x32xbf16> loc(#loc2486)
      %2362 = stablehlo.slice %2359 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2487)
      %2363 = stablehlo.multiply %2362, %65 : tensor<1x16x1x32xbf16> loc(#loc2488)
      %2364 = stablehlo.subtract %2361, %2363 : tensor<1x16x1x32xbf16> loc(#loc2489)
      %2365 = stablehlo.multiply %2362, %59 : tensor<1x16x1x32xbf16> loc(#loc2490)
      %2366 = stablehlo.multiply %2360, %65 : tensor<1x16x1x32xbf16> loc(#loc2491)
      %2367 = stablehlo.add %2365, %2366 : tensor<1x16x1x32xbf16> loc(#loc2492)
      %2368 = stablehlo.concatenate %2364, %2367, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2493)
      %2369 = stablehlo.reshape %arg691 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2494)
      %2370 = stablehlo.reshape %2369 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2495)
      %2371 = stablehlo.transpose %2370, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2496)
      %2372 = stablehlo.dot_general %2350, %2371, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2497)
      %2373 = "stablehlo.all_reduce"(%2372) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6051"), %arg933: tensor<bf16> loc("dot.6051")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2497)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2497)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2497)
      %2374 = stablehlo.reshape %2373 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2498)
      %2375 = stablehlo.reshape %arg690 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2499)
      %2376 = stablehlo.add %2374, %2375 : tensor<1x1x128xbf16> loc(#loc2500)
      %2377 = stablehlo.reshape %2376 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2501)
      %2378 = stablehlo.slice %2377 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2502)
      %2379 = stablehlo.multiply %2378, %86 : tensor<1x2x1x32xbf16> loc(#loc2503)
      %2380 = stablehlo.slice %2377 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2504)
      %2381 = stablehlo.multiply %2380, %89 : tensor<1x2x1x32xbf16> loc(#loc2505)
      %2382 = stablehlo.subtract %2379, %2381 : tensor<1x2x1x32xbf16> loc(#loc2506)
      %2383 = stablehlo.multiply %2380, %86 : tensor<1x2x1x32xbf16> loc(#loc2507)
      %2384 = stablehlo.multiply %2378, %89 : tensor<1x2x1x32xbf16> loc(#loc2508)
      %2385 = stablehlo.add %2383, %2384 : tensor<1x2x1x32xbf16> loc(#loc2509)
      %2386 = stablehlo.concatenate %2382, %2385, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2510)
      %2387 = "stablehlo.scatter"(%arg705, %75, %2386) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.6098"), %arg933: tensor<bf16> loc("scatter.6098")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2511)
      %2388 = stablehlo.broadcast_in_dim %2387, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2512)
      %2389 = stablehlo.reshape %2388 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2513)
      %2390 = stablehlo.transpose %2389, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc2514)
      %2391 = stablehlo.dot_general %2368, %2390, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2515)
      %2392 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %2393 = stablehlo.multiply %2391, %2392 : tensor<1x16x1x256xbf16> loc(#loc2516)
      %2394 = stablehlo.add %2393, %126 : tensor<1x16x1x256xbf16> loc(#loc2517)
      %2395 = stablehlo.reshape %arg715 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc250)
      %2396 = "stablehlo.all_to_all"(%2395) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc250)
      %2397 = stablehlo.slice %2396 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc250)
      %2398 = stablehlo.reshape %2397 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc250)
      %2399 = stablehlo.reshape %2398 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2518)
      %2400 = stablehlo.reshape %2399 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc2519)
      %2401 = stablehlo.concatenate %2394, %2400, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2520)
      %2402 = stablehlo.reshape %arg723 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2521)
      %2403 = stablehlo.reshape %2402 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2522)
      %2404 = stablehlo.convert %2403 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2523)
      %2405 = stablehlo.reshape %2404 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2524)
      %2406 = stablehlo.reduce(%2401 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2525)
      %2407 = stablehlo.broadcast_in_dim %2406, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2526)
      %2408 = stablehlo.subtract %2401, %2407 : tensor<1x16x1x257xbf16> loc(#loc2527)
      %2409 = stablehlo.reduce(%2408 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2528)
      %2410 = stablehlo.broadcast_in_dim %2409, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2529)
      %2411 = stablehlo.subtract %2408, %2410 : tensor<1x16x1x257xbf16> loc(#loc2530)
      %2412 = stablehlo.exponential %2411 : tensor<1x16x1x257xbf16> loc(#loc2531)
      %2413 = stablehlo.reduce(%2412 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2532)
      %2414 = stablehlo.broadcast_in_dim %2413, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2533)
      %2415 = stablehlo.divide %2412, %2414 : tensor<1x16x1x257xbf16> loc(#loc2534)
      %2416 = stablehlo.slice %2415 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2535)
      %2417 = stablehlo.reshape %arg707 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2536)
      %2418 = stablehlo.reshape %2417 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2537)
      %2419 = stablehlo.transpose %2418, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2538)
      %2420 = stablehlo.dot_general %2350, %2419, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2539)
      %2421 = "stablehlo.all_reduce"(%2420) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6111"), %arg933: tensor<bf16> loc("dot.6111")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2539)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2539)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2539)
      %2422 = stablehlo.reshape %2421 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2540)
      %2423 = stablehlo.reshape %arg706 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2541)
      %2424 = stablehlo.add %2422, %2423 : tensor<1x1x128xbf16> loc(#loc2542)
      %2425 = stablehlo.reshape %2424 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2543)
      %2426 = "stablehlo.scatter"(%arg708, %75, %2425) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.6135"), %arg933: tensor<bf16> loc("scatter.6135")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2544)
      %2427 = stablehlo.broadcast_in_dim %2426, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2545)
      %2428 = stablehlo.reshape %2427 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2546)
      %2429 = stablehlo.dot_general %2416, %2428, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2547)
      %2430 = stablehlo.reshape %2429 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc2548)
      %2431 = stablehlo.reshape %arg714 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc2549)
      %2432 = stablehlo.reshape %2431 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc2550)
      %2433 = stablehlo.transpose %2432, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc2551)
      %2434 = stablehlo.dot_general %2430, %2433, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2552)
      %2435 = "stablehlo.all_reduce"(%2434) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6322"), %arg933: tensor<bf16> loc("dot.6322")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2552)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2552)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2552)
      %2436 = stablehlo.reshape %2435 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2553)
      %2437 = stablehlo.reshape %arg713 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2554)
      %2438 = stablehlo.add %2436, %2437 : tensor<1x1x1440xbf16> loc(#loc2555)
      %2439 = stablehlo.add %2335, %2438 : tensor<1x1x1440xbf16> loc(#loc2556)
      %2440 = stablehlo.reshape %arg718 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2557)
      %2441 = stablehlo.reshape %2440 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2558)
      %2442 = stablehlo.convert %2441 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2559)
      %2443 = stablehlo.reshape %2442 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2560)
      %2444 = stablehlo.convert %2439 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2561)
      %2445 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2446 = stablehlo.power %2444, %2445 : tensor<1x1x1440xf32> loc(#loc2562)
      %2447 = stablehlo.reduce(%2446 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2563)
      %2448 = "stablehlo.all_reduce"(%2447) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.6340"), %arg933: tensor<f32> loc("reduce.6340")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2563)
        stablehlo.return %4605 : tensor<f32> loc(#loc2563)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2563)
      %2449 = stablehlo.multiply %2448, %cst_1 : tensor<1x1xf32> loc(#loc2564)
      %2450 = stablehlo.reshape %2449 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2565)
      %2451 = stablehlo.add %2450, %cst_2 : tensor<1x1x1xf32> loc(#loc2566)
      %2452 = stablehlo.rsqrt %2451 : tensor<1x1x1xf32> loc(#loc2567)
      %2453 = stablehlo.reshape %2452 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2568)
      %2454 = stablehlo.broadcast_in_dim %2453, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2569)
      %2455 = stablehlo.multiply %2444, %2454 : tensor<1x1x1440xf32> loc(#loc2570)
      %2456 = stablehlo.multiply %2443, %2455 : tensor<1x1x1440xf32> loc(#loc2571)
      %2457 = stablehlo.convert %2456 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2572)
      %2458 = stablehlo.reshape %2457 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2573)
      %2459 = stablehlo.broadcast_in_dim %2458, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2574)
      %2460 = stablehlo.dot_general %2459, %arg722, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2575)
      %2461 = "stablehlo.all_reduce"(%2460) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6454"), %arg933: tensor<bf16> loc("dot.6454")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2575)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2575)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2575)
      %2462 = stablehlo.reshape %arg721 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc2576)
      %2463 = stablehlo.reshape %2462 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2577)
      %2464 = stablehlo.add %2461, %2463 : tensor<8x1x5760xbf16> loc(#loc2578)
      %2465 = stablehlo.slice %2464 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2579)
      %2466 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2467 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2468 = stablehlo.clamp %2467, %2465, %2466 : tensor<8x1x2880xbf16> loc(#loc2580)
      %2469 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2470 = stablehlo.add %2468, %2469 : tensor<8x1x2880xbf16> loc(#loc2581)
      %2471 = stablehlo.slice %2464 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2582)
      %2472 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2473 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2474 = stablehlo.clamp %2472, %2471, %2473 : tensor<8x1x2880xbf16> loc(#loc2583)
      %2475 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2476 = stablehlo.multiply %2474, %2475 : tensor<8x1x2880xbf16> loc(#loc2584)
      %2477 = stablehlo.logistic %2476 : tensor<8x1x2880xbf16> loc(#loc2585)
      %2478 = stablehlo.multiply %2474, %2477 : tensor<8x1x2880xbf16> loc(#loc2586)
      %2479 = stablehlo.multiply %2470, %2478 : tensor<8x1x2880xbf16> loc(#loc2587)
      %2480 = stablehlo.dot_general %2479, %arg720, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2588)
      %2481 = stablehlo.reshape %arg719 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc2589)
      %2482 = stablehlo.reshape %2481 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2590)
      %2483 = stablehlo.add %2480, %2482 : tensor<8x1x1440xbf16> loc(#loc2591)
      %2484 = stablehlo.reshape %2483 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2592)
      %2485 = stablehlo.convert %2458 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc2593)
      %2486 = stablehlo.reshape %arg712 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc2594)
      %2487 = stablehlo.reshape %2486 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc2595)
      %2488 = stablehlo.transpose %2487, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc2596)
      %2489 = stablehlo.convert %2488 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc2597)
      %2490 = stablehlo.dot_general %2485, %2489, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc2598)
      %2491 = "stablehlo.all_reduce"(%2490) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.6369"), %arg933: tensor<f32> loc("dot.6369")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2598)
        stablehlo.return %4605 : tensor<f32> loc(#loc2598)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc2598)
      %2492 = stablehlo.reshape %arg711 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc2599)
      %2493 = stablehlo.reshape %2492 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc2600)
      %2494 = stablehlo.convert %2493 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc2601)
      %2495 = stablehlo.reshape %2494 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc2602)
      %2496 = stablehlo.add %2491, %2495 : tensor<1x32xf32> loc(#loc2603)
      %2497 = stablehlo.convert %2496 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc2604)
      %2498:2 = "stablehlo.sort"(%2497, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.6390"), %arg933: tensor<bf16> loc("sort.6390"), %arg934: tensor<i32> loc("sort.6390"), %arg935: tensor<i32> loc("sort.6390")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2606)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc2605)
      %2499 = stablehlo.slice %2498#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc2607)
      %2500 = stablehlo.convert %2499 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc2608)
      %2501 = stablehlo.reshape %2500 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc2609)
      %2502 = stablehlo.concatenate %c_16, %2501, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc2610)
      %2503 = stablehlo.slice %2498#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc2611)
      %2504 = stablehlo.reduce(%2503 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2612)
      %2505 = stablehlo.broadcast_in_dim %2504, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2613)
      %2506 = stablehlo.subtract %2503, %2505 : tensor<1x4xbf16> loc(#loc2614)
      %2507 = stablehlo.exponential %2506 : tensor<1x4xbf16> loc(#loc2615)
      %2508 = stablehlo.reduce(%2507 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2616)
      %2509 = stablehlo.broadcast_in_dim %2508, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2617)
      %2510 = stablehlo.divide %2507, %2509 : tensor<1x4xbf16> loc(#loc2618)
      %2511 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %2512 = "stablehlo.all_gather"(%2511) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %2513 = "stablehlo.scatter"(%2512, %2502, %2510) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.6424"), %arg933: tensor<bf16> loc("scatter.6424")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc2619)
      %2514 = stablehlo.reshape %2513 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc2619)
      %2515 = "stablehlo.all_to_all"(%2514) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc2619)
      %2516 = stablehlo.slice %2515 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc2619)
      %2517 = stablehlo.reshape %2516 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc2619)
      %2518 = stablehlo.reshape %2517 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc2620)
      %2519 = stablehlo.broadcast_in_dim %2518, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2621)
      %2520 = stablehlo.multiply %2484, %2519 : tensor<8x1x1x1440xbf16> loc(#loc2622)
      %2521 = stablehlo.reduce(%2520 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc2623)
      %2522 = "stablehlo.all_reduce"(%2521) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.6492"), %arg933: tensor<bf16> loc("reduce.6492")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2623)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2623)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2623)
      %2523 = stablehlo.add %2439, %2522 : tensor<1x1x1440xbf16> loc(#loc2624)
      %2524 = stablehlo.convert %2523 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2625)
      %2525 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2526 = stablehlo.power %2524, %2525 : tensor<1x1x1440xf32> loc(#loc2626)
      %2527 = stablehlo.reduce(%2526 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2627)
      %2528 = "stablehlo.all_reduce"(%2527) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.6505"), %arg933: tensor<f32> loc("reduce.6505")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2627)
        stablehlo.return %4605 : tensor<f32> loc(#loc2627)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2627)
      %2529 = stablehlo.multiply %2528, %cst_1 : tensor<1x1xf32> loc(#loc2628)
      %2530 = stablehlo.reshape %2529 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2629)
      %2531 = stablehlo.add %2530, %cst_2 : tensor<1x1x1xf32> loc(#loc2630)
      %2532 = stablehlo.rsqrt %2531 : tensor<1x1x1xf32> loc(#loc2631)
      %2533 = stablehlo.reshape %2532 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2632)
      %2534 = stablehlo.broadcast_in_dim %2533, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2633)
      %2535 = stablehlo.multiply %2524, %2534 : tensor<1x1x1440xf32> loc(#loc2634)
      %2536 = stablehlo.multiply %2405, %2535 : tensor<1x1x1440xf32> loc(#loc2635)
      %2537 = stablehlo.convert %2536 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2636)
      %2538 = stablehlo.reshape %2537 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2637)
      %2539 = stablehlo.reshape %arg736 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc2638)
      %2540 = stablehlo.reshape %2539 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc2639)
      %2541 = stablehlo.transpose %2540, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc2640)
      %2542 = stablehlo.dot_general %2538, %2541, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2641)
      %2543 = "stablehlo.all_reduce"(%2542) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6697"), %arg933: tensor<bf16> loc("dot.6697")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2641)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2641)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2641)
      %2544 = stablehlo.reshape %2543 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2642)
      %2545 = stablehlo.reshape %arg735 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2643)
      %2546 = stablehlo.add %2544, %2545 : tensor<1x1x1024xbf16> loc(#loc2644)
      %2547 = stablehlo.reshape %2546 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2645)
      %2548 = stablehlo.slice %2547 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2646)
      %2549 = stablehlo.multiply %2548, %59 : tensor<1x16x1x32xbf16> loc(#loc2647)
      %2550 = stablehlo.slice %2547 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2648)
      %2551 = stablehlo.multiply %2550, %65 : tensor<1x16x1x32xbf16> loc(#loc2649)
      %2552 = stablehlo.subtract %2549, %2551 : tensor<1x16x1x32xbf16> loc(#loc2650)
      %2553 = stablehlo.multiply %2550, %59 : tensor<1x16x1x32xbf16> loc(#loc2651)
      %2554 = stablehlo.multiply %2548, %65 : tensor<1x16x1x32xbf16> loc(#loc2652)
      %2555 = stablehlo.add %2553, %2554 : tensor<1x16x1x32xbf16> loc(#loc2653)
      %2556 = stablehlo.concatenate %2552, %2555, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2654)
      %2557 = stablehlo.reshape %arg710 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2655)
      %2558 = stablehlo.reshape %2557 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2656)
      %2559 = stablehlo.transpose %2558, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2657)
      %2560 = stablehlo.dot_general %2538, %2559, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2658)
      %2561 = "stablehlo.all_reduce"(%2560) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6533"), %arg933: tensor<bf16> loc("dot.6533")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2658)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2658)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2658)
      %2562 = stablehlo.reshape %2561 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2659)
      %2563 = stablehlo.reshape %arg709 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2660)
      %2564 = stablehlo.add %2562, %2563 : tensor<1x1x128xbf16> loc(#loc2661)
      %2565 = stablehlo.reshape %2564 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2662)
      %2566 = stablehlo.slice %2565 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2663)
      %2567 = stablehlo.multiply %2566, %86 : tensor<1x2x1x32xbf16> loc(#loc2664)
      %2568 = stablehlo.slice %2565 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2665)
      %2569 = stablehlo.multiply %2568, %89 : tensor<1x2x1x32xbf16> loc(#loc2666)
      %2570 = stablehlo.subtract %2567, %2569 : tensor<1x2x1x32xbf16> loc(#loc2667)
      %2571 = stablehlo.multiply %2568, %86 : tensor<1x2x1x32xbf16> loc(#loc2668)
      %2572 = stablehlo.multiply %2566, %89 : tensor<1x2x1x32xbf16> loc(#loc2669)
      %2573 = stablehlo.add %2571, %2572 : tensor<1x2x1x32xbf16> loc(#loc2670)
      %2574 = stablehlo.concatenate %2570, %2573, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2671)
      %2575 = "stablehlo.scatter"(%arg724, %75, %2574) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.6580"), %arg933: tensor<bf16> loc("scatter.6580")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2672)
      %2576 = stablehlo.broadcast_in_dim %2575, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2673)
      %2577 = stablehlo.reshape %2576 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2674)
      %2578 = stablehlo.transpose %2577, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc2675)
      %2579 = stablehlo.dot_general %2556, %2578, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2676)
      %2580 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %2581 = stablehlo.multiply %2579, %2580 : tensor<1x16x1x256xbf16> loc(#loc2677)
      %2582 = stablehlo.add %2581, %325 : tensor<1x16x1x256xbf16> loc(#loc2678)
      %2583 = stablehlo.reshape %arg734 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc269)
      %2584 = "stablehlo.all_to_all"(%2583) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc269)
      %2585 = stablehlo.slice %2584 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc269)
      %2586 = stablehlo.reshape %2585 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc269)
      %2587 = stablehlo.reshape %2586 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2679)
      %2588 = stablehlo.reshape %2587 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc2680)
      %2589 = stablehlo.concatenate %2582, %2588, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2681)
      %2590 = stablehlo.reshape %arg742 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2682)
      %2591 = stablehlo.reshape %2590 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2683)
      %2592 = stablehlo.convert %2591 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2684)
      %2593 = stablehlo.reshape %2592 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2685)
      %2594 = stablehlo.reduce(%2589 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2686)
      %2595 = stablehlo.broadcast_in_dim %2594, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2687)
      %2596 = stablehlo.subtract %2589, %2595 : tensor<1x16x1x257xbf16> loc(#loc2688)
      %2597 = stablehlo.reduce(%2596 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2689)
      %2598 = stablehlo.broadcast_in_dim %2597, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2690)
      %2599 = stablehlo.subtract %2596, %2598 : tensor<1x16x1x257xbf16> loc(#loc2691)
      %2600 = stablehlo.exponential %2599 : tensor<1x16x1x257xbf16> loc(#loc2692)
      %2601 = stablehlo.reduce(%2600 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2693)
      %2602 = stablehlo.broadcast_in_dim %2601, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2694)
      %2603 = stablehlo.divide %2600, %2602 : tensor<1x16x1x257xbf16> loc(#loc2695)
      %2604 = stablehlo.slice %2603 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2696)
      %2605 = stablehlo.reshape %arg726 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2697)
      %2606 = stablehlo.reshape %2605 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2698)
      %2607 = stablehlo.transpose %2606, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2699)
      %2608 = stablehlo.dot_general %2538, %2607, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2700)
      %2609 = "stablehlo.all_reduce"(%2608) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6593"), %arg933: tensor<bf16> loc("dot.6593")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2700)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2700)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2700)
      %2610 = stablehlo.reshape %2609 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2701)
      %2611 = stablehlo.reshape %arg725 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2702)
      %2612 = stablehlo.add %2610, %2611 : tensor<1x1x128xbf16> loc(#loc2703)
      %2613 = stablehlo.reshape %2612 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2704)
      %2614 = "stablehlo.scatter"(%arg727, %75, %2613) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.6617"), %arg933: tensor<bf16> loc("scatter.6617")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2705)
      %2615 = stablehlo.broadcast_in_dim %2614, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2706)
      %2616 = stablehlo.reshape %2615 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2707)
      %2617 = stablehlo.dot_general %2604, %2616, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2708)
      %2618 = stablehlo.reshape %2617 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc2709)
      %2619 = stablehlo.reshape %arg733 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc2710)
      %2620 = stablehlo.reshape %2619 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc2711)
      %2621 = stablehlo.transpose %2620, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc2712)
      %2622 = stablehlo.dot_general %2618, %2621, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2713)
      %2623 = "stablehlo.all_reduce"(%2622) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6804"), %arg933: tensor<bf16> loc("dot.6804")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2713)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2713)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2713)
      %2624 = stablehlo.reshape %2623 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2714)
      %2625 = stablehlo.reshape %arg732 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2715)
      %2626 = stablehlo.add %2624, %2625 : tensor<1x1x1440xbf16> loc(#loc2716)
      %2627 = stablehlo.add %2523, %2626 : tensor<1x1x1440xbf16> loc(#loc2717)
      %2628 = stablehlo.reshape %arg737 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2718)
      %2629 = stablehlo.reshape %2628 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2719)
      %2630 = stablehlo.convert %2629 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2720)
      %2631 = stablehlo.reshape %2630 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2721)
      %2632 = stablehlo.convert %2627 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2722)
      %2633 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2634 = stablehlo.power %2632, %2633 : tensor<1x1x1440xf32> loc(#loc2723)
      %2635 = stablehlo.reduce(%2634 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2724)
      %2636 = "stablehlo.all_reduce"(%2635) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.6822"), %arg933: tensor<f32> loc("reduce.6822")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2724)
        stablehlo.return %4605 : tensor<f32> loc(#loc2724)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2724)
      %2637 = stablehlo.multiply %2636, %cst_1 : tensor<1x1xf32> loc(#loc2725)
      %2638 = stablehlo.reshape %2637 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2726)
      %2639 = stablehlo.add %2638, %cst_2 : tensor<1x1x1xf32> loc(#loc2727)
      %2640 = stablehlo.rsqrt %2639 : tensor<1x1x1xf32> loc(#loc2728)
      %2641 = stablehlo.reshape %2640 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2729)
      %2642 = stablehlo.broadcast_in_dim %2641, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2730)
      %2643 = stablehlo.multiply %2632, %2642 : tensor<1x1x1440xf32> loc(#loc2731)
      %2644 = stablehlo.multiply %2631, %2643 : tensor<1x1x1440xf32> loc(#loc2732)
      %2645 = stablehlo.convert %2644 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2733)
      %2646 = stablehlo.reshape %2645 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2734)
      %2647 = stablehlo.broadcast_in_dim %2646, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2735)
      %2648 = stablehlo.dot_general %2647, %arg741, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2736)
      %2649 = "stablehlo.all_reduce"(%2648) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.6936"), %arg933: tensor<bf16> loc("dot.6936")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2736)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2736)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2736)
      %2650 = stablehlo.reshape %arg740 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc2737)
      %2651 = stablehlo.reshape %2650 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2738)
      %2652 = stablehlo.add %2649, %2651 : tensor<8x1x5760xbf16> loc(#loc2739)
      %2653 = stablehlo.slice %2652 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2740)
      %2654 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2655 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2656 = stablehlo.clamp %2655, %2653, %2654 : tensor<8x1x2880xbf16> loc(#loc2741)
      %2657 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2658 = stablehlo.add %2656, %2657 : tensor<8x1x2880xbf16> loc(#loc2742)
      %2659 = stablehlo.slice %2652 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2743)
      %2660 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2661 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2662 = stablehlo.clamp %2660, %2659, %2661 : tensor<8x1x2880xbf16> loc(#loc2744)
      %2663 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2664 = stablehlo.multiply %2662, %2663 : tensor<8x1x2880xbf16> loc(#loc2745)
      %2665 = stablehlo.logistic %2664 : tensor<8x1x2880xbf16> loc(#loc2746)
      %2666 = stablehlo.multiply %2662, %2665 : tensor<8x1x2880xbf16> loc(#loc2747)
      %2667 = stablehlo.multiply %2658, %2666 : tensor<8x1x2880xbf16> loc(#loc2748)
      %2668 = stablehlo.dot_general %2667, %arg739, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2749)
      %2669 = stablehlo.reshape %arg738 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc2750)
      %2670 = stablehlo.reshape %2669 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2751)
      %2671 = stablehlo.add %2668, %2670 : tensor<8x1x1440xbf16> loc(#loc2752)
      %2672 = stablehlo.reshape %2671 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2753)
      %2673 = stablehlo.convert %2646 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc2754)
      %2674 = stablehlo.reshape %arg731 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc2755)
      %2675 = stablehlo.reshape %2674 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc2756)
      %2676 = stablehlo.transpose %2675, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc2757)
      %2677 = stablehlo.convert %2676 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc2758)
      %2678 = stablehlo.dot_general %2673, %2677, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc2759)
      %2679 = "stablehlo.all_reduce"(%2678) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.6851"), %arg933: tensor<f32> loc("dot.6851")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2759)
        stablehlo.return %4605 : tensor<f32> loc(#loc2759)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc2759)
      %2680 = stablehlo.reshape %arg730 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc2760)
      %2681 = stablehlo.reshape %2680 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc2761)
      %2682 = stablehlo.convert %2681 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc2762)
      %2683 = stablehlo.reshape %2682 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc2763)
      %2684 = stablehlo.add %2679, %2683 : tensor<1x32xf32> loc(#loc2764)
      %2685 = stablehlo.convert %2684 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc2765)
      %2686:2 = "stablehlo.sort"(%2685, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.6872"), %arg933: tensor<bf16> loc("sort.6872"), %arg934: tensor<i32> loc("sort.6872"), %arg935: tensor<i32> loc("sort.6872")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2767)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc2766)
      %2687 = stablehlo.slice %2686#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc2768)
      %2688 = stablehlo.convert %2687 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc2769)
      %2689 = stablehlo.reshape %2688 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc2770)
      %2690 = stablehlo.concatenate %c_16, %2689, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc2771)
      %2691 = stablehlo.slice %2686#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc2772)
      %2692 = stablehlo.reduce(%2691 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2773)
      %2693 = stablehlo.broadcast_in_dim %2692, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2774)
      %2694 = stablehlo.subtract %2691, %2693 : tensor<1x4xbf16> loc(#loc2775)
      %2695 = stablehlo.exponential %2694 : tensor<1x4xbf16> loc(#loc2776)
      %2696 = stablehlo.reduce(%2695 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2777)
      %2697 = stablehlo.broadcast_in_dim %2696, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2778)
      %2698 = stablehlo.divide %2695, %2697 : tensor<1x4xbf16> loc(#loc2779)
      %2699 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %2700 = "stablehlo.all_gather"(%2699) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %2701 = "stablehlo.scatter"(%2700, %2690, %2698) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.6906"), %arg933: tensor<bf16> loc("scatter.6906")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc2780)
      %2702 = stablehlo.reshape %2701 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc2780)
      %2703 = "stablehlo.all_to_all"(%2702) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc2780)
      %2704 = stablehlo.slice %2703 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc2780)
      %2705 = stablehlo.reshape %2704 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc2780)
      %2706 = stablehlo.reshape %2705 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc2781)
      %2707 = stablehlo.broadcast_in_dim %2706, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2782)
      %2708 = stablehlo.multiply %2672, %2707 : tensor<8x1x1x1440xbf16> loc(#loc2783)
      %2709 = stablehlo.reduce(%2708 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc2784)
      %2710 = "stablehlo.all_reduce"(%2709) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.6974"), %arg933: tensor<bf16> loc("reduce.6974")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2784)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2784)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2784)
      %2711 = stablehlo.add %2627, %2710 : tensor<1x1x1440xbf16> loc(#loc2785)
      %2712 = stablehlo.convert %2711 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2786)
      %2713 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2714 = stablehlo.power %2712, %2713 : tensor<1x1x1440xf32> loc(#loc2787)
      %2715 = stablehlo.reduce(%2714 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2788)
      %2716 = "stablehlo.all_reduce"(%2715) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.6987"), %arg933: tensor<f32> loc("reduce.6987")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2788)
        stablehlo.return %4605 : tensor<f32> loc(#loc2788)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2788)
      %2717 = stablehlo.multiply %2716, %cst_1 : tensor<1x1xf32> loc(#loc2789)
      %2718 = stablehlo.reshape %2717 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2790)
      %2719 = stablehlo.add %2718, %cst_2 : tensor<1x1x1xf32> loc(#loc2791)
      %2720 = stablehlo.rsqrt %2719 : tensor<1x1x1xf32> loc(#loc2792)
      %2721 = stablehlo.reshape %2720 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2793)
      %2722 = stablehlo.broadcast_in_dim %2721, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2794)
      %2723 = stablehlo.multiply %2712, %2722 : tensor<1x1x1440xf32> loc(#loc2795)
      %2724 = stablehlo.multiply %2593, %2723 : tensor<1x1x1440xf32> loc(#loc2796)
      %2725 = stablehlo.convert %2724 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2797)
      %2726 = stablehlo.reshape %2725 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2798)
      %2727 = stablehlo.reshape %arg755 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc2799)
      %2728 = stablehlo.reshape %2727 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc2800)
      %2729 = stablehlo.transpose %2728, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc2801)
      %2730 = stablehlo.dot_general %2726, %2729, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2802)
      %2731 = "stablehlo.all_reduce"(%2730) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7179"), %arg933: tensor<bf16> loc("dot.7179")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2802)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2802)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2802)
      %2732 = stablehlo.reshape %2731 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2803)
      %2733 = stablehlo.reshape %arg754 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2804)
      %2734 = stablehlo.add %2732, %2733 : tensor<1x1x1024xbf16> loc(#loc2805)
      %2735 = stablehlo.reshape %2734 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2806)
      %2736 = stablehlo.slice %2735 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2807)
      %2737 = stablehlo.multiply %2736, %59 : tensor<1x16x1x32xbf16> loc(#loc2808)
      %2738 = stablehlo.slice %2735 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2809)
      %2739 = stablehlo.multiply %2738, %65 : tensor<1x16x1x32xbf16> loc(#loc2810)
      %2740 = stablehlo.subtract %2737, %2739 : tensor<1x16x1x32xbf16> loc(#loc2811)
      %2741 = stablehlo.multiply %2738, %59 : tensor<1x16x1x32xbf16> loc(#loc2812)
      %2742 = stablehlo.multiply %2736, %65 : tensor<1x16x1x32xbf16> loc(#loc2813)
      %2743 = stablehlo.add %2741, %2742 : tensor<1x16x1x32xbf16> loc(#loc2814)
      %2744 = stablehlo.concatenate %2740, %2743, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2815)
      %2745 = stablehlo.reshape %arg729 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2816)
      %2746 = stablehlo.reshape %2745 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2817)
      %2747 = stablehlo.transpose %2746, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2818)
      %2748 = stablehlo.dot_general %2726, %2747, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2819)
      %2749 = "stablehlo.all_reduce"(%2748) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7015"), %arg933: tensor<bf16> loc("dot.7015")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2819)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2819)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2819)
      %2750 = stablehlo.reshape %2749 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2820)
      %2751 = stablehlo.reshape %arg728 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2821)
      %2752 = stablehlo.add %2750, %2751 : tensor<1x1x128xbf16> loc(#loc2822)
      %2753 = stablehlo.reshape %2752 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2823)
      %2754 = stablehlo.slice %2753 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2824)
      %2755 = stablehlo.multiply %2754, %86 : tensor<1x2x1x32xbf16> loc(#loc2825)
      %2756 = stablehlo.slice %2753 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2826)
      %2757 = stablehlo.multiply %2756, %89 : tensor<1x2x1x32xbf16> loc(#loc2827)
      %2758 = stablehlo.subtract %2755, %2757 : tensor<1x2x1x32xbf16> loc(#loc2828)
      %2759 = stablehlo.multiply %2756, %86 : tensor<1x2x1x32xbf16> loc(#loc2829)
      %2760 = stablehlo.multiply %2754, %89 : tensor<1x2x1x32xbf16> loc(#loc2830)
      %2761 = stablehlo.add %2759, %2760 : tensor<1x2x1x32xbf16> loc(#loc2831)
      %2762 = stablehlo.concatenate %2758, %2761, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2832)
      %2763 = "stablehlo.scatter"(%arg743, %75, %2762) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.7062"), %arg933: tensor<bf16> loc("scatter.7062")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2833)
      %2764 = stablehlo.broadcast_in_dim %2763, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2834)
      %2765 = stablehlo.reshape %2764 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2835)
      %2766 = stablehlo.transpose %2765, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc2836)
      %2767 = stablehlo.dot_general %2744, %2766, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2837)
      %2768 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %2769 = stablehlo.multiply %2767, %2768 : tensor<1x16x1x256xbf16> loc(#loc2838)
      %2770 = stablehlo.add %2769, %126 : tensor<1x16x1x256xbf16> loc(#loc2839)
      %2771 = stablehlo.reshape %arg753 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc288)
      %2772 = "stablehlo.all_to_all"(%2771) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc288)
      %2773 = stablehlo.slice %2772 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc288)
      %2774 = stablehlo.reshape %2773 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc288)
      %2775 = stablehlo.reshape %2774 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2840)
      %2776 = stablehlo.reshape %2775 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc2841)
      %2777 = stablehlo.concatenate %2770, %2776, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2842)
      %2778 = stablehlo.reshape %arg761 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2843)
      %2779 = stablehlo.reshape %2778 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2844)
      %2780 = stablehlo.convert %2779 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2845)
      %2781 = stablehlo.reshape %2780 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2846)
      %2782 = stablehlo.reduce(%2777 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2847)
      %2783 = stablehlo.broadcast_in_dim %2782, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2848)
      %2784 = stablehlo.subtract %2777, %2783 : tensor<1x16x1x257xbf16> loc(#loc2849)
      %2785 = stablehlo.reduce(%2784 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2850)
      %2786 = stablehlo.broadcast_in_dim %2785, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2851)
      %2787 = stablehlo.subtract %2784, %2786 : tensor<1x16x1x257xbf16> loc(#loc2852)
      %2788 = stablehlo.exponential %2787 : tensor<1x16x1x257xbf16> loc(#loc2853)
      %2789 = stablehlo.reduce(%2788 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc2854)
      %2790 = stablehlo.broadcast_in_dim %2789, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc2855)
      %2791 = stablehlo.divide %2788, %2790 : tensor<1x16x1x257xbf16> loc(#loc2856)
      %2792 = stablehlo.slice %2791 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2857)
      %2793 = stablehlo.reshape %arg745 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2858)
      %2794 = stablehlo.reshape %2793 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2859)
      %2795 = stablehlo.transpose %2794, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2860)
      %2796 = stablehlo.dot_general %2726, %2795, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2861)
      %2797 = "stablehlo.all_reduce"(%2796) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7075"), %arg933: tensor<bf16> loc("dot.7075")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2861)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2861)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2861)
      %2798 = stablehlo.reshape %2797 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2862)
      %2799 = stablehlo.reshape %arg744 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2863)
      %2800 = stablehlo.add %2798, %2799 : tensor<1x1x128xbf16> loc(#loc2864)
      %2801 = stablehlo.reshape %2800 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2865)
      %2802 = "stablehlo.scatter"(%arg746, %75, %2801) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.7099"), %arg933: tensor<bf16> loc("scatter.7099")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2866)
      %2803 = stablehlo.broadcast_in_dim %2802, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2867)
      %2804 = stablehlo.reshape %2803 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2868)
      %2805 = stablehlo.dot_general %2792, %2804, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2869)
      %2806 = stablehlo.reshape %2805 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc2870)
      %2807 = stablehlo.reshape %arg752 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc2871)
      %2808 = stablehlo.reshape %2807 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc2872)
      %2809 = stablehlo.transpose %2808, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc2873)
      %2810 = stablehlo.dot_general %2806, %2809, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2874)
      %2811 = "stablehlo.all_reduce"(%2810) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7286"), %arg933: tensor<bf16> loc("dot.7286")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2874)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2874)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2874)
      %2812 = stablehlo.reshape %2811 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2875)
      %2813 = stablehlo.reshape %arg751 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2876)
      %2814 = stablehlo.add %2812, %2813 : tensor<1x1x1440xbf16> loc(#loc2877)
      %2815 = stablehlo.add %2711, %2814 : tensor<1x1x1440xbf16> loc(#loc2878)
      %2816 = stablehlo.reshape %arg756 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2879)
      %2817 = stablehlo.reshape %2816 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc2880)
      %2818 = stablehlo.convert %2817 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc2881)
      %2819 = stablehlo.reshape %2818 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc2882)
      %2820 = stablehlo.convert %2815 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2883)
      %2821 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2822 = stablehlo.power %2820, %2821 : tensor<1x1x1440xf32> loc(#loc2884)
      %2823 = stablehlo.reduce(%2822 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2885)
      %2824 = "stablehlo.all_reduce"(%2823) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.7304"), %arg933: tensor<f32> loc("reduce.7304")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2885)
        stablehlo.return %4605 : tensor<f32> loc(#loc2885)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2885)
      %2825 = stablehlo.multiply %2824, %cst_1 : tensor<1x1xf32> loc(#loc2886)
      %2826 = stablehlo.reshape %2825 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2887)
      %2827 = stablehlo.add %2826, %cst_2 : tensor<1x1x1xf32> loc(#loc2888)
      %2828 = stablehlo.rsqrt %2827 : tensor<1x1x1xf32> loc(#loc2889)
      %2829 = stablehlo.reshape %2828 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2890)
      %2830 = stablehlo.broadcast_in_dim %2829, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2891)
      %2831 = stablehlo.multiply %2820, %2830 : tensor<1x1x1440xf32> loc(#loc2892)
      %2832 = stablehlo.multiply %2819, %2831 : tensor<1x1x1440xf32> loc(#loc2893)
      %2833 = stablehlo.convert %2832 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2894)
      %2834 = stablehlo.reshape %2833 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2895)
      %2835 = stablehlo.broadcast_in_dim %2834, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2896)
      %2836 = stablehlo.dot_general %2835, %arg760, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2897)
      %2837 = "stablehlo.all_reduce"(%2836) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7418"), %arg933: tensor<bf16> loc("dot.7418")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2897)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2897)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2897)
      %2838 = stablehlo.reshape %arg759 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc2898)
      %2839 = stablehlo.reshape %2838 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc2899)
      %2840 = stablehlo.add %2837, %2839 : tensor<8x1x5760xbf16> loc(#loc2900)
      %2841 = stablehlo.slice %2840 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2901)
      %2842 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2843 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2844 = stablehlo.clamp %2843, %2841, %2842 : tensor<8x1x2880xbf16> loc(#loc2902)
      %2845 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2846 = stablehlo.add %2844, %2845 : tensor<8x1x2880xbf16> loc(#loc2903)
      %2847 = stablehlo.slice %2840 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc2904)
      %2848 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2849 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2850 = stablehlo.clamp %2848, %2847, %2849 : tensor<8x1x2880xbf16> loc(#loc2905)
      %2851 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %2852 = stablehlo.multiply %2850, %2851 : tensor<8x1x2880xbf16> loc(#loc2906)
      %2853 = stablehlo.logistic %2852 : tensor<8x1x2880xbf16> loc(#loc2907)
      %2854 = stablehlo.multiply %2850, %2853 : tensor<8x1x2880xbf16> loc(#loc2908)
      %2855 = stablehlo.multiply %2846, %2854 : tensor<8x1x2880xbf16> loc(#loc2909)
      %2856 = stablehlo.dot_general %2855, %arg758, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2910)
      %2857 = stablehlo.reshape %arg757 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc2911)
      %2858 = stablehlo.reshape %2857 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc2912)
      %2859 = stablehlo.add %2856, %2858 : tensor<8x1x1440xbf16> loc(#loc2913)
      %2860 = stablehlo.reshape %2859 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2914)
      %2861 = stablehlo.convert %2834 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc2915)
      %2862 = stablehlo.reshape %arg750 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc2916)
      %2863 = stablehlo.reshape %2862 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc2917)
      %2864 = stablehlo.transpose %2863, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc2918)
      %2865 = stablehlo.convert %2864 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc2919)
      %2866 = stablehlo.dot_general %2861, %2865, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc2920)
      %2867 = "stablehlo.all_reduce"(%2866) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.7333"), %arg933: tensor<f32> loc("dot.7333")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2920)
        stablehlo.return %4605 : tensor<f32> loc(#loc2920)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc2920)
      %2868 = stablehlo.reshape %arg749 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc2921)
      %2869 = stablehlo.reshape %2868 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc2922)
      %2870 = stablehlo.convert %2869 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc2923)
      %2871 = stablehlo.reshape %2870 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc2924)
      %2872 = stablehlo.add %2867, %2871 : tensor<1x32xf32> loc(#loc2925)
      %2873 = stablehlo.convert %2872 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc2926)
      %2874:2 = "stablehlo.sort"(%2873, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.7354"), %arg933: tensor<bf16> loc("sort.7354"), %arg934: tensor<i32> loc("sort.7354"), %arg935: tensor<i32> loc("sort.7354")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2928)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc2927)
      %2875 = stablehlo.slice %2874#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc2929)
      %2876 = stablehlo.convert %2875 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc2930)
      %2877 = stablehlo.reshape %2876 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc2931)
      %2878 = stablehlo.concatenate %c_16, %2877, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc2932)
      %2879 = stablehlo.slice %2874#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc2933)
      %2880 = stablehlo.reduce(%2879 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2934)
      %2881 = stablehlo.broadcast_in_dim %2880, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2935)
      %2882 = stablehlo.subtract %2879, %2881 : tensor<1x4xbf16> loc(#loc2936)
      %2883 = stablehlo.exponential %2882 : tensor<1x4xbf16> loc(#loc2937)
      %2884 = stablehlo.reduce(%2883 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc2938)
      %2885 = stablehlo.broadcast_in_dim %2884, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc2939)
      %2886 = stablehlo.divide %2883, %2885 : tensor<1x4xbf16> loc(#loc2940)
      %2887 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %2888 = "stablehlo.all_gather"(%2887) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %2889 = "stablehlo.scatter"(%2888, %2878, %2886) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.7388"), %arg933: tensor<bf16> loc("scatter.7388")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc2941)
      %2890 = stablehlo.reshape %2889 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc2941)
      %2891 = "stablehlo.all_to_all"(%2890) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc2941)
      %2892 = stablehlo.slice %2891 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc2941)
      %2893 = stablehlo.reshape %2892 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc2941)
      %2894 = stablehlo.reshape %2893 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc2942)
      %2895 = stablehlo.broadcast_in_dim %2894, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc2943)
      %2896 = stablehlo.multiply %2860, %2895 : tensor<8x1x1x1440xbf16> loc(#loc2944)
      %2897 = stablehlo.reduce(%2896 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc2945)
      %2898 = "stablehlo.all_reduce"(%2897) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.7456"), %arg933: tensor<bf16> loc("reduce.7456")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2945)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2945)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc2945)
      %2899 = stablehlo.add %2815, %2898 : tensor<1x1x1440xbf16> loc(#loc2946)
      %2900 = stablehlo.convert %2899 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc2947)
      %2901 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %2902 = stablehlo.power %2900, %2901 : tensor<1x1x1440xf32> loc(#loc2948)
      %2903 = stablehlo.reduce(%2902 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc2949)
      %2904 = "stablehlo.all_reduce"(%2903) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.7469"), %arg933: tensor<f32> loc("reduce.7469")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc2949)
        stablehlo.return %4605 : tensor<f32> loc(#loc2949)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc2949)
      %2905 = stablehlo.multiply %2904, %cst_1 : tensor<1x1xf32> loc(#loc2950)
      %2906 = stablehlo.reshape %2905 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc2951)
      %2907 = stablehlo.add %2906, %cst_2 : tensor<1x1x1xf32> loc(#loc2952)
      %2908 = stablehlo.rsqrt %2907 : tensor<1x1x1xf32> loc(#loc2953)
      %2909 = stablehlo.reshape %2908 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc2954)
      %2910 = stablehlo.broadcast_in_dim %2909, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc2955)
      %2911 = stablehlo.multiply %2900, %2910 : tensor<1x1x1440xf32> loc(#loc2956)
      %2912 = stablehlo.multiply %2781, %2911 : tensor<1x1x1440xf32> loc(#loc2957)
      %2913 = stablehlo.convert %2912 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc2958)
      %2914 = stablehlo.reshape %2913 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc2959)
      %2915 = stablehlo.reshape %arg774 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc2960)
      %2916 = stablehlo.reshape %2915 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc2961)
      %2917 = stablehlo.transpose %2916, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc2962)
      %2918 = stablehlo.dot_general %2914, %2917, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2963)
      %2919 = "stablehlo.all_reduce"(%2918) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7661"), %arg933: tensor<bf16> loc("dot.7661")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2963)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2963)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc2963)
      %2920 = stablehlo.reshape %2919 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2964)
      %2921 = stablehlo.reshape %arg773 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2965)
      %2922 = stablehlo.add %2920, %2921 : tensor<1x1x1024xbf16> loc(#loc2966)
      %2923 = stablehlo.reshape %2922 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2967)
      %2924 = stablehlo.slice %2923 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2968)
      %2925 = stablehlo.multiply %2924, %59 : tensor<1x16x1x32xbf16> loc(#loc2969)
      %2926 = stablehlo.slice %2923 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc2970)
      %2927 = stablehlo.multiply %2926, %65 : tensor<1x16x1x32xbf16> loc(#loc2971)
      %2928 = stablehlo.subtract %2925, %2927 : tensor<1x16x1x32xbf16> loc(#loc2972)
      %2929 = stablehlo.multiply %2926, %59 : tensor<1x16x1x32xbf16> loc(#loc2973)
      %2930 = stablehlo.multiply %2924, %65 : tensor<1x16x1x32xbf16> loc(#loc2974)
      %2931 = stablehlo.add %2929, %2930 : tensor<1x16x1x32xbf16> loc(#loc2975)
      %2932 = stablehlo.concatenate %2928, %2931, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc2976)
      %2933 = stablehlo.reshape %arg748 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc2977)
      %2934 = stablehlo.reshape %2933 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc2978)
      %2935 = stablehlo.transpose %2934, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc2979)
      %2936 = stablehlo.dot_general %2914, %2935, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc2980)
      %2937 = "stablehlo.all_reduce"(%2936) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7497"), %arg933: tensor<bf16> loc("dot.7497")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc2980)
        stablehlo.return %4605 : tensor<bf16> loc(#loc2980)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc2980)
      %2938 = stablehlo.reshape %2937 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2981)
      %2939 = stablehlo.reshape %arg747 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2982)
      %2940 = stablehlo.add %2938, %2939 : tensor<1x1x128xbf16> loc(#loc2983)
      %2941 = stablehlo.reshape %2940 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2984)
      %2942 = stablehlo.slice %2941 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2985)
      %2943 = stablehlo.multiply %2942, %86 : tensor<1x2x1x32xbf16> loc(#loc2986)
      %2944 = stablehlo.slice %2941 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc2987)
      %2945 = stablehlo.multiply %2944, %89 : tensor<1x2x1x32xbf16> loc(#loc2988)
      %2946 = stablehlo.subtract %2943, %2945 : tensor<1x2x1x32xbf16> loc(#loc2989)
      %2947 = stablehlo.multiply %2944, %86 : tensor<1x2x1x32xbf16> loc(#loc2990)
      %2948 = stablehlo.multiply %2942, %89 : tensor<1x2x1x32xbf16> loc(#loc2991)
      %2949 = stablehlo.add %2947, %2948 : tensor<1x2x1x32xbf16> loc(#loc2992)
      %2950 = stablehlo.concatenate %2946, %2949, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc2993)
      %2951 = "stablehlo.scatter"(%arg762, %75, %2950) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.7544"), %arg933: tensor<bf16> loc("scatter.7544")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc2994)
      %2952 = stablehlo.broadcast_in_dim %2951, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc2995)
      %2953 = stablehlo.reshape %2952 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc2996)
      %2954 = stablehlo.transpose %2953, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc2997)
      %2955 = stablehlo.dot_general %2932, %2954, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc2998)
      %2956 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %2957 = stablehlo.multiply %2955, %2956 : tensor<1x16x1x256xbf16> loc(#loc2999)
      %2958 = stablehlo.add %2957, %325 : tensor<1x16x1x256xbf16> loc(#loc3000)
      %2959 = stablehlo.reshape %arg772 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc307)
      %2960 = "stablehlo.all_to_all"(%2959) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc307)
      %2961 = stablehlo.slice %2960 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc307)
      %2962 = stablehlo.reshape %2961 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc307)
      %2963 = stablehlo.reshape %2962 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3001)
      %2964 = stablehlo.reshape %2963 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc3002)
      %2965 = stablehlo.concatenate %2958, %2964, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3003)
      %2966 = stablehlo.reshape %arg780 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3004)
      %2967 = stablehlo.reshape %2966 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3005)
      %2968 = stablehlo.convert %2967 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3006)
      %2969 = stablehlo.reshape %2968 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3007)
      %2970 = stablehlo.reduce(%2965 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3008)
      %2971 = stablehlo.broadcast_in_dim %2970, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3009)
      %2972 = stablehlo.subtract %2965, %2971 : tensor<1x16x1x257xbf16> loc(#loc3010)
      %2973 = stablehlo.reduce(%2972 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3011)
      %2974 = stablehlo.broadcast_in_dim %2973, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3012)
      %2975 = stablehlo.subtract %2972, %2974 : tensor<1x16x1x257xbf16> loc(#loc3013)
      %2976 = stablehlo.exponential %2975 : tensor<1x16x1x257xbf16> loc(#loc3014)
      %2977 = stablehlo.reduce(%2976 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3015)
      %2978 = stablehlo.broadcast_in_dim %2977, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3016)
      %2979 = stablehlo.divide %2976, %2978 : tensor<1x16x1x257xbf16> loc(#loc3017)
      %2980 = stablehlo.slice %2979 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3018)
      %2981 = stablehlo.reshape %arg764 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3019)
      %2982 = stablehlo.reshape %2981 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3020)
      %2983 = stablehlo.transpose %2982, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3021)
      %2984 = stablehlo.dot_general %2914, %2983, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3022)
      %2985 = "stablehlo.all_reduce"(%2984) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7557"), %arg933: tensor<bf16> loc("dot.7557")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3022)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3022)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3022)
      %2986 = stablehlo.reshape %2985 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3023)
      %2987 = stablehlo.reshape %arg763 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3024)
      %2988 = stablehlo.add %2986, %2987 : tensor<1x1x128xbf16> loc(#loc3025)
      %2989 = stablehlo.reshape %2988 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3026)
      %2990 = "stablehlo.scatter"(%arg765, %75, %2989) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.7581"), %arg933: tensor<bf16> loc("scatter.7581")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3027)
      %2991 = stablehlo.broadcast_in_dim %2990, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3028)
      %2992 = stablehlo.reshape %2991 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3029)
      %2993 = stablehlo.dot_general %2980, %2992, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3030)
      %2994 = stablehlo.reshape %2993 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc3031)
      %2995 = stablehlo.reshape %arg771 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc3032)
      %2996 = stablehlo.reshape %2995 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc3033)
      %2997 = stablehlo.transpose %2996, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc3034)
      %2998 = stablehlo.dot_general %2994, %2997, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3035)
      %2999 = "stablehlo.all_reduce"(%2998) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7768"), %arg933: tensor<bf16> loc("dot.7768")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3035)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3035)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3035)
      %3000 = stablehlo.reshape %2999 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3036)
      %3001 = stablehlo.reshape %arg770 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3037)
      %3002 = stablehlo.add %3000, %3001 : tensor<1x1x1440xbf16> loc(#loc3038)
      %3003 = stablehlo.add %2899, %3002 : tensor<1x1x1440xbf16> loc(#loc3039)
      %3004 = stablehlo.reshape %arg775 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3040)
      %3005 = stablehlo.reshape %3004 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3041)
      %3006 = stablehlo.convert %3005 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3042)
      %3007 = stablehlo.reshape %3006 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3043)
      %3008 = stablehlo.convert %3003 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3044)
      %3009 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3010 = stablehlo.power %3008, %3009 : tensor<1x1x1440xf32> loc(#loc3045)
      %3011 = stablehlo.reduce(%3010 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3046)
      %3012 = "stablehlo.all_reduce"(%3011) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.7786"), %arg933: tensor<f32> loc("reduce.7786")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3046)
        stablehlo.return %4605 : tensor<f32> loc(#loc3046)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3046)
      %3013 = stablehlo.multiply %3012, %cst_1 : tensor<1x1xf32> loc(#loc3047)
      %3014 = stablehlo.reshape %3013 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3048)
      %3015 = stablehlo.add %3014, %cst_2 : tensor<1x1x1xf32> loc(#loc3049)
      %3016 = stablehlo.rsqrt %3015 : tensor<1x1x1xf32> loc(#loc3050)
      %3017 = stablehlo.reshape %3016 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3051)
      %3018 = stablehlo.broadcast_in_dim %3017, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3052)
      %3019 = stablehlo.multiply %3008, %3018 : tensor<1x1x1440xf32> loc(#loc3053)
      %3020 = stablehlo.multiply %3007, %3019 : tensor<1x1x1440xf32> loc(#loc3054)
      %3021 = stablehlo.convert %3020 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3055)
      %3022 = stablehlo.reshape %3021 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3056)
      %3023 = stablehlo.broadcast_in_dim %3022, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3057)
      %3024 = stablehlo.dot_general %3023, %arg779, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3058)
      %3025 = "stablehlo.all_reduce"(%3024) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7900"), %arg933: tensor<bf16> loc("dot.7900")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3058)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3058)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3058)
      %3026 = stablehlo.reshape %arg778 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc3059)
      %3027 = stablehlo.reshape %3026 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3060)
      %3028 = stablehlo.add %3025, %3027 : tensor<8x1x5760xbf16> loc(#loc3061)
      %3029 = stablehlo.slice %3028 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3062)
      %3030 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3031 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3032 = stablehlo.clamp %3031, %3029, %3030 : tensor<8x1x2880xbf16> loc(#loc3063)
      %3033 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3034 = stablehlo.add %3032, %3033 : tensor<8x1x2880xbf16> loc(#loc3064)
      %3035 = stablehlo.slice %3028 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3065)
      %3036 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3037 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3038 = stablehlo.clamp %3036, %3035, %3037 : tensor<8x1x2880xbf16> loc(#loc3066)
      %3039 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3040 = stablehlo.multiply %3038, %3039 : tensor<8x1x2880xbf16> loc(#loc3067)
      %3041 = stablehlo.logistic %3040 : tensor<8x1x2880xbf16> loc(#loc3068)
      %3042 = stablehlo.multiply %3038, %3041 : tensor<8x1x2880xbf16> loc(#loc3069)
      %3043 = stablehlo.multiply %3034, %3042 : tensor<8x1x2880xbf16> loc(#loc3070)
      %3044 = stablehlo.dot_general %3043, %arg777, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3071)
      %3045 = stablehlo.reshape %arg776 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc3072)
      %3046 = stablehlo.reshape %3045 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3073)
      %3047 = stablehlo.add %3044, %3046 : tensor<8x1x1440xbf16> loc(#loc3074)
      %3048 = stablehlo.reshape %3047 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3075)
      %3049 = stablehlo.convert %3022 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc3076)
      %3050 = stablehlo.reshape %arg769 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc3077)
      %3051 = stablehlo.reshape %3050 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc3078)
      %3052 = stablehlo.transpose %3051, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc3079)
      %3053 = stablehlo.convert %3052 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc3080)
      %3054 = stablehlo.dot_general %3049, %3053, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc3081)
      %3055 = "stablehlo.all_reduce"(%3054) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.7815"), %arg933: tensor<f32> loc("dot.7815")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3081)
        stablehlo.return %4605 : tensor<f32> loc(#loc3081)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc3081)
      %3056 = stablehlo.reshape %arg768 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3082)
      %3057 = stablehlo.reshape %3056 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3083)
      %3058 = stablehlo.convert %3057 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc3084)
      %3059 = stablehlo.reshape %3058 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc3085)
      %3060 = stablehlo.add %3055, %3059 : tensor<1x32xf32> loc(#loc3086)
      %3061 = stablehlo.convert %3060 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc3087)
      %3062:2 = "stablehlo.sort"(%3061, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.7836"), %arg933: tensor<bf16> loc("sort.7836"), %arg934: tensor<i32> loc("sort.7836"), %arg935: tensor<i32> loc("sort.7836")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3089)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc3088)
      %3063 = stablehlo.slice %3062#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc3090)
      %3064 = stablehlo.convert %3063 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc3091)
      %3065 = stablehlo.reshape %3064 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc3092)
      %3066 = stablehlo.concatenate %c_16, %3065, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc3093)
      %3067 = stablehlo.slice %3062#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc3094)
      %3068 = stablehlo.reduce(%3067 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3095)
      %3069 = stablehlo.broadcast_in_dim %3068, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3096)
      %3070 = stablehlo.subtract %3067, %3069 : tensor<1x4xbf16> loc(#loc3097)
      %3071 = stablehlo.exponential %3070 : tensor<1x4xbf16> loc(#loc3098)
      %3072 = stablehlo.reduce(%3071 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3099)
      %3073 = stablehlo.broadcast_in_dim %3072, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3100)
      %3074 = stablehlo.divide %3071, %3073 : tensor<1x4xbf16> loc(#loc3101)
      %3075 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %3076 = "stablehlo.all_gather"(%3075) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %3077 = "stablehlo.scatter"(%3076, %3066, %3074) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.7870"), %arg933: tensor<bf16> loc("scatter.7870")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc3102)
      %3078 = stablehlo.reshape %3077 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc3102)
      %3079 = "stablehlo.all_to_all"(%3078) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc3102)
      %3080 = stablehlo.slice %3079 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc3102)
      %3081 = stablehlo.reshape %3080 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc3102)
      %3082 = stablehlo.reshape %3081 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc3103)
      %3083 = stablehlo.broadcast_in_dim %3082, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3104)
      %3084 = stablehlo.multiply %3048, %3083 : tensor<8x1x1x1440xbf16> loc(#loc3105)
      %3085 = stablehlo.reduce(%3084 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc3106)
      %3086 = "stablehlo.all_reduce"(%3085) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.7938"), %arg933: tensor<bf16> loc("reduce.7938")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3106)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3106)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3106)
      %3087 = stablehlo.add %3003, %3086 : tensor<1x1x1440xbf16> loc(#loc3107)
      %3088 = stablehlo.convert %3087 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3108)
      %3089 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3090 = stablehlo.power %3088, %3089 : tensor<1x1x1440xf32> loc(#loc3109)
      %3091 = stablehlo.reduce(%3090 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3110)
      %3092 = "stablehlo.all_reduce"(%3091) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.7951"), %arg933: tensor<f32> loc("reduce.7951")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3110)
        stablehlo.return %4605 : tensor<f32> loc(#loc3110)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3110)
      %3093 = stablehlo.multiply %3092, %cst_1 : tensor<1x1xf32> loc(#loc3111)
      %3094 = stablehlo.reshape %3093 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3112)
      %3095 = stablehlo.add %3094, %cst_2 : tensor<1x1x1xf32> loc(#loc3113)
      %3096 = stablehlo.rsqrt %3095 : tensor<1x1x1xf32> loc(#loc3114)
      %3097 = stablehlo.reshape %3096 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3115)
      %3098 = stablehlo.broadcast_in_dim %3097, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3116)
      %3099 = stablehlo.multiply %3088, %3098 : tensor<1x1x1440xf32> loc(#loc3117)
      %3100 = stablehlo.multiply %2969, %3099 : tensor<1x1x1440xf32> loc(#loc3118)
      %3101 = stablehlo.convert %3100 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3119)
      %3102 = stablehlo.reshape %3101 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3120)
      %3103 = stablehlo.reshape %arg793 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc3121)
      %3104 = stablehlo.reshape %3103 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc3122)
      %3105 = stablehlo.transpose %3104, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc3123)
      %3106 = stablehlo.dot_general %3102, %3105, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3124)
      %3107 = "stablehlo.all_reduce"(%3106) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8143"), %arg933: tensor<bf16> loc("dot.8143")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3124)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3124)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3124)
      %3108 = stablehlo.reshape %3107 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3125)
      %3109 = stablehlo.reshape %arg792 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3126)
      %3110 = stablehlo.add %3108, %3109 : tensor<1x1x1024xbf16> loc(#loc3127)
      %3111 = stablehlo.reshape %3110 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3128)
      %3112 = stablehlo.slice %3111 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3129)
      %3113 = stablehlo.multiply %3112, %59 : tensor<1x16x1x32xbf16> loc(#loc3130)
      %3114 = stablehlo.slice %3111 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3131)
      %3115 = stablehlo.multiply %3114, %65 : tensor<1x16x1x32xbf16> loc(#loc3132)
      %3116 = stablehlo.subtract %3113, %3115 : tensor<1x16x1x32xbf16> loc(#loc3133)
      %3117 = stablehlo.multiply %3114, %59 : tensor<1x16x1x32xbf16> loc(#loc3134)
      %3118 = stablehlo.multiply %3112, %65 : tensor<1x16x1x32xbf16> loc(#loc3135)
      %3119 = stablehlo.add %3117, %3118 : tensor<1x16x1x32xbf16> loc(#loc3136)
      %3120 = stablehlo.concatenate %3116, %3119, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3137)
      %3121 = stablehlo.reshape %arg767 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3138)
      %3122 = stablehlo.reshape %3121 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3139)
      %3123 = stablehlo.transpose %3122, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3140)
      %3124 = stablehlo.dot_general %3102, %3123, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3141)
      %3125 = "stablehlo.all_reduce"(%3124) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.7979"), %arg933: tensor<bf16> loc("dot.7979")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3141)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3141)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3141)
      %3126 = stablehlo.reshape %3125 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3142)
      %3127 = stablehlo.reshape %arg766 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3143)
      %3128 = stablehlo.add %3126, %3127 : tensor<1x1x128xbf16> loc(#loc3144)
      %3129 = stablehlo.reshape %3128 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3145)
      %3130 = stablehlo.slice %3129 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3146)
      %3131 = stablehlo.multiply %3130, %86 : tensor<1x2x1x32xbf16> loc(#loc3147)
      %3132 = stablehlo.slice %3129 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3148)
      %3133 = stablehlo.multiply %3132, %89 : tensor<1x2x1x32xbf16> loc(#loc3149)
      %3134 = stablehlo.subtract %3131, %3133 : tensor<1x2x1x32xbf16> loc(#loc3150)
      %3135 = stablehlo.multiply %3132, %86 : tensor<1x2x1x32xbf16> loc(#loc3151)
      %3136 = stablehlo.multiply %3130, %89 : tensor<1x2x1x32xbf16> loc(#loc3152)
      %3137 = stablehlo.add %3135, %3136 : tensor<1x2x1x32xbf16> loc(#loc3153)
      %3138 = stablehlo.concatenate %3134, %3137, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3154)
      %3139 = "stablehlo.scatter"(%arg781, %75, %3138) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.8026"), %arg933: tensor<bf16> loc("scatter.8026")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3155)
      %3140 = stablehlo.broadcast_in_dim %3139, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3156)
      %3141 = stablehlo.reshape %3140 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3157)
      %3142 = stablehlo.transpose %3141, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc3158)
      %3143 = stablehlo.dot_general %3120, %3142, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3159)
      %3144 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %3145 = stablehlo.multiply %3143, %3144 : tensor<1x16x1x256xbf16> loc(#loc3160)
      %3146 = stablehlo.add %3145, %126 : tensor<1x16x1x256xbf16> loc(#loc3161)
      %3147 = stablehlo.reshape %arg791 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc326)
      %3148 = "stablehlo.all_to_all"(%3147) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc326)
      %3149 = stablehlo.slice %3148 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc326)
      %3150 = stablehlo.reshape %3149 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc326)
      %3151 = stablehlo.reshape %3150 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3162)
      %3152 = stablehlo.reshape %3151 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc3163)
      %3153 = stablehlo.concatenate %3146, %3152, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3164)
      %3154 = stablehlo.reshape %arg799 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3165)
      %3155 = stablehlo.reshape %3154 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3166)
      %3156 = stablehlo.convert %3155 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3167)
      %3157 = stablehlo.reshape %3156 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3168)
      %3158 = stablehlo.reduce(%3153 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3169)
      %3159 = stablehlo.broadcast_in_dim %3158, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3170)
      %3160 = stablehlo.subtract %3153, %3159 : tensor<1x16x1x257xbf16> loc(#loc3171)
      %3161 = stablehlo.reduce(%3160 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3172)
      %3162 = stablehlo.broadcast_in_dim %3161, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3173)
      %3163 = stablehlo.subtract %3160, %3162 : tensor<1x16x1x257xbf16> loc(#loc3174)
      %3164 = stablehlo.exponential %3163 : tensor<1x16x1x257xbf16> loc(#loc3175)
      %3165 = stablehlo.reduce(%3164 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3176)
      %3166 = stablehlo.broadcast_in_dim %3165, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3177)
      %3167 = stablehlo.divide %3164, %3166 : tensor<1x16x1x257xbf16> loc(#loc3178)
      %3168 = stablehlo.slice %3167 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3179)
      %3169 = stablehlo.reshape %arg783 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3180)
      %3170 = stablehlo.reshape %3169 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3181)
      %3171 = stablehlo.transpose %3170, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3182)
      %3172 = stablehlo.dot_general %3102, %3171, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3183)
      %3173 = "stablehlo.all_reduce"(%3172) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8039"), %arg933: tensor<bf16> loc("dot.8039")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3183)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3183)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3183)
      %3174 = stablehlo.reshape %3173 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3184)
      %3175 = stablehlo.reshape %arg782 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3185)
      %3176 = stablehlo.add %3174, %3175 : tensor<1x1x128xbf16> loc(#loc3186)
      %3177 = stablehlo.reshape %3176 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3187)
      %3178 = "stablehlo.scatter"(%arg784, %75, %3177) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.8063"), %arg933: tensor<bf16> loc("scatter.8063")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3188)
      %3179 = stablehlo.broadcast_in_dim %3178, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3189)
      %3180 = stablehlo.reshape %3179 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3190)
      %3181 = stablehlo.dot_general %3168, %3180, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3191)
      %3182 = stablehlo.reshape %3181 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc3192)
      %3183 = stablehlo.reshape %arg790 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc3193)
      %3184 = stablehlo.reshape %3183 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc3194)
      %3185 = stablehlo.transpose %3184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc3195)
      %3186 = stablehlo.dot_general %3182, %3185, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3196)
      %3187 = "stablehlo.all_reduce"(%3186) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8250"), %arg933: tensor<bf16> loc("dot.8250")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3196)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3196)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3196)
      %3188 = stablehlo.reshape %3187 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3197)
      %3189 = stablehlo.reshape %arg789 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3198)
      %3190 = stablehlo.add %3188, %3189 : tensor<1x1x1440xbf16> loc(#loc3199)
      %3191 = stablehlo.add %3087, %3190 : tensor<1x1x1440xbf16> loc(#loc3200)
      %3192 = stablehlo.reshape %arg794 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3201)
      %3193 = stablehlo.reshape %3192 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3202)
      %3194 = stablehlo.convert %3193 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3203)
      %3195 = stablehlo.reshape %3194 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3204)
      %3196 = stablehlo.convert %3191 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3205)
      %3197 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3198 = stablehlo.power %3196, %3197 : tensor<1x1x1440xf32> loc(#loc3206)
      %3199 = stablehlo.reduce(%3198 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3207)
      %3200 = "stablehlo.all_reduce"(%3199) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.8268"), %arg933: tensor<f32> loc("reduce.8268")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3207)
        stablehlo.return %4605 : tensor<f32> loc(#loc3207)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3207)
      %3201 = stablehlo.multiply %3200, %cst_1 : tensor<1x1xf32> loc(#loc3208)
      %3202 = stablehlo.reshape %3201 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3209)
      %3203 = stablehlo.add %3202, %cst_2 : tensor<1x1x1xf32> loc(#loc3210)
      %3204 = stablehlo.rsqrt %3203 : tensor<1x1x1xf32> loc(#loc3211)
      %3205 = stablehlo.reshape %3204 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3212)
      %3206 = stablehlo.broadcast_in_dim %3205, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3213)
      %3207 = stablehlo.multiply %3196, %3206 : tensor<1x1x1440xf32> loc(#loc3214)
      %3208 = stablehlo.multiply %3195, %3207 : tensor<1x1x1440xf32> loc(#loc3215)
      %3209 = stablehlo.convert %3208 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3216)
      %3210 = stablehlo.reshape %3209 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3217)
      %3211 = stablehlo.broadcast_in_dim %3210, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3218)
      %3212 = stablehlo.dot_general %3211, %arg798, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3219)
      %3213 = "stablehlo.all_reduce"(%3212) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8382"), %arg933: tensor<bf16> loc("dot.8382")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3219)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3219)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3219)
      %3214 = stablehlo.reshape %arg797 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc3220)
      %3215 = stablehlo.reshape %3214 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3221)
      %3216 = stablehlo.add %3213, %3215 : tensor<8x1x5760xbf16> loc(#loc3222)
      %3217 = stablehlo.slice %3216 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3223)
      %3218 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3219 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3220 = stablehlo.clamp %3219, %3217, %3218 : tensor<8x1x2880xbf16> loc(#loc3224)
      %3221 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3222 = stablehlo.add %3220, %3221 : tensor<8x1x2880xbf16> loc(#loc3225)
      %3223 = stablehlo.slice %3216 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3226)
      %3224 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3225 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3226 = stablehlo.clamp %3224, %3223, %3225 : tensor<8x1x2880xbf16> loc(#loc3227)
      %3227 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3228 = stablehlo.multiply %3226, %3227 : tensor<8x1x2880xbf16> loc(#loc3228)
      %3229 = stablehlo.logistic %3228 : tensor<8x1x2880xbf16> loc(#loc3229)
      %3230 = stablehlo.multiply %3226, %3229 : tensor<8x1x2880xbf16> loc(#loc3230)
      %3231 = stablehlo.multiply %3222, %3230 : tensor<8x1x2880xbf16> loc(#loc3231)
      %3232 = stablehlo.dot_general %3231, %arg796, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3232)
      %3233 = stablehlo.reshape %arg795 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc3233)
      %3234 = stablehlo.reshape %3233 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3234)
      %3235 = stablehlo.add %3232, %3234 : tensor<8x1x1440xbf16> loc(#loc3235)
      %3236 = stablehlo.reshape %3235 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3236)
      %3237 = stablehlo.convert %3210 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc3237)
      %3238 = stablehlo.reshape %arg788 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc3238)
      %3239 = stablehlo.reshape %3238 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc3239)
      %3240 = stablehlo.transpose %3239, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc3240)
      %3241 = stablehlo.convert %3240 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc3241)
      %3242 = stablehlo.dot_general %3237, %3241, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc3242)
      %3243 = "stablehlo.all_reduce"(%3242) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.8297"), %arg933: tensor<f32> loc("dot.8297")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3242)
        stablehlo.return %4605 : tensor<f32> loc(#loc3242)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc3242)
      %3244 = stablehlo.reshape %arg787 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3243)
      %3245 = stablehlo.reshape %3244 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3244)
      %3246 = stablehlo.convert %3245 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc3245)
      %3247 = stablehlo.reshape %3246 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc3246)
      %3248 = stablehlo.add %3243, %3247 : tensor<1x32xf32> loc(#loc3247)
      %3249 = stablehlo.convert %3248 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc3248)
      %3250:2 = "stablehlo.sort"(%3249, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.8318"), %arg933: tensor<bf16> loc("sort.8318"), %arg934: tensor<i32> loc("sort.8318"), %arg935: tensor<i32> loc("sort.8318")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3250)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc3249)
      %3251 = stablehlo.slice %3250#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc3251)
      %3252 = stablehlo.convert %3251 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc3252)
      %3253 = stablehlo.reshape %3252 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc3253)
      %3254 = stablehlo.concatenate %c_16, %3253, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc3254)
      %3255 = stablehlo.slice %3250#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc3255)
      %3256 = stablehlo.reduce(%3255 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3256)
      %3257 = stablehlo.broadcast_in_dim %3256, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3257)
      %3258 = stablehlo.subtract %3255, %3257 : tensor<1x4xbf16> loc(#loc3258)
      %3259 = stablehlo.exponential %3258 : tensor<1x4xbf16> loc(#loc3259)
      %3260 = stablehlo.reduce(%3259 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3260)
      %3261 = stablehlo.broadcast_in_dim %3260, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3261)
      %3262 = stablehlo.divide %3259, %3261 : tensor<1x4xbf16> loc(#loc3262)
      %3263 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %3264 = "stablehlo.all_gather"(%3263) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %3265 = "stablehlo.scatter"(%3264, %3254, %3262) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.8352"), %arg933: tensor<bf16> loc("scatter.8352")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc3263)
      %3266 = stablehlo.reshape %3265 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc3263)
      %3267 = "stablehlo.all_to_all"(%3266) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc3263)
      %3268 = stablehlo.slice %3267 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc3263)
      %3269 = stablehlo.reshape %3268 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc3263)
      %3270 = stablehlo.reshape %3269 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc3264)
      %3271 = stablehlo.broadcast_in_dim %3270, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3265)
      %3272 = stablehlo.multiply %3236, %3271 : tensor<8x1x1x1440xbf16> loc(#loc3266)
      %3273 = stablehlo.reduce(%3272 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc3267)
      %3274 = "stablehlo.all_reduce"(%3273) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.8420"), %arg933: tensor<bf16> loc("reduce.8420")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3267)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3267)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3267)
      %3275 = stablehlo.add %3191, %3274 : tensor<1x1x1440xbf16> loc(#loc3268)
      %3276 = stablehlo.convert %3275 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3269)
      %3277 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3278 = stablehlo.power %3276, %3277 : tensor<1x1x1440xf32> loc(#loc3270)
      %3279 = stablehlo.reduce(%3278 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3271)
      %3280 = "stablehlo.all_reduce"(%3279) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.8433"), %arg933: tensor<f32> loc("reduce.8433")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3271)
        stablehlo.return %4605 : tensor<f32> loc(#loc3271)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3271)
      %3281 = stablehlo.multiply %3280, %cst_1 : tensor<1x1xf32> loc(#loc3272)
      %3282 = stablehlo.reshape %3281 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3273)
      %3283 = stablehlo.add %3282, %cst_2 : tensor<1x1x1xf32> loc(#loc3274)
      %3284 = stablehlo.rsqrt %3283 : tensor<1x1x1xf32> loc(#loc3275)
      %3285 = stablehlo.reshape %3284 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3276)
      %3286 = stablehlo.broadcast_in_dim %3285, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3277)
      %3287 = stablehlo.multiply %3276, %3286 : tensor<1x1x1440xf32> loc(#loc3278)
      %3288 = stablehlo.multiply %3157, %3287 : tensor<1x1x1440xf32> loc(#loc3279)
      %3289 = stablehlo.convert %3288 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3280)
      %3290 = stablehlo.reshape %3289 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3281)
      %3291 = stablehlo.reshape %arg812 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc3282)
      %3292 = stablehlo.reshape %3291 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc3283)
      %3293 = stablehlo.transpose %3292, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc3284)
      %3294 = stablehlo.dot_general %3290, %3293, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3285)
      %3295 = "stablehlo.all_reduce"(%3294) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8625"), %arg933: tensor<bf16> loc("dot.8625")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3285)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3285)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3285)
      %3296 = stablehlo.reshape %3295 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3286)
      %3297 = stablehlo.reshape %arg811 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3287)
      %3298 = stablehlo.add %3296, %3297 : tensor<1x1x1024xbf16> loc(#loc3288)
      %3299 = stablehlo.reshape %3298 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3289)
      %3300 = stablehlo.slice %3299 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3290)
      %3301 = stablehlo.multiply %3300, %59 : tensor<1x16x1x32xbf16> loc(#loc3291)
      %3302 = stablehlo.slice %3299 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3292)
      %3303 = stablehlo.multiply %3302, %65 : tensor<1x16x1x32xbf16> loc(#loc3293)
      %3304 = stablehlo.subtract %3301, %3303 : tensor<1x16x1x32xbf16> loc(#loc3294)
      %3305 = stablehlo.multiply %3302, %59 : tensor<1x16x1x32xbf16> loc(#loc3295)
      %3306 = stablehlo.multiply %3300, %65 : tensor<1x16x1x32xbf16> loc(#loc3296)
      %3307 = stablehlo.add %3305, %3306 : tensor<1x16x1x32xbf16> loc(#loc3297)
      %3308 = stablehlo.concatenate %3304, %3307, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3298)
      %3309 = stablehlo.reshape %arg786 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3299)
      %3310 = stablehlo.reshape %3309 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3300)
      %3311 = stablehlo.transpose %3310, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3301)
      %3312 = stablehlo.dot_general %3290, %3311, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3302)
      %3313 = "stablehlo.all_reduce"(%3312) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8461"), %arg933: tensor<bf16> loc("dot.8461")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3302)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3302)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3302)
      %3314 = stablehlo.reshape %3313 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3303)
      %3315 = stablehlo.reshape %arg785 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3304)
      %3316 = stablehlo.add %3314, %3315 : tensor<1x1x128xbf16> loc(#loc3305)
      %3317 = stablehlo.reshape %3316 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3306)
      %3318 = stablehlo.slice %3317 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3307)
      %3319 = stablehlo.multiply %3318, %86 : tensor<1x2x1x32xbf16> loc(#loc3308)
      %3320 = stablehlo.slice %3317 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3309)
      %3321 = stablehlo.multiply %3320, %89 : tensor<1x2x1x32xbf16> loc(#loc3310)
      %3322 = stablehlo.subtract %3319, %3321 : tensor<1x2x1x32xbf16> loc(#loc3311)
      %3323 = stablehlo.multiply %3320, %86 : tensor<1x2x1x32xbf16> loc(#loc3312)
      %3324 = stablehlo.multiply %3318, %89 : tensor<1x2x1x32xbf16> loc(#loc3313)
      %3325 = stablehlo.add %3323, %3324 : tensor<1x2x1x32xbf16> loc(#loc3314)
      %3326 = stablehlo.concatenate %3322, %3325, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3315)
      %3327 = "stablehlo.scatter"(%arg800, %75, %3326) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.8508"), %arg933: tensor<bf16> loc("scatter.8508")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3316)
      %3328 = stablehlo.broadcast_in_dim %3327, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3317)
      %3329 = stablehlo.reshape %3328 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3318)
      %3330 = stablehlo.transpose %3329, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc3319)
      %3331 = stablehlo.dot_general %3308, %3330, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3320)
      %3332 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %3333 = stablehlo.multiply %3331, %3332 : tensor<1x16x1x256xbf16> loc(#loc3321)
      %3334 = stablehlo.add %3333, %325 : tensor<1x16x1x256xbf16> loc(#loc3322)
      %3335 = stablehlo.reshape %arg810 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc345)
      %3336 = "stablehlo.all_to_all"(%3335) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc345)
      %3337 = stablehlo.slice %3336 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc345)
      %3338 = stablehlo.reshape %3337 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc345)
      %3339 = stablehlo.reshape %3338 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3323)
      %3340 = stablehlo.reshape %3339 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc3324)
      %3341 = stablehlo.concatenate %3334, %3340, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3325)
      %3342 = stablehlo.reshape %arg818 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3326)
      %3343 = stablehlo.reshape %3342 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3327)
      %3344 = stablehlo.convert %3343 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3328)
      %3345 = stablehlo.reshape %3344 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3329)
      %3346 = stablehlo.reduce(%3341 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3330)
      %3347 = stablehlo.broadcast_in_dim %3346, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3331)
      %3348 = stablehlo.subtract %3341, %3347 : tensor<1x16x1x257xbf16> loc(#loc3332)
      %3349 = stablehlo.reduce(%3348 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3333)
      %3350 = stablehlo.broadcast_in_dim %3349, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3334)
      %3351 = stablehlo.subtract %3348, %3350 : tensor<1x16x1x257xbf16> loc(#loc3335)
      %3352 = stablehlo.exponential %3351 : tensor<1x16x1x257xbf16> loc(#loc3336)
      %3353 = stablehlo.reduce(%3352 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3337)
      %3354 = stablehlo.broadcast_in_dim %3353, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3338)
      %3355 = stablehlo.divide %3352, %3354 : tensor<1x16x1x257xbf16> loc(#loc3339)
      %3356 = stablehlo.slice %3355 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3340)
      %3357 = stablehlo.reshape %arg802 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3341)
      %3358 = stablehlo.reshape %3357 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3342)
      %3359 = stablehlo.transpose %3358, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3343)
      %3360 = stablehlo.dot_general %3290, %3359, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3344)
      %3361 = "stablehlo.all_reduce"(%3360) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8521"), %arg933: tensor<bf16> loc("dot.8521")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3344)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3344)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3344)
      %3362 = stablehlo.reshape %3361 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3345)
      %3363 = stablehlo.reshape %arg801 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3346)
      %3364 = stablehlo.add %3362, %3363 : tensor<1x1x128xbf16> loc(#loc3347)
      %3365 = stablehlo.reshape %3364 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3348)
      %3366 = "stablehlo.scatter"(%arg803, %75, %3365) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.8545"), %arg933: tensor<bf16> loc("scatter.8545")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3349)
      %3367 = stablehlo.broadcast_in_dim %3366, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3350)
      %3368 = stablehlo.reshape %3367 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3351)
      %3369 = stablehlo.dot_general %3356, %3368, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3352)
      %3370 = stablehlo.reshape %3369 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc3353)
      %3371 = stablehlo.reshape %arg809 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc3354)
      %3372 = stablehlo.reshape %3371 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc3355)
      %3373 = stablehlo.transpose %3372, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc3356)
      %3374 = stablehlo.dot_general %3370, %3373, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3357)
      %3375 = "stablehlo.all_reduce"(%3374) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8732"), %arg933: tensor<bf16> loc("dot.8732")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3357)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3357)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3357)
      %3376 = stablehlo.reshape %3375 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3358)
      %3377 = stablehlo.reshape %arg808 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3359)
      %3378 = stablehlo.add %3376, %3377 : tensor<1x1x1440xbf16> loc(#loc3360)
      %3379 = stablehlo.add %3275, %3378 : tensor<1x1x1440xbf16> loc(#loc3361)
      %3380 = stablehlo.reshape %arg813 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3362)
      %3381 = stablehlo.reshape %3380 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3363)
      %3382 = stablehlo.convert %3381 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3364)
      %3383 = stablehlo.reshape %3382 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3365)
      %3384 = stablehlo.convert %3379 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3366)
      %3385 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3386 = stablehlo.power %3384, %3385 : tensor<1x1x1440xf32> loc(#loc3367)
      %3387 = stablehlo.reduce(%3386 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3368)
      %3388 = "stablehlo.all_reduce"(%3387) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.8750"), %arg933: tensor<f32> loc("reduce.8750")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3368)
        stablehlo.return %4605 : tensor<f32> loc(#loc3368)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3368)
      %3389 = stablehlo.multiply %3388, %cst_1 : tensor<1x1xf32> loc(#loc3369)
      %3390 = stablehlo.reshape %3389 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3370)
      %3391 = stablehlo.add %3390, %cst_2 : tensor<1x1x1xf32> loc(#loc3371)
      %3392 = stablehlo.rsqrt %3391 : tensor<1x1x1xf32> loc(#loc3372)
      %3393 = stablehlo.reshape %3392 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3373)
      %3394 = stablehlo.broadcast_in_dim %3393, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3374)
      %3395 = stablehlo.multiply %3384, %3394 : tensor<1x1x1440xf32> loc(#loc3375)
      %3396 = stablehlo.multiply %3383, %3395 : tensor<1x1x1440xf32> loc(#loc3376)
      %3397 = stablehlo.convert %3396 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3377)
      %3398 = stablehlo.reshape %3397 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3378)
      %3399 = stablehlo.broadcast_in_dim %3398, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3379)
      %3400 = stablehlo.dot_general %3399, %arg817, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3380)
      %3401 = "stablehlo.all_reduce"(%3400) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8864"), %arg933: tensor<bf16> loc("dot.8864")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3380)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3380)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3380)
      %3402 = stablehlo.reshape %arg816 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc3381)
      %3403 = stablehlo.reshape %3402 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3382)
      %3404 = stablehlo.add %3401, %3403 : tensor<8x1x5760xbf16> loc(#loc3383)
      %3405 = stablehlo.slice %3404 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3384)
      %3406 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3407 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3408 = stablehlo.clamp %3407, %3405, %3406 : tensor<8x1x2880xbf16> loc(#loc3385)
      %3409 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3410 = stablehlo.add %3408, %3409 : tensor<8x1x2880xbf16> loc(#loc3386)
      %3411 = stablehlo.slice %3404 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3387)
      %3412 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3413 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3414 = stablehlo.clamp %3412, %3411, %3413 : tensor<8x1x2880xbf16> loc(#loc3388)
      %3415 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3416 = stablehlo.multiply %3414, %3415 : tensor<8x1x2880xbf16> loc(#loc3389)
      %3417 = stablehlo.logistic %3416 : tensor<8x1x2880xbf16> loc(#loc3390)
      %3418 = stablehlo.multiply %3414, %3417 : tensor<8x1x2880xbf16> loc(#loc3391)
      %3419 = stablehlo.multiply %3410, %3418 : tensor<8x1x2880xbf16> loc(#loc3392)
      %3420 = stablehlo.dot_general %3419, %arg815, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3393)
      %3421 = stablehlo.reshape %arg814 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc3394)
      %3422 = stablehlo.reshape %3421 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3395)
      %3423 = stablehlo.add %3420, %3422 : tensor<8x1x1440xbf16> loc(#loc3396)
      %3424 = stablehlo.reshape %3423 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3397)
      %3425 = stablehlo.convert %3398 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc3398)
      %3426 = stablehlo.reshape %arg807 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc3399)
      %3427 = stablehlo.reshape %3426 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc3400)
      %3428 = stablehlo.transpose %3427, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc3401)
      %3429 = stablehlo.convert %3428 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc3402)
      %3430 = stablehlo.dot_general %3425, %3429, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc3403)
      %3431 = "stablehlo.all_reduce"(%3430) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.8779"), %arg933: tensor<f32> loc("dot.8779")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3403)
        stablehlo.return %4605 : tensor<f32> loc(#loc3403)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc3403)
      %3432 = stablehlo.reshape %arg806 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3404)
      %3433 = stablehlo.reshape %3432 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3405)
      %3434 = stablehlo.convert %3433 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc3406)
      %3435 = stablehlo.reshape %3434 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc3407)
      %3436 = stablehlo.add %3431, %3435 : tensor<1x32xf32> loc(#loc3408)
      %3437 = stablehlo.convert %3436 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc3409)
      %3438:2 = "stablehlo.sort"(%3437, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.8800"), %arg933: tensor<bf16> loc("sort.8800"), %arg934: tensor<i32> loc("sort.8800"), %arg935: tensor<i32> loc("sort.8800")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3411)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc3410)
      %3439 = stablehlo.slice %3438#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc3412)
      %3440 = stablehlo.convert %3439 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc3413)
      %3441 = stablehlo.reshape %3440 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc3414)
      %3442 = stablehlo.concatenate %c_16, %3441, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc3415)
      %3443 = stablehlo.slice %3438#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc3416)
      %3444 = stablehlo.reduce(%3443 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3417)
      %3445 = stablehlo.broadcast_in_dim %3444, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3418)
      %3446 = stablehlo.subtract %3443, %3445 : tensor<1x4xbf16> loc(#loc3419)
      %3447 = stablehlo.exponential %3446 : tensor<1x4xbf16> loc(#loc3420)
      %3448 = stablehlo.reduce(%3447 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3421)
      %3449 = stablehlo.broadcast_in_dim %3448, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3422)
      %3450 = stablehlo.divide %3447, %3449 : tensor<1x4xbf16> loc(#loc3423)
      %3451 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %3452 = "stablehlo.all_gather"(%3451) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %3453 = "stablehlo.scatter"(%3452, %3442, %3450) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.8834"), %arg933: tensor<bf16> loc("scatter.8834")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc3424)
      %3454 = stablehlo.reshape %3453 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc3424)
      %3455 = "stablehlo.all_to_all"(%3454) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc3424)
      %3456 = stablehlo.slice %3455 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc3424)
      %3457 = stablehlo.reshape %3456 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc3424)
      %3458 = stablehlo.reshape %3457 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc3425)
      %3459 = stablehlo.broadcast_in_dim %3458, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3426)
      %3460 = stablehlo.multiply %3424, %3459 : tensor<8x1x1x1440xbf16> loc(#loc3427)
      %3461 = stablehlo.reduce(%3460 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc3428)
      %3462 = "stablehlo.all_reduce"(%3461) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.8902"), %arg933: tensor<bf16> loc("reduce.8902")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3428)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3428)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3428)
      %3463 = stablehlo.add %3379, %3462 : tensor<1x1x1440xbf16> loc(#loc3429)
      %3464 = stablehlo.convert %3463 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3430)
      %3465 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3466 = stablehlo.power %3464, %3465 : tensor<1x1x1440xf32> loc(#loc3431)
      %3467 = stablehlo.reduce(%3466 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3432)
      %3468 = "stablehlo.all_reduce"(%3467) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.8915"), %arg933: tensor<f32> loc("reduce.8915")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3432)
        stablehlo.return %4605 : tensor<f32> loc(#loc3432)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3432)
      %3469 = stablehlo.multiply %3468, %cst_1 : tensor<1x1xf32> loc(#loc3433)
      %3470 = stablehlo.reshape %3469 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3434)
      %3471 = stablehlo.add %3470, %cst_2 : tensor<1x1x1xf32> loc(#loc3435)
      %3472 = stablehlo.rsqrt %3471 : tensor<1x1x1xf32> loc(#loc3436)
      %3473 = stablehlo.reshape %3472 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3437)
      %3474 = stablehlo.broadcast_in_dim %3473, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3438)
      %3475 = stablehlo.multiply %3464, %3474 : tensor<1x1x1440xf32> loc(#loc3439)
      %3476 = stablehlo.multiply %3345, %3475 : tensor<1x1x1440xf32> loc(#loc3440)
      %3477 = stablehlo.convert %3476 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3441)
      %3478 = stablehlo.reshape %3477 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3442)
      %3479 = stablehlo.reshape %arg831 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc3443)
      %3480 = stablehlo.reshape %3479 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc3444)
      %3481 = stablehlo.transpose %3480, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc3445)
      %3482 = stablehlo.dot_general %3478, %3481, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3446)
      %3483 = "stablehlo.all_reduce"(%3482) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9107"), %arg933: tensor<bf16> loc("dot.9107")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3446)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3446)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3446)
      %3484 = stablehlo.reshape %3483 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3447)
      %3485 = stablehlo.reshape %arg830 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3448)
      %3486 = stablehlo.add %3484, %3485 : tensor<1x1x1024xbf16> loc(#loc3449)
      %3487 = stablehlo.reshape %3486 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3450)
      %3488 = stablehlo.slice %3487 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3451)
      %3489 = stablehlo.multiply %3488, %59 : tensor<1x16x1x32xbf16> loc(#loc3452)
      %3490 = stablehlo.slice %3487 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3453)
      %3491 = stablehlo.multiply %3490, %65 : tensor<1x16x1x32xbf16> loc(#loc3454)
      %3492 = stablehlo.subtract %3489, %3491 : tensor<1x16x1x32xbf16> loc(#loc3455)
      %3493 = stablehlo.multiply %3490, %59 : tensor<1x16x1x32xbf16> loc(#loc3456)
      %3494 = stablehlo.multiply %3488, %65 : tensor<1x16x1x32xbf16> loc(#loc3457)
      %3495 = stablehlo.add %3493, %3494 : tensor<1x16x1x32xbf16> loc(#loc3458)
      %3496 = stablehlo.concatenate %3492, %3495, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3459)
      %3497 = stablehlo.reshape %arg805 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3460)
      %3498 = stablehlo.reshape %3497 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3461)
      %3499 = stablehlo.transpose %3498, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3462)
      %3500 = stablehlo.dot_general %3478, %3499, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3463)
      %3501 = "stablehlo.all_reduce"(%3500) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.8943"), %arg933: tensor<bf16> loc("dot.8943")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3463)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3463)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3463)
      %3502 = stablehlo.reshape %3501 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3464)
      %3503 = stablehlo.reshape %arg804 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3465)
      %3504 = stablehlo.add %3502, %3503 : tensor<1x1x128xbf16> loc(#loc3466)
      %3505 = stablehlo.reshape %3504 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3467)
      %3506 = stablehlo.slice %3505 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3468)
      %3507 = stablehlo.multiply %3506, %86 : tensor<1x2x1x32xbf16> loc(#loc3469)
      %3508 = stablehlo.slice %3505 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3470)
      %3509 = stablehlo.multiply %3508, %89 : tensor<1x2x1x32xbf16> loc(#loc3471)
      %3510 = stablehlo.subtract %3507, %3509 : tensor<1x2x1x32xbf16> loc(#loc3472)
      %3511 = stablehlo.multiply %3508, %86 : tensor<1x2x1x32xbf16> loc(#loc3473)
      %3512 = stablehlo.multiply %3506, %89 : tensor<1x2x1x32xbf16> loc(#loc3474)
      %3513 = stablehlo.add %3511, %3512 : tensor<1x2x1x32xbf16> loc(#loc3475)
      %3514 = stablehlo.concatenate %3510, %3513, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3476)
      %3515 = "stablehlo.scatter"(%arg819, %75, %3514) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.8990"), %arg933: tensor<bf16> loc("scatter.8990")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3477)
      %3516 = stablehlo.broadcast_in_dim %3515, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3478)
      %3517 = stablehlo.reshape %3516 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3479)
      %3518 = stablehlo.transpose %3517, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc3480)
      %3519 = stablehlo.dot_general %3496, %3518, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3481)
      %3520 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %3521 = stablehlo.multiply %3519, %3520 : tensor<1x16x1x256xbf16> loc(#loc3482)
      %3522 = stablehlo.add %3521, %126 : tensor<1x16x1x256xbf16> loc(#loc3483)
      %3523 = stablehlo.reshape %arg829 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc364)
      %3524 = "stablehlo.all_to_all"(%3523) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc364)
      %3525 = stablehlo.slice %3524 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc364)
      %3526 = stablehlo.reshape %3525 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc364)
      %3527 = stablehlo.reshape %3526 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3484)
      %3528 = stablehlo.reshape %3527 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc3485)
      %3529 = stablehlo.concatenate %3522, %3528, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3486)
      %3530 = stablehlo.reshape %arg837 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3487)
      %3531 = stablehlo.reshape %3530 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3488)
      %3532 = stablehlo.convert %3531 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3489)
      %3533 = stablehlo.reshape %3532 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3490)
      %3534 = stablehlo.reduce(%3529 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3491)
      %3535 = stablehlo.broadcast_in_dim %3534, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3492)
      %3536 = stablehlo.subtract %3529, %3535 : tensor<1x16x1x257xbf16> loc(#loc3493)
      %3537 = stablehlo.reduce(%3536 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3494)
      %3538 = stablehlo.broadcast_in_dim %3537, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3495)
      %3539 = stablehlo.subtract %3536, %3538 : tensor<1x16x1x257xbf16> loc(#loc3496)
      %3540 = stablehlo.exponential %3539 : tensor<1x16x1x257xbf16> loc(#loc3497)
      %3541 = stablehlo.reduce(%3540 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3498)
      %3542 = stablehlo.broadcast_in_dim %3541, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3499)
      %3543 = stablehlo.divide %3540, %3542 : tensor<1x16x1x257xbf16> loc(#loc3500)
      %3544 = stablehlo.slice %3543 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3501)
      %3545 = stablehlo.reshape %arg821 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3502)
      %3546 = stablehlo.reshape %3545 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3503)
      %3547 = stablehlo.transpose %3546, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3504)
      %3548 = stablehlo.dot_general %3478, %3547, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3505)
      %3549 = "stablehlo.all_reduce"(%3548) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9003"), %arg933: tensor<bf16> loc("dot.9003")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3505)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3505)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3505)
      %3550 = stablehlo.reshape %3549 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3506)
      %3551 = stablehlo.reshape %arg820 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3507)
      %3552 = stablehlo.add %3550, %3551 : tensor<1x1x128xbf16> loc(#loc3508)
      %3553 = stablehlo.reshape %3552 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3509)
      %3554 = "stablehlo.scatter"(%arg822, %75, %3553) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.9027"), %arg933: tensor<bf16> loc("scatter.9027")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3510)
      %3555 = stablehlo.broadcast_in_dim %3554, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3511)
      %3556 = stablehlo.reshape %3555 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3512)
      %3557 = stablehlo.dot_general %3544, %3556, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3513)
      %3558 = stablehlo.reshape %3557 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc3514)
      %3559 = stablehlo.reshape %arg828 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc3515)
      %3560 = stablehlo.reshape %3559 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc3516)
      %3561 = stablehlo.transpose %3560, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc3517)
      %3562 = stablehlo.dot_general %3558, %3561, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3518)
      %3563 = "stablehlo.all_reduce"(%3562) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9214"), %arg933: tensor<bf16> loc("dot.9214")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3518)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3518)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3518)
      %3564 = stablehlo.reshape %3563 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3519)
      %3565 = stablehlo.reshape %arg827 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3520)
      %3566 = stablehlo.add %3564, %3565 : tensor<1x1x1440xbf16> loc(#loc3521)
      %3567 = stablehlo.add %3463, %3566 : tensor<1x1x1440xbf16> loc(#loc3522)
      %3568 = stablehlo.reshape %arg832 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3523)
      %3569 = stablehlo.reshape %3568 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3524)
      %3570 = stablehlo.convert %3569 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3525)
      %3571 = stablehlo.reshape %3570 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3526)
      %3572 = stablehlo.convert %3567 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3527)
      %3573 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3574 = stablehlo.power %3572, %3573 : tensor<1x1x1440xf32> loc(#loc3528)
      %3575 = stablehlo.reduce(%3574 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3529)
      %3576 = "stablehlo.all_reduce"(%3575) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.9232"), %arg933: tensor<f32> loc("reduce.9232")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3529)
        stablehlo.return %4605 : tensor<f32> loc(#loc3529)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3529)
      %3577 = stablehlo.multiply %3576, %cst_1 : tensor<1x1xf32> loc(#loc3530)
      %3578 = stablehlo.reshape %3577 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3531)
      %3579 = stablehlo.add %3578, %cst_2 : tensor<1x1x1xf32> loc(#loc3532)
      %3580 = stablehlo.rsqrt %3579 : tensor<1x1x1xf32> loc(#loc3533)
      %3581 = stablehlo.reshape %3580 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3534)
      %3582 = stablehlo.broadcast_in_dim %3581, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3535)
      %3583 = stablehlo.multiply %3572, %3582 : tensor<1x1x1440xf32> loc(#loc3536)
      %3584 = stablehlo.multiply %3571, %3583 : tensor<1x1x1440xf32> loc(#loc3537)
      %3585 = stablehlo.convert %3584 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3538)
      %3586 = stablehlo.reshape %3585 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3539)
      %3587 = stablehlo.broadcast_in_dim %3586, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3540)
      %3588 = stablehlo.dot_general %3587, %arg836, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3541)
      %3589 = "stablehlo.all_reduce"(%3588) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9346"), %arg933: tensor<bf16> loc("dot.9346")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3541)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3541)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3541)
      %3590 = stablehlo.reshape %arg835 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc3542)
      %3591 = stablehlo.reshape %3590 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3543)
      %3592 = stablehlo.add %3589, %3591 : tensor<8x1x5760xbf16> loc(#loc3544)
      %3593 = stablehlo.slice %3592 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3545)
      %3594 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3595 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3596 = stablehlo.clamp %3595, %3593, %3594 : tensor<8x1x2880xbf16> loc(#loc3546)
      %3597 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3598 = stablehlo.add %3596, %3597 : tensor<8x1x2880xbf16> loc(#loc3547)
      %3599 = stablehlo.slice %3592 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3548)
      %3600 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3601 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3602 = stablehlo.clamp %3600, %3599, %3601 : tensor<8x1x2880xbf16> loc(#loc3549)
      %3603 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3604 = stablehlo.multiply %3602, %3603 : tensor<8x1x2880xbf16> loc(#loc3550)
      %3605 = stablehlo.logistic %3604 : tensor<8x1x2880xbf16> loc(#loc3551)
      %3606 = stablehlo.multiply %3602, %3605 : tensor<8x1x2880xbf16> loc(#loc3552)
      %3607 = stablehlo.multiply %3598, %3606 : tensor<8x1x2880xbf16> loc(#loc3553)
      %3608 = stablehlo.dot_general %3607, %arg834, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3554)
      %3609 = stablehlo.reshape %arg833 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc3555)
      %3610 = stablehlo.reshape %3609 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3556)
      %3611 = stablehlo.add %3608, %3610 : tensor<8x1x1440xbf16> loc(#loc3557)
      %3612 = stablehlo.reshape %3611 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3558)
      %3613 = stablehlo.convert %3586 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc3559)
      %3614 = stablehlo.reshape %arg826 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc3560)
      %3615 = stablehlo.reshape %3614 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc3561)
      %3616 = stablehlo.transpose %3615, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc3562)
      %3617 = stablehlo.convert %3616 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc3563)
      %3618 = stablehlo.dot_general %3613, %3617, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc3564)
      %3619 = "stablehlo.all_reduce"(%3618) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.9261"), %arg933: tensor<f32> loc("dot.9261")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3564)
        stablehlo.return %4605 : tensor<f32> loc(#loc3564)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc3564)
      %3620 = stablehlo.reshape %arg825 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3565)
      %3621 = stablehlo.reshape %3620 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3566)
      %3622 = stablehlo.convert %3621 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc3567)
      %3623 = stablehlo.reshape %3622 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc3568)
      %3624 = stablehlo.add %3619, %3623 : tensor<1x32xf32> loc(#loc3569)
      %3625 = stablehlo.convert %3624 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc3570)
      %3626:2 = "stablehlo.sort"(%3625, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.9282"), %arg933: tensor<bf16> loc("sort.9282"), %arg934: tensor<i32> loc("sort.9282"), %arg935: tensor<i32> loc("sort.9282")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3572)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc3571)
      %3627 = stablehlo.slice %3626#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc3573)
      %3628 = stablehlo.convert %3627 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc3574)
      %3629 = stablehlo.reshape %3628 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc3575)
      %3630 = stablehlo.concatenate %c_16, %3629, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc3576)
      %3631 = stablehlo.slice %3626#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc3577)
      %3632 = stablehlo.reduce(%3631 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3578)
      %3633 = stablehlo.broadcast_in_dim %3632, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3579)
      %3634 = stablehlo.subtract %3631, %3633 : tensor<1x4xbf16> loc(#loc3580)
      %3635 = stablehlo.exponential %3634 : tensor<1x4xbf16> loc(#loc3581)
      %3636 = stablehlo.reduce(%3635 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3582)
      %3637 = stablehlo.broadcast_in_dim %3636, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3583)
      %3638 = stablehlo.divide %3635, %3637 : tensor<1x4xbf16> loc(#loc3584)
      %3639 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %3640 = "stablehlo.all_gather"(%3639) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %3641 = "stablehlo.scatter"(%3640, %3630, %3638) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.9316"), %arg933: tensor<bf16> loc("scatter.9316")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc3585)
      %3642 = stablehlo.reshape %3641 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc3585)
      %3643 = "stablehlo.all_to_all"(%3642) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc3585)
      %3644 = stablehlo.slice %3643 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc3585)
      %3645 = stablehlo.reshape %3644 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc3585)
      %3646 = stablehlo.reshape %3645 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc3586)
      %3647 = stablehlo.broadcast_in_dim %3646, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3587)
      %3648 = stablehlo.multiply %3612, %3647 : tensor<8x1x1x1440xbf16> loc(#loc3588)
      %3649 = stablehlo.reduce(%3648 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc3589)
      %3650 = "stablehlo.all_reduce"(%3649) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.9384"), %arg933: tensor<bf16> loc("reduce.9384")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3589)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3589)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3589)
      %3651 = stablehlo.add %3567, %3650 : tensor<1x1x1440xbf16> loc(#loc3590)
      %3652 = stablehlo.convert %3651 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3591)
      %3653 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3654 = stablehlo.power %3652, %3653 : tensor<1x1x1440xf32> loc(#loc3592)
      %3655 = stablehlo.reduce(%3654 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3593)
      %3656 = "stablehlo.all_reduce"(%3655) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.9397"), %arg933: tensor<f32> loc("reduce.9397")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3593)
        stablehlo.return %4605 : tensor<f32> loc(#loc3593)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3593)
      %3657 = stablehlo.multiply %3656, %cst_1 : tensor<1x1xf32> loc(#loc3594)
      %3658 = stablehlo.reshape %3657 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3595)
      %3659 = stablehlo.add %3658, %cst_2 : tensor<1x1x1xf32> loc(#loc3596)
      %3660 = stablehlo.rsqrt %3659 : tensor<1x1x1xf32> loc(#loc3597)
      %3661 = stablehlo.reshape %3660 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3598)
      %3662 = stablehlo.broadcast_in_dim %3661, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3599)
      %3663 = stablehlo.multiply %3652, %3662 : tensor<1x1x1440xf32> loc(#loc3600)
      %3664 = stablehlo.multiply %3533, %3663 : tensor<1x1x1440xf32> loc(#loc3601)
      %3665 = stablehlo.convert %3664 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3602)
      %3666 = stablehlo.reshape %3665 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3603)
      %3667 = stablehlo.reshape %arg850 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc3604)
      %3668 = stablehlo.reshape %3667 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc3605)
      %3669 = stablehlo.transpose %3668, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc3606)
      %3670 = stablehlo.dot_general %3666, %3669, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3607)
      %3671 = "stablehlo.all_reduce"(%3670) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9589"), %arg933: tensor<bf16> loc("dot.9589")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3607)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3607)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3607)
      %3672 = stablehlo.reshape %3671 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3608)
      %3673 = stablehlo.reshape %arg849 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3609)
      %3674 = stablehlo.add %3672, %3673 : tensor<1x1x1024xbf16> loc(#loc3610)
      %3675 = stablehlo.reshape %3674 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3611)
      %3676 = stablehlo.slice %3675 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3612)
      %3677 = stablehlo.multiply %3676, %59 : tensor<1x16x1x32xbf16> loc(#loc3613)
      %3678 = stablehlo.slice %3675 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3614)
      %3679 = stablehlo.multiply %3678, %65 : tensor<1x16x1x32xbf16> loc(#loc3615)
      %3680 = stablehlo.subtract %3677, %3679 : tensor<1x16x1x32xbf16> loc(#loc3616)
      %3681 = stablehlo.multiply %3678, %59 : tensor<1x16x1x32xbf16> loc(#loc3617)
      %3682 = stablehlo.multiply %3676, %65 : tensor<1x16x1x32xbf16> loc(#loc3618)
      %3683 = stablehlo.add %3681, %3682 : tensor<1x16x1x32xbf16> loc(#loc3619)
      %3684 = stablehlo.concatenate %3680, %3683, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3620)
      %3685 = stablehlo.reshape %arg824 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3621)
      %3686 = stablehlo.reshape %3685 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3622)
      %3687 = stablehlo.transpose %3686, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3623)
      %3688 = stablehlo.dot_general %3666, %3687, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3624)
      %3689 = "stablehlo.all_reduce"(%3688) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9425"), %arg933: tensor<bf16> loc("dot.9425")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3624)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3624)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3624)
      %3690 = stablehlo.reshape %3689 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3625)
      %3691 = stablehlo.reshape %arg823 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3626)
      %3692 = stablehlo.add %3690, %3691 : tensor<1x1x128xbf16> loc(#loc3627)
      %3693 = stablehlo.reshape %3692 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3628)
      %3694 = stablehlo.slice %3693 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3629)
      %3695 = stablehlo.multiply %3694, %86 : tensor<1x2x1x32xbf16> loc(#loc3630)
      %3696 = stablehlo.slice %3693 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3631)
      %3697 = stablehlo.multiply %3696, %89 : tensor<1x2x1x32xbf16> loc(#loc3632)
      %3698 = stablehlo.subtract %3695, %3697 : tensor<1x2x1x32xbf16> loc(#loc3633)
      %3699 = stablehlo.multiply %3696, %86 : tensor<1x2x1x32xbf16> loc(#loc3634)
      %3700 = stablehlo.multiply %3694, %89 : tensor<1x2x1x32xbf16> loc(#loc3635)
      %3701 = stablehlo.add %3699, %3700 : tensor<1x2x1x32xbf16> loc(#loc3636)
      %3702 = stablehlo.concatenate %3698, %3701, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3637)
      %3703 = "stablehlo.scatter"(%arg838, %75, %3702) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.9472"), %arg933: tensor<bf16> loc("scatter.9472")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3638)
      %3704 = stablehlo.broadcast_in_dim %3703, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3639)
      %3705 = stablehlo.reshape %3704 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3640)
      %3706 = stablehlo.transpose %3705, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc3641)
      %3707 = stablehlo.dot_general %3684, %3706, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3642)
      %3708 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %3709 = stablehlo.multiply %3707, %3708 : tensor<1x16x1x256xbf16> loc(#loc3643)
      %3710 = stablehlo.add %3709, %325 : tensor<1x16x1x256xbf16> loc(#loc3644)
      %3711 = stablehlo.reshape %arg848 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc383)
      %3712 = "stablehlo.all_to_all"(%3711) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc383)
      %3713 = stablehlo.slice %3712 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc383)
      %3714 = stablehlo.reshape %3713 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc383)
      %3715 = stablehlo.reshape %3714 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3645)
      %3716 = stablehlo.reshape %3715 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc3646)
      %3717 = stablehlo.concatenate %3710, %3716, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3647)
      %3718 = stablehlo.reshape %arg856 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3648)
      %3719 = stablehlo.reshape %3718 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3649)
      %3720 = stablehlo.convert %3719 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3650)
      %3721 = stablehlo.reshape %3720 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3651)
      %3722 = stablehlo.reduce(%3717 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3652)
      %3723 = stablehlo.broadcast_in_dim %3722, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3653)
      %3724 = stablehlo.subtract %3717, %3723 : tensor<1x16x1x257xbf16> loc(#loc3654)
      %3725 = stablehlo.reduce(%3724 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3655)
      %3726 = stablehlo.broadcast_in_dim %3725, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3656)
      %3727 = stablehlo.subtract %3724, %3726 : tensor<1x16x1x257xbf16> loc(#loc3657)
      %3728 = stablehlo.exponential %3727 : tensor<1x16x1x257xbf16> loc(#loc3658)
      %3729 = stablehlo.reduce(%3728 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3659)
      %3730 = stablehlo.broadcast_in_dim %3729, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3660)
      %3731 = stablehlo.divide %3728, %3730 : tensor<1x16x1x257xbf16> loc(#loc3661)
      %3732 = stablehlo.slice %3731 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3662)
      %3733 = stablehlo.reshape %arg840 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3663)
      %3734 = stablehlo.reshape %3733 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3664)
      %3735 = stablehlo.transpose %3734, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3665)
      %3736 = stablehlo.dot_general %3666, %3735, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3666)
      %3737 = "stablehlo.all_reduce"(%3736) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9485"), %arg933: tensor<bf16> loc("dot.9485")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3666)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3666)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3666)
      %3738 = stablehlo.reshape %3737 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3667)
      %3739 = stablehlo.reshape %arg839 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3668)
      %3740 = stablehlo.add %3738, %3739 : tensor<1x1x128xbf16> loc(#loc3669)
      %3741 = stablehlo.reshape %3740 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3670)
      %3742 = "stablehlo.scatter"(%arg841, %75, %3741) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.9509"), %arg933: tensor<bf16> loc("scatter.9509")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3671)
      %3743 = stablehlo.broadcast_in_dim %3742, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3672)
      %3744 = stablehlo.reshape %3743 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3673)
      %3745 = stablehlo.dot_general %3732, %3744, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3674)
      %3746 = stablehlo.reshape %3745 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc3675)
      %3747 = stablehlo.reshape %arg847 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc3676)
      %3748 = stablehlo.reshape %3747 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc3677)
      %3749 = stablehlo.transpose %3748, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc3678)
      %3750 = stablehlo.dot_general %3746, %3749, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3679)
      %3751 = "stablehlo.all_reduce"(%3750) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9696"), %arg933: tensor<bf16> loc("dot.9696")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3679)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3679)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3679)
      %3752 = stablehlo.reshape %3751 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3680)
      %3753 = stablehlo.reshape %arg846 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3681)
      %3754 = stablehlo.add %3752, %3753 : tensor<1x1x1440xbf16> loc(#loc3682)
      %3755 = stablehlo.add %3651, %3754 : tensor<1x1x1440xbf16> loc(#loc3683)
      %3756 = stablehlo.reshape %arg851 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3684)
      %3757 = stablehlo.reshape %3756 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3685)
      %3758 = stablehlo.convert %3757 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3686)
      %3759 = stablehlo.reshape %3758 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3687)
      %3760 = stablehlo.convert %3755 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3688)
      %3761 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3762 = stablehlo.power %3760, %3761 : tensor<1x1x1440xf32> loc(#loc3689)
      %3763 = stablehlo.reduce(%3762 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3690)
      %3764 = "stablehlo.all_reduce"(%3763) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.9714"), %arg933: tensor<f32> loc("reduce.9714")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3690)
        stablehlo.return %4605 : tensor<f32> loc(#loc3690)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3690)
      %3765 = stablehlo.multiply %3764, %cst_1 : tensor<1x1xf32> loc(#loc3691)
      %3766 = stablehlo.reshape %3765 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3692)
      %3767 = stablehlo.add %3766, %cst_2 : tensor<1x1x1xf32> loc(#loc3693)
      %3768 = stablehlo.rsqrt %3767 : tensor<1x1x1xf32> loc(#loc3694)
      %3769 = stablehlo.reshape %3768 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3695)
      %3770 = stablehlo.broadcast_in_dim %3769, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3696)
      %3771 = stablehlo.multiply %3760, %3770 : tensor<1x1x1440xf32> loc(#loc3697)
      %3772 = stablehlo.multiply %3759, %3771 : tensor<1x1x1440xf32> loc(#loc3698)
      %3773 = stablehlo.convert %3772 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3699)
      %3774 = stablehlo.reshape %3773 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3700)
      %3775 = stablehlo.broadcast_in_dim %3774, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3701)
      %3776 = stablehlo.dot_general %3775, %arg855, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3702)
      %3777 = "stablehlo.all_reduce"(%3776) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9828"), %arg933: tensor<bf16> loc("dot.9828")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3702)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3702)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3702)
      %3778 = stablehlo.reshape %arg854 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc3703)
      %3779 = stablehlo.reshape %3778 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3704)
      %3780 = stablehlo.add %3777, %3779 : tensor<8x1x5760xbf16> loc(#loc3705)
      %3781 = stablehlo.slice %3780 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3706)
      %3782 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3783 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3784 = stablehlo.clamp %3783, %3781, %3782 : tensor<8x1x2880xbf16> loc(#loc3707)
      %3785 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3786 = stablehlo.add %3784, %3785 : tensor<8x1x2880xbf16> loc(#loc3708)
      %3787 = stablehlo.slice %3780 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3709)
      %3788 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3789 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3790 = stablehlo.clamp %3788, %3787, %3789 : tensor<8x1x2880xbf16> loc(#loc3710)
      %3791 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3792 = stablehlo.multiply %3790, %3791 : tensor<8x1x2880xbf16> loc(#loc3711)
      %3793 = stablehlo.logistic %3792 : tensor<8x1x2880xbf16> loc(#loc3712)
      %3794 = stablehlo.multiply %3790, %3793 : tensor<8x1x2880xbf16> loc(#loc3713)
      %3795 = stablehlo.multiply %3786, %3794 : tensor<8x1x2880xbf16> loc(#loc3714)
      %3796 = stablehlo.dot_general %3795, %arg853, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3715)
      %3797 = stablehlo.reshape %arg852 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc3716)
      %3798 = stablehlo.reshape %3797 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3717)
      %3799 = stablehlo.add %3796, %3798 : tensor<8x1x1440xbf16> loc(#loc3718)
      %3800 = stablehlo.reshape %3799 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3719)
      %3801 = stablehlo.convert %3774 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc3720)
      %3802 = stablehlo.reshape %arg845 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc3721)
      %3803 = stablehlo.reshape %3802 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc3722)
      %3804 = stablehlo.transpose %3803, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc3723)
      %3805 = stablehlo.convert %3804 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc3724)
      %3806 = stablehlo.dot_general %3801, %3805, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc3725)
      %3807 = "stablehlo.all_reduce"(%3806) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.9743"), %arg933: tensor<f32> loc("dot.9743")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3725)
        stablehlo.return %4605 : tensor<f32> loc(#loc3725)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc3725)
      %3808 = stablehlo.reshape %arg844 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3726)
      %3809 = stablehlo.reshape %3808 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3727)
      %3810 = stablehlo.convert %3809 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc3728)
      %3811 = stablehlo.reshape %3810 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc3729)
      %3812 = stablehlo.add %3807, %3811 : tensor<1x32xf32> loc(#loc3730)
      %3813 = stablehlo.convert %3812 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc3731)
      %3814:2 = "stablehlo.sort"(%3813, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.9764"), %arg933: tensor<bf16> loc("sort.9764"), %arg934: tensor<i32> loc("sort.9764"), %arg935: tensor<i32> loc("sort.9764")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3733)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc3732)
      %3815 = stablehlo.slice %3814#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc3734)
      %3816 = stablehlo.convert %3815 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc3735)
      %3817 = stablehlo.reshape %3816 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc3736)
      %3818 = stablehlo.concatenate %c_16, %3817, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc3737)
      %3819 = stablehlo.slice %3814#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc3738)
      %3820 = stablehlo.reduce(%3819 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3739)
      %3821 = stablehlo.broadcast_in_dim %3820, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3740)
      %3822 = stablehlo.subtract %3819, %3821 : tensor<1x4xbf16> loc(#loc3741)
      %3823 = stablehlo.exponential %3822 : tensor<1x4xbf16> loc(#loc3742)
      %3824 = stablehlo.reduce(%3823 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3743)
      %3825 = stablehlo.broadcast_in_dim %3824, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3744)
      %3826 = stablehlo.divide %3823, %3825 : tensor<1x4xbf16> loc(#loc3745)
      %3827 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %3828 = "stablehlo.all_gather"(%3827) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %3829 = "stablehlo.scatter"(%3828, %3818, %3826) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.9798"), %arg933: tensor<bf16> loc("scatter.9798")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc3746)
      %3830 = stablehlo.reshape %3829 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc3746)
      %3831 = "stablehlo.all_to_all"(%3830) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc3746)
      %3832 = stablehlo.slice %3831 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc3746)
      %3833 = stablehlo.reshape %3832 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc3746)
      %3834 = stablehlo.reshape %3833 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc3747)
      %3835 = stablehlo.broadcast_in_dim %3834, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3748)
      %3836 = stablehlo.multiply %3800, %3835 : tensor<8x1x1x1440xbf16> loc(#loc3749)
      %3837 = stablehlo.reduce(%3836 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc3750)
      %3838 = "stablehlo.all_reduce"(%3837) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.9866"), %arg933: tensor<bf16> loc("reduce.9866")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3750)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3750)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3750)
      %3839 = stablehlo.add %3755, %3838 : tensor<1x1x1440xbf16> loc(#loc3751)
      %3840 = stablehlo.convert %3839 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3752)
      %3841 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3842 = stablehlo.power %3840, %3841 : tensor<1x1x1440xf32> loc(#loc3753)
      %3843 = stablehlo.reduce(%3842 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3754)
      %3844 = "stablehlo.all_reduce"(%3843) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.9879"), %arg933: tensor<f32> loc("reduce.9879")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3754)
        stablehlo.return %4605 : tensor<f32> loc(#loc3754)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3754)
      %3845 = stablehlo.multiply %3844, %cst_1 : tensor<1x1xf32> loc(#loc3755)
      %3846 = stablehlo.reshape %3845 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3756)
      %3847 = stablehlo.add %3846, %cst_2 : tensor<1x1x1xf32> loc(#loc3757)
      %3848 = stablehlo.rsqrt %3847 : tensor<1x1x1xf32> loc(#loc3758)
      %3849 = stablehlo.reshape %3848 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3759)
      %3850 = stablehlo.broadcast_in_dim %3849, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3760)
      %3851 = stablehlo.multiply %3840, %3850 : tensor<1x1x1440xf32> loc(#loc3761)
      %3852 = stablehlo.multiply %3721, %3851 : tensor<1x1x1440xf32> loc(#loc3762)
      %3853 = stablehlo.convert %3852 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3763)
      %3854 = stablehlo.reshape %3853 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3764)
      %3855 = stablehlo.reshape %arg869 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc3765)
      %3856 = stablehlo.reshape %3855 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc3766)
      %3857 = stablehlo.transpose %3856, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc3767)
      %3858 = stablehlo.dot_general %3854, %3857, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3768)
      %3859 = "stablehlo.all_reduce"(%3858) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10071"), %arg933: tensor<bf16> loc("dot.10071")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3768)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3768)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3768)
      %3860 = stablehlo.reshape %3859 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3769)
      %3861 = stablehlo.reshape %arg868 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3770)
      %3862 = stablehlo.add %3860, %3861 : tensor<1x1x1024xbf16> loc(#loc3771)
      %3863 = stablehlo.reshape %3862 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3772)
      %3864 = stablehlo.slice %3863 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3773)
      %3865 = stablehlo.multiply %3864, %59 : tensor<1x16x1x32xbf16> loc(#loc3774)
      %3866 = stablehlo.slice %3863 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3775)
      %3867 = stablehlo.multiply %3866, %65 : tensor<1x16x1x32xbf16> loc(#loc3776)
      %3868 = stablehlo.subtract %3865, %3867 : tensor<1x16x1x32xbf16> loc(#loc3777)
      %3869 = stablehlo.multiply %3866, %59 : tensor<1x16x1x32xbf16> loc(#loc3778)
      %3870 = stablehlo.multiply %3864, %65 : tensor<1x16x1x32xbf16> loc(#loc3779)
      %3871 = stablehlo.add %3869, %3870 : tensor<1x16x1x32xbf16> loc(#loc3780)
      %3872 = stablehlo.concatenate %3868, %3871, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3781)
      %3873 = stablehlo.reshape %arg843 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3782)
      %3874 = stablehlo.reshape %3873 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3783)
      %3875 = stablehlo.transpose %3874, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3784)
      %3876 = stablehlo.dot_general %3854, %3875, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3785)
      %3877 = "stablehlo.all_reduce"(%3876) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9907"), %arg933: tensor<bf16> loc("dot.9907")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3785)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3785)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3785)
      %3878 = stablehlo.reshape %3877 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3786)
      %3879 = stablehlo.reshape %arg842 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3787)
      %3880 = stablehlo.add %3878, %3879 : tensor<1x1x128xbf16> loc(#loc3788)
      %3881 = stablehlo.reshape %3880 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3789)
      %3882 = stablehlo.slice %3881 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3790)
      %3883 = stablehlo.multiply %3882, %86 : tensor<1x2x1x32xbf16> loc(#loc3791)
      %3884 = stablehlo.slice %3881 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3792)
      %3885 = stablehlo.multiply %3884, %89 : tensor<1x2x1x32xbf16> loc(#loc3793)
      %3886 = stablehlo.subtract %3883, %3885 : tensor<1x2x1x32xbf16> loc(#loc3794)
      %3887 = stablehlo.multiply %3884, %86 : tensor<1x2x1x32xbf16> loc(#loc3795)
      %3888 = stablehlo.multiply %3882, %89 : tensor<1x2x1x32xbf16> loc(#loc3796)
      %3889 = stablehlo.add %3887, %3888 : tensor<1x2x1x32xbf16> loc(#loc3797)
      %3890 = stablehlo.concatenate %3886, %3889, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3798)
      %3891 = "stablehlo.scatter"(%arg857, %75, %3890) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.9954"), %arg933: tensor<bf16> loc("scatter.9954")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3799)
      %3892 = stablehlo.broadcast_in_dim %3891, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3800)
      %3893 = stablehlo.reshape %3892 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3801)
      %3894 = stablehlo.transpose %3893, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc3802)
      %3895 = stablehlo.dot_general %3872, %3894, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3803)
      %3896 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %3897 = stablehlo.multiply %3895, %3896 : tensor<1x16x1x256xbf16> loc(#loc3804)
      %3898 = stablehlo.add %3897, %126 : tensor<1x16x1x256xbf16> loc(#loc3805)
      %3899 = stablehlo.reshape %arg867 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc402)
      %3900 = "stablehlo.all_to_all"(%3899) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc402)
      %3901 = stablehlo.slice %3900 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc402)
      %3902 = stablehlo.reshape %3901 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc402)
      %3903 = stablehlo.reshape %3902 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3806)
      %3904 = stablehlo.reshape %3903 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc3807)
      %3905 = stablehlo.concatenate %3898, %3904, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3808)
      %3906 = stablehlo.reshape %arg875 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3809)
      %3907 = stablehlo.reshape %3906 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3810)
      %3908 = stablehlo.convert %3907 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3811)
      %3909 = stablehlo.reshape %3908 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3812)
      %3910 = stablehlo.reduce(%3905 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3813)
      %3911 = stablehlo.broadcast_in_dim %3910, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3814)
      %3912 = stablehlo.subtract %3905, %3911 : tensor<1x16x1x257xbf16> loc(#loc3815)
      %3913 = stablehlo.reduce(%3912 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3816)
      %3914 = stablehlo.broadcast_in_dim %3913, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3817)
      %3915 = stablehlo.subtract %3912, %3914 : tensor<1x16x1x257xbf16> loc(#loc3818)
      %3916 = stablehlo.exponential %3915 : tensor<1x16x1x257xbf16> loc(#loc3819)
      %3917 = stablehlo.reduce(%3916 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3820)
      %3918 = stablehlo.broadcast_in_dim %3917, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3821)
      %3919 = stablehlo.divide %3916, %3918 : tensor<1x16x1x257xbf16> loc(#loc3822)
      %3920 = stablehlo.slice %3919 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3823)
      %3921 = stablehlo.reshape %arg859 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3824)
      %3922 = stablehlo.reshape %3921 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3825)
      %3923 = stablehlo.transpose %3922, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3826)
      %3924 = stablehlo.dot_general %3854, %3923, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3827)
      %3925 = "stablehlo.all_reduce"(%3924) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.9967"), %arg933: tensor<bf16> loc("dot.9967")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3827)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3827)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3827)
      %3926 = stablehlo.reshape %3925 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3828)
      %3927 = stablehlo.reshape %arg858 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3829)
      %3928 = stablehlo.add %3926, %3927 : tensor<1x1x128xbf16> loc(#loc3830)
      %3929 = stablehlo.reshape %3928 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3831)
      %3930 = "stablehlo.scatter"(%arg860, %75, %3929) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.9991"), %arg933: tensor<bf16> loc("scatter.9991")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3832)
      %3931 = stablehlo.broadcast_in_dim %3930, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3833)
      %3932 = stablehlo.reshape %3931 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3834)
      %3933 = stablehlo.dot_general %3920, %3932, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3835)
      %3934 = stablehlo.reshape %3933 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc3836)
      %3935 = stablehlo.reshape %arg866 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc3837)
      %3936 = stablehlo.reshape %3935 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc3838)
      %3937 = stablehlo.transpose %3936, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc3839)
      %3938 = stablehlo.dot_general %3934, %3937, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3840)
      %3939 = "stablehlo.all_reduce"(%3938) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10178"), %arg933: tensor<bf16> loc("dot.10178")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3840)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3840)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3840)
      %3940 = stablehlo.reshape %3939 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3841)
      %3941 = stablehlo.reshape %arg865 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3842)
      %3942 = stablehlo.add %3940, %3941 : tensor<1x1x1440xbf16> loc(#loc3843)
      %3943 = stablehlo.add %3839, %3942 : tensor<1x1x1440xbf16> loc(#loc3844)
      %3944 = stablehlo.reshape %arg870 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3845)
      %3945 = stablehlo.reshape %3944 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3846)
      %3946 = stablehlo.convert %3945 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3847)
      %3947 = stablehlo.reshape %3946 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3848)
      %3948 = stablehlo.convert %3943 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3849)
      %3949 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %3950 = stablehlo.power %3948, %3949 : tensor<1x1x1440xf32> loc(#loc3850)
      %3951 = stablehlo.reduce(%3950 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3851)
      %3952 = "stablehlo.all_reduce"(%3951) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.10196"), %arg933: tensor<f32> loc("reduce.10196")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3851)
        stablehlo.return %4605 : tensor<f32> loc(#loc3851)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3851)
      %3953 = stablehlo.multiply %3952, %cst_1 : tensor<1x1xf32> loc(#loc3852)
      %3954 = stablehlo.reshape %3953 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3853)
      %3955 = stablehlo.add %3954, %cst_2 : tensor<1x1x1xf32> loc(#loc3854)
      %3956 = stablehlo.rsqrt %3955 : tensor<1x1x1xf32> loc(#loc3855)
      %3957 = stablehlo.reshape %3956 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3856)
      %3958 = stablehlo.broadcast_in_dim %3957, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3857)
      %3959 = stablehlo.multiply %3948, %3958 : tensor<1x1x1440xf32> loc(#loc3858)
      %3960 = stablehlo.multiply %3947, %3959 : tensor<1x1x1440xf32> loc(#loc3859)
      %3961 = stablehlo.convert %3960 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3860)
      %3962 = stablehlo.reshape %3961 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3861)
      %3963 = stablehlo.broadcast_in_dim %3962, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3862)
      %3964 = stablehlo.dot_general %3963, %arg874, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3863)
      %3965 = "stablehlo.all_reduce"(%3964) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10310"), %arg933: tensor<bf16> loc("dot.10310")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3863)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3863)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3863)
      %3966 = stablehlo.reshape %arg873 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc3864)
      %3967 = stablehlo.reshape %3966 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc3865)
      %3968 = stablehlo.add %3965, %3967 : tensor<8x1x5760xbf16> loc(#loc3866)
      %3969 = stablehlo.slice %3968 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3867)
      %3970 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3971 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3972 = stablehlo.clamp %3971, %3969, %3970 : tensor<8x1x2880xbf16> loc(#loc3868)
      %3973 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3974 = stablehlo.add %3972, %3973 : tensor<8x1x2880xbf16> loc(#loc3869)
      %3975 = stablehlo.slice %3968 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc3870)
      %3976 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3977 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3978 = stablehlo.clamp %3976, %3975, %3977 : tensor<8x1x2880xbf16> loc(#loc3871)
      %3979 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %3980 = stablehlo.multiply %3978, %3979 : tensor<8x1x2880xbf16> loc(#loc3872)
      %3981 = stablehlo.logistic %3980 : tensor<8x1x2880xbf16> loc(#loc3873)
      %3982 = stablehlo.multiply %3978, %3981 : tensor<8x1x2880xbf16> loc(#loc3874)
      %3983 = stablehlo.multiply %3974, %3982 : tensor<8x1x2880xbf16> loc(#loc3875)
      %3984 = stablehlo.dot_general %3983, %arg872, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3876)
      %3985 = stablehlo.reshape %arg871 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc3877)
      %3986 = stablehlo.reshape %3985 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc3878)
      %3987 = stablehlo.add %3984, %3986 : tensor<8x1x1440xbf16> loc(#loc3879)
      %3988 = stablehlo.reshape %3987 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3880)
      %3989 = stablehlo.convert %3962 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc3881)
      %3990 = stablehlo.reshape %arg864 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc3882)
      %3991 = stablehlo.reshape %3990 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc3883)
      %3992 = stablehlo.transpose %3991, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc3884)
      %3993 = stablehlo.convert %3992 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc3885)
      %3994 = stablehlo.dot_general %3989, %3993, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc3886)
      %3995 = "stablehlo.all_reduce"(%3994) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.10225"), %arg933: tensor<f32> loc("dot.10225")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3886)
        stablehlo.return %4605 : tensor<f32> loc(#loc3886)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc3886)
      %3996 = stablehlo.reshape %arg863 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc3887)
      %3997 = stablehlo.reshape %3996 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc3888)
      %3998 = stablehlo.convert %3997 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc3889)
      %3999 = stablehlo.reshape %3998 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc3890)
      %4000 = stablehlo.add %3995, %3999 : tensor<1x32xf32> loc(#loc3891)
      %4001 = stablehlo.convert %4000 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc3892)
      %4002:2 = "stablehlo.sort"(%4001, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.10246"), %arg933: tensor<bf16> loc("sort.10246"), %arg934: tensor<i32> loc("sort.10246"), %arg935: tensor<i32> loc("sort.10246")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3894)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc3893)
      %4003 = stablehlo.slice %4002#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc3895)
      %4004 = stablehlo.convert %4003 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc3896)
      %4005 = stablehlo.reshape %4004 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc3897)
      %4006 = stablehlo.concatenate %c_16, %4005, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc3898)
      %4007 = stablehlo.slice %4002#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc3899)
      %4008 = stablehlo.reduce(%4007 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3900)
      %4009 = stablehlo.broadcast_in_dim %4008, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3901)
      %4010 = stablehlo.subtract %4007, %4009 : tensor<1x4xbf16> loc(#loc3902)
      %4011 = stablehlo.exponential %4010 : tensor<1x4xbf16> loc(#loc3903)
      %4012 = stablehlo.reduce(%4011 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc3904)
      %4013 = stablehlo.broadcast_in_dim %4012, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc3905)
      %4014 = stablehlo.divide %4011, %4013 : tensor<1x4xbf16> loc(#loc3906)
      %4015 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %4016 = "stablehlo.all_gather"(%4015) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %4017 = "stablehlo.scatter"(%4016, %4006, %4014) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.10280"), %arg933: tensor<bf16> loc("scatter.10280")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc3907)
      %4018 = stablehlo.reshape %4017 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc3907)
      %4019 = "stablehlo.all_to_all"(%4018) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc3907)
      %4020 = stablehlo.slice %4019 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc3907)
      %4021 = stablehlo.reshape %4020 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc3907)
      %4022 = stablehlo.reshape %4021 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc3908)
      %4023 = stablehlo.broadcast_in_dim %4022, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc3909)
      %4024 = stablehlo.multiply %3988, %4023 : tensor<8x1x1x1440xbf16> loc(#loc3910)
      %4025 = stablehlo.reduce(%4024 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc3911)
      %4026 = "stablehlo.all_reduce"(%4025) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.10348"), %arg933: tensor<bf16> loc("reduce.10348")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3911)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3911)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3911)
      %4027 = stablehlo.add %3943, %4026 : tensor<1x1x1440xbf16> loc(#loc3912)
      %4028 = stablehlo.convert %4027 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc3913)
      %4029 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %4030 = stablehlo.power %4028, %4029 : tensor<1x1x1440xf32> loc(#loc3914)
      %4031 = stablehlo.reduce(%4030 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc3915)
      %4032 = "stablehlo.all_reduce"(%4031) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.10361"), %arg933: tensor<f32> loc("reduce.10361")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc3915)
        stablehlo.return %4605 : tensor<f32> loc(#loc3915)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc3915)
      %4033 = stablehlo.multiply %4032, %cst_1 : tensor<1x1xf32> loc(#loc3916)
      %4034 = stablehlo.reshape %4033 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc3917)
      %4035 = stablehlo.add %4034, %cst_2 : tensor<1x1x1xf32> loc(#loc3918)
      %4036 = stablehlo.rsqrt %4035 : tensor<1x1x1xf32> loc(#loc3919)
      %4037 = stablehlo.reshape %4036 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc3920)
      %4038 = stablehlo.broadcast_in_dim %4037, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc3921)
      %4039 = stablehlo.multiply %4028, %4038 : tensor<1x1x1440xf32> loc(#loc3922)
      %4040 = stablehlo.multiply %3909, %4039 : tensor<1x1x1440xf32> loc(#loc3923)
      %4041 = stablehlo.convert %4040 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc3924)
      %4042 = stablehlo.reshape %4041 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc3925)
      %4043 = stablehlo.reshape %arg888 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc3926)
      %4044 = stablehlo.reshape %4043 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc3927)
      %4045 = stablehlo.transpose %4044, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc3928)
      %4046 = stablehlo.dot_general %4042, %4045, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3929)
      %4047 = "stablehlo.all_reduce"(%4046) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10553"), %arg933: tensor<bf16> loc("dot.10553")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3929)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3929)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc3929)
      %4048 = stablehlo.reshape %4047 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3930)
      %4049 = stablehlo.reshape %arg887 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3931)
      %4050 = stablehlo.add %4048, %4049 : tensor<1x1x1024xbf16> loc(#loc3932)
      %4051 = stablehlo.reshape %4050 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3933)
      %4052 = stablehlo.slice %4051 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3934)
      %4053 = stablehlo.multiply %4052, %59 : tensor<1x16x1x32xbf16> loc(#loc3935)
      %4054 = stablehlo.slice %4051 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc3936)
      %4055 = stablehlo.multiply %4054, %65 : tensor<1x16x1x32xbf16> loc(#loc3937)
      %4056 = stablehlo.subtract %4053, %4055 : tensor<1x16x1x32xbf16> loc(#loc3938)
      %4057 = stablehlo.multiply %4054, %59 : tensor<1x16x1x32xbf16> loc(#loc3939)
      %4058 = stablehlo.multiply %4052, %65 : tensor<1x16x1x32xbf16> loc(#loc3940)
      %4059 = stablehlo.add %4057, %4058 : tensor<1x16x1x32xbf16> loc(#loc3941)
      %4060 = stablehlo.concatenate %4056, %4059, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3942)
      %4061 = stablehlo.reshape %arg862 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3943)
      %4062 = stablehlo.reshape %4061 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3944)
      %4063 = stablehlo.transpose %4062, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3945)
      %4064 = stablehlo.dot_general %4042, %4063, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3946)
      %4065 = "stablehlo.all_reduce"(%4064) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10389"), %arg933: tensor<bf16> loc("dot.10389")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3946)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3946)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3946)
      %4066 = stablehlo.reshape %4065 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3947)
      %4067 = stablehlo.reshape %arg861 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3948)
      %4068 = stablehlo.add %4066, %4067 : tensor<1x1x128xbf16> loc(#loc3949)
      %4069 = stablehlo.reshape %4068 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3950)
      %4070 = stablehlo.slice %4069 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3951)
      %4071 = stablehlo.multiply %4070, %86 : tensor<1x2x1x32xbf16> loc(#loc3952)
      %4072 = stablehlo.slice %4069 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc3953)
      %4073 = stablehlo.multiply %4072, %89 : tensor<1x2x1x32xbf16> loc(#loc3954)
      %4074 = stablehlo.subtract %4071, %4073 : tensor<1x2x1x32xbf16> loc(#loc3955)
      %4075 = stablehlo.multiply %4072, %86 : tensor<1x2x1x32xbf16> loc(#loc3956)
      %4076 = stablehlo.multiply %4070, %89 : tensor<1x2x1x32xbf16> loc(#loc3957)
      %4077 = stablehlo.add %4075, %4076 : tensor<1x2x1x32xbf16> loc(#loc3958)
      %4078 = stablehlo.concatenate %4074, %4077, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3959)
      %4079 = "stablehlo.scatter"(%arg876, %75, %4078) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.10436"), %arg933: tensor<bf16> loc("scatter.10436")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3960)
      %4080 = stablehlo.broadcast_in_dim %4079, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3961)
      %4081 = stablehlo.reshape %4080 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3962)
      %4082 = stablehlo.transpose %4081, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc3963)
      %4083 = stablehlo.dot_general %4060, %4082, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3964)
      %4084 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %4085 = stablehlo.multiply %4083, %4084 : tensor<1x16x1x256xbf16> loc(#loc3965)
      %4086 = stablehlo.add %4085, %325 : tensor<1x16x1x256xbf16> loc(#loc3966)
      %4087 = stablehlo.reshape %arg886 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc421)
      %4088 = "stablehlo.all_to_all"(%4087) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc421)
      %4089 = stablehlo.slice %4088 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc421)
      %4090 = stablehlo.reshape %4089 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc421)
      %4091 = stablehlo.reshape %4090 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3967)
      %4092 = stablehlo.reshape %4091 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc3968)
      %4093 = stablehlo.concatenate %4086, %4092, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3969)
      %4094 = stablehlo.reshape %arg894 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc3970)
      %4095 = stablehlo.reshape %4094 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc3971)
      %4096 = stablehlo.convert %4095 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc3972)
      %4097 = stablehlo.reshape %4096 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc3973)
      %4098 = stablehlo.reduce(%4093 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3974)
      %4099 = stablehlo.broadcast_in_dim %4098, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3975)
      %4100 = stablehlo.subtract %4093, %4099 : tensor<1x16x1x257xbf16> loc(#loc3976)
      %4101 = stablehlo.reduce(%4100 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3977)
      %4102 = stablehlo.broadcast_in_dim %4101, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3978)
      %4103 = stablehlo.subtract %4100, %4102 : tensor<1x16x1x257xbf16> loc(#loc3979)
      %4104 = stablehlo.exponential %4103 : tensor<1x16x1x257xbf16> loc(#loc3980)
      %4105 = stablehlo.reduce(%4104 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc3981)
      %4106 = stablehlo.broadcast_in_dim %4105, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc3982)
      %4107 = stablehlo.divide %4104, %4106 : tensor<1x16x1x257xbf16> loc(#loc3983)
      %4108 = stablehlo.slice %4107 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc3984)
      %4109 = stablehlo.reshape %arg878 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc3985)
      %4110 = stablehlo.reshape %4109 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc3986)
      %4111 = stablehlo.transpose %4110, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc3987)
      %4112 = stablehlo.dot_general %4042, %4111, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc3988)
      %4113 = "stablehlo.all_reduce"(%4112) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10449"), %arg933: tensor<bf16> loc("dot.10449")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc3988)
        stablehlo.return %4605 : tensor<bf16> loc(#loc3988)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc3988)
      %4114 = stablehlo.reshape %4113 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3989)
      %4115 = stablehlo.reshape %arg877 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3990)
      %4116 = stablehlo.add %4114, %4115 : tensor<1x1x128xbf16> loc(#loc3991)
      %4117 = stablehlo.reshape %4116 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc3992)
      %4118 = "stablehlo.scatter"(%arg879, %75, %4117) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.10473"), %arg933: tensor<bf16> loc("scatter.10473")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc3993)
      %4119 = stablehlo.broadcast_in_dim %4118, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc3994)
      %4120 = stablehlo.reshape %4119 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc3995)
      %4121 = stablehlo.dot_general %4108, %4120, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc3996)
      %4122 = stablehlo.reshape %4121 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc3997)
      %4123 = stablehlo.reshape %arg885 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc3998)
      %4124 = stablehlo.reshape %4123 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc3999)
      %4125 = stablehlo.transpose %4124, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc4000)
      %4126 = stablehlo.dot_general %4122, %4125, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4001)
      %4127 = "stablehlo.all_reduce"(%4126) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10660"), %arg933: tensor<bf16> loc("dot.10660")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4001)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4001)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4001)
      %4128 = stablehlo.reshape %4127 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4002)
      %4129 = stablehlo.reshape %arg884 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4003)
      %4130 = stablehlo.add %4128, %4129 : tensor<1x1x1440xbf16> loc(#loc4004)
      %4131 = stablehlo.add %4027, %4130 : tensor<1x1x1440xbf16> loc(#loc4005)
      %4132 = stablehlo.reshape %arg889 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4006)
      %4133 = stablehlo.reshape %4132 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc4007)
      %4134 = stablehlo.convert %4133 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc4008)
      %4135 = stablehlo.reshape %4134 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc4009)
      %4136 = stablehlo.convert %4131 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc4010)
      %4137 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %4138 = stablehlo.power %4136, %4137 : tensor<1x1x1440xf32> loc(#loc4011)
      %4139 = stablehlo.reduce(%4138 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc4012)
      %4140 = "stablehlo.all_reduce"(%4139) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.10678"), %arg933: tensor<f32> loc("reduce.10678")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4012)
        stablehlo.return %4605 : tensor<f32> loc(#loc4012)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc4012)
      %4141 = stablehlo.multiply %4140, %cst_1 : tensor<1x1xf32> loc(#loc4013)
      %4142 = stablehlo.reshape %4141 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc4014)
      %4143 = stablehlo.add %4142, %cst_2 : tensor<1x1x1xf32> loc(#loc4015)
      %4144 = stablehlo.rsqrt %4143 : tensor<1x1x1xf32> loc(#loc4016)
      %4145 = stablehlo.reshape %4144 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc4017)
      %4146 = stablehlo.broadcast_in_dim %4145, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc4018)
      %4147 = stablehlo.multiply %4136, %4146 : tensor<1x1x1440xf32> loc(#loc4019)
      %4148 = stablehlo.multiply %4135, %4147 : tensor<1x1x1440xf32> loc(#loc4020)
      %4149 = stablehlo.convert %4148 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc4021)
      %4150 = stablehlo.reshape %4149 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4022)
      %4151 = stablehlo.broadcast_in_dim %4150, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4023)
      %4152 = stablehlo.dot_general %4151, %arg893, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4024)
      %4153 = "stablehlo.all_reduce"(%4152) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10792"), %arg933: tensor<bf16> loc("dot.10792")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4024)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4024)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4024)
      %4154 = stablehlo.reshape %arg892 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc4025)
      %4155 = stablehlo.reshape %4154 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4026)
      %4156 = stablehlo.add %4153, %4155 : tensor<8x1x5760xbf16> loc(#loc4027)
      %4157 = stablehlo.slice %4156 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc4028)
      %4158 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4159 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4160 = stablehlo.clamp %4159, %4157, %4158 : tensor<8x1x2880xbf16> loc(#loc4029)
      %4161 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4162 = stablehlo.add %4160, %4161 : tensor<8x1x2880xbf16> loc(#loc4030)
      %4163 = stablehlo.slice %4156 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc4031)
      %4164 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4165 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4166 = stablehlo.clamp %4164, %4163, %4165 : tensor<8x1x2880xbf16> loc(#loc4032)
      %4167 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4168 = stablehlo.multiply %4166, %4167 : tensor<8x1x2880xbf16> loc(#loc4033)
      %4169 = stablehlo.logistic %4168 : tensor<8x1x2880xbf16> loc(#loc4034)
      %4170 = stablehlo.multiply %4166, %4169 : tensor<8x1x2880xbf16> loc(#loc4035)
      %4171 = stablehlo.multiply %4162, %4170 : tensor<8x1x2880xbf16> loc(#loc4036)
      %4172 = stablehlo.dot_general %4171, %arg891, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4037)
      %4173 = stablehlo.reshape %arg890 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc4038)
      %4174 = stablehlo.reshape %4173 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4039)
      %4175 = stablehlo.add %4172, %4174 : tensor<8x1x1440xbf16> loc(#loc4040)
      %4176 = stablehlo.reshape %4175 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc4041)
      %4177 = stablehlo.convert %4150 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc4042)
      %4178 = stablehlo.reshape %arg883 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc4043)
      %4179 = stablehlo.reshape %4178 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc4044)
      %4180 = stablehlo.transpose %4179, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc4045)
      %4181 = stablehlo.convert %4180 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc4046)
      %4182 = stablehlo.dot_general %4177, %4181, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc4047)
      %4183 = "stablehlo.all_reduce"(%4182) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.10707"), %arg933: tensor<f32> loc("dot.10707")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4047)
        stablehlo.return %4605 : tensor<f32> loc(#loc4047)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc4047)
      %4184 = stablehlo.reshape %arg882 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc4048)
      %4185 = stablehlo.reshape %4184 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc4049)
      %4186 = stablehlo.convert %4185 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc4050)
      %4187 = stablehlo.reshape %4186 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc4051)
      %4188 = stablehlo.add %4183, %4187 : tensor<1x32xf32> loc(#loc4052)
      %4189 = stablehlo.convert %4188 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc4053)
      %4190:2 = "stablehlo.sort"(%4189, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.10728"), %arg933: tensor<bf16> loc("sort.10728"), %arg934: tensor<i32> loc("sort.10728"), %arg935: tensor<i32> loc("sort.10728")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4055)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc4054)
      %4191 = stablehlo.slice %4190#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc4056)
      %4192 = stablehlo.convert %4191 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc4057)
      %4193 = stablehlo.reshape %4192 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc4058)
      %4194 = stablehlo.concatenate %c_16, %4193, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc4059)
      %4195 = stablehlo.slice %4190#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc4060)
      %4196 = stablehlo.reduce(%4195 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc4061)
      %4197 = stablehlo.broadcast_in_dim %4196, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc4062)
      %4198 = stablehlo.subtract %4195, %4197 : tensor<1x4xbf16> loc(#loc4063)
      %4199 = stablehlo.exponential %4198 : tensor<1x4xbf16> loc(#loc4064)
      %4200 = stablehlo.reduce(%4199 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc4065)
      %4201 = stablehlo.broadcast_in_dim %4200, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc4066)
      %4202 = stablehlo.divide %4199, %4201 : tensor<1x4xbf16> loc(#loc4067)
      %4203 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %4204 = "stablehlo.all_gather"(%4203) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %4205 = "stablehlo.scatter"(%4204, %4194, %4202) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.10762"), %arg933: tensor<bf16> loc("scatter.10762")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc4068)
      %4206 = stablehlo.reshape %4205 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc4068)
      %4207 = "stablehlo.all_to_all"(%4206) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc4068)
      %4208 = stablehlo.slice %4207 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc4068)
      %4209 = stablehlo.reshape %4208 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc4068)
      %4210 = stablehlo.reshape %4209 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc4069)
      %4211 = stablehlo.broadcast_in_dim %4210, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc4070)
      %4212 = stablehlo.multiply %4176, %4211 : tensor<8x1x1x1440xbf16> loc(#loc4071)
      %4213 = stablehlo.reduce(%4212 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc4072)
      %4214 = "stablehlo.all_reduce"(%4213) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.10830"), %arg933: tensor<bf16> loc("reduce.10830")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4072)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4072)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4072)
      %4215 = stablehlo.add %4131, %4214 : tensor<1x1x1440xbf16> loc(#loc4073)
      %4216 = stablehlo.convert %4215 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc4074)
      %4217 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %4218 = stablehlo.power %4216, %4217 : tensor<1x1x1440xf32> loc(#loc4075)
      %4219 = stablehlo.reduce(%4218 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc4076)
      %4220 = "stablehlo.all_reduce"(%4219) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.10843"), %arg933: tensor<f32> loc("reduce.10843")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4076)
        stablehlo.return %4605 : tensor<f32> loc(#loc4076)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc4076)
      %4221 = stablehlo.multiply %4220, %cst_1 : tensor<1x1xf32> loc(#loc4077)
      %4222 = stablehlo.reshape %4221 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc4078)
      %4223 = stablehlo.add %4222, %cst_2 : tensor<1x1x1xf32> loc(#loc4079)
      %4224 = stablehlo.rsqrt %4223 : tensor<1x1x1xf32> loc(#loc4080)
      %4225 = stablehlo.reshape %4224 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc4081)
      %4226 = stablehlo.broadcast_in_dim %4225, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc4082)
      %4227 = stablehlo.multiply %4216, %4226 : tensor<1x1x1440xf32> loc(#loc4083)
      %4228 = stablehlo.multiply %4097, %4227 : tensor<1x1x1440xf32> loc(#loc4084)
      %4229 = stablehlo.convert %4228 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc4085)
      %4230 = stablehlo.reshape %4229 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4086)
      %4231 = stablehlo.reshape %arg907 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc4087)
      %4232 = stablehlo.reshape %4231 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc4088)
      %4233 = stablehlo.transpose %4232, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc4089)
      %4234 = stablehlo.dot_general %4230, %4233, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc4090)
      %4235 = "stablehlo.all_reduce"(%4234) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11035"), %arg933: tensor<bf16> loc("dot.11035")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4090)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4090)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc4090)
      %4236 = stablehlo.reshape %4235 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4091)
      %4237 = stablehlo.reshape %arg906 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4092)
      %4238 = stablehlo.add %4236, %4237 : tensor<1x1x1024xbf16> loc(#loc4093)
      %4239 = stablehlo.reshape %4238 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc4094)
      %4240 = stablehlo.slice %4239 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc4095)
      %4241 = stablehlo.multiply %4240, %59 : tensor<1x16x1x32xbf16> loc(#loc4096)
      %4242 = stablehlo.slice %4239 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc4097)
      %4243 = stablehlo.multiply %4242, %65 : tensor<1x16x1x32xbf16> loc(#loc4098)
      %4244 = stablehlo.subtract %4241, %4243 : tensor<1x16x1x32xbf16> loc(#loc4099)
      %4245 = stablehlo.multiply %4242, %59 : tensor<1x16x1x32xbf16> loc(#loc4100)
      %4246 = stablehlo.multiply %4240, %65 : tensor<1x16x1x32xbf16> loc(#loc4101)
      %4247 = stablehlo.add %4245, %4246 : tensor<1x16x1x32xbf16> loc(#loc4102)
      %4248 = stablehlo.concatenate %4244, %4247, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc4103)
      %4249 = stablehlo.reshape %arg881 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc4104)
      %4250 = stablehlo.reshape %4249 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc4105)
      %4251 = stablehlo.transpose %4250, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc4106)
      %4252 = stablehlo.dot_general %4230, %4251, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc4107)
      %4253 = "stablehlo.all_reduce"(%4252) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10871"), %arg933: tensor<bf16> loc("dot.10871")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4107)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4107)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc4107)
      %4254 = stablehlo.reshape %4253 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4108)
      %4255 = stablehlo.reshape %arg880 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4109)
      %4256 = stablehlo.add %4254, %4255 : tensor<1x1x128xbf16> loc(#loc4110)
      %4257 = stablehlo.reshape %4256 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc4111)
      %4258 = stablehlo.slice %4257 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc4112)
      %4259 = stablehlo.multiply %4258, %86 : tensor<1x2x1x32xbf16> loc(#loc4113)
      %4260 = stablehlo.slice %4257 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc4114)
      %4261 = stablehlo.multiply %4260, %89 : tensor<1x2x1x32xbf16> loc(#loc4115)
      %4262 = stablehlo.subtract %4259, %4261 : tensor<1x2x1x32xbf16> loc(#loc4116)
      %4263 = stablehlo.multiply %4260, %86 : tensor<1x2x1x32xbf16> loc(#loc4117)
      %4264 = stablehlo.multiply %4258, %89 : tensor<1x2x1x32xbf16> loc(#loc4118)
      %4265 = stablehlo.add %4263, %4264 : tensor<1x2x1x32xbf16> loc(#loc4119)
      %4266 = stablehlo.concatenate %4262, %4265, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc4120)
      %4267 = "stablehlo.scatter"(%arg895, %75, %4266) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.10918"), %arg933: tensor<bf16> loc("scatter.10918")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc4121)
      %4268 = stablehlo.broadcast_in_dim %4267, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc4122)
      %4269 = stablehlo.reshape %4268 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc4123)
      %4270 = stablehlo.transpose %4269, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc4124)
      %4271 = stablehlo.dot_general %4248, %4270, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc4125)
      %4272 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x1x256xbf16> loc(#loc)
      %4273 = stablehlo.multiply %4271, %4272 : tensor<1x16x1x256xbf16> loc(#loc4126)
      %4274 = stablehlo.add %4273, %126 : tensor<1x16x1x256xbf16> loc(#loc4127)
      %4275 = stablehlo.reshape %arg905 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc440)
      %4276 = "stablehlo.all_to_all"(%4275) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc440)
      %4277 = stablehlo.slice %4276 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc440)
      %4278 = stablehlo.reshape %4277 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc440)
      %4279 = stablehlo.reshape %4278 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc4128)
      %4280 = stablehlo.reshape %4279 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc4129)
      %4281 = stablehlo.concatenate %4274, %4280, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4130)
      %4282 = stablehlo.reshape %arg913 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4131)
      %4283 = stablehlo.reshape %4282 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc4132)
      %4284 = stablehlo.convert %4283 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc4133)
      %4285 = stablehlo.reshape %4284 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc4134)
      %4286 = stablehlo.reduce(%4281 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc4135)
      %4287 = stablehlo.broadcast_in_dim %4286, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4136)
      %4288 = stablehlo.subtract %4281, %4287 : tensor<1x16x1x257xbf16> loc(#loc4137)
      %4289 = stablehlo.reduce(%4288 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc4138)
      %4290 = stablehlo.broadcast_in_dim %4289, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4139)
      %4291 = stablehlo.subtract %4288, %4290 : tensor<1x16x1x257xbf16> loc(#loc4140)
      %4292 = stablehlo.exponential %4291 : tensor<1x16x1x257xbf16> loc(#loc4141)
      %4293 = stablehlo.reduce(%4292 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc4142)
      %4294 = stablehlo.broadcast_in_dim %4293, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4143)
      %4295 = stablehlo.divide %4292, %4294 : tensor<1x16x1x257xbf16> loc(#loc4144)
      %4296 = stablehlo.slice %4295 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc4145)
      %4297 = stablehlo.reshape %arg897 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc4146)
      %4298 = stablehlo.reshape %4297 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc4147)
      %4299 = stablehlo.transpose %4298, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc4148)
      %4300 = stablehlo.dot_general %4230, %4299, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc4149)
      %4301 = "stablehlo.all_reduce"(%4300) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.10931"), %arg933: tensor<bf16> loc("dot.10931")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4149)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4149)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc4149)
      %4302 = stablehlo.reshape %4301 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4150)
      %4303 = stablehlo.reshape %arg896 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4151)
      %4304 = stablehlo.add %4302, %4303 : tensor<1x1x128xbf16> loc(#loc4152)
      %4305 = stablehlo.reshape %4304 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc4153)
      %4306 = "stablehlo.scatter"(%arg898, %75, %4305) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.10955"), %arg933: tensor<bf16> loc("scatter.10955")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc4154)
      %4307 = stablehlo.broadcast_in_dim %4306, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc4155)
      %4308 = stablehlo.reshape %4307 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc4156)
      %4309 = stablehlo.dot_general %4296, %4308, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc4157)
      %4310 = stablehlo.reshape %4309 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc4158)
      %4311 = stablehlo.reshape %arg904 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc4159)
      %4312 = stablehlo.reshape %4311 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc4160)
      %4313 = stablehlo.transpose %4312, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc4161)
      %4314 = stablehlo.dot_general %4310, %4313, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4162)
      %4315 = "stablehlo.all_reduce"(%4314) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11142"), %arg933: tensor<bf16> loc("dot.11142")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4162)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4162)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4162)
      %4316 = stablehlo.reshape %4315 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4163)
      %4317 = stablehlo.reshape %arg903 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4164)
      %4318 = stablehlo.add %4316, %4317 : tensor<1x1x1440xbf16> loc(#loc4165)
      %4319 = stablehlo.add %4215, %4318 : tensor<1x1x1440xbf16> loc(#loc4166)
      %4320 = stablehlo.reshape %arg908 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4167)
      %4321 = stablehlo.reshape %4320 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc4168)
      %4322 = stablehlo.convert %4321 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc4169)
      %4323 = stablehlo.reshape %4322 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc4170)
      %4324 = stablehlo.convert %4319 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc4171)
      %4325 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %4326 = stablehlo.power %4324, %4325 : tensor<1x1x1440xf32> loc(#loc4172)
      %4327 = stablehlo.reduce(%4326 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc4173)
      %4328 = "stablehlo.all_reduce"(%4327) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.11160"), %arg933: tensor<f32> loc("reduce.11160")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4173)
        stablehlo.return %4605 : tensor<f32> loc(#loc4173)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc4173)
      %4329 = stablehlo.multiply %4328, %cst_1 : tensor<1x1xf32> loc(#loc4174)
      %4330 = stablehlo.reshape %4329 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc4175)
      %4331 = stablehlo.add %4330, %cst_2 : tensor<1x1x1xf32> loc(#loc4176)
      %4332 = stablehlo.rsqrt %4331 : tensor<1x1x1xf32> loc(#loc4177)
      %4333 = stablehlo.reshape %4332 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc4178)
      %4334 = stablehlo.broadcast_in_dim %4333, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc4179)
      %4335 = stablehlo.multiply %4324, %4334 : tensor<1x1x1440xf32> loc(#loc4180)
      %4336 = stablehlo.multiply %4323, %4335 : tensor<1x1x1440xf32> loc(#loc4181)
      %4337 = stablehlo.convert %4336 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc4182)
      %4338 = stablehlo.reshape %4337 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4183)
      %4339 = stablehlo.broadcast_in_dim %4338, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4184)
      %4340 = stablehlo.dot_general %4339, %arg912, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4185)
      %4341 = "stablehlo.all_reduce"(%4340) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11274"), %arg933: tensor<bf16> loc("dot.11274")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4185)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4185)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4185)
      %4342 = stablehlo.reshape %arg911 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc4186)
      %4343 = stablehlo.reshape %4342 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4187)
      %4344 = stablehlo.add %4341, %4343 : tensor<8x1x5760xbf16> loc(#loc4188)
      %4345 = stablehlo.slice %4344 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc4189)
      %4346 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4347 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4348 = stablehlo.clamp %4347, %4345, %4346 : tensor<8x1x2880xbf16> loc(#loc4190)
      %4349 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4350 = stablehlo.add %4348, %4349 : tensor<8x1x2880xbf16> loc(#loc4191)
      %4351 = stablehlo.slice %4344 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc4192)
      %4352 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4353 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4354 = stablehlo.clamp %4352, %4351, %4353 : tensor<8x1x2880xbf16> loc(#loc4193)
      %4355 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4356 = stablehlo.multiply %4354, %4355 : tensor<8x1x2880xbf16> loc(#loc4194)
      %4357 = stablehlo.logistic %4356 : tensor<8x1x2880xbf16> loc(#loc4195)
      %4358 = stablehlo.multiply %4354, %4357 : tensor<8x1x2880xbf16> loc(#loc4196)
      %4359 = stablehlo.multiply %4350, %4358 : tensor<8x1x2880xbf16> loc(#loc4197)
      %4360 = stablehlo.dot_general %4359, %arg910, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4198)
      %4361 = stablehlo.reshape %arg909 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc4199)
      %4362 = stablehlo.reshape %4361 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4200)
      %4363 = stablehlo.add %4360, %4362 : tensor<8x1x1440xbf16> loc(#loc4201)
      %4364 = stablehlo.reshape %4363 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc4202)
      %4365 = stablehlo.convert %4338 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc4203)
      %4366 = stablehlo.reshape %arg902 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc4204)
      %4367 = stablehlo.reshape %4366 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc4205)
      %4368 = stablehlo.transpose %4367, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc4206)
      %4369 = stablehlo.convert %4368 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc4207)
      %4370 = stablehlo.dot_general %4365, %4369, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc4208)
      %4371 = "stablehlo.all_reduce"(%4370) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.11189"), %arg933: tensor<f32> loc("dot.11189")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4208)
        stablehlo.return %4605 : tensor<f32> loc(#loc4208)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc4208)
      %4372 = stablehlo.reshape %arg901 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc4209)
      %4373 = stablehlo.reshape %4372 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc4210)
      %4374 = stablehlo.convert %4373 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc4211)
      %4375 = stablehlo.reshape %4374 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc4212)
      %4376 = stablehlo.add %4371, %4375 : tensor<1x32xf32> loc(#loc4213)
      %4377 = stablehlo.convert %4376 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc4214)
      %4378:2 = "stablehlo.sort"(%4377, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.11210"), %arg933: tensor<bf16> loc("sort.11210"), %arg934: tensor<i32> loc("sort.11210"), %arg935: tensor<i32> loc("sort.11210")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4216)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc4215)
      %4379 = stablehlo.slice %4378#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc4217)
      %4380 = stablehlo.convert %4379 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc4218)
      %4381 = stablehlo.reshape %4380 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc4219)
      %4382 = stablehlo.concatenate %c_16, %4381, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc4220)
      %4383 = stablehlo.slice %4378#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc4221)
      %4384 = stablehlo.reduce(%4383 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc4222)
      %4385 = stablehlo.broadcast_in_dim %4384, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc4223)
      %4386 = stablehlo.subtract %4383, %4385 : tensor<1x4xbf16> loc(#loc4224)
      %4387 = stablehlo.exponential %4386 : tensor<1x4xbf16> loc(#loc4225)
      %4388 = stablehlo.reduce(%4387 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc4226)
      %4389 = stablehlo.broadcast_in_dim %4388, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc4227)
      %4390 = stablehlo.divide %4387, %4389 : tensor<1x4xbf16> loc(#loc4228)
      %4391 = stablehlo.broadcast_in_dim %cst_17, dims = [] : (tensor<bf16>) -> tensor<1x8xbf16> loc(#loc)
      %4392 = "stablehlo.all_gather"(%4391) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %4393 = "stablehlo.scatter"(%4392, %4382, %4390) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.11244"), %arg933: tensor<bf16> loc("scatter.11244")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc4229)
      %4394 = stablehlo.reshape %4393 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc4229)
      %4395 = "stablehlo.all_to_all"(%4394) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc4229)
      %4396 = stablehlo.slice %4395 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc4229)
      %4397 = stablehlo.reshape %4396 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc4229)
      %4398 = stablehlo.reshape %4397 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc4230)
      %4399 = stablehlo.broadcast_in_dim %4398, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc4231)
      %4400 = stablehlo.multiply %4364, %4399 : tensor<8x1x1x1440xbf16> loc(#loc4232)
      %4401 = stablehlo.reduce(%4400 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc4233)
      %4402 = "stablehlo.all_reduce"(%4401) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.11312"), %arg933: tensor<bf16> loc("reduce.11312")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4233)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4233)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4233)
      %4403 = stablehlo.add %4319, %4402 : tensor<1x1x1440xbf16> loc(#loc4234)
      %4404 = stablehlo.convert %4403 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc4235)
      %4405 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %4406 = stablehlo.power %4404, %4405 : tensor<1x1x1440xf32> loc(#loc4236)
      %4407 = stablehlo.reduce(%4406 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc4237)
      %4408 = "stablehlo.all_reduce"(%4407) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.11325"), %arg933: tensor<f32> loc("reduce.11325")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4237)
        stablehlo.return %4605 : tensor<f32> loc(#loc4237)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc4237)
      %4409 = stablehlo.multiply %4408, %cst_1 : tensor<1x1xf32> loc(#loc4238)
      %4410 = stablehlo.reshape %4409 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc4239)
      %4411 = stablehlo.add %4410, %cst_2 : tensor<1x1x1xf32> loc(#loc4240)
      %4412 = stablehlo.rsqrt %4411 : tensor<1x1x1xf32> loc(#loc4241)
      %4413 = stablehlo.reshape %4412 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc4242)
      %4414 = stablehlo.broadcast_in_dim %4413, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc4243)
      %4415 = stablehlo.multiply %4404, %4414 : tensor<1x1x1440xf32> loc(#loc4244)
      %4416 = stablehlo.multiply %4285, %4415 : tensor<1x1x1440xf32> loc(#loc4245)
      %4417 = stablehlo.convert %4416 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc4246)
      %4418 = stablehlo.reshape %4417 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4247)
      %4419 = stablehlo.reshape %arg925 : (tensor<1024x1440xbf16>) -> tensor<1x1024x1440xbf16> loc(#loc4248)
      %4420 = stablehlo.reshape %4419 : (tensor<1x1024x1440xbf16>) -> tensor<1024x1440xbf16> loc(#loc4249)
      %4421 = stablehlo.transpose %4420, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x1440xbf16>) -> tensor<1440x1024xbf16> loc(#loc4250)
      %4422 = stablehlo.dot_general %4418, %4421, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc4251)
      %4423 = "stablehlo.all_reduce"(%4422) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11510"), %arg933: tensor<bf16> loc("dot.11510")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4251)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4251)
      }) : (tensor<1x1024xbf16>) -> tensor<1x1024xbf16> loc(#loc4251)
      %4424 = stablehlo.reshape %4423 : (tensor<1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4252)
      %4425 = stablehlo.reshape %arg924 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4253)
      %4426 = stablehlo.add %4424, %4425 : tensor<1x1x1024xbf16> loc(#loc4254)
      %4427 = stablehlo.reshape %4426 : (tensor<1x1x1024xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc4255)
      %4428 = stablehlo.slice %4427 [0:1, 0:16, 0:1, 0:32] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc4256)
      %4429 = stablehlo.multiply %4428, %59 : tensor<1x16x1x32xbf16> loc(#loc4257)
      %4430 = stablehlo.slice %4427 [0:1, 0:16, 0:1, 32:64] : (tensor<1x16x1x64xbf16>) -> tensor<1x16x1x32xbf16> loc(#loc4258)
      %4431 = stablehlo.multiply %4430, %65 : tensor<1x16x1x32xbf16> loc(#loc4259)
      %4432 = stablehlo.subtract %4429, %4431 : tensor<1x16x1x32xbf16> loc(#loc4260)
      %4433 = stablehlo.multiply %4430, %59 : tensor<1x16x1x32xbf16> loc(#loc4261)
      %4434 = stablehlo.multiply %4428, %65 : tensor<1x16x1x32xbf16> loc(#loc4262)
      %4435 = stablehlo.add %4433, %4434 : tensor<1x16x1x32xbf16> loc(#loc4263)
      %4436 = stablehlo.concatenate %4432, %4435, dim = 3 : (tensor<1x16x1x32xbf16>, tensor<1x16x1x32xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc4264)
      %4437 = stablehlo.reshape %arg900 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc4265)
      %4438 = stablehlo.reshape %4437 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc4266)
      %4439 = stablehlo.transpose %4438, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc4267)
      %4440 = stablehlo.dot_general %4418, %4439, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc4268)
      %4441 = "stablehlo.all_reduce"(%4440) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11353"), %arg933: tensor<bf16> loc("dot.11353")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4268)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4268)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc4268)
      %4442 = stablehlo.reshape %4441 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4269)
      %4443 = stablehlo.reshape %arg899 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4270)
      %4444 = stablehlo.add %4442, %4443 : tensor<1x1x128xbf16> loc(#loc4271)
      %4445 = stablehlo.reshape %4444 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc4272)
      %4446 = stablehlo.slice %4445 [0:1, 0:2, 0:1, 0:32] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc4273)
      %4447 = stablehlo.multiply %4446, %86 : tensor<1x2x1x32xbf16> loc(#loc4274)
      %4448 = stablehlo.slice %4445 [0:1, 0:2, 0:1, 32:64] : (tensor<1x2x1x64xbf16>) -> tensor<1x2x1x32xbf16> loc(#loc4275)
      %4449 = stablehlo.multiply %4448, %89 : tensor<1x2x1x32xbf16> loc(#loc4276)
      %4450 = stablehlo.subtract %4447, %4449 : tensor<1x2x1x32xbf16> loc(#loc4277)
      %4451 = stablehlo.multiply %4448, %86 : tensor<1x2x1x32xbf16> loc(#loc4278)
      %4452 = stablehlo.multiply %4446, %89 : tensor<1x2x1x32xbf16> loc(#loc4279)
      %4453 = stablehlo.add %4451, %4452 : tensor<1x2x1x32xbf16> loc(#loc4280)
      %4454 = stablehlo.concatenate %4450, %4453, dim = 3 : (tensor<1x2x1x32xbf16>, tensor<1x2x1x32xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc4281)
      %4455 = "stablehlo.scatter"(%arg914, %75, %4454) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.11400"), %arg933: tensor<bf16> loc("scatter.11400")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc4282)
      %4456 = stablehlo.broadcast_in_dim %4455, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc4283)
      %4457 = stablehlo.reshape %4456 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc4284)
      %4458 = stablehlo.transpose %4457, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,256]{2,3,1,0}"} : (tensor<1x16x256x64xbf16>) -> tensor<1x16x64x256xbf16> loc(#loc4285)
      %4459 = stablehlo.dot_general %4436, %4458, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x64xbf16>, tensor<1x16x64x256xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc4286)
      %4460 = stablehlo.multiply %4459, %8 : tensor<1x16x1x256xbf16> loc(#loc4287)
      %4461 = stablehlo.add %4460, %325 : tensor<1x16x1x256xbf16> loc(#loc4288)
      %4462 = stablehlo.reshape %arg923 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc458)
      %4463 = "stablehlo.all_to_all"(%4462) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc458)
      %4464 = stablehlo.slice %4463 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc458)
      %4465 = stablehlo.reshape %4464 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc458)
      %4466 = stablehlo.reshape %4465 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc4289)
      %4467 = stablehlo.reshape %4466 : (tensor<1x1x16xbf16>) -> tensor<1x16x1x1xbf16> loc(#loc4290)
      %4468 = stablehlo.concatenate %4461, %4467, dim = 3 : (tensor<1x16x1x256xbf16>, tensor<1x16x1x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4291)
      %4469 = stablehlo.reshape %arg916 : (tensor<128x1440xbf16>) -> tensor<1x128x1440xbf16> loc(#loc4292)
      %4470 = stablehlo.reshape %4469 : (tensor<1x128x1440xbf16>) -> tensor<128x1440xbf16> loc(#loc4293)
      %4471 = stablehlo.transpose %4470, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x1440xbf16>) -> tensor<1440x128xbf16> loc(#loc4294)
      %4472 = stablehlo.dot_general %4418, %4471, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x128xbf16>) -> tensor<1x128xbf16> loc(#loc4295)
      %4473 = "stablehlo.all_reduce"(%4472) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11413"), %arg933: tensor<bf16> loc("dot.11413")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4295)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4295)
      }) : (tensor<1x128xbf16>) -> tensor<1x128xbf16> loc(#loc4295)
      %4474 = stablehlo.reshape %4473 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4296)
      %4475 = stablehlo.reshape %arg915 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4297)
      %4476 = stablehlo.add %4474, %4475 : tensor<1x1x128xbf16> loc(#loc4298)
      %4477 = stablehlo.reshape %4476 : (tensor<1x1x128xbf16>) -> tensor<1x2x1x64xbf16> loc(#loc4299)
      %4478 = "stablehlo.scatter"(%arg917, %75, %4477) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.11437"), %arg933: tensor<bf16> loc("scatter.11437")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x2x256x64xbf16>, tensor<1x1xi64>, tensor<1x2x1x64xbf16>) -> tensor<1x2x256x64xbf16> loc(#loc4300)
      %4479 = stablehlo.reshape %arg931 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4301)
      %4480 = stablehlo.reshape %4479 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc4302)
      %4481 = stablehlo.convert %4480 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc4303)
      %4482 = stablehlo.reshape %4481 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc4304)
      %4483 = stablehlo.reduce(%4468 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc4305)
      %4484 = stablehlo.broadcast_in_dim %4483, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4306)
      %4485 = stablehlo.subtract %4468, %4484 : tensor<1x16x1x257xbf16> loc(#loc4307)
      %4486 = stablehlo.reduce(%4485 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc4308)
      %4487 = stablehlo.broadcast_in_dim %4486, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4309)
      %4488 = stablehlo.subtract %4485, %4487 : tensor<1x16x1x257xbf16> loc(#loc4310)
      %4489 = stablehlo.exponential %4488 : tensor<1x16x1x257xbf16> loc(#loc4311)
      %4490 = stablehlo.reduce(%4489 init: %cst_17) applies stablehlo.add across dimensions = [3] : (tensor<1x16x1x257xbf16>, tensor<bf16>) -> tensor<1x16x1xbf16> loc(#loc4312)
      %4491 = stablehlo.broadcast_in_dim %4490, dims = [0, 1, 2] : (tensor<1x16x1xbf16>) -> tensor<1x16x1x257xbf16> loc(#loc4313)
      %4492 = stablehlo.divide %4489, %4491 : tensor<1x16x1x257xbf16> loc(#loc4314)
      %4493 = stablehlo.slice %4492 [0:1, 0:16, 0:1, 0:256] : (tensor<1x16x1x257xbf16>) -> tensor<1x16x1x256xbf16> loc(#loc4315)
      %4494 = stablehlo.broadcast_in_dim %4478, dims = [0, 1, 3, 4] : (tensor<1x2x256x64xbf16>) -> tensor<1x2x8x256x64xbf16> loc(#loc4316)
      %4495 = stablehlo.reshape %4494 : (tensor<1x2x8x256x64xbf16>) -> tensor<1x16x256x64xbf16> loc(#loc4317)
      %4496 = stablehlo.dot_general %4493, %4495, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x1x256xbf16>, tensor<1x16x256x64xbf16>) -> tensor<1x16x1x64xbf16> loc(#loc4318)
      %4497 = stablehlo.reshape %4496 : (tensor<1x16x1x64xbf16>) -> tensor<1x1024xbf16> loc(#loc4319)
      %4498 = stablehlo.reshape %arg922 : (tensor<1440x1024xbf16>) -> tensor<1x1440x1024xbf16> loc(#loc4320)
      %4499 = stablehlo.reshape %4498 : (tensor<1x1440x1024xbf16>) -> tensor<1440x1024xbf16> loc(#loc4321)
      %4500 = stablehlo.transpose %4499, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<1440x1024xbf16>) -> tensor<1024x1440xbf16> loc(#loc4322)
      %4501 = stablehlo.dot_general %4497, %4500, contracting_dims = [1] x [0] : (tensor<1x1024xbf16>, tensor<1024x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4323)
      %4502 = "stablehlo.all_reduce"(%4501) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11617"), %arg933: tensor<bf16> loc("dot.11617")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4323)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4323)
      }) : (tensor<1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4323)
      %4503 = stablehlo.reshape %4502 : (tensor<1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4324)
      %4504 = stablehlo.reshape %arg921 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4325)
      %4505 = stablehlo.add %4503, %4504 : tensor<1x1x1440xbf16> loc(#loc4326)
      %4506 = stablehlo.add %4403, %4505 : tensor<1x1x1440xbf16> loc(#loc4327)
      %4507 = stablehlo.reshape %arg926 : (tensor<1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4328)
      %4508 = stablehlo.reshape %4507 : (tensor<1x1x1440xbf16>) -> tensor<1440xbf16> loc(#loc4329)
      %4509 = stablehlo.convert %4508 : (tensor<1440xbf16>) -> tensor<1440xf32> loc(#loc4330)
      %4510 = stablehlo.reshape %4509 : (tensor<1440xf32>) -> tensor<1x1x1440xf32> loc(#loc4331)
      %4511 = stablehlo.convert %4506 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc4332)
      %4512 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x1x1440xf32> loc(#loc)
      %4513 = stablehlo.power %4511, %4512 : tensor<1x1x1440xf32> loc(#loc4333)
      %4514 = stablehlo.reduce(%4513 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc4334)
      %4515 = "stablehlo.all_reduce"(%4514) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.11635"), %arg933: tensor<f32> loc("reduce.11635")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4334)
        stablehlo.return %4605 : tensor<f32> loc(#loc4334)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc4334)
      %4516 = stablehlo.multiply %4515, %cst_1 : tensor<1x1xf32> loc(#loc4335)
      %4517 = stablehlo.reshape %4516 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc4336)
      %4518 = stablehlo.add %4517, %cst_2 : tensor<1x1x1xf32> loc(#loc4337)
      %4519 = stablehlo.rsqrt %4518 : tensor<1x1x1xf32> loc(#loc4338)
      %4520 = stablehlo.reshape %4519 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc4339)
      %4521 = stablehlo.broadcast_in_dim %4520, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc4340)
      %4522 = stablehlo.multiply %4511, %4521 : tensor<1x1x1440xf32> loc(#loc4341)
      %4523 = stablehlo.multiply %4510, %4522 : tensor<1x1x1440xf32> loc(#loc4342)
      %4524 = stablehlo.convert %4523 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc4343)
      %4525 = stablehlo.reshape %4524 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4344)
      %4526 = stablehlo.broadcast_in_dim %4525, dims = [1, 2] : (tensor<1x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4345)
      %4527 = stablehlo.dot_general %4526, %arg930, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x1440xbf16>, tensor<8x1440x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4346)
      %4528 = "stablehlo.all_reduce"(%4527) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11749"), %arg933: tensor<bf16> loc("dot.11749")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4346)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4346)
      }) : (tensor<8x1x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4346)
      %4529 = stablehlo.reshape %arg929 : (tensor<8x5760xbf16>) -> tensor<1x8x5760xbf16> loc(#loc4347)
      %4530 = stablehlo.reshape %4529 : (tensor<1x8x5760xbf16>) -> tensor<8x1x5760xbf16> loc(#loc4348)
      %4531 = stablehlo.add %4528, %4530 : tensor<8x1x5760xbf16> loc(#loc4349)
      %4532 = stablehlo.slice %4531 [0:8, 0:1, 1:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc4350)
      %4533 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<8x1x2880xbf16> loc(#loc)
      %4534 = stablehlo.clamp %6, %4532, %4533 : tensor<8x1x2880xbf16> loc(#loc4351)
      %4535 = stablehlo.add %4534, %4 : tensor<8x1x2880xbf16> loc(#loc4352)
      %4536 = stablehlo.slice %4531 [0:8, 0:1, 0:5760:2] : (tensor<8x1x5760xbf16>) -> tensor<8x1x2880xbf16> loc(#loc4353)
      %4537 = stablehlo.clamp %3, %4536, %5 : tensor<8x1x2880xbf16> loc(#loc4354)
      %4538 = stablehlo.multiply %4537, %2 : tensor<8x1x2880xbf16> loc(#loc4355)
      %4539 = stablehlo.logistic %4538 : tensor<8x1x2880xbf16> loc(#loc4356)
      %4540 = stablehlo.multiply %4537, %4539 : tensor<8x1x2880xbf16> loc(#loc4357)
      %4541 = stablehlo.multiply %4535, %4540 : tensor<8x1x2880xbf16> loc(#loc4358)
      %4542 = stablehlo.dot_general %4541, %arg928, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<8x1x2880xbf16>, tensor<8x2880x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4359)
      %4543 = stablehlo.reshape %arg927 : (tensor<8x1440xbf16>) -> tensor<1x8x1440xbf16> loc(#loc4360)
      %4544 = stablehlo.reshape %4543 : (tensor<1x8x1440xbf16>) -> tensor<8x1x1440xbf16> loc(#loc4361)
      %4545 = stablehlo.add %4542, %4544 : tensor<8x1x1440xbf16> loc(#loc4362)
      %4546 = stablehlo.reshape %4545 : (tensor<8x1x1440xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc4363)
      %4547 = stablehlo.convert %4525 : (tensor<1x1440xbf16>) -> tensor<1x1440xf32> loc(#loc4364)
      %4548 = stablehlo.reshape %arg920 : (tensor<32x1440xbf16>) -> tensor<1x32x1440xbf16> loc(#loc4365)
      %4549 = stablehlo.reshape %4548 : (tensor<1x32x1440xbf16>) -> tensor<32x1440xbf16> loc(#loc4366)
      %4550 = stablehlo.transpose %4549, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,32]{0,1}"} : (tensor<32x1440xbf16>) -> tensor<1440x32xbf16> loc(#loc4367)
      %4551 = stablehlo.convert %4550 {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "f32[2880,32]{0,1}"} : (tensor<1440x32xbf16>) -> tensor<1440x32xf32> loc(#loc4368)
      %4552 = stablehlo.dot_general %4547, %4551, contracting_dims = [1] x [0] : (tensor<1x1440xf32>, tensor<1440x32xf32>) -> tensor<1x32xf32> loc(#loc4369)
      %4553 = "stablehlo.all_reduce"(%4552) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("dot.11664"), %arg933: tensor<f32> loc("dot.11664")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4369)
        stablehlo.return %4605 : tensor<f32> loc(#loc4369)
      }) : (tensor<1x32xf32>) -> tensor<1x32xf32> loc(#loc4369)
      %4554 = stablehlo.reshape %arg919 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc4370)
      %4555 = stablehlo.reshape %4554 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc4371)
      %4556 = stablehlo.convert %4555 : (tensor<32xbf16>) -> tensor<32xf32> loc(#loc4372)
      %4557 = stablehlo.reshape %4556 : (tensor<32xf32>) -> tensor<1x32xf32> loc(#loc4373)
      %4558 = stablehlo.add %4553, %4557 : tensor<1x32xf32> loc(#loc4374)
      %4559 = stablehlo.convert %4558 : (tensor<1x32xf32>) -> tensor<1x32xbf16> loc(#loc4375)
      %4560:2 = "stablehlo.sort"(%4559, %232) <{dimension = 1 : i64}> ({
      ^bb0(%arg932: tensor<bf16> loc("sort.11685"), %arg933: tensor<bf16> loc("sort.11685"), %arg934: tensor<i32> loc("sort.11685"), %arg935: tensor<i32> loc("sort.11685")):
        %4605 = stablehlo.compare  GT, %arg932, %arg933,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4377)
        stablehlo.return %4605 : tensor<i1> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x32xi32>) -> (tensor<1x32xbf16>, tensor<1x32xi32>) loc(#loc4376)
      %4561 = stablehlo.slice %4560#1 [0:1, 0:4] : (tensor<1x32xi32>) -> tensor<1x4xi32> loc(#loc4378)
      %4562 = stablehlo.convert %4561 : (tensor<1x4xi32>) -> tensor<1x4xi64> loc(#loc4379)
      %4563 = stablehlo.reshape %4562 : (tensor<1x4xi64>) -> tensor<1x4x1xi64> loc(#loc4380)
      %4564 = stablehlo.concatenate %c_16, %4563, dim = 2 : (tensor<1x4x1xi64>, tensor<1x4x1xi64>) -> tensor<1x4x2xi64> loc(#loc4381)
      %4565 = stablehlo.slice %4560#0 [0:1, 0:4] : (tensor<1x32xbf16>) -> tensor<1x4xbf16> loc(#loc4382)
      %4566 = stablehlo.reduce(%4565 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc4383)
      %4567 = stablehlo.broadcast_in_dim %4566, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc4384)
      %4568 = stablehlo.subtract %4565, %4567 : tensor<1x4xbf16> loc(#loc4385)
      %4569 = stablehlo.exponential %4568 : tensor<1x4xbf16> loc(#loc4386)
      %4570 = stablehlo.reduce(%4569 init: %cst_17) applies stablehlo.add across dimensions = [1] : (tensor<1x4xbf16>, tensor<bf16>) -> tensor<1xbf16> loc(#loc4387)
      %4571 = stablehlo.broadcast_in_dim %4570, dims = [0] : (tensor<1xbf16>) -> tensor<1x4xbf16> loc(#loc4388)
      %4572 = stablehlo.divide %4569, %4571 : tensor<1x4xbf16> loc(#loc4389)
      %4573 = "stablehlo.all_gather"(%1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> : (tensor<1x8xbf16>) -> tensor<1x32xbf16> loc(#loc)
      %4574 = "stablehlo.scatter"(%4573, %4564, %4572) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg932: tensor<bf16> loc("scatter.11719"), %arg933: tensor<bf16> loc("scatter.11719")):
        stablehlo.return %arg933 : tensor<bf16> loc(#loc)
      }) : (tensor<1x32xbf16>, tensor<1x4x2xi64>, tensor<1x4xbf16>) -> tensor<1x32xbf16> loc(#loc4390)
      %4575 = stablehlo.reshape %4574 : (tensor<1x32xbf16>) -> tensor<1x4x8xbf16> loc(#loc4390)
      %4576 = "stablehlo.all_to_all"(%4575) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<1x4x8xbf16>) -> tensor<1x4x8xbf16> loc(#loc4390)
      %4577 = stablehlo.slice %4576 [0:1, 0:1, 0:8] : (tensor<1x4x8xbf16>) -> tensor<1x1x8xbf16> loc(#loc4390)
      %4578 = stablehlo.reshape %4577 : (tensor<1x1x8xbf16>) -> tensor<1x8xbf16> loc(#loc4390)
      %4579 = stablehlo.reshape %4578 : (tensor<1x8xbf16>) -> tensor<8x1x1xbf16> loc(#loc4391)
      %4580 = stablehlo.broadcast_in_dim %4579, dims = [0, 1, 2] : (tensor<8x1x1xbf16>) -> tensor<8x1x1x1440xbf16> loc(#loc4392)
      %4581 = stablehlo.multiply %4546, %4580 : tensor<8x1x1x1440xbf16> loc(#loc4393)
      %4582 = stablehlo.reduce(%4581 init: %cst_17) applies stablehlo.add across dimensions = [0] : (tensor<8x1x1x1440xbf16>, tensor<bf16>) -> tensor<1x1x1440xbf16> loc(#loc4394)
      %4583 = "stablehlo.all_reduce"(%4582) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7]]> : tensor<2x4xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("reduce.11787"), %arg933: tensor<bf16> loc("reduce.11787")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4394)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4394)
      }) : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xbf16> loc(#loc4394)
      %4584 = stablehlo.add %4506, %4583 : tensor<1x1x1440xbf16> loc(#loc4395)
      %4585 = stablehlo.convert %4584 : (tensor<1x1x1440xbf16>) -> tensor<1x1x1440xf32> loc(#loc4396)
      %4586 = stablehlo.power %4585, %10 : tensor<1x1x1440xf32> loc(#loc4397)
      %4587 = stablehlo.reduce(%4586 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x1440xf32>, tensor<f32>) -> tensor<1x1xf32> loc(#loc4398)
      %4588 = "stablehlo.all_reduce"(%4587) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<f32> loc("reduce.11800"), %arg933: tensor<f32> loc("reduce.11800")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<f32> loc(#loc4398)
        stablehlo.return %4605 : tensor<f32> loc(#loc4398)
      }) : (tensor<1x1xf32>) -> tensor<1x1xf32> loc(#loc4398)
      %4589 = stablehlo.multiply %4588, %cst_1 : tensor<1x1xf32> loc(#loc4399)
      %4590 = stablehlo.reshape %4589 : (tensor<1x1xf32>) -> tensor<1x1x1xf32> loc(#loc4400)
      %4591 = stablehlo.add %4590, %cst_2 : tensor<1x1x1xf32> loc(#loc4401)
      %4592 = stablehlo.rsqrt %4591 : tensor<1x1x1xf32> loc(#loc4402)
      %4593 = stablehlo.reshape %4592 : (tensor<1x1x1xf32>) -> tensor<1x1xf32> loc(#loc4403)
      %4594 = stablehlo.broadcast_in_dim %4593, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x1440xf32> loc(#loc4404)
      %4595 = stablehlo.multiply %4585, %4594 : tensor<1x1x1440xf32> loc(#loc4405)
      %4596 = stablehlo.multiply %4482, %4595 : tensor<1x1x1440xf32> loc(#loc4406)
      %4597 = stablehlo.convert %4596 : (tensor<1x1x1440xf32>) -> tensor<1x1x1440xbf16> loc(#loc4407)
      %4598 = stablehlo.reshape %4597 : (tensor<1x1x1440xbf16>) -> tensor<1x1440xbf16> loc(#loc4408)
      %4599 = stablehlo.reshape %arg918 : (tensor<50272x1440xbf16>) -> tensor<1x50272x1440xbf16> loc(#loc4409)
      %4600 = stablehlo.reshape %4599 : (tensor<1x50272x1440xbf16>) -> tensor<50272x1440xbf16> loc(#loc4410)
      %4601 = stablehlo.transpose %4600, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<50272x1440xbf16>) -> tensor<1440x50272xbf16> loc(#loc4411)
      %4602 = stablehlo.dot_general %4598, %4601, contracting_dims = [1] x [0] : (tensor<1x1440xbf16>, tensor<1440x50272xbf16>) -> tensor<1x50272xbf16> loc(#loc4412)
      %4603 = "stablehlo.all_reduce"(%4602) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4], [1, 5], [2, 6], [3, 7]]> : tensor<4x2xi64>}> ({
      ^bb0(%arg932: tensor<bf16> loc("dot.11828"), %arg933: tensor<bf16> loc("dot.11828")):
        %4605 = stablehlo.add %arg932, %arg933 : tensor<bf16> loc(#loc4412)
        stablehlo.return %4605 : tensor<bf16> loc(#loc4412)
      }) : (tensor<1x50272xbf16>) -> tensor<1x50272xbf16> loc(#loc4412)
      %4604 = stablehlo.reshape %4603 : (tensor<1x50272xbf16>) -> tensor<1x1x50272xbf16> loc(#loc4413)
      sdy.return %96, %159, %310, %358, %507, %546, %695, %734, %883, %922, %1071, %1110, %1259, %1298, %1447, %1486, %1635, %1674, %1823, %1862, %2011, %2050, %2199, %2238, %2387, %2426, %2575, %2614, %2763, %2802, %2951, %2990, %3139, %3178, %3327, %3366, %3515, %3554, %3703, %3742, %3891, %3930, %4079, %4118, %4267, %4306, %4455, %4478, %4604 : tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x2x256x64xbf16>, tensor<1x1x50272xbf16> loc(#loc)
    } : (tensor<1xi64>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x1xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<1x256xi64>, tensor<i1>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<bf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>, tensor<1x8x256x64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x8x256x64xbf16>, tensor<201088x2880xbf16>, tensor<32xbf16>, tensor<32x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<64xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<32x2880xbf16>, tensor<32x2880x2880xbf16>, tensor<32x5760xbf16>, tensor<32x2880x5760xbf16>, tensor<2880xbf16>) -> (tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x1x201088xbf16>) loc(#loc)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5, %0#6, %0#7, %0#8, %0#9, %0#10, %0#11, %0#12, %0#13, %0#14, %0#15, %0#16, %0#17, %0#18, %0#19, %0#20, %0#21, %0#22, %0#23, %0#24, %0#25, %0#26, %0#27, %0#28, %0#29, %0#30, %0#31, %0#32, %0#33, %0#34, %0#35, %0#36, %0#37, %0#38, %0#39, %0#40, %0#41, %0#42, %0#43, %0#44, %0#45, %0#46, %0#47, %0#48 : tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x8x256x64xbf16>, tensor<1x1x201088xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc467 = loc("reshape.80")
#loc468 = loc("reshape.82")
#loc469 = loc("convert.83")
#loc470 = loc("broadcast.84")
#loc471 = loc("reshape.45")
#loc472 = loc("reshape.47")
#loc473 = loc("reshape.40")
#loc474 = loc("reshape.43")
#loc475 = loc("convert.48")
#loc476 = loc("gather.49")
#loc477 = loc("reshape.50")
#loc478 = loc("convert.51")
#loc479 = loc("power.53")
#loc481 = loc("multiply.69")
#loc482 = loc("reshape.70")
#loc483 = loc("add.74")
#loc484 = loc("rsqrt.75")
#loc485 = loc("reshape.76")
#loc486 = loc("broadcast.77")
#loc487 = loc("multiply.78")
#loc488 = loc("multiply.85")
#loc489 = loc("convert.86")
#loc490 = loc("reshape.356")
#loc491 = loc("reshape.352")
#loc492 = loc("reshape.354")
#loc493 = loc("transpose.355")
#loc495 = loc("reshape.358")
#loc496 = loc("reshape.348")
#loc497 = loc("add.362")
#loc498 = loc("transpose.364")
#loc499 = loc("slice.365")
#loc500 = loc("reshape.12")
#loc501 = loc("reshape.16")
#loc502 = loc("reshape.4")
#loc503 = loc("reshape.6")
#loc504 = loc("convert.9")
#loc505 = loc("dot.19")
#loc506 = loc("transpose.20")
#loc507 = loc("cosine.101")
#loc508 = loc("multiply.103")
#loc509 = loc("convert.104")
#loc510 = loc("broadcast.381")
#loc511 = loc("multiply.382")
#loc512 = loc("slice.369")
#loc513 = loc("sine.21")
#loc514 = loc("multiply.23")
#loc515 = loc("convert.24")
#loc516 = loc("broadcast.378")
#loc517 = loc("multiply.379")
#loc518 = loc("subtract.385")
#loc519 = loc("multiply.372")
#loc520 = loc("multiply.368")
#loc521 = loc("add.375")
#loc522 = loc("concatenate.386")
#loc523 = loc("compare.130")
#loc524 = loc("add.127")
#loc525 = loc("select.131")
#loc526 = loc("reshape.132")
#loc527 = loc("reshape.32")
#loc528 = loc("reshape.34")
#loc529 = loc("transpose.35")
#loc531 = loc("reshape.89")
#loc532 = loc("reshape.28")
#loc533 = loc("add.93")
#loc534 = loc("transpose.95")
#loc535 = loc("slice.96")
#loc536 = loc("broadcast.118")
#loc537 = loc("multiply.119")
#loc538 = loc("slice.106")
#loc539 = loc("broadcast.115")
#loc540 = loc("multiply.116")
#loc541 = loc("subtract.122")
#loc542 = loc("multiply.109")
#loc543 = loc("multiply.99")
#loc544 = loc("add.112")
#loc545 = loc("concatenate.123")
#loc547 = loc("broadcast.342")
#loc548 = loc("reshape.343")
#loc549 = loc("transpose.344")
#loc550 = loc("dot.387")
#loc551 = loc("multiply.390")
#loc552 = loc("reshape.322")
#loc553 = loc("broadcast.328")
#loc554 = loc("subtract.304")
#loc555 = loc("broadcast.308")
#loc556 = loc("compare.309")
#loc557 = loc("and.318")
#loc558 = loc("broadcast.298")
#loc559 = loc("compare.299")
#loc560 = loc("and.321")
#loc561 = loc("and.329")
#loc562 = loc("broadcast.332")
#loc563 = loc("reshape.289")
#loc564 = loc("reshape.291")
#loc565 = loc("convert.292")
#loc566 = loc("gather.293")
#loc567 = loc("reshape.294")
#loc568 = loc("and.333")
#loc569 = loc("reshape.334")
#loc570 = loc("reshape.245")
#loc571 = loc("broadcast.251")
#loc572 = loc("select.337")
#loc573 = loc("reshape.393")
#loc574 = loc("broadcast.394")
#loc575 = loc("add.395")
#loc576 = loc("reshape.232")
#loc577 = loc("reshape.235")
#loc578 = loc("concatenate.396")
#loc579 = loc("reshape.667")
#loc580 = loc("reshape.669")
#loc581 = loc("convert.670")
#loc582 = loc("broadcast.671")
#loc583 = loc("reduce.403")
#loc584 = loc("broadcast.439")
#loc585 = loc("subtract.440")
#loc586 = loc("reduce.446")
#loc587 = loc("broadcast.447")
#loc588 = loc("subtract.448")
#loc589 = loc("exponential.449")
#loc590 = loc("reduce.455")
#loc591 = loc("broadcast.456")
#loc592 = loc("divide.457")
#loc593 = loc("slice.458")
#loc594 = loc("reshape.148")
#loc595 = loc("reshape.150")
#loc596 = loc("transpose.151")
#loc598 = loc("reshape.154")
#loc599 = loc("reshape.144")
#loc600 = loc("add.158")
#loc601 = loc("transpose.160")
#loc603 = loc("broadcast.228")
#loc604 = loc("reshape.229")
#loc605 = loc("dot.459")
#loc606 = loc("reshape.463")
#loc607 = loc("reshape.221")
#loc608 = loc("reshape.223")
#loc609 = loc("transpose.224")
#loc611 = loc("reshape.465")
#loc612 = loc("reshape.217")
#loc613 = loc("add.469")
#loc614 = loc("add.472")
#loc615 = loc("reshape.502")
#loc616 = loc("reshape.504")
#loc617 = loc("convert.505")
#loc618 = loc("broadcast.506")
#loc619 = loc("convert.473")
#loc620 = loc("power.475")
#loc622 = loc("multiply.491")
#loc623 = loc("reshape.492")
#loc624 = loc("add.496")
#loc625 = loc("rsqrt.497")
#loc626 = loc("reshape.498")
#loc627 = loc("broadcast.499")
#loc628 = loc("multiply.500")
#loc629 = loc("multiply.507")
#loc630 = loc("convert.508")
#loc631 = loc("reshape.589")
#loc632 = loc("broadcast.593")
#loc634 = loc("reshape.583")
#loc635 = loc("reshape.586")
#loc636 = loc("add.599")
#loc637 = loc("slice.612")
#loc638 = loc("clamp.615")
#loc639 = loc("add.618")
#loc640 = loc("slice.600")
#loc641 = loc("clamp.603")
#loc642 = loc("multiply.605")
#loc643 = loc("logistic.606")
#loc644 = loc("multiply.607")
#loc645 = loc("multiply.619")
#loc646 = loc("dot.620")
#loc647 = loc("reshape.572")
#loc648 = loc("reshape.575")
#loc649 = loc("add.623")
#loc650 = loc("reshape.624")
#loc651 = loc("convert.510")
#loc652 = loc("reshape.206")
#loc653 = loc("reshape.208")
#loc654 = loc("transpose.209")
#loc655 = loc("convert.210")
#loc657 = loc("reshape.198")
#loc658 = loc("reshape.200")
#loc659 = loc("convert.201")
#loc660 = loc("broadcast.516")
#loc661 = loc("add.517")
#loc662 = loc("convert.518")
#loc663 = loc("iota.519")
#loc665 = loc("compare.531")
#loc666 = loc("slice.536")
#loc667 = loc("convert.537")
#loc668 = loc("reshape.561")
#loc669 = loc("concatenate.562")
#loc670 = loc("slice.534")
#loc671 = loc("reduce.543")
#loc672 = loc("broadcast.544")
#loc673 = loc("subtract.545")
#loc674 = loc("exponential.546")
#loc675 = loc("reduce.552")
#loc676 = loc("broadcast.553")
#loc677 = loc("divide.554")
#loc679 = loc("reshape.568")
#loc680 = loc("broadcast.626")
#loc681 = loc("multiply.627")
#loc683 = loc("add.637")
#loc684 = loc("convert.638")
#loc685 = loc("power.640")
#loc687 = loc("multiply.656")
#loc688 = loc("reshape.657")
#loc689 = loc("add.661")
#loc690 = loc("rsqrt.662")
#loc691 = loc("reshape.663")
#loc692 = loc("broadcast.664")
#loc693 = loc("multiply.665")
#loc694 = loc("multiply.672")
#loc695 = loc("convert.673")
#loc696 = loc("reshape.912")
#loc697 = loc("reshape.908")
#loc698 = loc("reshape.910")
#loc699 = loc("transpose.911")
#loc701 = loc("reshape.914")
#loc702 = loc("reshape.904")
#loc703 = loc("add.918")
#loc704 = loc("transpose.920")
#loc705 = loc("slice.921")
#loc706 = loc("multiply.938")
#loc707 = loc("slice.925")
#loc708 = loc("multiply.935")
#loc709 = loc("subtract.941")
#loc710 = loc("multiply.928")
#loc711 = loc("multiply.924")
#loc712 = loc("add.931")
#loc713 = loc("concatenate.942")
#loc714 = loc("reshape.187")
#loc715 = loc("reshape.189")
#loc716 = loc("transpose.190")
#loc718 = loc("reshape.676")
#loc719 = loc("reshape.183")
#loc720 = loc("add.680")
#loc721 = loc("transpose.682")
#loc722 = loc("slice.683")
#loc723 = loc("multiply.701")
#loc724 = loc("slice.688")
#loc725 = loc("multiply.698")
#loc726 = loc("subtract.704")
#loc727 = loc("multiply.691")
#loc728 = loc("multiply.686")
#loc729 = loc("add.694")
#loc730 = loc("concatenate.705")
#loc732 = loc("broadcast.898")
#loc733 = loc("reshape.899")
#loc734 = loc("transpose.900")
#loc735 = loc("dot.943")
#loc736 = loc("multiply.946")
#loc737 = loc("and.885")
#loc738 = loc("broadcast.888")
#loc739 = loc("and.889")
#loc740 = loc("reshape.890")
#loc741 = loc("reshape.827")
#loc742 = loc("broadcast.833")
#loc743 = loc("select.893")
#loc744 = loc("reshape.949")
#loc745 = loc("broadcast.950")
#loc746 = loc("add.951")
#loc747 = loc("reshape.814")
#loc748 = loc("reshape.817")
#loc749 = loc("concatenate.952")
#loc750 = loc("reshape.1223")
#loc751 = loc("reshape.1225")
#loc752 = loc("convert.1226")
#loc753 = loc("broadcast.1227")
#loc754 = loc("reduce.959")
#loc755 = loc("broadcast.995")
#loc756 = loc("subtract.996")
#loc757 = loc("reduce.1002")
#loc758 = loc("broadcast.1003")
#loc759 = loc("subtract.1004")
#loc760 = loc("exponential.1005")
#loc761 = loc("reduce.1011")
#loc762 = loc("broadcast.1012")
#loc763 = loc("divide.1013")
#loc764 = loc("slice.1014")
#loc765 = loc("reshape.730")
#loc766 = loc("reshape.732")
#loc767 = loc("transpose.733")
#loc769 = loc("reshape.736")
#loc770 = loc("reshape.726")
#loc771 = loc("add.740")
#loc772 = loc("transpose.742")
#loc774 = loc("broadcast.810")
#loc775 = loc("reshape.811")
#loc776 = loc("dot.1015")
#loc777 = loc("reshape.1019")
#loc778 = loc("reshape.803")
#loc779 = loc("reshape.805")
#loc780 = loc("transpose.806")
#loc782 = loc("reshape.1021")
#loc783 = loc("reshape.799")
#loc784 = loc("add.1025")
#loc785 = loc("add.1028")
#loc786 = loc("reshape.1058")
#loc787 = loc("reshape.1060")
#loc788 = loc("convert.1061")
#loc789 = loc("broadcast.1062")
#loc790 = loc("convert.1029")
#loc791 = loc("power.1031")
#loc793 = loc("multiply.1047")
#loc794 = loc("reshape.1048")
#loc795 = loc("add.1052")
#loc796 = loc("rsqrt.1053")
#loc797 = loc("reshape.1054")
#loc798 = loc("broadcast.1055")
#loc799 = loc("multiply.1056")
#loc800 = loc("multiply.1063")
#loc801 = loc("convert.1064")
#loc802 = loc("reshape.1145")
#loc803 = loc("broadcast.1149")
#loc805 = loc("reshape.1139")
#loc806 = loc("reshape.1142")
#loc807 = loc("add.1155")
#loc808 = loc("slice.1168")
#loc809 = loc("clamp.1171")
#loc810 = loc("add.1174")
#loc811 = loc("slice.1156")
#loc812 = loc("clamp.1159")
#loc813 = loc("multiply.1161")
#loc814 = loc("logistic.1162")
#loc815 = loc("multiply.1163")
#loc816 = loc("multiply.1175")
#loc817 = loc("dot.1176")
#loc818 = loc("reshape.1128")
#loc819 = loc("reshape.1131")
#loc820 = loc("add.1179")
#loc821 = loc("reshape.1180")
#loc822 = loc("convert.1066")
#loc823 = loc("reshape.788")
#loc824 = loc("reshape.790")
#loc825 = loc("transpose.791")
#loc826 = loc("convert.792")
#loc828 = loc("reshape.780")
#loc829 = loc("reshape.782")
#loc830 = loc("convert.783")
#loc831 = loc("broadcast.1072")
#loc832 = loc("add.1073")
#loc833 = loc("convert.1074")
#loc835 = loc("compare.1087")
#loc836 = loc("slice.1092")
#loc837 = loc("convert.1093")
#loc838 = loc("reshape.1117")
#loc839 = loc("concatenate.1118")
#loc840 = loc("slice.1090")
#loc841 = loc("reduce.1099")
#loc842 = loc("broadcast.1100")
#loc843 = loc("subtract.1101")
#loc844 = loc("exponential.1102")
#loc845 = loc("reduce.1108")
#loc846 = loc("broadcast.1109")
#loc847 = loc("divide.1110")
#loc849 = loc("reshape.1124")
#loc850 = loc("broadcast.1182")
#loc851 = loc("multiply.1183")
#loc853 = loc("add.1193")
#loc854 = loc("convert.1194")
#loc855 = loc("power.1196")
#loc857 = loc("multiply.1212")
#loc858 = loc("reshape.1213")
#loc859 = loc("add.1217")
#loc860 = loc("rsqrt.1218")
#loc861 = loc("reshape.1219")
#loc862 = loc("broadcast.1220")
#loc863 = loc("multiply.1221")
#loc864 = loc("multiply.1228")
#loc865 = loc("convert.1229")
#loc866 = loc("reshape.1394")
#loc867 = loc("reshape.1390")
#loc868 = loc("reshape.1392")
#loc869 = loc("transpose.1393")
#loc871 = loc("reshape.1396")
#loc872 = loc("reshape.1386")
#loc873 = loc("add.1400")
#loc874 = loc("transpose.1402")
#loc875 = loc("slice.1403")
#loc876 = loc("multiply.1420")
#loc877 = loc("slice.1407")
#loc878 = loc("multiply.1417")
#loc879 = loc("subtract.1423")
#loc880 = loc("multiply.1410")
#loc881 = loc("multiply.1406")
#loc882 = loc("add.1413")
#loc883 = loc("concatenate.1424")
#loc884 = loc("reshape.769")
#loc885 = loc("reshape.771")
#loc886 = loc("transpose.772")
#loc888 = loc("reshape.1232")
#loc889 = loc("reshape.765")
#loc890 = loc("add.1236")
#loc891 = loc("transpose.1238")
#loc892 = loc("slice.1239")
#loc893 = loc("multiply.1257")
#loc894 = loc("slice.1244")
#loc895 = loc("multiply.1254")
#loc896 = loc("subtract.1260")
#loc897 = loc("multiply.1247")
#loc898 = loc("multiply.1242")
#loc899 = loc("add.1250")
#loc900 = loc("concatenate.1261")
#loc902 = loc("broadcast.1380")
#loc903 = loc("reshape.1381")
#loc904 = loc("transpose.1382")
#loc905 = loc("dot.1425")
#loc906 = loc("multiply.1428")
#loc907 = loc("add.1433")
#loc908 = loc("reshape.1370")
#loc909 = loc("reshape.1373")
#loc910 = loc("concatenate.1434")
#loc911 = loc("reshape.1705")
#loc912 = loc("reshape.1707")
#loc913 = loc("convert.1708")
#loc914 = loc("broadcast.1709")
#loc915 = loc("reduce.1441")
#loc916 = loc("broadcast.1477")
#loc917 = loc("subtract.1478")
#loc918 = loc("reduce.1484")
#loc919 = loc("broadcast.1485")
#loc920 = loc("subtract.1486")
#loc921 = loc("exponential.1487")
#loc922 = loc("reduce.1493")
#loc923 = loc("broadcast.1494")
#loc924 = loc("divide.1495")
#loc925 = loc("slice.1496")
#loc926 = loc("reshape.1286")
#loc927 = loc("reshape.1288")
#loc928 = loc("transpose.1289")
#loc930 = loc("reshape.1292")
#loc931 = loc("reshape.1282")
#loc932 = loc("add.1296")
#loc933 = loc("transpose.1298")
#loc935 = loc("broadcast.1366")
#loc936 = loc("reshape.1367")
#loc937 = loc("dot.1497")
#loc938 = loc("reshape.1501")
#loc939 = loc("reshape.1359")
#loc940 = loc("reshape.1361")
#loc941 = loc("transpose.1362")
#loc943 = loc("reshape.1503")
#loc944 = loc("reshape.1355")
#loc945 = loc("add.1507")
#loc946 = loc("add.1510")
#loc947 = loc("reshape.1540")
#loc948 = loc("reshape.1542")
#loc949 = loc("convert.1543")
#loc950 = loc("broadcast.1544")
#loc951 = loc("convert.1511")
#loc952 = loc("power.1513")
#loc954 = loc("multiply.1529")
#loc955 = loc("reshape.1530")
#loc956 = loc("add.1534")
#loc957 = loc("rsqrt.1535")
#loc958 = loc("reshape.1536")
#loc959 = loc("broadcast.1537")
#loc960 = loc("multiply.1538")
#loc961 = loc("multiply.1545")
#loc962 = loc("convert.1546")
#loc963 = loc("reshape.1627")
#loc964 = loc("broadcast.1631")
#loc966 = loc("reshape.1621")
#loc967 = loc("reshape.1624")
#loc968 = loc("add.1637")
#loc969 = loc("slice.1650")
#loc970 = loc("clamp.1653")
#loc971 = loc("add.1656")
#loc972 = loc("slice.1638")
#loc973 = loc("clamp.1641")
#loc974 = loc("multiply.1643")
#loc975 = loc("logistic.1644")
#loc976 = loc("multiply.1645")
#loc977 = loc("multiply.1657")
#loc978 = loc("dot.1658")
#loc979 = loc("reshape.1610")
#loc980 = loc("reshape.1613")
#loc981 = loc("add.1661")
#loc982 = loc("reshape.1662")
#loc983 = loc("convert.1548")
#loc984 = loc("reshape.1344")
#loc985 = loc("reshape.1346")
#loc986 = loc("transpose.1347")
#loc987 = loc("convert.1348")
#loc989 = loc("reshape.1336")
#loc990 = loc("reshape.1338")
#loc991 = loc("convert.1339")
#loc992 = loc("broadcast.1554")
#loc993 = loc("add.1555")
#loc994 = loc("convert.1556")
#loc996 = loc("compare.1569")
#loc997 = loc("slice.1574")
#loc998 = loc("convert.1575")
#loc999 = loc("reshape.1599")
#loc1000 = loc("concatenate.1600")
#loc1001 = loc("slice.1572")
#loc1002 = loc("reduce.1581")
#loc1003 = loc("broadcast.1582")
#loc1004 = loc("subtract.1583")
#loc1005 = loc("exponential.1584")
#loc1006 = loc("reduce.1590")
#loc1007 = loc("broadcast.1591")
#loc1008 = loc("divide.1592")
#loc1010 = loc("reshape.1606")
#loc1011 = loc("broadcast.1664")
#loc1012 = loc("multiply.1665")
#loc1014 = loc("add.1675")
#loc1015 = loc("convert.1676")
#loc1016 = loc("power.1678")
#loc1018 = loc("multiply.1694")
#loc1019 = loc("reshape.1695")
#loc1020 = loc("add.1699")
#loc1021 = loc("rsqrt.1700")
#loc1022 = loc("reshape.1701")
#loc1023 = loc("broadcast.1702")
#loc1024 = loc("multiply.1703")
#loc1025 = loc("multiply.1710")
#loc1026 = loc("convert.1711")
#loc1027 = loc("reshape.1876")
#loc1028 = loc("reshape.1872")
#loc1029 = loc("reshape.1874")
#loc1030 = loc("transpose.1875")
#loc1032 = loc("reshape.1878")
#loc1033 = loc("reshape.1868")
#loc1034 = loc("add.1882")
#loc1035 = loc("transpose.1884")
#loc1036 = loc("slice.1885")
#loc1037 = loc("multiply.1902")
#loc1038 = loc("slice.1889")
#loc1039 = loc("multiply.1899")
#loc1040 = loc("subtract.1905")
#loc1041 = loc("multiply.1892")
#loc1042 = loc("multiply.1888")
#loc1043 = loc("add.1895")
#loc1044 = loc("concatenate.1906")
#loc1045 = loc("reshape.1325")
#loc1046 = loc("reshape.1327")
#loc1047 = loc("transpose.1328")
#loc1049 = loc("reshape.1714")
#loc1050 = loc("reshape.1321")
#loc1051 = loc("add.1718")
#loc1052 = loc("transpose.1720")
#loc1053 = loc("slice.1721")
#loc1054 = loc("multiply.1739")
#loc1055 = loc("slice.1726")
#loc1056 = loc("multiply.1736")
#loc1057 = loc("subtract.1742")
#loc1058 = loc("multiply.1729")
#loc1059 = loc("multiply.1724")
#loc1060 = loc("add.1732")
#loc1061 = loc("concatenate.1743")
#loc1063 = loc("broadcast.1862")
#loc1064 = loc("reshape.1863")
#loc1065 = loc("transpose.1864")
#loc1066 = loc("dot.1907")
#loc1067 = loc("multiply.1910")
#loc1068 = loc("add.1915")
#loc1069 = loc("reshape.1852")
#loc1070 = loc("reshape.1855")
#loc1071 = loc("concatenate.1916")
#loc1072 = loc("reshape.2187")
#loc1073 = loc("reshape.2189")
#loc1074 = loc("convert.2190")
#loc1075 = loc("broadcast.2191")
#loc1076 = loc("reduce.1923")
#loc1077 = loc("broadcast.1959")
#loc1078 = loc("subtract.1960")
#loc1079 = loc("reduce.1966")
#loc1080 = loc("broadcast.1967")
#loc1081 = loc("subtract.1968")
#loc1082 = loc("exponential.1969")
#loc1083 = loc("reduce.1975")
#loc1084 = loc("broadcast.1976")
#loc1085 = loc("divide.1977")
#loc1086 = loc("slice.1978")
#loc1087 = loc("reshape.1768")
#loc1088 = loc("reshape.1770")
#loc1089 = loc("transpose.1771")
#loc1091 = loc("reshape.1774")
#loc1092 = loc("reshape.1764")
#loc1093 = loc("add.1778")
#loc1094 = loc("transpose.1780")
#loc1096 = loc("broadcast.1848")
#loc1097 = loc("reshape.1849")
#loc1098 = loc("dot.1979")
#loc1099 = loc("reshape.1983")
#loc1100 = loc("reshape.1841")
#loc1101 = loc("reshape.1843")
#loc1102 = loc("transpose.1844")
#loc1104 = loc("reshape.1985")
#loc1105 = loc("reshape.1837")
#loc1106 = loc("add.1989")
#loc1107 = loc("add.1992")
#loc1108 = loc("reshape.2022")
#loc1109 = loc("reshape.2024")
#loc1110 = loc("convert.2025")
#loc1111 = loc("broadcast.2026")
#loc1112 = loc("convert.1993")
#loc1113 = loc("power.1995")
#loc1115 = loc("multiply.2011")
#loc1116 = loc("reshape.2012")
#loc1117 = loc("add.2016")
#loc1118 = loc("rsqrt.2017")
#loc1119 = loc("reshape.2018")
#loc1120 = loc("broadcast.2019")
#loc1121 = loc("multiply.2020")
#loc1122 = loc("multiply.2027")
#loc1123 = loc("convert.2028")
#loc1124 = loc("reshape.2109")
#loc1125 = loc("broadcast.2113")
#loc1127 = loc("reshape.2103")
#loc1128 = loc("reshape.2106")
#loc1129 = loc("add.2119")
#loc1130 = loc("slice.2132")
#loc1131 = loc("clamp.2135")
#loc1132 = loc("add.2138")
#loc1133 = loc("slice.2120")
#loc1134 = loc("clamp.2123")
#loc1135 = loc("multiply.2125")
#loc1136 = loc("logistic.2126")
#loc1137 = loc("multiply.2127")
#loc1138 = loc("multiply.2139")
#loc1139 = loc("dot.2140")
#loc1140 = loc("reshape.2092")
#loc1141 = loc("reshape.2095")
#loc1142 = loc("add.2143")
#loc1143 = loc("reshape.2144")
#loc1144 = loc("convert.2030")
#loc1145 = loc("reshape.1826")
#loc1146 = loc("reshape.1828")
#loc1147 = loc("transpose.1829")
#loc1148 = loc("convert.1830")
#loc1150 = loc("reshape.1818")
#loc1151 = loc("reshape.1820")
#loc1152 = loc("convert.1821")
#loc1153 = loc("broadcast.2036")
#loc1154 = loc("add.2037")
#loc1155 = loc("convert.2038")
#loc1157 = loc("compare.2051")
#loc1158 = loc("slice.2056")
#loc1159 = loc("convert.2057")
#loc1160 = loc("reshape.2081")
#loc1161 = loc("concatenate.2082")
#loc1162 = loc("slice.2054")
#loc1163 = loc("reduce.2063")
#loc1164 = loc("broadcast.2064")
#loc1165 = loc("subtract.2065")
#loc1166 = loc("exponential.2066")
#loc1167 = loc("reduce.2072")
#loc1168 = loc("broadcast.2073")
#loc1169 = loc("divide.2074")
#loc1171 = loc("reshape.2088")
#loc1172 = loc("broadcast.2146")
#loc1173 = loc("multiply.2147")
#loc1175 = loc("add.2157")
#loc1176 = loc("convert.2158")
#loc1177 = loc("power.2160")
#loc1179 = loc("multiply.2176")
#loc1180 = loc("reshape.2177")
#loc1181 = loc("add.2181")
#loc1182 = loc("rsqrt.2182")
#loc1183 = loc("reshape.2183")
#loc1184 = loc("broadcast.2184")
#loc1185 = loc("multiply.2185")
#loc1186 = loc("multiply.2192")
#loc1187 = loc("convert.2193")
#loc1188 = loc("reshape.2358")
#loc1189 = loc("reshape.2354")
#loc1190 = loc("reshape.2356")
#loc1191 = loc("transpose.2357")
#loc1193 = loc("reshape.2360")
#loc1194 = loc("reshape.2350")
#loc1195 = loc("add.2364")
#loc1196 = loc("transpose.2366")
#loc1197 = loc("slice.2367")
#loc1198 = loc("multiply.2384")
#loc1199 = loc("slice.2371")
#loc1200 = loc("multiply.2381")
#loc1201 = loc("subtract.2387")
#loc1202 = loc("multiply.2374")
#loc1203 = loc("multiply.2370")
#loc1204 = loc("add.2377")
#loc1205 = loc("concatenate.2388")
#loc1206 = loc("reshape.1807")
#loc1207 = loc("reshape.1809")
#loc1208 = loc("transpose.1810")
#loc1210 = loc("reshape.2196")
#loc1211 = loc("reshape.1803")
#loc1212 = loc("add.2200")
#loc1213 = loc("transpose.2202")
#loc1214 = loc("slice.2203")
#loc1215 = loc("multiply.2221")
#loc1216 = loc("slice.2208")
#loc1217 = loc("multiply.2218")
#loc1218 = loc("subtract.2224")
#loc1219 = loc("multiply.2211")
#loc1220 = loc("multiply.2206")
#loc1221 = loc("add.2214")
#loc1222 = loc("concatenate.2225")
#loc1224 = loc("broadcast.2344")
#loc1225 = loc("reshape.2345")
#loc1226 = loc("transpose.2346")
#loc1227 = loc("dot.2389")
#loc1228 = loc("multiply.2392")
#loc1229 = loc("add.2397")
#loc1230 = loc("reshape.2334")
#loc1231 = loc("reshape.2337")
#loc1232 = loc("concatenate.2398")
#loc1233 = loc("reshape.2669")
#loc1234 = loc("reshape.2671")
#loc1235 = loc("convert.2672")
#loc1236 = loc("broadcast.2673")
#loc1237 = loc("reduce.2405")
#loc1238 = loc("broadcast.2441")
#loc1239 = loc("subtract.2442")
#loc1240 = loc("reduce.2448")
#loc1241 = loc("broadcast.2449")
#loc1242 = loc("subtract.2450")
#loc1243 = loc("exponential.2451")
#loc1244 = loc("reduce.2457")
#loc1245 = loc("broadcast.2458")
#loc1246 = loc("divide.2459")
#loc1247 = loc("slice.2460")
#loc1248 = loc("reshape.2250")
#loc1249 = loc("reshape.2252")
#loc1250 = loc("transpose.2253")
#loc1252 = loc("reshape.2256")
#loc1253 = loc("reshape.2246")
#loc1254 = loc("add.2260")
#loc1255 = loc("transpose.2262")
#loc1257 = loc("broadcast.2330")
#loc1258 = loc("reshape.2331")
#loc1259 = loc("dot.2461")
#loc1260 = loc("reshape.2465")
#loc1261 = loc("reshape.2323")
#loc1262 = loc("reshape.2325")
#loc1263 = loc("transpose.2326")
#loc1265 = loc("reshape.2467")
#loc1266 = loc("reshape.2319")
#loc1267 = loc("add.2471")
#loc1268 = loc("add.2474")
#loc1269 = loc("reshape.2504")
#loc1270 = loc("reshape.2506")
#loc1271 = loc("convert.2507")
#loc1272 = loc("broadcast.2508")
#loc1273 = loc("convert.2475")
#loc1274 = loc("power.2477")
#loc1276 = loc("multiply.2493")
#loc1277 = loc("reshape.2494")
#loc1278 = loc("add.2498")
#loc1279 = loc("rsqrt.2499")
#loc1280 = loc("reshape.2500")
#loc1281 = loc("broadcast.2501")
#loc1282 = loc("multiply.2502")
#loc1283 = loc("multiply.2509")
#loc1284 = loc("convert.2510")
#loc1285 = loc("reshape.2591")
#loc1286 = loc("broadcast.2595")
#loc1288 = loc("reshape.2585")
#loc1289 = loc("reshape.2588")
#loc1290 = loc("add.2601")
#loc1291 = loc("slice.2614")
#loc1292 = loc("clamp.2617")
#loc1293 = loc("add.2620")
#loc1294 = loc("slice.2602")
#loc1295 = loc("clamp.2605")
#loc1296 = loc("multiply.2607")
#loc1297 = loc("logistic.2608")
#loc1298 = loc("multiply.2609")
#loc1299 = loc("multiply.2621")
#loc1300 = loc("dot.2622")
#loc1301 = loc("reshape.2574")
#loc1302 = loc("reshape.2577")
#loc1303 = loc("add.2625")
#loc1304 = loc("reshape.2626")
#loc1305 = loc("convert.2512")
#loc1306 = loc("reshape.2308")
#loc1307 = loc("reshape.2310")
#loc1308 = loc("transpose.2311")
#loc1309 = loc("convert.2312")
#loc1311 = loc("reshape.2300")
#loc1312 = loc("reshape.2302")
#loc1313 = loc("convert.2303")
#loc1314 = loc("broadcast.2518")
#loc1315 = loc("add.2519")
#loc1316 = loc("convert.2520")
#loc1318 = loc("compare.2533")
#loc1319 = loc("slice.2538")
#loc1320 = loc("convert.2539")
#loc1321 = loc("reshape.2563")
#loc1322 = loc("concatenate.2564")
#loc1323 = loc("slice.2536")
#loc1324 = loc("reduce.2545")
#loc1325 = loc("broadcast.2546")
#loc1326 = loc("subtract.2547")
#loc1327 = loc("exponential.2548")
#loc1328 = loc("reduce.2554")
#loc1329 = loc("broadcast.2555")
#loc1330 = loc("divide.2556")
#loc1332 = loc("reshape.2570")
#loc1333 = loc("broadcast.2628")
#loc1334 = loc("multiply.2629")
#loc1336 = loc("add.2639")
#loc1337 = loc("convert.2640")
#loc1338 = loc("power.2642")
#loc1340 = loc("multiply.2658")
#loc1341 = loc("reshape.2659")
#loc1342 = loc("add.2663")
#loc1343 = loc("rsqrt.2664")
#loc1344 = loc("reshape.2665")
#loc1345 = loc("broadcast.2666")
#loc1346 = loc("multiply.2667")
#loc1347 = loc("multiply.2674")
#loc1348 = loc("convert.2675")
#loc1349 = loc("reshape.2840")
#loc1350 = loc("reshape.2836")
#loc1351 = loc("reshape.2838")
#loc1352 = loc("transpose.2839")
#loc1354 = loc("reshape.2842")
#loc1355 = loc("reshape.2832")
#loc1356 = loc("add.2846")
#loc1357 = loc("transpose.2848")
#loc1358 = loc("slice.2849")
#loc1359 = loc("multiply.2866")
#loc1360 = loc("slice.2853")
#loc1361 = loc("multiply.2863")
#loc1362 = loc("subtract.2869")
#loc1363 = loc("multiply.2856")
#loc1364 = loc("multiply.2852")
#loc1365 = loc("add.2859")
#loc1366 = loc("concatenate.2870")
#loc1367 = loc("reshape.2289")
#loc1368 = loc("reshape.2291")
#loc1369 = loc("transpose.2292")
#loc1371 = loc("reshape.2678")
#loc1372 = loc("reshape.2285")
#loc1373 = loc("add.2682")
#loc1374 = loc("transpose.2684")
#loc1375 = loc("slice.2685")
#loc1376 = loc("multiply.2703")
#loc1377 = loc("slice.2690")
#loc1378 = loc("multiply.2700")
#loc1379 = loc("subtract.2706")
#loc1380 = loc("multiply.2693")
#loc1381 = loc("multiply.2688")
#loc1382 = loc("add.2696")
#loc1383 = loc("concatenate.2707")
#loc1385 = loc("broadcast.2826")
#loc1386 = loc("reshape.2827")
#loc1387 = loc("transpose.2828")
#loc1388 = loc("dot.2871")
#loc1389 = loc("multiply.2874")
#loc1390 = loc("add.2879")
#loc1391 = loc("reshape.2816")
#loc1392 = loc("reshape.2819")
#loc1393 = loc("concatenate.2880")
#loc1394 = loc("reshape.3151")
#loc1395 = loc("reshape.3153")
#loc1396 = loc("convert.3154")
#loc1397 = loc("broadcast.3155")
#loc1398 = loc("reduce.2887")
#loc1399 = loc("broadcast.2923")
#loc1400 = loc("subtract.2924")
#loc1401 = loc("reduce.2930")
#loc1402 = loc("broadcast.2931")
#loc1403 = loc("subtract.2932")
#loc1404 = loc("exponential.2933")
#loc1405 = loc("reduce.2939")
#loc1406 = loc("broadcast.2940")
#loc1407 = loc("divide.2941")
#loc1408 = loc("slice.2942")
#loc1409 = loc("reshape.2732")
#loc1410 = loc("reshape.2734")
#loc1411 = loc("transpose.2735")
#loc1413 = loc("reshape.2738")
#loc1414 = loc("reshape.2728")
#loc1415 = loc("add.2742")
#loc1416 = loc("transpose.2744")
#loc1418 = loc("broadcast.2812")
#loc1419 = loc("reshape.2813")
#loc1420 = loc("dot.2943")
#loc1421 = loc("reshape.2947")
#loc1422 = loc("reshape.2805")
#loc1423 = loc("reshape.2807")
#loc1424 = loc("transpose.2808")
#loc1426 = loc("reshape.2949")
#loc1427 = loc("reshape.2801")
#loc1428 = loc("add.2953")
#loc1429 = loc("add.2956")
#loc1430 = loc("reshape.2986")
#loc1431 = loc("reshape.2988")
#loc1432 = loc("convert.2989")
#loc1433 = loc("broadcast.2990")
#loc1434 = loc("convert.2957")
#loc1435 = loc("power.2959")
#loc1437 = loc("multiply.2975")
#loc1438 = loc("reshape.2976")
#loc1439 = loc("add.2980")
#loc1440 = loc("rsqrt.2981")
#loc1441 = loc("reshape.2982")
#loc1442 = loc("broadcast.2983")
#loc1443 = loc("multiply.2984")
#loc1444 = loc("multiply.2991")
#loc1445 = loc("convert.2992")
#loc1446 = loc("reshape.3073")
#loc1447 = loc("broadcast.3077")
#loc1449 = loc("reshape.3067")
#loc1450 = loc("reshape.3070")
#loc1451 = loc("add.3083")
#loc1452 = loc("slice.3096")
#loc1453 = loc("clamp.3099")
#loc1454 = loc("add.3102")
#loc1455 = loc("slice.3084")
#loc1456 = loc("clamp.3087")
#loc1457 = loc("multiply.3089")
#loc1458 = loc("logistic.3090")
#loc1459 = loc("multiply.3091")
#loc1460 = loc("multiply.3103")
#loc1461 = loc("dot.3104")
#loc1462 = loc("reshape.3056")
#loc1463 = loc("reshape.3059")
#loc1464 = loc("add.3107")
#loc1465 = loc("reshape.3108")
#loc1466 = loc("convert.2994")
#loc1467 = loc("reshape.2790")
#loc1468 = loc("reshape.2792")
#loc1469 = loc("transpose.2793")
#loc1470 = loc("convert.2794")
#loc1472 = loc("reshape.2782")
#loc1473 = loc("reshape.2784")
#loc1474 = loc("convert.2785")
#loc1475 = loc("broadcast.3000")
#loc1476 = loc("add.3001")
#loc1477 = loc("convert.3002")
#loc1479 = loc("compare.3015")
#loc1480 = loc("slice.3020")
#loc1481 = loc("convert.3021")
#loc1482 = loc("reshape.3045")
#loc1483 = loc("concatenate.3046")
#loc1484 = loc("slice.3018")
#loc1485 = loc("reduce.3027")
#loc1486 = loc("broadcast.3028")
#loc1487 = loc("subtract.3029")
#loc1488 = loc("exponential.3030")
#loc1489 = loc("reduce.3036")
#loc1490 = loc("broadcast.3037")
#loc1491 = loc("divide.3038")
#loc1493 = loc("reshape.3052")
#loc1494 = loc("broadcast.3110")
#loc1495 = loc("multiply.3111")
#loc1497 = loc("add.3121")
#loc1498 = loc("convert.3122")
#loc1499 = loc("power.3124")
#loc1501 = loc("multiply.3140")
#loc1502 = loc("reshape.3141")
#loc1503 = loc("add.3145")
#loc1504 = loc("rsqrt.3146")
#loc1505 = loc("reshape.3147")
#loc1506 = loc("broadcast.3148")
#loc1507 = loc("multiply.3149")
#loc1508 = loc("multiply.3156")
#loc1509 = loc("convert.3157")
#loc1510 = loc("reshape.3322")
#loc1511 = loc("reshape.3318")
#loc1512 = loc("reshape.3320")
#loc1513 = loc("transpose.3321")
#loc1515 = loc("reshape.3324")
#loc1516 = loc("reshape.3314")
#loc1517 = loc("add.3328")
#loc1518 = loc("transpose.3330")
#loc1519 = loc("slice.3331")
#loc1520 = loc("multiply.3348")
#loc1521 = loc("slice.3335")
#loc1522 = loc("multiply.3345")
#loc1523 = loc("subtract.3351")
#loc1524 = loc("multiply.3338")
#loc1525 = loc("multiply.3334")
#loc1526 = loc("add.3341")
#loc1527 = loc("concatenate.3352")
#loc1528 = loc("reshape.2771")
#loc1529 = loc("reshape.2773")
#loc1530 = loc("transpose.2774")
#loc1532 = loc("reshape.3160")
#loc1533 = loc("reshape.2767")
#loc1534 = loc("add.3164")
#loc1535 = loc("transpose.3166")
#loc1536 = loc("slice.3167")
#loc1537 = loc("multiply.3185")
#loc1538 = loc("slice.3172")
#loc1539 = loc("multiply.3182")
#loc1540 = loc("subtract.3188")
#loc1541 = loc("multiply.3175")
#loc1542 = loc("multiply.3170")
#loc1543 = loc("add.3178")
#loc1544 = loc("concatenate.3189")
#loc1546 = loc("broadcast.3308")
#loc1547 = loc("reshape.3309")
#loc1548 = loc("transpose.3310")
#loc1549 = loc("dot.3353")
#loc1550 = loc("multiply.3356")
#loc1551 = loc("add.3361")
#loc1552 = loc("reshape.3298")
#loc1553 = loc("reshape.3301")
#loc1554 = loc("concatenate.3362")
#loc1555 = loc("reshape.3633")
#loc1556 = loc("reshape.3635")
#loc1557 = loc("convert.3636")
#loc1558 = loc("broadcast.3637")
#loc1559 = loc("reduce.3369")
#loc1560 = loc("broadcast.3405")
#loc1561 = loc("subtract.3406")
#loc1562 = loc("reduce.3412")
#loc1563 = loc("broadcast.3413")
#loc1564 = loc("subtract.3414")
#loc1565 = loc("exponential.3415")
#loc1566 = loc("reduce.3421")
#loc1567 = loc("broadcast.3422")
#loc1568 = loc("divide.3423")
#loc1569 = loc("slice.3424")
#loc1570 = loc("reshape.3214")
#loc1571 = loc("reshape.3216")
#loc1572 = loc("transpose.3217")
#loc1574 = loc("reshape.3220")
#loc1575 = loc("reshape.3210")
#loc1576 = loc("add.3224")
#loc1577 = loc("transpose.3226")
#loc1579 = loc("broadcast.3294")
#loc1580 = loc("reshape.3295")
#loc1581 = loc("dot.3425")
#loc1582 = loc("reshape.3429")
#loc1583 = loc("reshape.3287")
#loc1584 = loc("reshape.3289")
#loc1585 = loc("transpose.3290")
#loc1587 = loc("reshape.3431")
#loc1588 = loc("reshape.3283")
#loc1589 = loc("add.3435")
#loc1590 = loc("add.3438")
#loc1591 = loc("reshape.3468")
#loc1592 = loc("reshape.3470")
#loc1593 = loc("convert.3471")
#loc1594 = loc("broadcast.3472")
#loc1595 = loc("convert.3439")
#loc1596 = loc("power.3441")
#loc1598 = loc("multiply.3457")
#loc1599 = loc("reshape.3458")
#loc1600 = loc("add.3462")
#loc1601 = loc("rsqrt.3463")
#loc1602 = loc("reshape.3464")
#loc1603 = loc("broadcast.3465")
#loc1604 = loc("multiply.3466")
#loc1605 = loc("multiply.3473")
#loc1606 = loc("convert.3474")
#loc1607 = loc("reshape.3555")
#loc1608 = loc("broadcast.3559")
#loc1610 = loc("reshape.3549")
#loc1611 = loc("reshape.3552")
#loc1612 = loc("add.3565")
#loc1613 = loc("slice.3578")
#loc1614 = loc("clamp.3581")
#loc1615 = loc("add.3584")
#loc1616 = loc("slice.3566")
#loc1617 = loc("clamp.3569")
#loc1618 = loc("multiply.3571")
#loc1619 = loc("logistic.3572")
#loc1620 = loc("multiply.3573")
#loc1621 = loc("multiply.3585")
#loc1622 = loc("dot.3586")
#loc1623 = loc("reshape.3538")
#loc1624 = loc("reshape.3541")
#loc1625 = loc("add.3589")
#loc1626 = loc("reshape.3590")
#loc1627 = loc("convert.3476")
#loc1628 = loc("reshape.3272")
#loc1629 = loc("reshape.3274")
#loc1630 = loc("transpose.3275")
#loc1631 = loc("convert.3276")
#loc1633 = loc("reshape.3264")
#loc1634 = loc("reshape.3266")
#loc1635 = loc("convert.3267")
#loc1636 = loc("broadcast.3482")
#loc1637 = loc("add.3483")
#loc1638 = loc("convert.3484")
#loc1640 = loc("compare.3497")
#loc1641 = loc("slice.3502")
#loc1642 = loc("convert.3503")
#loc1643 = loc("reshape.3527")
#loc1644 = loc("concatenate.3528")
#loc1645 = loc("slice.3500")
#loc1646 = loc("reduce.3509")
#loc1647 = loc("broadcast.3510")
#loc1648 = loc("subtract.3511")
#loc1649 = loc("exponential.3512")
#loc1650 = loc("reduce.3518")
#loc1651 = loc("broadcast.3519")
#loc1652 = loc("divide.3520")
#loc1654 = loc("reshape.3534")
#loc1655 = loc("broadcast.3592")
#loc1656 = loc("multiply.3593")
#loc1658 = loc("add.3603")
#loc1659 = loc("convert.3604")
#loc1660 = loc("power.3606")
#loc1662 = loc("multiply.3622")
#loc1663 = loc("reshape.3623")
#loc1664 = loc("add.3627")
#loc1665 = loc("rsqrt.3628")
#loc1666 = loc("reshape.3629")
#loc1667 = loc("broadcast.3630")
#loc1668 = loc("multiply.3631")
#loc1669 = loc("multiply.3638")
#loc1670 = loc("convert.3639")
#loc1671 = loc("reshape.3804")
#loc1672 = loc("reshape.3800")
#loc1673 = loc("reshape.3802")
#loc1674 = loc("transpose.3803")
#loc1676 = loc("reshape.3806")
#loc1677 = loc("reshape.3796")
#loc1678 = loc("add.3810")
#loc1679 = loc("transpose.3812")
#loc1680 = loc("slice.3813")
#loc1681 = loc("multiply.3830")
#loc1682 = loc("slice.3817")
#loc1683 = loc("multiply.3827")
#loc1684 = loc("subtract.3833")
#loc1685 = loc("multiply.3820")
#loc1686 = loc("multiply.3816")
#loc1687 = loc("add.3823")
#loc1688 = loc("concatenate.3834")
#loc1689 = loc("reshape.3253")
#loc1690 = loc("reshape.3255")
#loc1691 = loc("transpose.3256")
#loc1693 = loc("reshape.3642")
#loc1694 = loc("reshape.3249")
#loc1695 = loc("add.3646")
#loc1696 = loc("transpose.3648")
#loc1697 = loc("slice.3649")
#loc1698 = loc("multiply.3667")
#loc1699 = loc("slice.3654")
#loc1700 = loc("multiply.3664")
#loc1701 = loc("subtract.3670")
#loc1702 = loc("multiply.3657")
#loc1703 = loc("multiply.3652")
#loc1704 = loc("add.3660")
#loc1705 = loc("concatenate.3671")
#loc1707 = loc("broadcast.3790")
#loc1708 = loc("reshape.3791")
#loc1709 = loc("transpose.3792")
#loc1710 = loc("dot.3835")
#loc1711 = loc("multiply.3838")
#loc1712 = loc("add.3843")
#loc1713 = loc("reshape.3780")
#loc1714 = loc("reshape.3783")
#loc1715 = loc("concatenate.3844")
#loc1716 = loc("reshape.4115")
#loc1717 = loc("reshape.4117")
#loc1718 = loc("convert.4118")
#loc1719 = loc("broadcast.4119")
#loc1720 = loc("reduce.3851")
#loc1721 = loc("broadcast.3887")
#loc1722 = loc("subtract.3888")
#loc1723 = loc("reduce.3894")
#loc1724 = loc("broadcast.3895")
#loc1725 = loc("subtract.3896")
#loc1726 = loc("exponential.3897")
#loc1727 = loc("reduce.3903")
#loc1728 = loc("broadcast.3904")
#loc1729 = loc("divide.3905")
#loc1730 = loc("slice.3906")
#loc1731 = loc("reshape.3696")
#loc1732 = loc("reshape.3698")
#loc1733 = loc("transpose.3699")
#loc1735 = loc("reshape.3702")
#loc1736 = loc("reshape.3692")
#loc1737 = loc("add.3706")
#loc1738 = loc("transpose.3708")
#loc1740 = loc("broadcast.3776")
#loc1741 = loc("reshape.3777")
#loc1742 = loc("dot.3907")
#loc1743 = loc("reshape.3911")
#loc1744 = loc("reshape.3769")
#loc1745 = loc("reshape.3771")
#loc1746 = loc("transpose.3772")
#loc1748 = loc("reshape.3913")
#loc1749 = loc("reshape.3765")
#loc1750 = loc("add.3917")
#loc1751 = loc("add.3920")
#loc1752 = loc("reshape.3950")
#loc1753 = loc("reshape.3952")
#loc1754 = loc("convert.3953")
#loc1755 = loc("broadcast.3954")
#loc1756 = loc("convert.3921")
#loc1757 = loc("power.3923")
#loc1759 = loc("multiply.3939")
#loc1760 = loc("reshape.3940")
#loc1761 = loc("add.3944")
#loc1762 = loc("rsqrt.3945")
#loc1763 = loc("reshape.3946")
#loc1764 = loc("broadcast.3947")
#loc1765 = loc("multiply.3948")
#loc1766 = loc("multiply.3955")
#loc1767 = loc("convert.3956")
#loc1768 = loc("reshape.4037")
#loc1769 = loc("broadcast.4041")
#loc1771 = loc("reshape.4031")
#loc1772 = loc("reshape.4034")
#loc1773 = loc("add.4047")
#loc1774 = loc("slice.4060")
#loc1775 = loc("clamp.4063")
#loc1776 = loc("add.4066")
#loc1777 = loc("slice.4048")
#loc1778 = loc("clamp.4051")
#loc1779 = loc("multiply.4053")
#loc1780 = loc("logistic.4054")
#loc1781 = loc("multiply.4055")
#loc1782 = loc("multiply.4067")
#loc1783 = loc("dot.4068")
#loc1784 = loc("reshape.4020")
#loc1785 = loc("reshape.4023")
#loc1786 = loc("add.4071")
#loc1787 = loc("reshape.4072")
#loc1788 = loc("convert.3958")
#loc1789 = loc("reshape.3754")
#loc1790 = loc("reshape.3756")
#loc1791 = loc("transpose.3757")
#loc1792 = loc("convert.3758")
#loc1794 = loc("reshape.3746")
#loc1795 = loc("reshape.3748")
#loc1796 = loc("convert.3749")
#loc1797 = loc("broadcast.3964")
#loc1798 = loc("add.3965")
#loc1799 = loc("convert.3966")
#loc1801 = loc("compare.3979")
#loc1802 = loc("slice.3984")
#loc1803 = loc("convert.3985")
#loc1804 = loc("reshape.4009")
#loc1805 = loc("concatenate.4010")
#loc1806 = loc("slice.3982")
#loc1807 = loc("reduce.3991")
#loc1808 = loc("broadcast.3992")
#loc1809 = loc("subtract.3993")
#loc1810 = loc("exponential.3994")
#loc1811 = loc("reduce.4000")
#loc1812 = loc("broadcast.4001")
#loc1813 = loc("divide.4002")
#loc1815 = loc("reshape.4016")
#loc1816 = loc("broadcast.4074")
#loc1817 = loc("multiply.4075")
#loc1819 = loc("add.4085")
#loc1820 = loc("convert.4086")
#loc1821 = loc("power.4088")
#loc1823 = loc("multiply.4104")
#loc1824 = loc("reshape.4105")
#loc1825 = loc("add.4109")
#loc1826 = loc("rsqrt.4110")
#loc1827 = loc("reshape.4111")
#loc1828 = loc("broadcast.4112")
#loc1829 = loc("multiply.4113")
#loc1830 = loc("multiply.4120")
#loc1831 = loc("convert.4121")
#loc1832 = loc("reshape.4286")
#loc1833 = loc("reshape.4282")
#loc1834 = loc("reshape.4284")
#loc1835 = loc("transpose.4285")
#loc1837 = loc("reshape.4288")
#loc1838 = loc("reshape.4278")
#loc1839 = loc("add.4292")
#loc1840 = loc("transpose.4294")
#loc1841 = loc("slice.4295")
#loc1842 = loc("multiply.4312")
#loc1843 = loc("slice.4299")
#loc1844 = loc("multiply.4309")
#loc1845 = loc("subtract.4315")
#loc1846 = loc("multiply.4302")
#loc1847 = loc("multiply.4298")
#loc1848 = loc("add.4305")
#loc1849 = loc("concatenate.4316")
#loc1850 = loc("reshape.3735")
#loc1851 = loc("reshape.3737")
#loc1852 = loc("transpose.3738")
#loc1854 = loc("reshape.4124")
#loc1855 = loc("reshape.3731")
#loc1856 = loc("add.4128")
#loc1857 = loc("transpose.4130")
#loc1858 = loc("slice.4131")
#loc1859 = loc("multiply.4149")
#loc1860 = loc("slice.4136")
#loc1861 = loc("multiply.4146")
#loc1862 = loc("subtract.4152")
#loc1863 = loc("multiply.4139")
#loc1864 = loc("multiply.4134")
#loc1865 = loc("add.4142")
#loc1866 = loc("concatenate.4153")
#loc1868 = loc("broadcast.4272")
#loc1869 = loc("reshape.4273")
#loc1870 = loc("transpose.4274")
#loc1871 = loc("dot.4317")
#loc1872 = loc("multiply.4320")
#loc1873 = loc("add.4325")
#loc1874 = loc("reshape.4262")
#loc1875 = loc("reshape.4265")
#loc1876 = loc("concatenate.4326")
#loc1877 = loc("reshape.4597")
#loc1878 = loc("reshape.4599")
#loc1879 = loc("convert.4600")
#loc1880 = loc("broadcast.4601")
#loc1881 = loc("reduce.4333")
#loc1882 = loc("broadcast.4369")
#loc1883 = loc("subtract.4370")
#loc1884 = loc("reduce.4376")
#loc1885 = loc("broadcast.4377")
#loc1886 = loc("subtract.4378")
#loc1887 = loc("exponential.4379")
#loc1888 = loc("reduce.4385")
#loc1889 = loc("broadcast.4386")
#loc1890 = loc("divide.4387")
#loc1891 = loc("slice.4388")
#loc1892 = loc("reshape.4178")
#loc1893 = loc("reshape.4180")
#loc1894 = loc("transpose.4181")
#loc1896 = loc("reshape.4184")
#loc1897 = loc("reshape.4174")
#loc1898 = loc("add.4188")
#loc1899 = loc("transpose.4190")
#loc1901 = loc("broadcast.4258")
#loc1902 = loc("reshape.4259")
#loc1903 = loc("dot.4389")
#loc1904 = loc("reshape.4393")
#loc1905 = loc("reshape.4251")
#loc1906 = loc("reshape.4253")
#loc1907 = loc("transpose.4254")
#loc1909 = loc("reshape.4395")
#loc1910 = loc("reshape.4247")
#loc1911 = loc("add.4399")
#loc1912 = loc("add.4402")
#loc1913 = loc("reshape.4432")
#loc1914 = loc("reshape.4434")
#loc1915 = loc("convert.4435")
#loc1916 = loc("broadcast.4436")
#loc1917 = loc("convert.4403")
#loc1918 = loc("power.4405")
#loc1920 = loc("multiply.4421")
#loc1921 = loc("reshape.4422")
#loc1922 = loc("add.4426")
#loc1923 = loc("rsqrt.4427")
#loc1924 = loc("reshape.4428")
#loc1925 = loc("broadcast.4429")
#loc1926 = loc("multiply.4430")
#loc1927 = loc("multiply.4437")
#loc1928 = loc("convert.4438")
#loc1929 = loc("reshape.4519")
#loc1930 = loc("broadcast.4523")
#loc1932 = loc("reshape.4513")
#loc1933 = loc("reshape.4516")
#loc1934 = loc("add.4529")
#loc1935 = loc("slice.4542")
#loc1936 = loc("clamp.4545")
#loc1937 = loc("add.4548")
#loc1938 = loc("slice.4530")
#loc1939 = loc("clamp.4533")
#loc1940 = loc("multiply.4535")
#loc1941 = loc("logistic.4536")
#loc1942 = loc("multiply.4537")
#loc1943 = loc("multiply.4549")
#loc1944 = loc("dot.4550")
#loc1945 = loc("reshape.4502")
#loc1946 = loc("reshape.4505")
#loc1947 = loc("add.4553")
#loc1948 = loc("reshape.4554")
#loc1949 = loc("convert.4440")
#loc1950 = loc("reshape.4236")
#loc1951 = loc("reshape.4238")
#loc1952 = loc("transpose.4239")
#loc1953 = loc("convert.4240")
#loc1955 = loc("reshape.4228")
#loc1956 = loc("reshape.4230")
#loc1957 = loc("convert.4231")
#loc1958 = loc("broadcast.4446")
#loc1959 = loc("add.4447")
#loc1960 = loc("convert.4448")
#loc1962 = loc("compare.4461")
#loc1963 = loc("slice.4466")
#loc1964 = loc("convert.4467")
#loc1965 = loc("reshape.4491")
#loc1966 = loc("concatenate.4492")
#loc1967 = loc("slice.4464")
#loc1968 = loc("reduce.4473")
#loc1969 = loc("broadcast.4474")
#loc1970 = loc("subtract.4475")
#loc1971 = loc("exponential.4476")
#loc1972 = loc("reduce.4482")
#loc1973 = loc("broadcast.4483")
#loc1974 = loc("divide.4484")
#loc1976 = loc("reshape.4498")
#loc1977 = loc("broadcast.4556")
#loc1978 = loc("multiply.4557")
#loc1980 = loc("add.4567")
#loc1981 = loc("convert.4568")
#loc1982 = loc("power.4570")
#loc1984 = loc("multiply.4586")
#loc1985 = loc("reshape.4587")
#loc1986 = loc("add.4591")
#loc1987 = loc("rsqrt.4592")
#loc1988 = loc("reshape.4593")
#loc1989 = loc("broadcast.4594")
#loc1990 = loc("multiply.4595")
#loc1991 = loc("multiply.4602")
#loc1992 = loc("convert.4603")
#loc1993 = loc("reshape.4768")
#loc1994 = loc("reshape.4764")
#loc1995 = loc("reshape.4766")
#loc1996 = loc("transpose.4767")
#loc1998 = loc("reshape.4770")
#loc1999 = loc("reshape.4760")
#loc2000 = loc("add.4774")
#loc2001 = loc("transpose.4776")
#loc2002 = loc("slice.4777")
#loc2003 = loc("multiply.4794")
#loc2004 = loc("slice.4781")
#loc2005 = loc("multiply.4791")
#loc2006 = loc("subtract.4797")
#loc2007 = loc("multiply.4784")
#loc2008 = loc("multiply.4780")
#loc2009 = loc("add.4787")
#loc2010 = loc("concatenate.4798")
#loc2011 = loc("reshape.4217")
#loc2012 = loc("reshape.4219")
#loc2013 = loc("transpose.4220")
#loc2015 = loc("reshape.4606")
#loc2016 = loc("reshape.4213")
#loc2017 = loc("add.4610")
#loc2018 = loc("transpose.4612")
#loc2019 = loc("slice.4613")
#loc2020 = loc("multiply.4631")
#loc2021 = loc("slice.4618")
#loc2022 = loc("multiply.4628")
#loc2023 = loc("subtract.4634")
#loc2024 = loc("multiply.4621")
#loc2025 = loc("multiply.4616")
#loc2026 = loc("add.4624")
#loc2027 = loc("concatenate.4635")
#loc2029 = loc("broadcast.4754")
#loc2030 = loc("reshape.4755")
#loc2031 = loc("transpose.4756")
#loc2032 = loc("dot.4799")
#loc2033 = loc("multiply.4802")
#loc2034 = loc("add.4807")
#loc2035 = loc("reshape.4744")
#loc2036 = loc("reshape.4747")
#loc2037 = loc("concatenate.4808")
#loc2038 = loc("reshape.5079")
#loc2039 = loc("reshape.5081")
#loc2040 = loc("convert.5082")
#loc2041 = loc("broadcast.5083")
#loc2042 = loc("reduce.4815")
#loc2043 = loc("broadcast.4851")
#loc2044 = loc("subtract.4852")
#loc2045 = loc("reduce.4858")
#loc2046 = loc("broadcast.4859")
#loc2047 = loc("subtract.4860")
#loc2048 = loc("exponential.4861")
#loc2049 = loc("reduce.4867")
#loc2050 = loc("broadcast.4868")
#loc2051 = loc("divide.4869")
#loc2052 = loc("slice.4870")
#loc2053 = loc("reshape.4660")
#loc2054 = loc("reshape.4662")
#loc2055 = loc("transpose.4663")
#loc2057 = loc("reshape.4666")
#loc2058 = loc("reshape.4656")
#loc2059 = loc("add.4670")
#loc2060 = loc("transpose.4672")
#loc2062 = loc("broadcast.4740")
#loc2063 = loc("reshape.4741")
#loc2064 = loc("dot.4871")
#loc2065 = loc("reshape.4875")
#loc2066 = loc("reshape.4733")
#loc2067 = loc("reshape.4735")
#loc2068 = loc("transpose.4736")
#loc2070 = loc("reshape.4877")
#loc2071 = loc("reshape.4729")
#loc2072 = loc("add.4881")
#loc2073 = loc("add.4884")
#loc2074 = loc("reshape.4914")
#loc2075 = loc("reshape.4916")
#loc2076 = loc("convert.4917")
#loc2077 = loc("broadcast.4918")
#loc2078 = loc("convert.4885")
#loc2079 = loc("power.4887")
#loc2081 = loc("multiply.4903")
#loc2082 = loc("reshape.4904")
#loc2083 = loc("add.4908")
#loc2084 = loc("rsqrt.4909")
#loc2085 = loc("reshape.4910")
#loc2086 = loc("broadcast.4911")
#loc2087 = loc("multiply.4912")
#loc2088 = loc("multiply.4919")
#loc2089 = loc("convert.4920")
#loc2090 = loc("reshape.5001")
#loc2091 = loc("broadcast.5005")
#loc2093 = loc("reshape.4995")
#loc2094 = loc("reshape.4998")
#loc2095 = loc("add.5011")
#loc2096 = loc("slice.5024")
#loc2097 = loc("clamp.5027")
#loc2098 = loc("add.5030")
#loc2099 = loc("slice.5012")
#loc2100 = loc("clamp.5015")
#loc2101 = loc("multiply.5017")
#loc2102 = loc("logistic.5018")
#loc2103 = loc("multiply.5019")
#loc2104 = loc("multiply.5031")
#loc2105 = loc("dot.5032")
#loc2106 = loc("reshape.4984")
#loc2107 = loc("reshape.4987")
#loc2108 = loc("add.5035")
#loc2109 = loc("reshape.5036")
#loc2110 = loc("convert.4922")
#loc2111 = loc("reshape.4718")
#loc2112 = loc("reshape.4720")
#loc2113 = loc("transpose.4721")
#loc2114 = loc("convert.4722")
#loc2116 = loc("reshape.4710")
#loc2117 = loc("reshape.4712")
#loc2118 = loc("convert.4713")
#loc2119 = loc("broadcast.4928")
#loc2120 = loc("add.4929")
#loc2121 = loc("convert.4930")
#loc2123 = loc("compare.4943")
#loc2124 = loc("slice.4948")
#loc2125 = loc("convert.4949")
#loc2126 = loc("reshape.4973")
#loc2127 = loc("concatenate.4974")
#loc2128 = loc("slice.4946")
#loc2129 = loc("reduce.4955")
#loc2130 = loc("broadcast.4956")
#loc2131 = loc("subtract.4957")
#loc2132 = loc("exponential.4958")
#loc2133 = loc("reduce.4964")
#loc2134 = loc("broadcast.4965")
#loc2135 = loc("divide.4966")
#loc2137 = loc("reshape.4980")
#loc2138 = loc("broadcast.5038")
#loc2139 = loc("multiply.5039")
#loc2141 = loc("add.5049")
#loc2142 = loc("convert.5050")
#loc2143 = loc("power.5052")
#loc2145 = loc("multiply.5068")
#loc2146 = loc("reshape.5069")
#loc2147 = loc("add.5073")
#loc2148 = loc("rsqrt.5074")
#loc2149 = loc("reshape.5075")
#loc2150 = loc("broadcast.5076")
#loc2151 = loc("multiply.5077")
#loc2152 = loc("multiply.5084")
#loc2153 = loc("convert.5085")
#loc2154 = loc("reshape.5250")
#loc2155 = loc("reshape.5246")
#loc2156 = loc("reshape.5248")
#loc2157 = loc("transpose.5249")
#loc2159 = loc("reshape.5252")
#loc2160 = loc("reshape.5242")
#loc2161 = loc("add.5256")
#loc2162 = loc("transpose.5258")
#loc2163 = loc("slice.5259")
#loc2164 = loc("multiply.5276")
#loc2165 = loc("slice.5263")
#loc2166 = loc("multiply.5273")
#loc2167 = loc("subtract.5279")
#loc2168 = loc("multiply.5266")
#loc2169 = loc("multiply.5262")
#loc2170 = loc("add.5269")
#loc2171 = loc("concatenate.5280")
#loc2172 = loc("reshape.4699")
#loc2173 = loc("reshape.4701")
#loc2174 = loc("transpose.4702")
#loc2176 = loc("reshape.5088")
#loc2177 = loc("reshape.4695")
#loc2178 = loc("add.5092")
#loc2179 = loc("transpose.5094")
#loc2180 = loc("slice.5095")
#loc2181 = loc("multiply.5113")
#loc2182 = loc("slice.5100")
#loc2183 = loc("multiply.5110")
#loc2184 = loc("subtract.5116")
#loc2185 = loc("multiply.5103")
#loc2186 = loc("multiply.5098")
#loc2187 = loc("add.5106")
#loc2188 = loc("concatenate.5117")
#loc2190 = loc("broadcast.5236")
#loc2191 = loc("reshape.5237")
#loc2192 = loc("transpose.5238")
#loc2193 = loc("dot.5281")
#loc2194 = loc("multiply.5284")
#loc2195 = loc("add.5289")
#loc2196 = loc("reshape.5226")
#loc2197 = loc("reshape.5229")
#loc2198 = loc("concatenate.5290")
#loc2199 = loc("reshape.5561")
#loc2200 = loc("reshape.5563")
#loc2201 = loc("convert.5564")
#loc2202 = loc("broadcast.5565")
#loc2203 = loc("reduce.5297")
#loc2204 = loc("broadcast.5333")
#loc2205 = loc("subtract.5334")
#loc2206 = loc("reduce.5340")
#loc2207 = loc("broadcast.5341")
#loc2208 = loc("subtract.5342")
#loc2209 = loc("exponential.5343")
#loc2210 = loc("reduce.5349")
#loc2211 = loc("broadcast.5350")
#loc2212 = loc("divide.5351")
#loc2213 = loc("slice.5352")
#loc2214 = loc("reshape.5142")
#loc2215 = loc("reshape.5144")
#loc2216 = loc("transpose.5145")
#loc2218 = loc("reshape.5148")
#loc2219 = loc("reshape.5138")
#loc2220 = loc("add.5152")
#loc2221 = loc("transpose.5154")
#loc2223 = loc("broadcast.5222")
#loc2224 = loc("reshape.5223")
#loc2225 = loc("dot.5353")
#loc2226 = loc("reshape.5357")
#loc2227 = loc("reshape.5215")
#loc2228 = loc("reshape.5217")
#loc2229 = loc("transpose.5218")
#loc2231 = loc("reshape.5359")
#loc2232 = loc("reshape.5211")
#loc2233 = loc("add.5363")
#loc2234 = loc("add.5366")
#loc2235 = loc("reshape.5396")
#loc2236 = loc("reshape.5398")
#loc2237 = loc("convert.5399")
#loc2238 = loc("broadcast.5400")
#loc2239 = loc("convert.5367")
#loc2240 = loc("power.5369")
#loc2242 = loc("multiply.5385")
#loc2243 = loc("reshape.5386")
#loc2244 = loc("add.5390")
#loc2245 = loc("rsqrt.5391")
#loc2246 = loc("reshape.5392")
#loc2247 = loc("broadcast.5393")
#loc2248 = loc("multiply.5394")
#loc2249 = loc("multiply.5401")
#loc2250 = loc("convert.5402")
#loc2251 = loc("reshape.5483")
#loc2252 = loc("broadcast.5487")
#loc2254 = loc("reshape.5477")
#loc2255 = loc("reshape.5480")
#loc2256 = loc("add.5493")
#loc2257 = loc("slice.5506")
#loc2258 = loc("clamp.5509")
#loc2259 = loc("add.5512")
#loc2260 = loc("slice.5494")
#loc2261 = loc("clamp.5497")
#loc2262 = loc("multiply.5499")
#loc2263 = loc("logistic.5500")
#loc2264 = loc("multiply.5501")
#loc2265 = loc("multiply.5513")
#loc2266 = loc("dot.5514")
#loc2267 = loc("reshape.5466")
#loc2268 = loc("reshape.5469")
#loc2269 = loc("add.5517")
#loc2270 = loc("reshape.5518")
#loc2271 = loc("convert.5404")
#loc2272 = loc("reshape.5200")
#loc2273 = loc("reshape.5202")
#loc2274 = loc("transpose.5203")
#loc2275 = loc("convert.5204")
#loc2277 = loc("reshape.5192")
#loc2278 = loc("reshape.5194")
#loc2279 = loc("convert.5195")
#loc2280 = loc("broadcast.5410")
#loc2281 = loc("add.5411")
#loc2282 = loc("convert.5412")
#loc2284 = loc("compare.5425")
#loc2285 = loc("slice.5430")
#loc2286 = loc("convert.5431")
#loc2287 = loc("reshape.5455")
#loc2288 = loc("concatenate.5456")
#loc2289 = loc("slice.5428")
#loc2290 = loc("reduce.5437")
#loc2291 = loc("broadcast.5438")
#loc2292 = loc("subtract.5439")
#loc2293 = loc("exponential.5440")
#loc2294 = loc("reduce.5446")
#loc2295 = loc("broadcast.5447")
#loc2296 = loc("divide.5448")
#loc2298 = loc("reshape.5462")
#loc2299 = loc("broadcast.5520")
#loc2300 = loc("multiply.5521")
#loc2302 = loc("add.5531")
#loc2303 = loc("convert.5532")
#loc2304 = loc("power.5534")
#loc2306 = loc("multiply.5550")
#loc2307 = loc("reshape.5551")
#loc2308 = loc("add.5555")
#loc2309 = loc("rsqrt.5556")
#loc2310 = loc("reshape.5557")
#loc2311 = loc("broadcast.5558")
#loc2312 = loc("multiply.5559")
#loc2313 = loc("multiply.5566")
#loc2314 = loc("convert.5567")
#loc2315 = loc("reshape.5732")
#loc2316 = loc("reshape.5728")
#loc2317 = loc("reshape.5730")
#loc2318 = loc("transpose.5731")
#loc2320 = loc("reshape.5734")
#loc2321 = loc("reshape.5724")
#loc2322 = loc("add.5738")
#loc2323 = loc("transpose.5740")
#loc2324 = loc("slice.5741")
#loc2325 = loc("multiply.5758")
#loc2326 = loc("slice.5745")
#loc2327 = loc("multiply.5755")
#loc2328 = loc("subtract.5761")
#loc2329 = loc("multiply.5748")
#loc2330 = loc("multiply.5744")
#loc2331 = loc("add.5751")
#loc2332 = loc("concatenate.5762")
#loc2333 = loc("reshape.5181")
#loc2334 = loc("reshape.5183")
#loc2335 = loc("transpose.5184")
#loc2337 = loc("reshape.5570")
#loc2338 = loc("reshape.5177")
#loc2339 = loc("add.5574")
#loc2340 = loc("transpose.5576")
#loc2341 = loc("slice.5577")
#loc2342 = loc("multiply.5595")
#loc2343 = loc("slice.5582")
#loc2344 = loc("multiply.5592")
#loc2345 = loc("subtract.5598")
#loc2346 = loc("multiply.5585")
#loc2347 = loc("multiply.5580")
#loc2348 = loc("add.5588")
#loc2349 = loc("concatenate.5599")
#loc2351 = loc("broadcast.5718")
#loc2352 = loc("reshape.5719")
#loc2353 = loc("transpose.5720")
#loc2354 = loc("dot.5763")
#loc2355 = loc("multiply.5766")
#loc2356 = loc("add.5771")
#loc2357 = loc("reshape.5708")
#loc2358 = loc("reshape.5711")
#loc2359 = loc("concatenate.5772")
#loc2360 = loc("reshape.6043")
#loc2361 = loc("reshape.6045")
#loc2362 = loc("convert.6046")
#loc2363 = loc("broadcast.6047")
#loc2364 = loc("reduce.5779")
#loc2365 = loc("broadcast.5815")
#loc2366 = loc("subtract.5816")
#loc2367 = loc("reduce.5822")
#loc2368 = loc("broadcast.5823")
#loc2369 = loc("subtract.5824")
#loc2370 = loc("exponential.5825")
#loc2371 = loc("reduce.5831")
#loc2372 = loc("broadcast.5832")
#loc2373 = loc("divide.5833")
#loc2374 = loc("slice.5834")
#loc2375 = loc("reshape.5624")
#loc2376 = loc("reshape.5626")
#loc2377 = loc("transpose.5627")
#loc2379 = loc("reshape.5630")
#loc2380 = loc("reshape.5620")
#loc2381 = loc("add.5634")
#loc2382 = loc("transpose.5636")
#loc2384 = loc("broadcast.5704")
#loc2385 = loc("reshape.5705")
#loc2386 = loc("dot.5835")
#loc2387 = loc("reshape.5839")
#loc2388 = loc("reshape.5697")
#loc2389 = loc("reshape.5699")
#loc2390 = loc("transpose.5700")
#loc2392 = loc("reshape.5841")
#loc2393 = loc("reshape.5693")
#loc2394 = loc("add.5845")
#loc2395 = loc("add.5848")
#loc2396 = loc("reshape.5878")
#loc2397 = loc("reshape.5880")
#loc2398 = loc("convert.5881")
#loc2399 = loc("broadcast.5882")
#loc2400 = loc("convert.5849")
#loc2401 = loc("power.5851")
#loc2403 = loc("multiply.5867")
#loc2404 = loc("reshape.5868")
#loc2405 = loc("add.5872")
#loc2406 = loc("rsqrt.5873")
#loc2407 = loc("reshape.5874")
#loc2408 = loc("broadcast.5875")
#loc2409 = loc("multiply.5876")
#loc2410 = loc("multiply.5883")
#loc2411 = loc("convert.5884")
#loc2412 = loc("reshape.5965")
#loc2413 = loc("broadcast.5969")
#loc2415 = loc("reshape.5959")
#loc2416 = loc("reshape.5962")
#loc2417 = loc("add.5975")
#loc2418 = loc("slice.5988")
#loc2419 = loc("clamp.5991")
#loc2420 = loc("add.5994")
#loc2421 = loc("slice.5976")
#loc2422 = loc("clamp.5979")
#loc2423 = loc("multiply.5981")
#loc2424 = loc("logistic.5982")
#loc2425 = loc("multiply.5983")
#loc2426 = loc("multiply.5995")
#loc2427 = loc("dot.5996")
#loc2428 = loc("reshape.5948")
#loc2429 = loc("reshape.5951")
#loc2430 = loc("add.5999")
#loc2431 = loc("reshape.6000")
#loc2432 = loc("convert.5886")
#loc2433 = loc("reshape.5682")
#loc2434 = loc("reshape.5684")
#loc2435 = loc("transpose.5685")
#loc2436 = loc("convert.5686")
#loc2438 = loc("reshape.5674")
#loc2439 = loc("reshape.5676")
#loc2440 = loc("convert.5677")
#loc2441 = loc("broadcast.5892")
#loc2442 = loc("add.5893")
#loc2443 = loc("convert.5894")
#loc2445 = loc("compare.5907")
#loc2446 = loc("slice.5912")
#loc2447 = loc("convert.5913")
#loc2448 = loc("reshape.5937")
#loc2449 = loc("concatenate.5938")
#loc2450 = loc("slice.5910")
#loc2451 = loc("reduce.5919")
#loc2452 = loc("broadcast.5920")
#loc2453 = loc("subtract.5921")
#loc2454 = loc("exponential.5922")
#loc2455 = loc("reduce.5928")
#loc2456 = loc("broadcast.5929")
#loc2457 = loc("divide.5930")
#loc2459 = loc("reshape.5944")
#loc2460 = loc("broadcast.6002")
#loc2461 = loc("multiply.6003")
#loc2463 = loc("add.6013")
#loc2464 = loc("convert.6014")
#loc2465 = loc("power.6016")
#loc2467 = loc("multiply.6032")
#loc2468 = loc("reshape.6033")
#loc2469 = loc("add.6037")
#loc2470 = loc("rsqrt.6038")
#loc2471 = loc("reshape.6039")
#loc2472 = loc("broadcast.6040")
#loc2473 = loc("multiply.6041")
#loc2474 = loc("multiply.6048")
#loc2475 = loc("convert.6049")
#loc2476 = loc("reshape.6214")
#loc2477 = loc("reshape.6210")
#loc2478 = loc("reshape.6212")
#loc2479 = loc("transpose.6213")
#loc2481 = loc("reshape.6216")
#loc2482 = loc("reshape.6206")
#loc2483 = loc("add.6220")
#loc2484 = loc("transpose.6222")
#loc2485 = loc("slice.6223")
#loc2486 = loc("multiply.6240")
#loc2487 = loc("slice.6227")
#loc2488 = loc("multiply.6237")
#loc2489 = loc("subtract.6243")
#loc2490 = loc("multiply.6230")
#loc2491 = loc("multiply.6226")
#loc2492 = loc("add.6233")
#loc2493 = loc("concatenate.6244")
#loc2494 = loc("reshape.5663")
#loc2495 = loc("reshape.5665")
#loc2496 = loc("transpose.5666")
#loc2498 = loc("reshape.6052")
#loc2499 = loc("reshape.5659")
#loc2500 = loc("add.6056")
#loc2501 = loc("transpose.6058")
#loc2502 = loc("slice.6059")
#loc2503 = loc("multiply.6077")
#loc2504 = loc("slice.6064")
#loc2505 = loc("multiply.6074")
#loc2506 = loc("subtract.6080")
#loc2507 = loc("multiply.6067")
#loc2508 = loc("multiply.6062")
#loc2509 = loc("add.6070")
#loc2510 = loc("concatenate.6081")
#loc2512 = loc("broadcast.6200")
#loc2513 = loc("reshape.6201")
#loc2514 = loc("transpose.6202")
#loc2515 = loc("dot.6245")
#loc2516 = loc("multiply.6248")
#loc2517 = loc("add.6253")
#loc2518 = loc("reshape.6190")
#loc2519 = loc("reshape.6193")
#loc2520 = loc("concatenate.6254")
#loc2521 = loc("reshape.6525")
#loc2522 = loc("reshape.6527")
#loc2523 = loc("convert.6528")
#loc2524 = loc("broadcast.6529")
#loc2525 = loc("reduce.6261")
#loc2526 = loc("broadcast.6297")
#loc2527 = loc("subtract.6298")
#loc2528 = loc("reduce.6304")
#loc2529 = loc("broadcast.6305")
#loc2530 = loc("subtract.6306")
#loc2531 = loc("exponential.6307")
#loc2532 = loc("reduce.6313")
#loc2533 = loc("broadcast.6314")
#loc2534 = loc("divide.6315")
#loc2535 = loc("slice.6316")
#loc2536 = loc("reshape.6106")
#loc2537 = loc("reshape.6108")
#loc2538 = loc("transpose.6109")
#loc2540 = loc("reshape.6112")
#loc2541 = loc("reshape.6102")
#loc2542 = loc("add.6116")
#loc2543 = loc("transpose.6118")
#loc2545 = loc("broadcast.6186")
#loc2546 = loc("reshape.6187")
#loc2547 = loc("dot.6317")
#loc2548 = loc("reshape.6321")
#loc2549 = loc("reshape.6179")
#loc2550 = loc("reshape.6181")
#loc2551 = loc("transpose.6182")
#loc2553 = loc("reshape.6323")
#loc2554 = loc("reshape.6175")
#loc2555 = loc("add.6327")
#loc2556 = loc("add.6330")
#loc2557 = loc("reshape.6360")
#loc2558 = loc("reshape.6362")
#loc2559 = loc("convert.6363")
#loc2560 = loc("broadcast.6364")
#loc2561 = loc("convert.6331")
#loc2562 = loc("power.6333")
#loc2564 = loc("multiply.6349")
#loc2565 = loc("reshape.6350")
#loc2566 = loc("add.6354")
#loc2567 = loc("rsqrt.6355")
#loc2568 = loc("reshape.6356")
#loc2569 = loc("broadcast.6357")
#loc2570 = loc("multiply.6358")
#loc2571 = loc("multiply.6365")
#loc2572 = loc("convert.6366")
#loc2573 = loc("reshape.6447")
#loc2574 = loc("broadcast.6451")
#loc2576 = loc("reshape.6441")
#loc2577 = loc("reshape.6444")
#loc2578 = loc("add.6457")
#loc2579 = loc("slice.6470")
#loc2580 = loc("clamp.6473")
#loc2581 = loc("add.6476")
#loc2582 = loc("slice.6458")
#loc2583 = loc("clamp.6461")
#loc2584 = loc("multiply.6463")
#loc2585 = loc("logistic.6464")
#loc2586 = loc("multiply.6465")
#loc2587 = loc("multiply.6477")
#loc2588 = loc("dot.6478")
#loc2589 = loc("reshape.6430")
#loc2590 = loc("reshape.6433")
#loc2591 = loc("add.6481")
#loc2592 = loc("reshape.6482")
#loc2593 = loc("convert.6368")
#loc2594 = loc("reshape.6164")
#loc2595 = loc("reshape.6166")
#loc2596 = loc("transpose.6167")
#loc2597 = loc("convert.6168")
#loc2599 = loc("reshape.6156")
#loc2600 = loc("reshape.6158")
#loc2601 = loc("convert.6159")
#loc2602 = loc("broadcast.6374")
#loc2603 = loc("add.6375")
#loc2604 = loc("convert.6376")
#loc2606 = loc("compare.6389")
#loc2607 = loc("slice.6394")
#loc2608 = loc("convert.6395")
#loc2609 = loc("reshape.6419")
#loc2610 = loc("concatenate.6420")
#loc2611 = loc("slice.6392")
#loc2612 = loc("reduce.6401")
#loc2613 = loc("broadcast.6402")
#loc2614 = loc("subtract.6403")
#loc2615 = loc("exponential.6404")
#loc2616 = loc("reduce.6410")
#loc2617 = loc("broadcast.6411")
#loc2618 = loc("divide.6412")
#loc2620 = loc("reshape.6426")
#loc2621 = loc("broadcast.6484")
#loc2622 = loc("multiply.6485")
#loc2624 = loc("add.6495")
#loc2625 = loc("convert.6496")
#loc2626 = loc("power.6498")
#loc2628 = loc("multiply.6514")
#loc2629 = loc("reshape.6515")
#loc2630 = loc("add.6519")
#loc2631 = loc("rsqrt.6520")
#loc2632 = loc("reshape.6521")
#loc2633 = loc("broadcast.6522")
#loc2634 = loc("multiply.6523")
#loc2635 = loc("multiply.6530")
#loc2636 = loc("convert.6531")
#loc2637 = loc("reshape.6696")
#loc2638 = loc("reshape.6692")
#loc2639 = loc("reshape.6694")
#loc2640 = loc("transpose.6695")
#loc2642 = loc("reshape.6698")
#loc2643 = loc("reshape.6688")
#loc2644 = loc("add.6702")
#loc2645 = loc("transpose.6704")
#loc2646 = loc("slice.6705")
#loc2647 = loc("multiply.6722")
#loc2648 = loc("slice.6709")
#loc2649 = loc("multiply.6719")
#loc2650 = loc("subtract.6725")
#loc2651 = loc("multiply.6712")
#loc2652 = loc("multiply.6708")
#loc2653 = loc("add.6715")
#loc2654 = loc("concatenate.6726")
#loc2655 = loc("reshape.6145")
#loc2656 = loc("reshape.6147")
#loc2657 = loc("transpose.6148")
#loc2659 = loc("reshape.6534")
#loc2660 = loc("reshape.6141")
#loc2661 = loc("add.6538")
#loc2662 = loc("transpose.6540")
#loc2663 = loc("slice.6541")
#loc2664 = loc("multiply.6559")
#loc2665 = loc("slice.6546")
#loc2666 = loc("multiply.6556")
#loc2667 = loc("subtract.6562")
#loc2668 = loc("multiply.6549")
#loc2669 = loc("multiply.6544")
#loc2670 = loc("add.6552")
#loc2671 = loc("concatenate.6563")
#loc2673 = loc("broadcast.6682")
#loc2674 = loc("reshape.6683")
#loc2675 = loc("transpose.6684")
#loc2676 = loc("dot.6727")
#loc2677 = loc("multiply.6730")
#loc2678 = loc("add.6735")
#loc2679 = loc("reshape.6672")
#loc2680 = loc("reshape.6675")
#loc2681 = loc("concatenate.6736")
#loc2682 = loc("reshape.7007")
#loc2683 = loc("reshape.7009")
#loc2684 = loc("convert.7010")
#loc2685 = loc("broadcast.7011")
#loc2686 = loc("reduce.6743")
#loc2687 = loc("broadcast.6779")
#loc2688 = loc("subtract.6780")
#loc2689 = loc("reduce.6786")
#loc2690 = loc("broadcast.6787")
#loc2691 = loc("subtract.6788")
#loc2692 = loc("exponential.6789")
#loc2693 = loc("reduce.6795")
#loc2694 = loc("broadcast.6796")
#loc2695 = loc("divide.6797")
#loc2696 = loc("slice.6798")
#loc2697 = loc("reshape.6588")
#loc2698 = loc("reshape.6590")
#loc2699 = loc("transpose.6591")
#loc2701 = loc("reshape.6594")
#loc2702 = loc("reshape.6584")
#loc2703 = loc("add.6598")
#loc2704 = loc("transpose.6600")
#loc2706 = loc("broadcast.6668")
#loc2707 = loc("reshape.6669")
#loc2708 = loc("dot.6799")
#loc2709 = loc("reshape.6803")
#loc2710 = loc("reshape.6661")
#loc2711 = loc("reshape.6663")
#loc2712 = loc("transpose.6664")
#loc2714 = loc("reshape.6805")
#loc2715 = loc("reshape.6657")
#loc2716 = loc("add.6809")
#loc2717 = loc("add.6812")
#loc2718 = loc("reshape.6842")
#loc2719 = loc("reshape.6844")
#loc2720 = loc("convert.6845")
#loc2721 = loc("broadcast.6846")
#loc2722 = loc("convert.6813")
#loc2723 = loc("power.6815")
#loc2725 = loc("multiply.6831")
#loc2726 = loc("reshape.6832")
#loc2727 = loc("add.6836")
#loc2728 = loc("rsqrt.6837")
#loc2729 = loc("reshape.6838")
#loc2730 = loc("broadcast.6839")
#loc2731 = loc("multiply.6840")
#loc2732 = loc("multiply.6847")
#loc2733 = loc("convert.6848")
#loc2734 = loc("reshape.6929")
#loc2735 = loc("broadcast.6933")
#loc2737 = loc("reshape.6923")
#loc2738 = loc("reshape.6926")
#loc2739 = loc("add.6939")
#loc2740 = loc("slice.6952")
#loc2741 = loc("clamp.6955")
#loc2742 = loc("add.6958")
#loc2743 = loc("slice.6940")
#loc2744 = loc("clamp.6943")
#loc2745 = loc("multiply.6945")
#loc2746 = loc("logistic.6946")
#loc2747 = loc("multiply.6947")
#loc2748 = loc("multiply.6959")
#loc2749 = loc("dot.6960")
#loc2750 = loc("reshape.6912")
#loc2751 = loc("reshape.6915")
#loc2752 = loc("add.6963")
#loc2753 = loc("reshape.6964")
#loc2754 = loc("convert.6850")
#loc2755 = loc("reshape.6646")
#loc2756 = loc("reshape.6648")
#loc2757 = loc("transpose.6649")
#loc2758 = loc("convert.6650")
#loc2760 = loc("reshape.6638")
#loc2761 = loc("reshape.6640")
#loc2762 = loc("convert.6641")
#loc2763 = loc("broadcast.6856")
#loc2764 = loc("add.6857")
#loc2765 = loc("convert.6858")
#loc2767 = loc("compare.6871")
#loc2768 = loc("slice.6876")
#loc2769 = loc("convert.6877")
#loc2770 = loc("reshape.6901")
#loc2771 = loc("concatenate.6902")
#loc2772 = loc("slice.6874")
#loc2773 = loc("reduce.6883")
#loc2774 = loc("broadcast.6884")
#loc2775 = loc("subtract.6885")
#loc2776 = loc("exponential.6886")
#loc2777 = loc("reduce.6892")
#loc2778 = loc("broadcast.6893")
#loc2779 = loc("divide.6894")
#loc2781 = loc("reshape.6908")
#loc2782 = loc("broadcast.6966")
#loc2783 = loc("multiply.6967")
#loc2785 = loc("add.6977")
#loc2786 = loc("convert.6978")
#loc2787 = loc("power.6980")
#loc2789 = loc("multiply.6996")
#loc2790 = loc("reshape.6997")
#loc2791 = loc("add.7001")
#loc2792 = loc("rsqrt.7002")
#loc2793 = loc("reshape.7003")
#loc2794 = loc("broadcast.7004")
#loc2795 = loc("multiply.7005")
#loc2796 = loc("multiply.7012")
#loc2797 = loc("convert.7013")
#loc2798 = loc("reshape.7178")
#loc2799 = loc("reshape.7174")
#loc2800 = loc("reshape.7176")
#loc2801 = loc("transpose.7177")
#loc2803 = loc("reshape.7180")
#loc2804 = loc("reshape.7170")
#loc2805 = loc("add.7184")
#loc2806 = loc("transpose.7186")
#loc2807 = loc("slice.7187")
#loc2808 = loc("multiply.7204")
#loc2809 = loc("slice.7191")
#loc2810 = loc("multiply.7201")
#loc2811 = loc("subtract.7207")
#loc2812 = loc("multiply.7194")
#loc2813 = loc("multiply.7190")
#loc2814 = loc("add.7197")
#loc2815 = loc("concatenate.7208")
#loc2816 = loc("reshape.6627")
#loc2817 = loc("reshape.6629")
#loc2818 = loc("transpose.6630")
#loc2820 = loc("reshape.7016")
#loc2821 = loc("reshape.6623")
#loc2822 = loc("add.7020")
#loc2823 = loc("transpose.7022")
#loc2824 = loc("slice.7023")
#loc2825 = loc("multiply.7041")
#loc2826 = loc("slice.7028")
#loc2827 = loc("multiply.7038")
#loc2828 = loc("subtract.7044")
#loc2829 = loc("multiply.7031")
#loc2830 = loc("multiply.7026")
#loc2831 = loc("add.7034")
#loc2832 = loc("concatenate.7045")
#loc2834 = loc("broadcast.7164")
#loc2835 = loc("reshape.7165")
#loc2836 = loc("transpose.7166")
#loc2837 = loc("dot.7209")
#loc2838 = loc("multiply.7212")
#loc2839 = loc("add.7217")
#loc2840 = loc("reshape.7154")
#loc2841 = loc("reshape.7157")
#loc2842 = loc("concatenate.7218")
#loc2843 = loc("reshape.7489")
#loc2844 = loc("reshape.7491")
#loc2845 = loc("convert.7492")
#loc2846 = loc("broadcast.7493")
#loc2847 = loc("reduce.7225")
#loc2848 = loc("broadcast.7261")
#loc2849 = loc("subtract.7262")
#loc2850 = loc("reduce.7268")
#loc2851 = loc("broadcast.7269")
#loc2852 = loc("subtract.7270")
#loc2853 = loc("exponential.7271")
#loc2854 = loc("reduce.7277")
#loc2855 = loc("broadcast.7278")
#loc2856 = loc("divide.7279")
#loc2857 = loc("slice.7280")
#loc2858 = loc("reshape.7070")
#loc2859 = loc("reshape.7072")
#loc2860 = loc("transpose.7073")
#loc2862 = loc("reshape.7076")
#loc2863 = loc("reshape.7066")
#loc2864 = loc("add.7080")
#loc2865 = loc("transpose.7082")
#loc2867 = loc("broadcast.7150")
#loc2868 = loc("reshape.7151")
#loc2869 = loc("dot.7281")
#loc2870 = loc("reshape.7285")
#loc2871 = loc("reshape.7143")
#loc2872 = loc("reshape.7145")
#loc2873 = loc("transpose.7146")
#loc2875 = loc("reshape.7287")
#loc2876 = loc("reshape.7139")
#loc2877 = loc("add.7291")
#loc2878 = loc("add.7294")
#loc2879 = loc("reshape.7324")
#loc2880 = loc("reshape.7326")
#loc2881 = loc("convert.7327")
#loc2882 = loc("broadcast.7328")
#loc2883 = loc("convert.7295")
#loc2884 = loc("power.7297")
#loc2886 = loc("multiply.7313")
#loc2887 = loc("reshape.7314")
#loc2888 = loc("add.7318")
#loc2889 = loc("rsqrt.7319")
#loc2890 = loc("reshape.7320")
#loc2891 = loc("broadcast.7321")
#loc2892 = loc("multiply.7322")
#loc2893 = loc("multiply.7329")
#loc2894 = loc("convert.7330")
#loc2895 = loc("reshape.7411")
#loc2896 = loc("broadcast.7415")
#loc2898 = loc("reshape.7405")
#loc2899 = loc("reshape.7408")
#loc2900 = loc("add.7421")
#loc2901 = loc("slice.7434")
#loc2902 = loc("clamp.7437")
#loc2903 = loc("add.7440")
#loc2904 = loc("slice.7422")
#loc2905 = loc("clamp.7425")
#loc2906 = loc("multiply.7427")
#loc2907 = loc("logistic.7428")
#loc2908 = loc("multiply.7429")
#loc2909 = loc("multiply.7441")
#loc2910 = loc("dot.7442")
#loc2911 = loc("reshape.7394")
#loc2912 = loc("reshape.7397")
#loc2913 = loc("add.7445")
#loc2914 = loc("reshape.7446")
#loc2915 = loc("convert.7332")
#loc2916 = loc("reshape.7128")
#loc2917 = loc("reshape.7130")
#loc2918 = loc("transpose.7131")
#loc2919 = loc("convert.7132")
#loc2921 = loc("reshape.7120")
#loc2922 = loc("reshape.7122")
#loc2923 = loc("convert.7123")
#loc2924 = loc("broadcast.7338")
#loc2925 = loc("add.7339")
#loc2926 = loc("convert.7340")
#loc2928 = loc("compare.7353")
#loc2929 = loc("slice.7358")
#loc2930 = loc("convert.7359")
#loc2931 = loc("reshape.7383")
#loc2932 = loc("concatenate.7384")
#loc2933 = loc("slice.7356")
#loc2934 = loc("reduce.7365")
#loc2935 = loc("broadcast.7366")
#loc2936 = loc("subtract.7367")
#loc2937 = loc("exponential.7368")
#loc2938 = loc("reduce.7374")
#loc2939 = loc("broadcast.7375")
#loc2940 = loc("divide.7376")
#loc2942 = loc("reshape.7390")
#loc2943 = loc("broadcast.7448")
#loc2944 = loc("multiply.7449")
#loc2946 = loc("add.7459")
#loc2947 = loc("convert.7460")
#loc2948 = loc("power.7462")
#loc2950 = loc("multiply.7478")
#loc2951 = loc("reshape.7479")
#loc2952 = loc("add.7483")
#loc2953 = loc("rsqrt.7484")
#loc2954 = loc("reshape.7485")
#loc2955 = loc("broadcast.7486")
#loc2956 = loc("multiply.7487")
#loc2957 = loc("multiply.7494")
#loc2958 = loc("convert.7495")
#loc2959 = loc("reshape.7660")
#loc2960 = loc("reshape.7656")
#loc2961 = loc("reshape.7658")
#loc2962 = loc("transpose.7659")
#loc2964 = loc("reshape.7662")
#loc2965 = loc("reshape.7652")
#loc2966 = loc("add.7666")
#loc2967 = loc("transpose.7668")
#loc2968 = loc("slice.7669")
#loc2969 = loc("multiply.7686")
#loc2970 = loc("slice.7673")
#loc2971 = loc("multiply.7683")
#loc2972 = loc("subtract.7689")
#loc2973 = loc("multiply.7676")
#loc2974 = loc("multiply.7672")
#loc2975 = loc("add.7679")
#loc2976 = loc("concatenate.7690")
#loc2977 = loc("reshape.7109")
#loc2978 = loc("reshape.7111")
#loc2979 = loc("transpose.7112")
#loc2981 = loc("reshape.7498")
#loc2982 = loc("reshape.7105")
#loc2983 = loc("add.7502")
#loc2984 = loc("transpose.7504")
#loc2985 = loc("slice.7505")
#loc2986 = loc("multiply.7523")
#loc2987 = loc("slice.7510")
#loc2988 = loc("multiply.7520")
#loc2989 = loc("subtract.7526")
#loc2990 = loc("multiply.7513")
#loc2991 = loc("multiply.7508")
#loc2992 = loc("add.7516")
#loc2993 = loc("concatenate.7527")
#loc2995 = loc("broadcast.7646")
#loc2996 = loc("reshape.7647")
#loc2997 = loc("transpose.7648")
#loc2998 = loc("dot.7691")
#loc2999 = loc("multiply.7694")
#loc3000 = loc("add.7699")
#loc3001 = loc("reshape.7636")
#loc3002 = loc("reshape.7639")
#loc3003 = loc("concatenate.7700")
#loc3004 = loc("reshape.7971")
#loc3005 = loc("reshape.7973")
#loc3006 = loc("convert.7974")
#loc3007 = loc("broadcast.7975")
#loc3008 = loc("reduce.7707")
#loc3009 = loc("broadcast.7743")
#loc3010 = loc("subtract.7744")
#loc3011 = loc("reduce.7750")
#loc3012 = loc("broadcast.7751")
#loc3013 = loc("subtract.7752")
#loc3014 = loc("exponential.7753")
#loc3015 = loc("reduce.7759")
#loc3016 = loc("broadcast.7760")
#loc3017 = loc("divide.7761")
#loc3018 = loc("slice.7762")
#loc3019 = loc("reshape.7552")
#loc3020 = loc("reshape.7554")
#loc3021 = loc("transpose.7555")
#loc3023 = loc("reshape.7558")
#loc3024 = loc("reshape.7548")
#loc3025 = loc("add.7562")
#loc3026 = loc("transpose.7564")
#loc3028 = loc("broadcast.7632")
#loc3029 = loc("reshape.7633")
#loc3030 = loc("dot.7763")
#loc3031 = loc("reshape.7767")
#loc3032 = loc("reshape.7625")
#loc3033 = loc("reshape.7627")
#loc3034 = loc("transpose.7628")
#loc3036 = loc("reshape.7769")
#loc3037 = loc("reshape.7621")
#loc3038 = loc("add.7773")
#loc3039 = loc("add.7776")
#loc3040 = loc("reshape.7806")
#loc3041 = loc("reshape.7808")
#loc3042 = loc("convert.7809")
#loc3043 = loc("broadcast.7810")
#loc3044 = loc("convert.7777")
#loc3045 = loc("power.7779")
#loc3047 = loc("multiply.7795")
#loc3048 = loc("reshape.7796")
#loc3049 = loc("add.7800")
#loc3050 = loc("rsqrt.7801")
#loc3051 = loc("reshape.7802")
#loc3052 = loc("broadcast.7803")
#loc3053 = loc("multiply.7804")
#loc3054 = loc("multiply.7811")
#loc3055 = loc("convert.7812")
#loc3056 = loc("reshape.7893")
#loc3057 = loc("broadcast.7897")
#loc3059 = loc("reshape.7887")
#loc3060 = loc("reshape.7890")
#loc3061 = loc("add.7903")
#loc3062 = loc("slice.7916")
#loc3063 = loc("clamp.7919")
#loc3064 = loc("add.7922")
#loc3065 = loc("slice.7904")
#loc3066 = loc("clamp.7907")
#loc3067 = loc("multiply.7909")
#loc3068 = loc("logistic.7910")
#loc3069 = loc("multiply.7911")
#loc3070 = loc("multiply.7923")
#loc3071 = loc("dot.7924")
#loc3072 = loc("reshape.7876")
#loc3073 = loc("reshape.7879")
#loc3074 = loc("add.7927")
#loc3075 = loc("reshape.7928")
#loc3076 = loc("convert.7814")
#loc3077 = loc("reshape.7610")
#loc3078 = loc("reshape.7612")
#loc3079 = loc("transpose.7613")
#loc3080 = loc("convert.7614")
#loc3082 = loc("reshape.7602")
#loc3083 = loc("reshape.7604")
#loc3084 = loc("convert.7605")
#loc3085 = loc("broadcast.7820")
#loc3086 = loc("add.7821")
#loc3087 = loc("convert.7822")
#loc3089 = loc("compare.7835")
#loc3090 = loc("slice.7840")
#loc3091 = loc("convert.7841")
#loc3092 = loc("reshape.7865")
#loc3093 = loc("concatenate.7866")
#loc3094 = loc("slice.7838")
#loc3095 = loc("reduce.7847")
#loc3096 = loc("broadcast.7848")
#loc3097 = loc("subtract.7849")
#loc3098 = loc("exponential.7850")
#loc3099 = loc("reduce.7856")
#loc3100 = loc("broadcast.7857")
#loc3101 = loc("divide.7858")
#loc3103 = loc("reshape.7872")
#loc3104 = loc("broadcast.7930")
#loc3105 = loc("multiply.7931")
#loc3107 = loc("add.7941")
#loc3108 = loc("convert.7942")
#loc3109 = loc("power.7944")
#loc3111 = loc("multiply.7960")
#loc3112 = loc("reshape.7961")
#loc3113 = loc("add.7965")
#loc3114 = loc("rsqrt.7966")
#loc3115 = loc("reshape.7967")
#loc3116 = loc("broadcast.7968")
#loc3117 = loc("multiply.7969")
#loc3118 = loc("multiply.7976")
#loc3119 = loc("convert.7977")
#loc3120 = loc("reshape.8142")
#loc3121 = loc("reshape.8138")
#loc3122 = loc("reshape.8140")
#loc3123 = loc("transpose.8141")
#loc3125 = loc("reshape.8144")
#loc3126 = loc("reshape.8134")
#loc3127 = loc("add.8148")
#loc3128 = loc("transpose.8150")
#loc3129 = loc("slice.8151")
#loc3130 = loc("multiply.8168")
#loc3131 = loc("slice.8155")
#loc3132 = loc("multiply.8165")
#loc3133 = loc("subtract.8171")
#loc3134 = loc("multiply.8158")
#loc3135 = loc("multiply.8154")
#loc3136 = loc("add.8161")
#loc3137 = loc("concatenate.8172")
#loc3138 = loc("reshape.7591")
#loc3139 = loc("reshape.7593")
#loc3140 = loc("transpose.7594")
#loc3142 = loc("reshape.7980")
#loc3143 = loc("reshape.7587")
#loc3144 = loc("add.7984")
#loc3145 = loc("transpose.7986")
#loc3146 = loc("slice.7987")
#loc3147 = loc("multiply.8005")
#loc3148 = loc("slice.7992")
#loc3149 = loc("multiply.8002")
#loc3150 = loc("subtract.8008")
#loc3151 = loc("multiply.7995")
#loc3152 = loc("multiply.7990")
#loc3153 = loc("add.7998")
#loc3154 = loc("concatenate.8009")
#loc3156 = loc("broadcast.8128")
#loc3157 = loc("reshape.8129")
#loc3158 = loc("transpose.8130")
#loc3159 = loc("dot.8173")
#loc3160 = loc("multiply.8176")
#loc3161 = loc("add.8181")
#loc3162 = loc("reshape.8118")
#loc3163 = loc("reshape.8121")
#loc3164 = loc("concatenate.8182")
#loc3165 = loc("reshape.8453")
#loc3166 = loc("reshape.8455")
#loc3167 = loc("convert.8456")
#loc3168 = loc("broadcast.8457")
#loc3169 = loc("reduce.8189")
#loc3170 = loc("broadcast.8225")
#loc3171 = loc("subtract.8226")
#loc3172 = loc("reduce.8232")
#loc3173 = loc("broadcast.8233")
#loc3174 = loc("subtract.8234")
#loc3175 = loc("exponential.8235")
#loc3176 = loc("reduce.8241")
#loc3177 = loc("broadcast.8242")
#loc3178 = loc("divide.8243")
#loc3179 = loc("slice.8244")
#loc3180 = loc("reshape.8034")
#loc3181 = loc("reshape.8036")
#loc3182 = loc("transpose.8037")
#loc3184 = loc("reshape.8040")
#loc3185 = loc("reshape.8030")
#loc3186 = loc("add.8044")
#loc3187 = loc("transpose.8046")
#loc3189 = loc("broadcast.8114")
#loc3190 = loc("reshape.8115")
#loc3191 = loc("dot.8245")
#loc3192 = loc("reshape.8249")
#loc3193 = loc("reshape.8107")
#loc3194 = loc("reshape.8109")
#loc3195 = loc("transpose.8110")
#loc3197 = loc("reshape.8251")
#loc3198 = loc("reshape.8103")
#loc3199 = loc("add.8255")
#loc3200 = loc("add.8258")
#loc3201 = loc("reshape.8288")
#loc3202 = loc("reshape.8290")
#loc3203 = loc("convert.8291")
#loc3204 = loc("broadcast.8292")
#loc3205 = loc("convert.8259")
#loc3206 = loc("power.8261")
#loc3208 = loc("multiply.8277")
#loc3209 = loc("reshape.8278")
#loc3210 = loc("add.8282")
#loc3211 = loc("rsqrt.8283")
#loc3212 = loc("reshape.8284")
#loc3213 = loc("broadcast.8285")
#loc3214 = loc("multiply.8286")
#loc3215 = loc("multiply.8293")
#loc3216 = loc("convert.8294")
#loc3217 = loc("reshape.8375")
#loc3218 = loc("broadcast.8379")
#loc3220 = loc("reshape.8369")
#loc3221 = loc("reshape.8372")
#loc3222 = loc("add.8385")
#loc3223 = loc("slice.8398")
#loc3224 = loc("clamp.8401")
#loc3225 = loc("add.8404")
#loc3226 = loc("slice.8386")
#loc3227 = loc("clamp.8389")
#loc3228 = loc("multiply.8391")
#loc3229 = loc("logistic.8392")
#loc3230 = loc("multiply.8393")
#loc3231 = loc("multiply.8405")
#loc3232 = loc("dot.8406")
#loc3233 = loc("reshape.8358")
#loc3234 = loc("reshape.8361")
#loc3235 = loc("add.8409")
#loc3236 = loc("reshape.8410")
#loc3237 = loc("convert.8296")
#loc3238 = loc("reshape.8092")
#loc3239 = loc("reshape.8094")
#loc3240 = loc("transpose.8095")
#loc3241 = loc("convert.8096")
#loc3243 = loc("reshape.8084")
#loc3244 = loc("reshape.8086")
#loc3245 = loc("convert.8087")
#loc3246 = loc("broadcast.8302")
#loc3247 = loc("add.8303")
#loc3248 = loc("convert.8304")
#loc3250 = loc("compare.8317")
#loc3251 = loc("slice.8322")
#loc3252 = loc("convert.8323")
#loc3253 = loc("reshape.8347")
#loc3254 = loc("concatenate.8348")
#loc3255 = loc("slice.8320")
#loc3256 = loc("reduce.8329")
#loc3257 = loc("broadcast.8330")
#loc3258 = loc("subtract.8331")
#loc3259 = loc("exponential.8332")
#loc3260 = loc("reduce.8338")
#loc3261 = loc("broadcast.8339")
#loc3262 = loc("divide.8340")
#loc3264 = loc("reshape.8354")
#loc3265 = loc("broadcast.8412")
#loc3266 = loc("multiply.8413")
#loc3268 = loc("add.8423")
#loc3269 = loc("convert.8424")
#loc3270 = loc("power.8426")
#loc3272 = loc("multiply.8442")
#loc3273 = loc("reshape.8443")
#loc3274 = loc("add.8447")
#loc3275 = loc("rsqrt.8448")
#loc3276 = loc("reshape.8449")
#loc3277 = loc("broadcast.8450")
#loc3278 = loc("multiply.8451")
#loc3279 = loc("multiply.8458")
#loc3280 = loc("convert.8459")
#loc3281 = loc("reshape.8624")
#loc3282 = loc("reshape.8620")
#loc3283 = loc("reshape.8622")
#loc3284 = loc("transpose.8623")
#loc3286 = loc("reshape.8626")
#loc3287 = loc("reshape.8616")
#loc3288 = loc("add.8630")
#loc3289 = loc("transpose.8632")
#loc3290 = loc("slice.8633")
#loc3291 = loc("multiply.8650")
#loc3292 = loc("slice.8637")
#loc3293 = loc("multiply.8647")
#loc3294 = loc("subtract.8653")
#loc3295 = loc("multiply.8640")
#loc3296 = loc("multiply.8636")
#loc3297 = loc("add.8643")
#loc3298 = loc("concatenate.8654")
#loc3299 = loc("reshape.8073")
#loc3300 = loc("reshape.8075")
#loc3301 = loc("transpose.8076")
#loc3303 = loc("reshape.8462")
#loc3304 = loc("reshape.8069")
#loc3305 = loc("add.8466")
#loc3306 = loc("transpose.8468")
#loc3307 = loc("slice.8469")
#loc3308 = loc("multiply.8487")
#loc3309 = loc("slice.8474")
#loc3310 = loc("multiply.8484")
#loc3311 = loc("subtract.8490")
#loc3312 = loc("multiply.8477")
#loc3313 = loc("multiply.8472")
#loc3314 = loc("add.8480")
#loc3315 = loc("concatenate.8491")
#loc3317 = loc("broadcast.8610")
#loc3318 = loc("reshape.8611")
#loc3319 = loc("transpose.8612")
#loc3320 = loc("dot.8655")
#loc3321 = loc("multiply.8658")
#loc3322 = loc("add.8663")
#loc3323 = loc("reshape.8600")
#loc3324 = loc("reshape.8603")
#loc3325 = loc("concatenate.8664")
#loc3326 = loc("reshape.8935")
#loc3327 = loc("reshape.8937")
#loc3328 = loc("convert.8938")
#loc3329 = loc("broadcast.8939")
#loc3330 = loc("reduce.8671")
#loc3331 = loc("broadcast.8707")
#loc3332 = loc("subtract.8708")
#loc3333 = loc("reduce.8714")
#loc3334 = loc("broadcast.8715")
#loc3335 = loc("subtract.8716")
#loc3336 = loc("exponential.8717")
#loc3337 = loc("reduce.8723")
#loc3338 = loc("broadcast.8724")
#loc3339 = loc("divide.8725")
#loc3340 = loc("slice.8726")
#loc3341 = loc("reshape.8516")
#loc3342 = loc("reshape.8518")
#loc3343 = loc("transpose.8519")
#loc3345 = loc("reshape.8522")
#loc3346 = loc("reshape.8512")
#loc3347 = loc("add.8526")
#loc3348 = loc("transpose.8528")
#loc3350 = loc("broadcast.8596")
#loc3351 = loc("reshape.8597")
#loc3352 = loc("dot.8727")
#loc3353 = loc("reshape.8731")
#loc3354 = loc("reshape.8589")
#loc3355 = loc("reshape.8591")
#loc3356 = loc("transpose.8592")
#loc3358 = loc("reshape.8733")
#loc3359 = loc("reshape.8585")
#loc3360 = loc("add.8737")
#loc3361 = loc("add.8740")
#loc3362 = loc("reshape.8770")
#loc3363 = loc("reshape.8772")
#loc3364 = loc("convert.8773")
#loc3365 = loc("broadcast.8774")
#loc3366 = loc("convert.8741")
#loc3367 = loc("power.8743")
#loc3369 = loc("multiply.8759")
#loc3370 = loc("reshape.8760")
#loc3371 = loc("add.8764")
#loc3372 = loc("rsqrt.8765")
#loc3373 = loc("reshape.8766")
#loc3374 = loc("broadcast.8767")
#loc3375 = loc("multiply.8768")
#loc3376 = loc("multiply.8775")
#loc3377 = loc("convert.8776")
#loc3378 = loc("reshape.8857")
#loc3379 = loc("broadcast.8861")
#loc3381 = loc("reshape.8851")
#loc3382 = loc("reshape.8854")
#loc3383 = loc("add.8867")
#loc3384 = loc("slice.8880")
#loc3385 = loc("clamp.8883")
#loc3386 = loc("add.8886")
#loc3387 = loc("slice.8868")
#loc3388 = loc("clamp.8871")
#loc3389 = loc("multiply.8873")
#loc3390 = loc("logistic.8874")
#loc3391 = loc("multiply.8875")
#loc3392 = loc("multiply.8887")
#loc3393 = loc("dot.8888")
#loc3394 = loc("reshape.8840")
#loc3395 = loc("reshape.8843")
#loc3396 = loc("add.8891")
#loc3397 = loc("reshape.8892")
#loc3398 = loc("convert.8778")
#loc3399 = loc("reshape.8574")
#loc3400 = loc("reshape.8576")
#loc3401 = loc("transpose.8577")
#loc3402 = loc("convert.8578")
#loc3404 = loc("reshape.8566")
#loc3405 = loc("reshape.8568")
#loc3406 = loc("convert.8569")
#loc3407 = loc("broadcast.8784")
#loc3408 = loc("add.8785")
#loc3409 = loc("convert.8786")
#loc3411 = loc("compare.8799")
#loc3412 = loc("slice.8804")
#loc3413 = loc("convert.8805")
#loc3414 = loc("reshape.8829")
#loc3415 = loc("concatenate.8830")
#loc3416 = loc("slice.8802")
#loc3417 = loc("reduce.8811")
#loc3418 = loc("broadcast.8812")
#loc3419 = loc("subtract.8813")
#loc3420 = loc("exponential.8814")
#loc3421 = loc("reduce.8820")
#loc3422 = loc("broadcast.8821")
#loc3423 = loc("divide.8822")
#loc3425 = loc("reshape.8836")
#loc3426 = loc("broadcast.8894")
#loc3427 = loc("multiply.8895")
#loc3429 = loc("add.8905")
#loc3430 = loc("convert.8906")
#loc3431 = loc("power.8908")
#loc3433 = loc("multiply.8924")
#loc3434 = loc("reshape.8925")
#loc3435 = loc("add.8929")
#loc3436 = loc("rsqrt.8930")
#loc3437 = loc("reshape.8931")
#loc3438 = loc("broadcast.8932")
#loc3439 = loc("multiply.8933")
#loc3440 = loc("multiply.8940")
#loc3441 = loc("convert.8941")
#loc3442 = loc("reshape.9106")
#loc3443 = loc("reshape.9102")
#loc3444 = loc("reshape.9104")
#loc3445 = loc("transpose.9105")
#loc3447 = loc("reshape.9108")
#loc3448 = loc("reshape.9098")
#loc3449 = loc("add.9112")
#loc3450 = loc("transpose.9114")
#loc3451 = loc("slice.9115")
#loc3452 = loc("multiply.9132")
#loc3453 = loc("slice.9119")
#loc3454 = loc("multiply.9129")
#loc3455 = loc("subtract.9135")
#loc3456 = loc("multiply.9122")
#loc3457 = loc("multiply.9118")
#loc3458 = loc("add.9125")
#loc3459 = loc("concatenate.9136")
#loc3460 = loc("reshape.8555")
#loc3461 = loc("reshape.8557")
#loc3462 = loc("transpose.8558")
#loc3464 = loc("reshape.8944")
#loc3465 = loc("reshape.8551")
#loc3466 = loc("add.8948")
#loc3467 = loc("transpose.8950")
#loc3468 = loc("slice.8951")
#loc3469 = loc("multiply.8969")
#loc3470 = loc("slice.8956")
#loc3471 = loc("multiply.8966")
#loc3472 = loc("subtract.8972")
#loc3473 = loc("multiply.8959")
#loc3474 = loc("multiply.8954")
#loc3475 = loc("add.8962")
#loc3476 = loc("concatenate.8973")
#loc3478 = loc("broadcast.9092")
#loc3479 = loc("reshape.9093")
#loc3480 = loc("transpose.9094")
#loc3481 = loc("dot.9137")
#loc3482 = loc("multiply.9140")
#loc3483 = loc("add.9145")
#loc3484 = loc("reshape.9082")
#loc3485 = loc("reshape.9085")
#loc3486 = loc("concatenate.9146")
#loc3487 = loc("reshape.9417")
#loc3488 = loc("reshape.9419")
#loc3489 = loc("convert.9420")
#loc3490 = loc("broadcast.9421")
#loc3491 = loc("reduce.9153")
#loc3492 = loc("broadcast.9189")
#loc3493 = loc("subtract.9190")
#loc3494 = loc("reduce.9196")
#loc3495 = loc("broadcast.9197")
#loc3496 = loc("subtract.9198")
#loc3497 = loc("exponential.9199")
#loc3498 = loc("reduce.9205")
#loc3499 = loc("broadcast.9206")
#loc3500 = loc("divide.9207")
#loc3501 = loc("slice.9208")
#loc3502 = loc("reshape.8998")
#loc3503 = loc("reshape.9000")
#loc3504 = loc("transpose.9001")
#loc3506 = loc("reshape.9004")
#loc3507 = loc("reshape.8994")
#loc3508 = loc("add.9008")
#loc3509 = loc("transpose.9010")
#loc3511 = loc("broadcast.9078")
#loc3512 = loc("reshape.9079")
#loc3513 = loc("dot.9209")
#loc3514 = loc("reshape.9213")
#loc3515 = loc("reshape.9071")
#loc3516 = loc("reshape.9073")
#loc3517 = loc("transpose.9074")
#loc3519 = loc("reshape.9215")
#loc3520 = loc("reshape.9067")
#loc3521 = loc("add.9219")
#loc3522 = loc("add.9222")
#loc3523 = loc("reshape.9252")
#loc3524 = loc("reshape.9254")
#loc3525 = loc("convert.9255")
#loc3526 = loc("broadcast.9256")
#loc3527 = loc("convert.9223")
#loc3528 = loc("power.9225")
#loc3530 = loc("multiply.9241")
#loc3531 = loc("reshape.9242")
#loc3532 = loc("add.9246")
#loc3533 = loc("rsqrt.9247")
#loc3534 = loc("reshape.9248")
#loc3535 = loc("broadcast.9249")
#loc3536 = loc("multiply.9250")
#loc3537 = loc("multiply.9257")
#loc3538 = loc("convert.9258")
#loc3539 = loc("reshape.9339")
#loc3540 = loc("broadcast.9343")
#loc3542 = loc("reshape.9333")
#loc3543 = loc("reshape.9336")
#loc3544 = loc("add.9349")
#loc3545 = loc("slice.9362")
#loc3546 = loc("clamp.9365")
#loc3547 = loc("add.9368")
#loc3548 = loc("slice.9350")
#loc3549 = loc("clamp.9353")
#loc3550 = loc("multiply.9355")
#loc3551 = loc("logistic.9356")
#loc3552 = loc("multiply.9357")
#loc3553 = loc("multiply.9369")
#loc3554 = loc("dot.9370")
#loc3555 = loc("reshape.9322")
#loc3556 = loc("reshape.9325")
#loc3557 = loc("add.9373")
#loc3558 = loc("reshape.9374")
#loc3559 = loc("convert.9260")
#loc3560 = loc("reshape.9056")
#loc3561 = loc("reshape.9058")
#loc3562 = loc("transpose.9059")
#loc3563 = loc("convert.9060")
#loc3565 = loc("reshape.9048")
#loc3566 = loc("reshape.9050")
#loc3567 = loc("convert.9051")
#loc3568 = loc("broadcast.9266")
#loc3569 = loc("add.9267")
#loc3570 = loc("convert.9268")
#loc3572 = loc("compare.9281")
#loc3573 = loc("slice.9286")
#loc3574 = loc("convert.9287")
#loc3575 = loc("reshape.9311")
#loc3576 = loc("concatenate.9312")
#loc3577 = loc("slice.9284")
#loc3578 = loc("reduce.9293")
#loc3579 = loc("broadcast.9294")
#loc3580 = loc("subtract.9295")
#loc3581 = loc("exponential.9296")
#loc3582 = loc("reduce.9302")
#loc3583 = loc("broadcast.9303")
#loc3584 = loc("divide.9304")
#loc3586 = loc("reshape.9318")
#loc3587 = loc("broadcast.9376")
#loc3588 = loc("multiply.9377")
#loc3590 = loc("add.9387")
#loc3591 = loc("convert.9388")
#loc3592 = loc("power.9390")
#loc3594 = loc("multiply.9406")
#loc3595 = loc("reshape.9407")
#loc3596 = loc("add.9411")
#loc3597 = loc("rsqrt.9412")
#loc3598 = loc("reshape.9413")
#loc3599 = loc("broadcast.9414")
#loc3600 = loc("multiply.9415")
#loc3601 = loc("multiply.9422")
#loc3602 = loc("convert.9423")
#loc3603 = loc("reshape.9588")
#loc3604 = loc("reshape.9584")
#loc3605 = loc("reshape.9586")
#loc3606 = loc("transpose.9587")
#loc3608 = loc("reshape.9590")
#loc3609 = loc("reshape.9580")
#loc3610 = loc("add.9594")
#loc3611 = loc("transpose.9596")
#loc3612 = loc("slice.9597")
#loc3613 = loc("multiply.9614")
#loc3614 = loc("slice.9601")
#loc3615 = loc("multiply.9611")
#loc3616 = loc("subtract.9617")
#loc3617 = loc("multiply.9604")
#loc3618 = loc("multiply.9600")
#loc3619 = loc("add.9607")
#loc3620 = loc("concatenate.9618")
#loc3621 = loc("reshape.9037")
#loc3622 = loc("reshape.9039")
#loc3623 = loc("transpose.9040")
#loc3625 = loc("reshape.9426")
#loc3626 = loc("reshape.9033")
#loc3627 = loc("add.9430")
#loc3628 = loc("transpose.9432")
#loc3629 = loc("slice.9433")
#loc3630 = loc("multiply.9451")
#loc3631 = loc("slice.9438")
#loc3632 = loc("multiply.9448")
#loc3633 = loc("subtract.9454")
#loc3634 = loc("multiply.9441")
#loc3635 = loc("multiply.9436")
#loc3636 = loc("add.9444")
#loc3637 = loc("concatenate.9455")
#loc3639 = loc("broadcast.9574")
#loc3640 = loc("reshape.9575")
#loc3641 = loc("transpose.9576")
#loc3642 = loc("dot.9619")
#loc3643 = loc("multiply.9622")
#loc3644 = loc("add.9627")
#loc3645 = loc("reshape.9564")
#loc3646 = loc("reshape.9567")
#loc3647 = loc("concatenate.9628")
#loc3648 = loc("reshape.9899")
#loc3649 = loc("reshape.9901")
#loc3650 = loc("convert.9902")
#loc3651 = loc("broadcast.9903")
#loc3652 = loc("reduce.9635")
#loc3653 = loc("broadcast.9671")
#loc3654 = loc("subtract.9672")
#loc3655 = loc("reduce.9678")
#loc3656 = loc("broadcast.9679")
#loc3657 = loc("subtract.9680")
#loc3658 = loc("exponential.9681")
#loc3659 = loc("reduce.9687")
#loc3660 = loc("broadcast.9688")
#loc3661 = loc("divide.9689")
#loc3662 = loc("slice.9690")
#loc3663 = loc("reshape.9480")
#loc3664 = loc("reshape.9482")
#loc3665 = loc("transpose.9483")
#loc3667 = loc("reshape.9486")
#loc3668 = loc("reshape.9476")
#loc3669 = loc("add.9490")
#loc3670 = loc("transpose.9492")
#loc3672 = loc("broadcast.9560")
#loc3673 = loc("reshape.9561")
#loc3674 = loc("dot.9691")
#loc3675 = loc("reshape.9695")
#loc3676 = loc("reshape.9553")
#loc3677 = loc("reshape.9555")
#loc3678 = loc("transpose.9556")
#loc3680 = loc("reshape.9697")
#loc3681 = loc("reshape.9549")
#loc3682 = loc("add.9701")
#loc3683 = loc("add.9704")
#loc3684 = loc("reshape.9734")
#loc3685 = loc("reshape.9736")
#loc3686 = loc("convert.9737")
#loc3687 = loc("broadcast.9738")
#loc3688 = loc("convert.9705")
#loc3689 = loc("power.9707")
#loc3691 = loc("multiply.9723")
#loc3692 = loc("reshape.9724")
#loc3693 = loc("add.9728")
#loc3694 = loc("rsqrt.9729")
#loc3695 = loc("reshape.9730")
#loc3696 = loc("broadcast.9731")
#loc3697 = loc("multiply.9732")
#loc3698 = loc("multiply.9739")
#loc3699 = loc("convert.9740")
#loc3700 = loc("reshape.9821")
#loc3701 = loc("broadcast.9825")
#loc3703 = loc("reshape.9815")
#loc3704 = loc("reshape.9818")
#loc3705 = loc("add.9831")
#loc3706 = loc("slice.9844")
#loc3707 = loc("clamp.9847")
#loc3708 = loc("add.9850")
#loc3709 = loc("slice.9832")
#loc3710 = loc("clamp.9835")
#loc3711 = loc("multiply.9837")
#loc3712 = loc("logistic.9838")
#loc3713 = loc("multiply.9839")
#loc3714 = loc("multiply.9851")
#loc3715 = loc("dot.9852")
#loc3716 = loc("reshape.9804")
#loc3717 = loc("reshape.9807")
#loc3718 = loc("add.9855")
#loc3719 = loc("reshape.9856")
#loc3720 = loc("convert.9742")
#loc3721 = loc("reshape.9538")
#loc3722 = loc("reshape.9540")
#loc3723 = loc("transpose.9541")
#loc3724 = loc("convert.9542")
#loc3726 = loc("reshape.9530")
#loc3727 = loc("reshape.9532")
#loc3728 = loc("convert.9533")
#loc3729 = loc("broadcast.9748")
#loc3730 = loc("add.9749")
#loc3731 = loc("convert.9750")
#loc3733 = loc("compare.9763")
#loc3734 = loc("slice.9768")
#loc3735 = loc("convert.9769")
#loc3736 = loc("reshape.9793")
#loc3737 = loc("concatenate.9794")
#loc3738 = loc("slice.9766")
#loc3739 = loc("reduce.9775")
#loc3740 = loc("broadcast.9776")
#loc3741 = loc("subtract.9777")
#loc3742 = loc("exponential.9778")
#loc3743 = loc("reduce.9784")
#loc3744 = loc("broadcast.9785")
#loc3745 = loc("divide.9786")
#loc3747 = loc("reshape.9800")
#loc3748 = loc("broadcast.9858")
#loc3749 = loc("multiply.9859")
#loc3751 = loc("add.9869")
#loc3752 = loc("convert.9870")
#loc3753 = loc("power.9872")
#loc3755 = loc("multiply.9888")
#loc3756 = loc("reshape.9889")
#loc3757 = loc("add.9893")
#loc3758 = loc("rsqrt.9894")
#loc3759 = loc("reshape.9895")
#loc3760 = loc("broadcast.9896")
#loc3761 = loc("multiply.9897")
#loc3762 = loc("multiply.9904")
#loc3763 = loc("convert.9905")
#loc3764 = loc("reshape.10070")
#loc3765 = loc("reshape.10066")
#loc3766 = loc("reshape.10068")
#loc3767 = loc("transpose.10069")
#loc3769 = loc("reshape.10072")
#loc3770 = loc("reshape.10062")
#loc3771 = loc("add.10076")
#loc3772 = loc("transpose.10078")
#loc3773 = loc("slice.10079")
#loc3774 = loc("multiply.10096")
#loc3775 = loc("slice.10083")
#loc3776 = loc("multiply.10093")
#loc3777 = loc("subtract.10099")
#loc3778 = loc("multiply.10086")
#loc3779 = loc("multiply.10082")
#loc3780 = loc("add.10089")
#loc3781 = loc("concatenate.10100")
#loc3782 = loc("reshape.9519")
#loc3783 = loc("reshape.9521")
#loc3784 = loc("transpose.9522")
#loc3786 = loc("reshape.9908")
#loc3787 = loc("reshape.9515")
#loc3788 = loc("add.9912")
#loc3789 = loc("transpose.9914")
#loc3790 = loc("slice.9915")
#loc3791 = loc("multiply.9933")
#loc3792 = loc("slice.9920")
#loc3793 = loc("multiply.9930")
#loc3794 = loc("subtract.9936")
#loc3795 = loc("multiply.9923")
#loc3796 = loc("multiply.9918")
#loc3797 = loc("add.9926")
#loc3798 = loc("concatenate.9937")
#loc3800 = loc("broadcast.10056")
#loc3801 = loc("reshape.10057")
#loc3802 = loc("transpose.10058")
#loc3803 = loc("dot.10101")
#loc3804 = loc("multiply.10104")
#loc3805 = loc("add.10109")
#loc3806 = loc("reshape.10046")
#loc3807 = loc("reshape.10049")
#loc3808 = loc("concatenate.10110")
#loc3809 = loc("reshape.10381")
#loc3810 = loc("reshape.10383")
#loc3811 = loc("convert.10384")
#loc3812 = loc("broadcast.10385")
#loc3813 = loc("reduce.10117")
#loc3814 = loc("broadcast.10153")
#loc3815 = loc("subtract.10154")
#loc3816 = loc("reduce.10160")
#loc3817 = loc("broadcast.10161")
#loc3818 = loc("subtract.10162")
#loc3819 = loc("exponential.10163")
#loc3820 = loc("reduce.10169")
#loc3821 = loc("broadcast.10170")
#loc3822 = loc("divide.10171")
#loc3823 = loc("slice.10172")
#loc3824 = loc("reshape.9962")
#loc3825 = loc("reshape.9964")
#loc3826 = loc("transpose.9965")
#loc3828 = loc("reshape.9968")
#loc3829 = loc("reshape.9958")
#loc3830 = loc("add.9972")
#loc3831 = loc("transpose.9974")
#loc3833 = loc("broadcast.10042")
#loc3834 = loc("reshape.10043")
#loc3835 = loc("dot.10173")
#loc3836 = loc("reshape.10177")
#loc3837 = loc("reshape.10035")
#loc3838 = loc("reshape.10037")
#loc3839 = loc("transpose.10038")
#loc3841 = loc("reshape.10179")
#loc3842 = loc("reshape.10031")
#loc3843 = loc("add.10183")
#loc3844 = loc("add.10186")
#loc3845 = loc("reshape.10216")
#loc3846 = loc("reshape.10218")
#loc3847 = loc("convert.10219")
#loc3848 = loc("broadcast.10220")
#loc3849 = loc("convert.10187")
#loc3850 = loc("power.10189")
#loc3852 = loc("multiply.10205")
#loc3853 = loc("reshape.10206")
#loc3854 = loc("add.10210")
#loc3855 = loc("rsqrt.10211")
#loc3856 = loc("reshape.10212")
#loc3857 = loc("broadcast.10213")
#loc3858 = loc("multiply.10214")
#loc3859 = loc("multiply.10221")
#loc3860 = loc("convert.10222")
#loc3861 = loc("reshape.10303")
#loc3862 = loc("broadcast.10307")
#loc3864 = loc("reshape.10297")
#loc3865 = loc("reshape.10300")
#loc3866 = loc("add.10313")
#loc3867 = loc("slice.10326")
#loc3868 = loc("clamp.10329")
#loc3869 = loc("add.10332")
#loc3870 = loc("slice.10314")
#loc3871 = loc("clamp.10317")
#loc3872 = loc("multiply.10319")
#loc3873 = loc("logistic.10320")
#loc3874 = loc("multiply.10321")
#loc3875 = loc("multiply.10333")
#loc3876 = loc("dot.10334")
#loc3877 = loc("reshape.10286")
#loc3878 = loc("reshape.10289")
#loc3879 = loc("add.10337")
#loc3880 = loc("reshape.10338")
#loc3881 = loc("convert.10224")
#loc3882 = loc("reshape.10020")
#loc3883 = loc("reshape.10022")
#loc3884 = loc("transpose.10023")
#loc3885 = loc("convert.10024")
#loc3887 = loc("reshape.10012")
#loc3888 = loc("reshape.10014")
#loc3889 = loc("convert.10015")
#loc3890 = loc("broadcast.10230")
#loc3891 = loc("add.10231")
#loc3892 = loc("convert.10232")
#loc3894 = loc("compare.10245")
#loc3895 = loc("slice.10250")
#loc3896 = loc("convert.10251")
#loc3897 = loc("reshape.10275")
#loc3898 = loc("concatenate.10276")
#loc3899 = loc("slice.10248")
#loc3900 = loc("reduce.10257")
#loc3901 = loc("broadcast.10258")
#loc3902 = loc("subtract.10259")
#loc3903 = loc("exponential.10260")
#loc3904 = loc("reduce.10266")
#loc3905 = loc("broadcast.10267")
#loc3906 = loc("divide.10268")
#loc3908 = loc("reshape.10282")
#loc3909 = loc("broadcast.10340")
#loc3910 = loc("multiply.10341")
#loc3912 = loc("add.10351")
#loc3913 = loc("convert.10352")
#loc3914 = loc("power.10354")
#loc3916 = loc("multiply.10370")
#loc3917 = loc("reshape.10371")
#loc3918 = loc("add.10375")
#loc3919 = loc("rsqrt.10376")
#loc3920 = loc("reshape.10377")
#loc3921 = loc("broadcast.10378")
#loc3922 = loc("multiply.10379")
#loc3923 = loc("multiply.10386")
#loc3924 = loc("convert.10387")
#loc3925 = loc("reshape.10552")
#loc3926 = loc("reshape.10548")
#loc3927 = loc("reshape.10550")
#loc3928 = loc("transpose.10551")
#loc3930 = loc("reshape.10554")
#loc3931 = loc("reshape.10544")
#loc3932 = loc("add.10558")
#loc3933 = loc("transpose.10560")
#loc3934 = loc("slice.10561")
#loc3935 = loc("multiply.10578")
#loc3936 = loc("slice.10565")
#loc3937 = loc("multiply.10575")
#loc3938 = loc("subtract.10581")
#loc3939 = loc("multiply.10568")
#loc3940 = loc("multiply.10564")
#loc3941 = loc("add.10571")
#loc3942 = loc("concatenate.10582")
#loc3943 = loc("reshape.10001")
#loc3944 = loc("reshape.10003")
#loc3945 = loc("transpose.10004")
#loc3947 = loc("reshape.10390")
#loc3948 = loc("reshape.9997")
#loc3949 = loc("add.10394")
#loc3950 = loc("transpose.10396")
#loc3951 = loc("slice.10397")
#loc3952 = loc("multiply.10415")
#loc3953 = loc("slice.10402")
#loc3954 = loc("multiply.10412")
#loc3955 = loc("subtract.10418")
#loc3956 = loc("multiply.10405")
#loc3957 = loc("multiply.10400")
#loc3958 = loc("add.10408")
#loc3959 = loc("concatenate.10419")
#loc3961 = loc("broadcast.10538")
#loc3962 = loc("reshape.10539")
#loc3963 = loc("transpose.10540")
#loc3964 = loc("dot.10583")
#loc3965 = loc("multiply.10586")
#loc3966 = loc("add.10591")
#loc3967 = loc("reshape.10528")
#loc3968 = loc("reshape.10531")
#loc3969 = loc("concatenate.10592")
#loc3970 = loc("reshape.10863")
#loc3971 = loc("reshape.10865")
#loc3972 = loc("convert.10866")
#loc3973 = loc("broadcast.10867")
#loc3974 = loc("reduce.10599")
#loc3975 = loc("broadcast.10635")
#loc3976 = loc("subtract.10636")
#loc3977 = loc("reduce.10642")
#loc3978 = loc("broadcast.10643")
#loc3979 = loc("subtract.10644")
#loc3980 = loc("exponential.10645")
#loc3981 = loc("reduce.10651")
#loc3982 = loc("broadcast.10652")
#loc3983 = loc("divide.10653")
#loc3984 = loc("slice.10654")
#loc3985 = loc("reshape.10444")
#loc3986 = loc("reshape.10446")
#loc3987 = loc("transpose.10447")
#loc3989 = loc("reshape.10450")
#loc3990 = loc("reshape.10440")
#loc3991 = loc("add.10454")
#loc3992 = loc("transpose.10456")
#loc3994 = loc("broadcast.10524")
#loc3995 = loc("reshape.10525")
#loc3996 = loc("dot.10655")
#loc3997 = loc("reshape.10659")
#loc3998 = loc("reshape.10517")
#loc3999 = loc("reshape.10519")
#loc4000 = loc("transpose.10520")
#loc4002 = loc("reshape.10661")
#loc4003 = loc("reshape.10513")
#loc4004 = loc("add.10665")
#loc4005 = loc("add.10668")
#loc4006 = loc("reshape.10698")
#loc4007 = loc("reshape.10700")
#loc4008 = loc("convert.10701")
#loc4009 = loc("broadcast.10702")
#loc4010 = loc("convert.10669")
#loc4011 = loc("power.10671")
#loc4013 = loc("multiply.10687")
#loc4014 = loc("reshape.10688")
#loc4015 = loc("add.10692")
#loc4016 = loc("rsqrt.10693")
#loc4017 = loc("reshape.10694")
#loc4018 = loc("broadcast.10695")
#loc4019 = loc("multiply.10696")
#loc4020 = loc("multiply.10703")
#loc4021 = loc("convert.10704")
#loc4022 = loc("reshape.10785")
#loc4023 = loc("broadcast.10789")
#loc4025 = loc("reshape.10779")
#loc4026 = loc("reshape.10782")
#loc4027 = loc("add.10795")
#loc4028 = loc("slice.10808")
#loc4029 = loc("clamp.10811")
#loc4030 = loc("add.10814")
#loc4031 = loc("slice.10796")
#loc4032 = loc("clamp.10799")
#loc4033 = loc("multiply.10801")
#loc4034 = loc("logistic.10802")
#loc4035 = loc("multiply.10803")
#loc4036 = loc("multiply.10815")
#loc4037 = loc("dot.10816")
#loc4038 = loc("reshape.10768")
#loc4039 = loc("reshape.10771")
#loc4040 = loc("add.10819")
#loc4041 = loc("reshape.10820")
#loc4042 = loc("convert.10706")
#loc4043 = loc("reshape.10502")
#loc4044 = loc("reshape.10504")
#loc4045 = loc("transpose.10505")
#loc4046 = loc("convert.10506")
#loc4048 = loc("reshape.10494")
#loc4049 = loc("reshape.10496")
#loc4050 = loc("convert.10497")
#loc4051 = loc("broadcast.10712")
#loc4052 = loc("add.10713")
#loc4053 = loc("convert.10714")
#loc4055 = loc("compare.10727")
#loc4056 = loc("slice.10732")
#loc4057 = loc("convert.10733")
#loc4058 = loc("reshape.10757")
#loc4059 = loc("concatenate.10758")
#loc4060 = loc("slice.10730")
#loc4061 = loc("reduce.10739")
#loc4062 = loc("broadcast.10740")
#loc4063 = loc("subtract.10741")
#loc4064 = loc("exponential.10742")
#loc4065 = loc("reduce.10748")
#loc4066 = loc("broadcast.10749")
#loc4067 = loc("divide.10750")
#loc4069 = loc("reshape.10764")
#loc4070 = loc("broadcast.10822")
#loc4071 = loc("multiply.10823")
#loc4073 = loc("add.10833")
#loc4074 = loc("convert.10834")
#loc4075 = loc("power.10836")
#loc4077 = loc("multiply.10852")
#loc4078 = loc("reshape.10853")
#loc4079 = loc("add.10857")
#loc4080 = loc("rsqrt.10858")
#loc4081 = loc("reshape.10859")
#loc4082 = loc("broadcast.10860")
#loc4083 = loc("multiply.10861")
#loc4084 = loc("multiply.10868")
#loc4085 = loc("convert.10869")
#loc4086 = loc("reshape.11034")
#loc4087 = loc("reshape.11030")
#loc4088 = loc("reshape.11032")
#loc4089 = loc("transpose.11033")
#loc4091 = loc("reshape.11036")
#loc4092 = loc("reshape.11026")
#loc4093 = loc("add.11040")
#loc4094 = loc("transpose.11042")
#loc4095 = loc("slice.11043")
#loc4096 = loc("multiply.11060")
#loc4097 = loc("slice.11047")
#loc4098 = loc("multiply.11057")
#loc4099 = loc("subtract.11063")
#loc4100 = loc("multiply.11050")
#loc4101 = loc("multiply.11046")
#loc4102 = loc("add.11053")
#loc4103 = loc("concatenate.11064")
#loc4104 = loc("reshape.10483")
#loc4105 = loc("reshape.10485")
#loc4106 = loc("transpose.10486")
#loc4108 = loc("reshape.10872")
#loc4109 = loc("reshape.10479")
#loc4110 = loc("add.10876")
#loc4111 = loc("transpose.10878")
#loc4112 = loc("slice.10879")
#loc4113 = loc("multiply.10897")
#loc4114 = loc("slice.10884")
#loc4115 = loc("multiply.10894")
#loc4116 = loc("subtract.10900")
#loc4117 = loc("multiply.10887")
#loc4118 = loc("multiply.10882")
#loc4119 = loc("add.10890")
#loc4120 = loc("concatenate.10901")
#loc4122 = loc("broadcast.11020")
#loc4123 = loc("reshape.11021")
#loc4124 = loc("transpose.11022")
#loc4125 = loc("dot.11065")
#loc4126 = loc("multiply.11068")
#loc4127 = loc("add.11073")
#loc4128 = loc("reshape.11010")
#loc4129 = loc("reshape.11013")
#loc4130 = loc("concatenate.11074")
#loc4131 = loc("reshape.11345")
#loc4132 = loc("reshape.11347")
#loc4133 = loc("convert.11348")
#loc4134 = loc("broadcast.11349")
#loc4135 = loc("reduce.11081")
#loc4136 = loc("broadcast.11117")
#loc4137 = loc("subtract.11118")
#loc4138 = loc("reduce.11124")
#loc4139 = loc("broadcast.11125")
#loc4140 = loc("subtract.11126")
#loc4141 = loc("exponential.11127")
#loc4142 = loc("reduce.11133")
#loc4143 = loc("broadcast.11134")
#loc4144 = loc("divide.11135")
#loc4145 = loc("slice.11136")
#loc4146 = loc("reshape.10926")
#loc4147 = loc("reshape.10928")
#loc4148 = loc("transpose.10929")
#loc4150 = loc("reshape.10932")
#loc4151 = loc("reshape.10922")
#loc4152 = loc("add.10936")
#loc4153 = loc("transpose.10938")
#loc4155 = loc("broadcast.11006")
#loc4156 = loc("reshape.11007")
#loc4157 = loc("dot.11137")
#loc4158 = loc("reshape.11141")
#loc4159 = loc("reshape.10999")
#loc4160 = loc("reshape.11001")
#loc4161 = loc("transpose.11002")
#loc4163 = loc("reshape.11143")
#loc4164 = loc("reshape.10995")
#loc4165 = loc("add.11147")
#loc4166 = loc("add.11150")
#loc4167 = loc("reshape.11180")
#loc4168 = loc("reshape.11182")
#loc4169 = loc("convert.11183")
#loc4170 = loc("broadcast.11184")
#loc4171 = loc("convert.11151")
#loc4172 = loc("power.11153")
#loc4174 = loc("multiply.11169")
#loc4175 = loc("reshape.11170")
#loc4176 = loc("add.11174")
#loc4177 = loc("rsqrt.11175")
#loc4178 = loc("reshape.11176")
#loc4179 = loc("broadcast.11177")
#loc4180 = loc("multiply.11178")
#loc4181 = loc("multiply.11185")
#loc4182 = loc("convert.11186")
#loc4183 = loc("reshape.11267")
#loc4184 = loc("broadcast.11271")
#loc4186 = loc("reshape.11261")
#loc4187 = loc("reshape.11264")
#loc4188 = loc("add.11277")
#loc4189 = loc("slice.11290")
#loc4190 = loc("clamp.11293")
#loc4191 = loc("add.11296")
#loc4192 = loc("slice.11278")
#loc4193 = loc("clamp.11281")
#loc4194 = loc("multiply.11283")
#loc4195 = loc("logistic.11284")
#loc4196 = loc("multiply.11285")
#loc4197 = loc("multiply.11297")
#loc4198 = loc("dot.11298")
#loc4199 = loc("reshape.11250")
#loc4200 = loc("reshape.11253")
#loc4201 = loc("add.11301")
#loc4202 = loc("reshape.11302")
#loc4203 = loc("convert.11188")
#loc4204 = loc("reshape.10984")
#loc4205 = loc("reshape.10986")
#loc4206 = loc("transpose.10987")
#loc4207 = loc("convert.10988")
#loc4209 = loc("reshape.10976")
#loc4210 = loc("reshape.10978")
#loc4211 = loc("convert.10979")
#loc4212 = loc("broadcast.11194")
#loc4213 = loc("add.11195")
#loc4214 = loc("convert.11196")
#loc4216 = loc("compare.11209")
#loc4217 = loc("slice.11214")
#loc4218 = loc("convert.11215")
#loc4219 = loc("reshape.11239")
#loc4220 = loc("concatenate.11240")
#loc4221 = loc("slice.11212")
#loc4222 = loc("reduce.11221")
#loc4223 = loc("broadcast.11222")
#loc4224 = loc("subtract.11223")
#loc4225 = loc("exponential.11224")
#loc4226 = loc("reduce.11230")
#loc4227 = loc("broadcast.11231")
#loc4228 = loc("divide.11232")
#loc4230 = loc("reshape.11246")
#loc4231 = loc("broadcast.11304")
#loc4232 = loc("multiply.11305")
#loc4234 = loc("add.11315")
#loc4235 = loc("convert.11316")
#loc4236 = loc("power.11318")
#loc4238 = loc("multiply.11334")
#loc4239 = loc("reshape.11335")
#loc4240 = loc("add.11339")
#loc4241 = loc("rsqrt.11340")
#loc4242 = loc("reshape.11341")
#loc4243 = loc("broadcast.11342")
#loc4244 = loc("multiply.11343")
#loc4245 = loc("multiply.11350")
#loc4246 = loc("convert.11351")
#loc4247 = loc("reshape.11509")
#loc4248 = loc("reshape.11505")
#loc4249 = loc("reshape.11507")
#loc4250 = loc("transpose.11508")
#loc4252 = loc("reshape.11511")
#loc4253 = loc("reshape.11501")
#loc4254 = loc("add.11515")
#loc4255 = loc("transpose.11517")
#loc4256 = loc("slice.11518")
#loc4257 = loc("multiply.11535")
#loc4258 = loc("slice.11522")
#loc4259 = loc("multiply.11532")
#loc4260 = loc("subtract.11538")
#loc4261 = loc("multiply.11525")
#loc4262 = loc("multiply.11521")
#loc4263 = loc("add.11528")
#loc4264 = loc("concatenate.11539")
#loc4265 = loc("reshape.10965")
#loc4266 = loc("reshape.10967")
#loc4267 = loc("transpose.10968")
#loc4269 = loc("reshape.11354")
#loc4270 = loc("reshape.10961")
#loc4271 = loc("add.11358")
#loc4272 = loc("transpose.11360")
#loc4273 = loc("slice.11361")
#loc4274 = loc("multiply.11379")
#loc4275 = loc("slice.11366")
#loc4276 = loc("multiply.11376")
#loc4277 = loc("subtract.11382")
#loc4278 = loc("multiply.11369")
#loc4279 = loc("multiply.11364")
#loc4280 = loc("add.11372")
#loc4281 = loc("concatenate.11383")
#loc4283 = loc("broadcast.11495")
#loc4284 = loc("reshape.11496")
#loc4285 = loc("transpose.11497")
#loc4286 = loc("dot.11540")
#loc4287 = loc("multiply.11543")
#loc4288 = loc("add.11548")
#loc4289 = loc("reshape.11485")
#loc4290 = loc("reshape.11488")
#loc4291 = loc("concatenate.11549")
#loc4292 = loc("reshape.11408")
#loc4293 = loc("reshape.11410")
#loc4294 = loc("transpose.11411")
#loc4296 = loc("reshape.11414")
#loc4297 = loc("reshape.11404")
#loc4298 = loc("add.11418")
#loc4299 = loc("transpose.11420")
#loc4301 = loc("reshape.11820")
#loc4302 = loc("reshape.11822")
#loc4303 = loc("convert.11823")
#loc4304 = loc("broadcast.11824")
#loc4305 = loc("reduce.11556")
#loc4306 = loc("broadcast.11592")
#loc4307 = loc("subtract.11593")
#loc4308 = loc("reduce.11599")
#loc4309 = loc("broadcast.11600")
#loc4310 = loc("subtract.11601")
#loc4311 = loc("exponential.11602")
#loc4312 = loc("reduce.11608")
#loc4313 = loc("broadcast.11609")
#loc4314 = loc("divide.11610")
#loc4315 = loc("slice.11611")
#loc4316 = loc("broadcast.11481")
#loc4317 = loc("reshape.11482")
#loc4318 = loc("dot.11612")
#loc4319 = loc("reshape.11616")
#loc4320 = loc("reshape.11474")
#loc4321 = loc("reshape.11476")
#loc4322 = loc("transpose.11477")
#loc4324 = loc("reshape.11618")
#loc4325 = loc("reshape.11470")
#loc4326 = loc("add.11622")
#loc4327 = loc("add.11625")
#loc4328 = loc("reshape.11655")
#loc4329 = loc("reshape.11657")
#loc4330 = loc("convert.11658")
#loc4331 = loc("broadcast.11659")
#loc4332 = loc("convert.11626")
#loc4333 = loc("power.11628")
#loc4335 = loc("multiply.11644")
#loc4336 = loc("reshape.11645")
#loc4337 = loc("add.11649")
#loc4338 = loc("rsqrt.11650")
#loc4339 = loc("reshape.11651")
#loc4340 = loc("broadcast.11652")
#loc4341 = loc("multiply.11653")
#loc4342 = loc("multiply.11660")
#loc4343 = loc("convert.11661")
#loc4344 = loc("reshape.11742")
#loc4345 = loc("broadcast.11746")
#loc4347 = loc("reshape.11736")
#loc4348 = loc("reshape.11739")
#loc4349 = loc("add.11752")
#loc4350 = loc("slice.11765")
#loc4351 = loc("clamp.11768")
#loc4352 = loc("add.11771")
#loc4353 = loc("slice.11753")
#loc4354 = loc("clamp.11756")
#loc4355 = loc("multiply.11758")
#loc4356 = loc("logistic.11759")
#loc4357 = loc("multiply.11760")
#loc4358 = loc("multiply.11772")
#loc4359 = loc("dot.11773")
#loc4360 = loc("reshape.11725")
#loc4361 = loc("reshape.11728")
#loc4362 = loc("add.11776")
#loc4363 = loc("reshape.11777")
#loc4364 = loc("convert.11663")
#loc4365 = loc("reshape.11459")
#loc4366 = loc("reshape.11461")
#loc4367 = loc("transpose.11462")
#loc4368 = loc("convert.11463")
#loc4370 = loc("reshape.11451")
#loc4371 = loc("reshape.11453")
#loc4372 = loc("convert.11454")
#loc4373 = loc("broadcast.11669")
#loc4374 = loc("add.11670")
#loc4375 = loc("convert.11671")
#loc4377 = loc("compare.11684")
#loc4378 = loc("slice.11689")
#loc4379 = loc("convert.11690")
#loc4380 = loc("reshape.11714")
#loc4381 = loc("concatenate.11715")
#loc4382 = loc("slice.11687")
#loc4383 = loc("reduce.11696")
#loc4384 = loc("broadcast.11697")
#loc4385 = loc("subtract.11698")
#loc4386 = loc("exponential.11699")
#loc4387 = loc("reduce.11705")
#loc4388 = loc("broadcast.11706")
#loc4389 = loc("divide.11707")
#loc4391 = loc("reshape.11721")
#loc4392 = loc("broadcast.11779")
#loc4393 = loc("multiply.11780")
#loc4395 = loc("add.11790")
#loc4396 = loc("convert.11791")
#loc4397 = loc("power.11793")
#loc4399 = loc("multiply.11809")
#loc4400 = loc("reshape.11810")
#loc4401 = loc("add.11814")
#loc4402 = loc("rsqrt.11815")
#loc4403 = loc("reshape.11816")
#loc4404 = loc("broadcast.11817")
#loc4405 = loc("multiply.11818")
#loc4406 = loc("multiply.11825")
#loc4407 = loc("convert.11826")
#loc4408 = loc("reshape.11827")
#loc4409 = loc("reshape.11440")
#loc4410 = loc("reshape.11442")
#loc4411 = loc("transpose.11443")
#loc4413 = loc("reshape.11829")
------------------ END OF MLIR MODULE ------------------
