#loc1 = loc("p0.1")
#loc2 = loc("p1.12")
#loc3 = loc("p2.19")
#loc4 = loc("p3.29")
#loc5 = loc("p4.33")
#loc6 = loc("p5.39")
#loc7 = loc("p6.43")
#loc8 = loc("p7.54")
#loc9 = loc("p8.61")
#loc10 = loc("p9.71")
#loc11 = loc("p10.75")
#loc12 = loc("p11.81")
#loc13 = loc("p12.85")
#loc14 = loc("p13.96")
#loc15 = loc("p14.103")
#loc16 = loc("p15.113")
#loc17 = loc("p16.117")
#loc18 = loc("p17.123")
#loc19 = loc("p18.127")
#loc20 = loc("p19.138")
#loc21 = loc("p20.145")
#loc22 = loc("p21.155")
#loc23 = loc("p22.159")
#loc24 = loc("p23.165")
#loc25 = loc("p24.169")
#loc26 = loc("p25.180")
#loc27 = loc("p26.187")
#loc28 = loc("p27.197")
#loc29 = loc("p28.201")
#loc30 = loc("p29.207")
#loc31 = loc("p30.211")
#loc32 = loc("p31.222")
#loc33 = loc("p32.229")
#loc34 = loc("p33.239")
#loc35 = loc("p34.243")
#loc36 = loc("p35.249")
#loc37 = loc("p36.253")
#loc38 = loc("p37.264")
#loc39 = loc("p38.271")
#loc40 = loc("p39.281")
#loc41 = loc("p40.285")
#loc42 = loc("p41.291")
#loc43 = loc("p42.295")
#loc44 = loc("p43.306")
#loc45 = loc("p44.313")
#loc46 = loc("p45.323")
#loc47 = loc("p46.327")
#loc48 = loc("p47.333")
#loc49 = loc("p48.337")
#loc50 = loc("p49.348")
#loc51 = loc("p50.355")
#loc52 = loc("p51.365")
#loc53 = loc("p52.369")
#loc54 = loc("p53.375")
#loc55 = loc("p54.379")
#loc56 = loc("p55.390")
#loc57 = loc("p56.397")
#loc58 = loc("p57.407")
#loc59 = loc("p58.411")
#loc60 = loc("p59.417")
#loc61 = loc("p60.421")
#loc62 = loc("p61.432")
#loc63 = loc("p62.439")
#loc64 = loc("p63.449")
#loc65 = loc("p64.453")
#loc66 = loc("p65.459")
#loc67 = loc("p66.463")
#loc68 = loc("p67.474")
#loc69 = loc("p68.481")
#loc70 = loc("p69.491")
#loc71 = loc("p70.495")
#loc72 = loc("p71.501")
#loc73 = loc("p72.505")
#loc74 = loc("p73.516")
#loc75 = loc("p74.523")
#loc76 = loc("p75.533")
#loc77 = loc("p76.537")
#loc78 = loc("p77.543")
#loc79 = loc("p78.547")
#loc80 = loc("p79.558")
#loc81 = loc("p80.565")
#loc82 = loc("p81.575")
#loc83 = loc("p82.579")
#loc84 = loc("p83.585")
#loc85 = loc("p84.589")
#loc86 = loc("p85.600")
#loc87 = loc("p86.607")
#loc88 = loc("p87.617")
#loc89 = loc("p88.621")
#loc90 = loc("p89.627")
#loc91 = loc("p90.631")
#loc92 = loc("p91.642")
#loc93 = loc("p92.649")
#loc94 = loc("p93.659")
#loc95 = loc("p94.663")
#loc96 = loc("p95.669")
#loc97 = loc("p96.673")
#loc98 = loc("p97.684")
#loc99 = loc("p98.691")
#loc100 = loc("p99.701")
#loc101 = loc("p100.705")
#loc102 = loc("p101.711")
#loc103 = loc("p102.715")
#loc104 = loc("p103.726")
#loc105 = loc("p104.733")
#loc106 = loc("p105.743")
#loc107 = loc("p106.747")
#loc108 = loc("p107.753")
#loc109 = loc("p108.757")
#loc110 = loc("p109.768")
#loc111 = loc("p110.775")
#loc112 = loc("p111.785")
#loc113 = loc("p112.789")
#loc114 = loc("p113.795")
#loc115 = loc("p114.799")
#loc116 = loc("p115.810")
#loc117 = loc("p116.817")
#loc118 = loc("p117.827")
#loc119 = loc("p118.831")
#loc120 = loc("p119.837")
#loc121 = loc("p120.841")
#loc122 = loc("p121.852")
#loc123 = loc("p122.859")
#loc124 = loc("p123.869")
#loc125 = loc("p124.873")
#loc126 = loc("p125.879")
#loc127 = loc("p126.883")
#loc128 = loc("p127.894")
#loc129 = loc("p128.901")
#loc130 = loc("p129.911")
#loc131 = loc("p130.915")
#loc132 = loc("p131.921")
#loc133 = loc("p132.925")
#loc134 = loc("p133.936")
#loc135 = loc("p134.943")
#loc136 = loc("p135.953")
#loc137 = loc("p136.957")
#loc138 = loc("p137.963")
#loc139 = loc("p138.967")
#loc140 = loc("p139.978")
#loc141 = loc("p140.985")
#loc142 = loc("p141.995")
#loc143 = loc("p142.999")
#loc144 = loc("p143.1005")
#loc145 = loc("p144.1009")
#loc146 = loc("p145.1020")
#loc147 = loc("p146.1027")
#loc148 = loc("p147.1037")
#loc149 = loc("p148.1041")
#loc150 = loc("p149.1047")
#loc151 = loc("p150.1051")
#loc152 = loc("p151.1062")
#loc153 = loc("p152.1069")
#loc154 = loc("p153.1079")
#loc155 = loc("p154.1083")
#loc156 = loc("p155.1089")
#loc157 = loc("p156.1093")
#loc158 = loc("p157.1104")
#loc159 = loc("p158.1111")
#loc160 = loc("p159.1121")
#loc161 = loc("p160.1125")
#loc162 = loc("p161.1131")
#loc163 = loc("p162.1135")
#loc164 = loc("p163.1146")
#loc165 = loc("p164.1153")
#loc166 = loc("p165.1163")
#loc167 = loc("p166.1167")
#loc168 = loc("p167.1173")
#loc169 = loc("p168.1177")
#loc170 = loc("p169.1188")
#loc171 = loc("p170.1195")
#loc172 = loc("p171.1205")
#loc173 = loc("p172.1209")
#loc174 = loc("p173.1215")
#loc175 = loc("p174.1219")
#loc176 = loc("p175.1230")
#loc177 = loc("p176.1237")
#loc178 = loc("p177.1247")
#loc179 = loc("p178.1251")
#loc180 = loc("p179.1257")
#loc181 = loc("p180.1261")
#loc182 = loc("p181.1272")
#loc183 = loc("p182.1279")
#loc184 = loc("p183.1289")
#loc185 = loc("p184.1293")
#loc186 = loc("p185.1299")
#loc187 = loc("p186.1303")
#loc188 = loc("p187.1314")
#loc189 = loc("p188.1321")
#loc190 = loc("p189.1331")
#loc191 = loc("p190.1335")
#loc192 = loc("p191.1341")
#loc193 = loc("p192.1345")
#loc194 = loc("p193.1356")
#loc195 = loc("p194.1363")
#loc196 = loc("p195.1373")
#loc197 = loc("p196.1377")
#loc198 = loc("p197.1383")
#loc199 = loc("p198.1387")
#loc200 = loc("p199.1398")
#loc201 = loc("p200.1405")
#loc202 = loc("p201.1415")
#loc203 = loc("p202.1419")
#loc204 = loc("p203.1425")
#loc205 = loc("p204.1429")
#loc206 = loc("p205.1440")
#loc207 = loc("p206.1447")
#loc208 = loc("p207.1457")
#loc209 = loc("p208.1461")
#loc210 = loc("p209.1467")
#loc211 = loc("p210.1471")
#loc212 = loc("p211.1482")
#loc213 = loc("p212.1489")
#loc214 = loc("p213.1499")
#loc215 = loc("p214.1503")
#loc216 = loc("p215.1509")
#loc217 = loc("p216.1513")
#loc218 = loc("p217.1521")
#loc219 = loc("p218.1526")
#loc220 = loc("p219.1561")
#loc221 = loc("p220.1584")
#loc222 = loc("p221.1640")
#loc223 = loc("p222.1663")
#loc224 = loc("p223.1704")
#loc225 = loc("p224.1720")
#loc226 = loc("p225.1724")
#loc227 = loc("p226.1774")
#loc228 = loc("p227.1778")
#loc229 = loc("p228.1928")
#loc230 = loc("p229.1996")
#loc231 = loc("p230.2001")
#loc232 = loc("p231.2007")
#loc233 = loc("p232.2012")
#loc234 = loc("p233.2095")
#loc235 = loc("p234.2118")
#loc236 = loc("p235.2206")
#loc237 = loc("p236.2210")
#loc238 = loc("p237.2255")
#loc239 = loc("p238.2259")
#loc240 = loc("p239.2409")
#loc241 = loc("p240.2477")
#loc242 = loc("p241.2482")
#loc243 = loc("p242.2488")
#loc244 = loc("p243.2493")
#loc245 = loc("p244.2576")
#loc246 = loc("p245.2599")
#loc247 = loc("p246.2612")
#loc248 = loc("p247.2616")
#loc249 = loc("p248.2661")
#loc250 = loc("p249.2665")
#loc251 = loc("p250.2815")
#loc252 = loc("p251.2883")
#loc253 = loc("p252.2888")
#loc254 = loc("p253.2894")
#loc255 = loc("p254.2899")
#loc256 = loc("p255.2982")
#loc257 = loc("p256.3005")
#loc258 = loc("p257.3018")
#loc259 = loc("p258.3022")
#loc260 = loc("p259.3067")
#loc261 = loc("p260.3071")
#loc262 = loc("p261.3221")
#loc263 = loc("p262.3289")
#loc264 = loc("p263.3294")
#loc265 = loc("p264.3300")
#loc266 = loc("p265.3305")
#loc267 = loc("p266.3388")
#loc268 = loc("p267.3411")
#loc269 = loc("p268.3424")
#loc270 = loc("p269.3428")
#loc271 = loc("p270.3473")
#loc272 = loc("p271.3477")
#loc273 = loc("p272.3627")
#loc274 = loc("p273.3695")
#loc275 = loc("p274.3700")
#loc276 = loc("p275.3706")
#loc277 = loc("p276.3711")
#loc278 = loc("p277.3794")
#loc279 = loc("p278.3817")
#loc280 = loc("p279.3830")
#loc281 = loc("p280.3834")
#loc282 = loc("p281.3879")
#loc283 = loc("p282.3883")
#loc284 = loc("p283.4033")
#loc285 = loc("p284.4101")
#loc286 = loc("p285.4106")
#loc287 = loc("p286.4112")
#loc288 = loc("p287.4117")
#loc289 = loc("p288.4200")
#loc290 = loc("p289.4223")
#loc291 = loc("p290.4236")
#loc292 = loc("p291.4240")
#loc293 = loc("p292.4285")
#loc294 = loc("p293.4289")
#loc295 = loc("p294.4439")
#loc296 = loc("p295.4507")
#loc297 = loc("p296.4512")
#loc298 = loc("p297.4518")
#loc299 = loc("p298.4523")
#loc300 = loc("p299.4606")
#loc301 = loc("p300.4629")
#loc302 = loc("p301.4642")
#loc303 = loc("p302.4646")
#loc304 = loc("p303.4691")
#loc305 = loc("p304.4695")
#loc306 = loc("p305.4845")
#loc307 = loc("p306.4913")
#loc308 = loc("p307.4918")
#loc309 = loc("p308.4924")
#loc310 = loc("p309.4929")
#loc311 = loc("p310.5012")
#loc312 = loc("p311.5035")
#loc313 = loc("p312.5048")
#loc314 = loc("p313.5052")
#loc315 = loc("p314.5097")
#loc316 = loc("p315.5101")
#loc317 = loc("p316.5251")
#loc318 = loc("p317.5319")
#loc319 = loc("p318.5324")
#loc320 = loc("p319.5330")
#loc321 = loc("p320.5335")
#loc322 = loc("p321.5418")
#loc323 = loc("p322.5441")
#loc324 = loc("p323.5454")
#loc325 = loc("p324.5458")
#loc326 = loc("p325.5503")
#loc327 = loc("p326.5507")
#loc328 = loc("p327.5657")
#loc329 = loc("p328.5725")
#loc330 = loc("p329.5730")
#loc331 = loc("p330.5736")
#loc332 = loc("p331.5741")
#loc333 = loc("p332.5824")
#loc334 = loc("p333.5847")
#loc335 = loc("p334.5860")
#loc336 = loc("p335.5864")
#loc337 = loc("p336.5909")
#loc338 = loc("p337.5913")
#loc339 = loc("p338.6063")
#loc340 = loc("p339.6131")
#loc341 = loc("p340.6136")
#loc342 = loc("p341.6142")
#loc343 = loc("p342.6147")
#loc344 = loc("p343.6230")
#loc345 = loc("p344.6253")
#loc346 = loc("p345.6266")
#loc347 = loc("p346.6270")
#loc348 = loc("p347.6315")
#loc349 = loc("p348.6319")
#loc350 = loc("p349.6469")
#loc351 = loc("p350.6537")
#loc352 = loc("p351.6542")
#loc353 = loc("p352.6548")
#loc354 = loc("p353.6553")
#loc355 = loc("p354.6636")
#loc356 = loc("p355.6659")
#loc357 = loc("p356.6672")
#loc358 = loc("p357.6676")
#loc359 = loc("p358.6721")
#loc360 = loc("p359.6725")
#loc361 = loc("p360.6875")
#loc362 = loc("p361.6943")
#loc363 = loc("p362.6948")
#loc364 = loc("p363.6954")
#loc365 = loc("p364.6959")
#loc366 = loc("p365.7042")
#loc367 = loc("p366.7065")
#loc368 = loc("p367.7078")
#loc369 = loc("p368.7082")
#loc370 = loc("p369.7127")
#loc371 = loc("p370.7131")
#loc372 = loc("p371.7281")
#loc373 = loc("p372.7349")
#loc374 = loc("p373.7354")
#loc375 = loc("p374.7360")
#loc376 = loc("p375.7365")
#loc377 = loc("p376.7448")
#loc378 = loc("p377.7471")
#loc379 = loc("p378.7484")
#loc380 = loc("p379.7488")
#loc381 = loc("p380.7533")
#loc382 = loc("p381.7537")
#loc383 = loc("p382.7687")
#loc384 = loc("p383.7755")
#loc385 = loc("p384.7760")
#loc386 = loc("p385.7766")
#loc387 = loc("p386.7771")
#loc388 = loc("p387.7854")
#loc389 = loc("p388.7877")
#loc390 = loc("p389.7890")
#loc391 = loc("p390.7894")
#loc392 = loc("p391.7939")
#loc393 = loc("p392.7943")
#loc394 = loc("p393.8093")
#loc395 = loc("p394.8161")
#loc396 = loc("p395.8166")
#loc397 = loc("p396.8172")
#loc398 = loc("p397.8177")
#loc399 = loc("p398.8260")
#loc400 = loc("p399.8283")
#loc401 = loc("p400.8296")
#loc402 = loc("p401.8300")
#loc403 = loc("p402.8345")
#loc404 = loc("p403.8349")
#loc405 = loc("p404.8499")
#loc406 = loc("p405.8567")
#loc407 = loc("p406.8572")
#loc408 = loc("p407.8578")
#loc409 = loc("p408.8583")
#loc410 = loc("p409.8666")
#loc411 = loc("p410.8689")
#loc412 = loc("p411.8702")
#loc413 = loc("p412.8706")
#loc414 = loc("p413.8751")
#loc415 = loc("p414.8755")
#loc416 = loc("p415.8905")
#loc417 = loc("p416.8973")
#loc418 = loc("p417.8978")
#loc419 = loc("p418.8984")
#loc420 = loc("p419.8989")
#loc421 = loc("p420.9072")
#loc422 = loc("p421.9095")
#loc423 = loc("p422.9108")
#loc424 = loc("p423.9112")
#loc425 = loc("p424.9157")
#loc426 = loc("p425.9161")
#loc427 = loc("p426.9311")
#loc428 = loc("p427.9379")
#loc429 = loc("p428.9384")
#loc430 = loc("p429.9390")
#loc431 = loc("p430.9395")
#loc432 = loc("p431.9478")
#loc433 = loc("p432.9501")
#loc434 = loc("p433.9514")
#loc435 = loc("p434.9518")
#loc436 = loc("p435.9563")
#loc437 = loc("p436.9567")
#loc438 = loc("p437.9717")
#loc439 = loc("p438.9785")
#loc440 = loc("p439.9790")
#loc441 = loc("p440.9796")
#loc442 = loc("p441.9801")
#loc443 = loc("p442.9884")
#loc444 = loc("p443.9907")
#loc445 = loc("p444.9920")
#loc446 = loc("p445.9924")
#loc447 = loc("p446.9969")
#loc448 = loc("p447.9973")
#loc449 = loc("p448.10123")
#loc450 = loc("p449.10191")
#loc451 = loc("p450.10196")
#loc452 = loc("p451.10202")
#loc453 = loc("p452.10207")
#loc454 = loc("p453.10290")
#loc455 = loc("p454.10313")
#loc456 = loc("p455.10326")
#loc457 = loc("p456.10330")
#loc458 = loc("p457.10375")
#loc459 = loc("p458.10379")
#loc460 = loc("p459.10529")
#loc461 = loc("p460.10597")
#loc462 = loc("p461.10602")
#loc463 = loc("p462.10608")
#loc464 = loc("p463.10613")
#loc465 = loc("p464.10696")
#loc466 = loc("p465.10719")
#loc467 = loc("p466.10732")
#loc468 = loc("p467.10736")
#loc469 = loc("p468.10781")
#loc470 = loc("p469.10785")
#loc471 = loc("p470.10935")
#loc472 = loc("p471.11003")
#loc473 = loc("p472.11008")
#loc474 = loc("p473.11014")
#loc475 = loc("p474.11019")
#loc476 = loc("p475.11102")
#loc477 = loc("p476.11125")
#loc478 = loc("p477.11138")
#loc479 = loc("p478.11142")
#loc480 = loc("p479.11187")
#loc481 = loc("p480.11191")
#loc482 = loc("p481.11341")
#loc483 = loc("p482.11409")
#loc484 = loc("p483.11414")
#loc485 = loc("p484.11420")
#loc486 = loc("p485.11425")
#loc487 = loc("p486.11508")
#loc488 = loc("p487.11531")
#loc489 = loc("p488.11544")
#loc490 = loc("p489.11548")
#loc491 = loc("p490.11593")
#loc492 = loc("p491.11597")
#loc493 = loc("p492.11747")
#loc494 = loc("p493.11815")
#loc495 = loc("p494.11820")
#loc496 = loc("p495.11826")
#loc497 = loc("p496.11831")
#loc498 = loc("p497.11914")
#loc499 = loc("p498.11937")
#loc500 = loc("p499.11950")
#loc501 = loc("p500.11954")
#loc502 = loc("p501.11999")
#loc503 = loc("p502.12003")
#loc504 = loc("p503.12153")
#loc505 = loc("p504.12221")
#loc506 = loc("p505.12226")
#loc507 = loc("p506.12232")
#loc508 = loc("p507.12237")
#loc509 = loc("p508.12320")
#loc510 = loc("p509.12343")
#loc511 = loc("p510.12356")
#loc512 = loc("p511.12360")
#loc513 = loc("p512.12405")
#loc514 = loc("p513.12409")
#loc515 = loc("p514.12559")
#loc516 = loc("p515.12627")
#loc517 = loc("p516.12632")
#loc518 = loc("p517.12638")
#loc519 = loc("p518.12643")
#loc520 = loc("p519.12726")
#loc521 = loc("p520.12749")
#loc522 = loc("p521.12762")
#loc523 = loc("p522.12766")
#loc524 = loc("p523.12811")
#loc525 = loc("p524.12815")
#loc526 = loc("p525.12965")
#loc527 = loc("p526.13033")
#loc528 = loc("p527.13038")
#loc529 = loc("p528.13044")
#loc530 = loc("p529.13049")
#loc531 = loc("p530.13132")
#loc532 = loc("p531.13155")
#loc533 = loc("p532.13168")
#loc534 = loc("p533.13172")
#loc535 = loc("p534.13217")
#loc536 = loc("p535.13221")
#loc537 = loc("p536.13371")
#loc538 = loc("p537.13439")
#loc539 = loc("p538.13444")
#loc540 = loc("p539.13450")
#loc541 = loc("p540.13455")
#loc542 = loc("p541.13538")
#loc543 = loc("p542.13561")
#loc544 = loc("p543.13574")
#loc545 = loc("p544.13578")
#loc546 = loc("p545.13623")
#loc547 = loc("p546.13627")
#loc548 = loc("p547.13777")
#loc549 = loc("p548.13845")
#loc550 = loc("p549.13850")
#loc551 = loc("p550.13856")
#loc552 = loc("p551.13861")
#loc553 = loc("p552.13944")
#loc554 = loc("p553.13967")
#loc555 = loc("p554.13980")
#loc556 = loc("p555.13984")
#loc557 = loc("p556.14029")
#loc558 = loc("p557.14033")
#loc559 = loc("p558.14183")
#loc560 = loc("p559.14251")
#loc561 = loc("p560.14256")
#loc562 = loc("p561.14262")
#loc563 = loc("p562.14267")
#loc564 = loc("p563.14350")
#loc565 = loc("p564.14373")
#loc566 = loc("p565.14386")
#loc567 = loc("p566.14390")
#loc568 = loc("p567.14435")
#loc569 = loc("p568.14439")
#loc570 = loc("p569.14589")
#loc571 = loc("p570.14657")
#loc572 = loc("p571.14662")
#loc573 = loc("p572.14668")
#loc574 = loc("p573.14673")
#loc575 = loc("p574.14756")
#loc576 = loc("p575.14779")
#loc577 = loc("p576.14792")
#loc578 = loc("p577.14796")
#loc579 = loc("p578.14841")
#loc580 = loc("p579.14845")
#loc581 = loc("p580.14995")
#loc582 = loc("p581.15063")
#loc583 = loc("p582.15068")
#loc584 = loc("p583.15074")
#loc585 = loc("p584.15079")
#loc586 = loc("p585.15162")
#loc587 = loc("p586.15185")
#loc588 = loc("p587.15198")
#loc589 = loc("p588.15202")
#loc590 = loc("p589.15247")
#loc591 = loc("p590.15251")
#loc592 = loc("p591.15401")
#loc593 = loc("p592.15469")
#loc594 = loc("p593.15474")
#loc595 = loc("p594.15480")
#loc596 = loc("p595.15485")
#loc597 = loc("p596.15568")
#loc598 = loc("p597.15591")
#loc599 = loc("p598.15604")
#loc600 = loc("p599.15608")
#loc601 = loc("p600.15653")
#loc602 = loc("p601.15657")
#loc603 = loc("p602.15807")
#loc604 = loc("p603.15875")
#loc605 = loc("p604.15880")
#loc606 = loc("p605.15886")
#loc607 = loc("p606.15891")
#loc608 = loc("p607.15974")
#loc609 = loc("p608.15997")
#loc610 = loc("p609.16010")
#loc611 = loc("p610.16014")
#loc612 = loc("p611.16059")
#loc613 = loc("p612.16063")
#loc614 = loc("p613.16213")
#loc615 = loc("p614.16281")
#loc616 = loc("p615.16286")
#loc617 = loc("p616.16292")
#loc618 = loc("p617.16297")
#loc619 = loc("p618.16380")
#loc633 = loc("reduce.1542")
#loc647 = loc("dot.1784")
#loc679 = loc("dot.1730")
#loc746 = loc("dot.1570")
#loc762 = loc("dot.1891")
#loc775 = loc("reduce.1909")
#loc787 = loc("dot.2021")
#loc811 = loc("dot.1937")
#loc817 = loc("sort.1957")
#loc831 = loc("scatter.1991")
#loc836 = loc("reduce.2063")
#loc840 = loc("reduce.2076")
#loc854 = loc("dot.2265")
#loc874 = loc("dot.2216")
#loc926 = loc("dot.2104")
#loc942 = loc("dot.2372")
#loc955 = loc("reduce.2390")
#loc967 = loc("dot.2502")
#loc990 = loc("dot.2418")
#loc995 = loc("sort.2438")
#loc1009 = loc("scatter.2472")
#loc1014 = loc("reduce.2544")
#loc1018 = loc("reduce.2557")
#loc1032 = loc("dot.2671")
#loc1052 = loc("dot.2622")
#loc1097 = loc("dot.2585")
#loc1113 = loc("dot.2778")
#loc1126 = loc("reduce.2796")
#loc1138 = loc("dot.2908")
#loc1161 = loc("dot.2824")
#loc1166 = loc("sort.2844")
#loc1180 = loc("scatter.2878")
#loc1185 = loc("reduce.2950")
#loc1189 = loc("reduce.2963")
#loc1203 = loc("dot.3077")
#loc1223 = loc("dot.3028")
#loc1268 = loc("dot.2991")
#loc1284 = loc("dot.3184")
#loc1297 = loc("reduce.3202")
#loc1309 = loc("dot.3314")
#loc1332 = loc("dot.3230")
#loc1337 = loc("sort.3250")
#loc1351 = loc("scatter.3284")
#loc1356 = loc("reduce.3356")
#loc1360 = loc("reduce.3369")
#loc1374 = loc("dot.3483")
#loc1394 = loc("dot.3434")
#loc1439 = loc("dot.3397")
#loc1455 = loc("dot.3590")
#loc1468 = loc("reduce.3608")
#loc1480 = loc("dot.3720")
#loc1503 = loc("dot.3636")
#loc1508 = loc("sort.3656")
#loc1522 = loc("scatter.3690")
#loc1527 = loc("reduce.3762")
#loc1531 = loc("reduce.3775")
#loc1545 = loc("dot.3889")
#loc1565 = loc("dot.3840")
#loc1610 = loc("dot.3803")
#loc1626 = loc("dot.3996")
#loc1639 = loc("reduce.4014")
#loc1651 = loc("dot.4126")
#loc1674 = loc("dot.4042")
#loc1679 = loc("sort.4062")
#loc1693 = loc("scatter.4096")
#loc1698 = loc("reduce.4168")
#loc1702 = loc("reduce.4181")
#loc1716 = loc("dot.4295")
#loc1736 = loc("dot.4246")
#loc1781 = loc("dot.4209")
#loc1797 = loc("dot.4402")
#loc1810 = loc("reduce.4420")
#loc1822 = loc("dot.4532")
#loc1845 = loc("dot.4448")
#loc1850 = loc("sort.4468")
#loc1864 = loc("scatter.4502")
#loc1869 = loc("reduce.4574")
#loc1873 = loc("reduce.4587")
#loc1887 = loc("dot.4701")
#loc1907 = loc("dot.4652")
#loc1952 = loc("dot.4615")
#loc1968 = loc("dot.4808")
#loc1981 = loc("reduce.4826")
#loc1993 = loc("dot.4938")
#loc2016 = loc("dot.4854")
#loc2021 = loc("sort.4874")
#loc2035 = loc("scatter.4908")
#loc2040 = loc("reduce.4980")
#loc2044 = loc("reduce.4993")
#loc2058 = loc("dot.5107")
#loc2078 = loc("dot.5058")
#loc2123 = loc("dot.5021")
#loc2139 = loc("dot.5214")
#loc2152 = loc("reduce.5232")
#loc2164 = loc("dot.5344")
#loc2187 = loc("dot.5260")
#loc2192 = loc("sort.5280")
#loc2206 = loc("scatter.5314")
#loc2211 = loc("reduce.5386")
#loc2215 = loc("reduce.5399")
#loc2229 = loc("dot.5513")
#loc2249 = loc("dot.5464")
#loc2294 = loc("dot.5427")
#loc2310 = loc("dot.5620")
#loc2323 = loc("reduce.5638")
#loc2335 = loc("dot.5750")
#loc2358 = loc("dot.5666")
#loc2363 = loc("sort.5686")
#loc2377 = loc("scatter.5720")
#loc2382 = loc("reduce.5792")
#loc2386 = loc("reduce.5805")
#loc2400 = loc("dot.5919")
#loc2420 = loc("dot.5870")
#loc2465 = loc("dot.5833")
#loc2481 = loc("dot.6026")
#loc2494 = loc("reduce.6044")
#loc2506 = loc("dot.6156")
#loc2529 = loc("dot.6072")
#loc2534 = loc("sort.6092")
#loc2548 = loc("scatter.6126")
#loc2553 = loc("reduce.6198")
#loc2557 = loc("reduce.6211")
#loc2571 = loc("dot.6325")
#loc2591 = loc("dot.6276")
#loc2636 = loc("dot.6239")
#loc2652 = loc("dot.6432")
#loc2665 = loc("reduce.6450")
#loc2677 = loc("dot.6562")
#loc2700 = loc("dot.6478")
#loc2705 = loc("sort.6498")
#loc2719 = loc("scatter.6532")
#loc2724 = loc("reduce.6604")
#loc2728 = loc("reduce.6617")
#loc2742 = loc("dot.6731")
#loc2762 = loc("dot.6682")
#loc2807 = loc("dot.6645")
#loc2823 = loc("dot.6838")
#loc2836 = loc("reduce.6856")
#loc2848 = loc("dot.6968")
#loc2871 = loc("dot.6884")
#loc2876 = loc("sort.6904")
#loc2890 = loc("scatter.6938")
#loc2895 = loc("reduce.7010")
#loc2899 = loc("reduce.7023")
#loc2913 = loc("dot.7137")
#loc2933 = loc("dot.7088")
#loc2978 = loc("dot.7051")
#loc2994 = loc("dot.7244")
#loc3007 = loc("reduce.7262")
#loc3019 = loc("dot.7374")
#loc3042 = loc("dot.7290")
#loc3047 = loc("sort.7310")
#loc3061 = loc("scatter.7344")
#loc3066 = loc("reduce.7416")
#loc3070 = loc("reduce.7429")
#loc3084 = loc("dot.7543")
#loc3104 = loc("dot.7494")
#loc3149 = loc("dot.7457")
#loc3165 = loc("dot.7650")
#loc3178 = loc("reduce.7668")
#loc3190 = loc("dot.7780")
#loc3213 = loc("dot.7696")
#loc3218 = loc("sort.7716")
#loc3232 = loc("scatter.7750")
#loc3237 = loc("reduce.7822")
#loc3241 = loc("reduce.7835")
#loc3255 = loc("dot.7949")
#loc3275 = loc("dot.7900")
#loc3320 = loc("dot.7863")
#loc3336 = loc("dot.8056")
#loc3349 = loc("reduce.8074")
#loc3361 = loc("dot.8186")
#loc3384 = loc("dot.8102")
#loc3389 = loc("sort.8122")
#loc3403 = loc("scatter.8156")
#loc3408 = loc("reduce.8228")
#loc3412 = loc("reduce.8241")
#loc3426 = loc("dot.8355")
#loc3446 = loc("dot.8306")
#loc3491 = loc("dot.8269")
#loc3507 = loc("dot.8462")
#loc3520 = loc("reduce.8480")
#loc3532 = loc("dot.8592")
#loc3555 = loc("dot.8508")
#loc3560 = loc("sort.8528")
#loc3574 = loc("scatter.8562")
#loc3579 = loc("reduce.8634")
#loc3583 = loc("reduce.8647")
#loc3597 = loc("dot.8761")
#loc3617 = loc("dot.8712")
#loc3662 = loc("dot.8675")
#loc3678 = loc("dot.8868")
#loc3691 = loc("reduce.8886")
#loc3703 = loc("dot.8998")
#loc3726 = loc("dot.8914")
#loc3731 = loc("sort.8934")
#loc3745 = loc("scatter.8968")
#loc3750 = loc("reduce.9040")
#loc3754 = loc("reduce.9053")
#loc3768 = loc("dot.9167")
#loc3788 = loc("dot.9118")
#loc3833 = loc("dot.9081")
#loc3849 = loc("dot.9274")
#loc3862 = loc("reduce.9292")
#loc3874 = loc("dot.9404")
#loc3897 = loc("dot.9320")
#loc3902 = loc("sort.9340")
#loc3916 = loc("scatter.9374")
#loc3921 = loc("reduce.9446")
#loc3925 = loc("reduce.9459")
#loc3939 = loc("dot.9573")
#loc3959 = loc("dot.9524")
#loc4004 = loc("dot.9487")
#loc4020 = loc("dot.9680")
#loc4033 = loc("reduce.9698")
#loc4045 = loc("dot.9810")
#loc4068 = loc("dot.9726")
#loc4073 = loc("sort.9746")
#loc4087 = loc("scatter.9780")
#loc4092 = loc("reduce.9852")
#loc4096 = loc("reduce.9865")
#loc4110 = loc("dot.9979")
#loc4130 = loc("dot.9930")
#loc4175 = loc("dot.9893")
#loc4191 = loc("dot.10086")
#loc4204 = loc("reduce.10104")
#loc4216 = loc("dot.10216")
#loc4239 = loc("dot.10132")
#loc4244 = loc("sort.10152")
#loc4258 = loc("scatter.10186")
#loc4263 = loc("reduce.10258")
#loc4267 = loc("reduce.10271")
#loc4281 = loc("dot.10385")
#loc4301 = loc("dot.10336")
#loc4346 = loc("dot.10299")
#loc4362 = loc("dot.10492")
#loc4375 = loc("reduce.10510")
#loc4387 = loc("dot.10622")
#loc4410 = loc("dot.10538")
#loc4415 = loc("sort.10558")
#loc4429 = loc("scatter.10592")
#loc4434 = loc("reduce.10664")
#loc4438 = loc("reduce.10677")
#loc4452 = loc("dot.10791")
#loc4472 = loc("dot.10742")
#loc4517 = loc("dot.10705")
#loc4533 = loc("dot.10898")
#loc4546 = loc("reduce.10916")
#loc4558 = loc("dot.11028")
#loc4581 = loc("dot.10944")
#loc4586 = loc("sort.10964")
#loc4600 = loc("scatter.10998")
#loc4605 = loc("reduce.11070")
#loc4609 = loc("reduce.11083")
#loc4623 = loc("dot.11197")
#loc4643 = loc("dot.11148")
#loc4688 = loc("dot.11111")
#loc4704 = loc("dot.11304")
#loc4717 = loc("reduce.11322")
#loc4729 = loc("dot.11434")
#loc4752 = loc("dot.11350")
#loc4757 = loc("sort.11370")
#loc4771 = loc("scatter.11404")
#loc4776 = loc("reduce.11476")
#loc4780 = loc("reduce.11489")
#loc4794 = loc("dot.11603")
#loc4814 = loc("dot.11554")
#loc4859 = loc("dot.11517")
#loc4875 = loc("dot.11710")
#loc4888 = loc("reduce.11728")
#loc4900 = loc("dot.11840")
#loc4923 = loc("dot.11756")
#loc4928 = loc("sort.11776")
#loc4942 = loc("scatter.11810")
#loc4947 = loc("reduce.11882")
#loc4951 = loc("reduce.11895")
#loc4965 = loc("dot.12009")
#loc4985 = loc("dot.11960")
#loc5030 = loc("dot.11923")
#loc5046 = loc("dot.12116")
#loc5059 = loc("reduce.12134")
#loc5071 = loc("dot.12246")
#loc5094 = loc("dot.12162")
#loc5099 = loc("sort.12182")
#loc5113 = loc("scatter.12216")
#loc5118 = loc("reduce.12288")
#loc5122 = loc("reduce.12301")
#loc5136 = loc("dot.12415")
#loc5156 = loc("dot.12366")
#loc5201 = loc("dot.12329")
#loc5217 = loc("dot.12522")
#loc5230 = loc("reduce.12540")
#loc5242 = loc("dot.12652")
#loc5265 = loc("dot.12568")
#loc5270 = loc("sort.12588")
#loc5284 = loc("scatter.12622")
#loc5289 = loc("reduce.12694")
#loc5293 = loc("reduce.12707")
#loc5307 = loc("dot.12821")
#loc5327 = loc("dot.12772")
#loc5372 = loc("dot.12735")
#loc5388 = loc("dot.12928")
#loc5401 = loc("reduce.12946")
#loc5413 = loc("dot.13058")
#loc5436 = loc("dot.12974")
#loc5441 = loc("sort.12994")
#loc5455 = loc("scatter.13028")
#loc5460 = loc("reduce.13100")
#loc5464 = loc("reduce.13113")
#loc5478 = loc("dot.13227")
#loc5498 = loc("dot.13178")
#loc5543 = loc("dot.13141")
#loc5559 = loc("dot.13334")
#loc5572 = loc("reduce.13352")
#loc5584 = loc("dot.13464")
#loc5607 = loc("dot.13380")
#loc5612 = loc("sort.13400")
#loc5626 = loc("scatter.13434")
#loc5631 = loc("reduce.13506")
#loc5635 = loc("reduce.13519")
#loc5649 = loc("dot.13633")
#loc5669 = loc("dot.13584")
#loc5714 = loc("dot.13547")
#loc5730 = loc("dot.13740")
#loc5743 = loc("reduce.13758")
#loc5755 = loc("dot.13870")
#loc5778 = loc("dot.13786")
#loc5783 = loc("sort.13806")
#loc5797 = loc("scatter.13840")
#loc5802 = loc("reduce.13912")
#loc5806 = loc("reduce.13925")
#loc5820 = loc("dot.14039")
#loc5840 = loc("dot.13990")
#loc5885 = loc("dot.13953")
#loc5901 = loc("dot.14146")
#loc5914 = loc("reduce.14164")
#loc5926 = loc("dot.14276")
#loc5949 = loc("dot.14192")
#loc5954 = loc("sort.14212")
#loc5968 = loc("scatter.14246")
#loc5973 = loc("reduce.14318")
#loc5977 = loc("reduce.14331")
#loc5991 = loc("dot.14445")
#loc6011 = loc("dot.14396")
#loc6056 = loc("dot.14359")
#loc6072 = loc("dot.14552")
#loc6085 = loc("reduce.14570")
#loc6097 = loc("dot.14682")
#loc6120 = loc("dot.14598")
#loc6125 = loc("sort.14618")
#loc6139 = loc("scatter.14652")
#loc6144 = loc("reduce.14724")
#loc6148 = loc("reduce.14737")
#loc6162 = loc("dot.14851")
#loc6182 = loc("dot.14802")
#loc6227 = loc("dot.14765")
#loc6243 = loc("dot.14958")
#loc6256 = loc("reduce.14976")
#loc6268 = loc("dot.15088")
#loc6291 = loc("dot.15004")
#loc6296 = loc("sort.15024")
#loc6310 = loc("scatter.15058")
#loc6315 = loc("reduce.15130")
#loc6319 = loc("reduce.15143")
#loc6333 = loc("dot.15257")
#loc6353 = loc("dot.15208")
#loc6398 = loc("dot.15171")
#loc6414 = loc("dot.15364")
#loc6427 = loc("reduce.15382")
#loc6439 = loc("dot.15494")
#loc6462 = loc("dot.15410")
#loc6467 = loc("sort.15430")
#loc6481 = loc("scatter.15464")
#loc6486 = loc("reduce.15536")
#loc6490 = loc("reduce.15549")
#loc6504 = loc("dot.15663")
#loc6524 = loc("dot.15614")
#loc6569 = loc("dot.15577")
#loc6585 = loc("dot.15770")
#loc6598 = loc("reduce.15788")
#loc6610 = loc("dot.15900")
#loc6633 = loc("dot.15816")
#loc6638 = loc("sort.15836")
#loc6652 = loc("scatter.15870")
#loc6657 = loc("reduce.15942")
#loc6661 = loc("reduce.15955")
#loc6675 = loc("dot.16069")
#loc6695 = loc("dot.16020")
#loc6740 = loc("dot.15983")
#loc6756 = loc("dot.16176")
#loc6769 = loc("reduce.16194")
#loc6781 = loc("dot.16306")
#loc6804 = loc("dot.16222")
#loc6809 = loc("sort.16242")
#loc6823 = loc("scatter.16276")
#loc6828 = loc("reduce.16348")
#loc6832 = loc("reduce.16361")
#loc6846 = loc("dot.16389")
module @SyncTensorsGraph.16392 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]> loc(#loc)
  func.func @main(%arg0: tensor<201088x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"} loc("p0.1"), %arg1: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_mlp_router_bias"} loc("p1.12"), %arg2: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_mlp_router_weight"} loc("p2.19"), %arg3: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_o_proj_bias"} loc("p3.29"), %arg4: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_o_proj_weight"} loc("p4.33"), %arg5: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_v_proj_bias"} loc("p5.39"), %arg6: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_v_proj_weight"} loc("p6.43"), %arg7: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_mlp_router_bias"} loc("p7.54"), %arg8: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_mlp_router_weight"} loc("p8.61"), %arg9: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_o_proj_bias"} loc("p9.71"), %arg10: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_o_proj_weight"} loc("p10.75"), %arg11: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_v_proj_bias"} loc("p11.81"), %arg12: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_v_proj_weight"} loc("p12.85"), %arg13: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_mlp_router_bias"} loc("p13.96"), %arg14: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_mlp_router_weight"} loc("p14.103"), %arg15: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_o_proj_bias"} loc("p15.113"), %arg16: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_o_proj_weight"} loc("p16.117"), %arg17: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_v_proj_bias"} loc("p17.123"), %arg18: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_v_proj_weight"} loc("p18.127"), %arg19: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_mlp_router_bias"} loc("p19.138"), %arg20: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_mlp_router_weight"} loc("p20.145"), %arg21: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_o_proj_bias"} loc("p21.155"), %arg22: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_o_proj_weight"} loc("p22.159"), %arg23: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_v_proj_bias"} loc("p23.165"), %arg24: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_v_proj_weight"} loc("p24.169"), %arg25: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_mlp_router_bias"} loc("p25.180"), %arg26: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_mlp_router_weight"} loc("p26.187"), %arg27: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_o_proj_bias"} loc("p27.197"), %arg28: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_o_proj_weight"} loc("p28.201"), %arg29: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_v_proj_bias"} loc("p29.207"), %arg30: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_v_proj_weight"} loc("p30.211"), %arg31: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_mlp_router_bias"} loc("p31.222"), %arg32: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_mlp_router_weight"} loc("p32.229"), %arg33: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_o_proj_bias"} loc("p33.239"), %arg34: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_o_proj_weight"} loc("p34.243"), %arg35: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_v_proj_bias"} loc("p35.249"), %arg36: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_v_proj_weight"} loc("p36.253"), %arg37: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_mlp_router_bias"} loc("p37.264"), %arg38: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_mlp_router_weight"} loc("p38.271"), %arg39: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_o_proj_bias"} loc("p39.281"), %arg40: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_o_proj_weight"} loc("p40.285"), %arg41: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_v_proj_bias"} loc("p41.291"), %arg42: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_v_proj_weight"} loc("p42.295"), %arg43: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_mlp_router_bias"} loc("p43.306"), %arg44: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_mlp_router_weight"} loc("p44.313"), %arg45: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_o_proj_bias"} loc("p45.323"), %arg46: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_o_proj_weight"} loc("p46.327"), %arg47: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_v_proj_bias"} loc("p47.333"), %arg48: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_v_proj_weight"} loc("p48.337"), %arg49: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_mlp_router_bias"} loc("p49.348"), %arg50: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_mlp_router_weight"} loc("p50.355"), %arg51: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_o_proj_bias"} loc("p51.365"), %arg52: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_o_proj_weight"} loc("p52.369"), %arg53: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_v_proj_bias"} loc("p53.375"), %arg54: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_v_proj_weight"} loc("p54.379"), %arg55: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_mlp_router_bias"} loc("p55.390"), %arg56: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_mlp_router_weight"} loc("p56.397"), %arg57: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_o_proj_bias"} loc("p57.407"), %arg58: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_o_proj_weight"} loc("p58.411"), %arg59: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_v_proj_bias"} loc("p59.417"), %arg60: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_v_proj_weight"} loc("p60.421"), %arg61: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_mlp_router_bias"} loc("p61.432"), %arg62: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_mlp_router_weight"} loc("p62.439"), %arg63: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_o_proj_bias"} loc("p63.449"), %arg64: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_o_proj_weight"} loc("p64.453"), %arg65: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_v_proj_bias"} loc("p65.459"), %arg66: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_v_proj_weight"} loc("p66.463"), %arg67: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_mlp_router_bias"} loc("p67.474"), %arg68: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_mlp_router_weight"} loc("p68.481"), %arg69: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_o_proj_bias"} loc("p69.491"), %arg70: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_o_proj_weight"} loc("p70.495"), %arg71: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_v_proj_bias"} loc("p71.501"), %arg72: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_v_proj_weight"} loc("p72.505"), %arg73: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_mlp_router_bias"} loc("p73.516"), %arg74: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_mlp_router_weight"} loc("p74.523"), %arg75: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_o_proj_bias"} loc("p75.533"), %arg76: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_o_proj_weight"} loc("p76.537"), %arg77: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_v_proj_bias"} loc("p77.543"), %arg78: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_v_proj_weight"} loc("p78.547"), %arg79: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_mlp_router_bias"} loc("p79.558"), %arg80: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_mlp_router_weight"} loc("p80.565"), %arg81: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_o_proj_bias"} loc("p81.575"), %arg82: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_o_proj_weight"} loc("p82.579"), %arg83: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_v_proj_bias"} loc("p83.585"), %arg84: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_v_proj_weight"} loc("p84.589"), %arg85: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_mlp_router_bias"} loc("p85.600"), %arg86: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_mlp_router_weight"} loc("p86.607"), %arg87: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_o_proj_bias"} loc("p87.617"), %arg88: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_o_proj_weight"} loc("p88.621"), %arg89: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_v_proj_bias"} loc("p89.627"), %arg90: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_v_proj_weight"} loc("p90.631"), %arg91: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_mlp_router_bias"} loc("p91.642"), %arg92: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_mlp_router_weight"} loc("p92.649"), %arg93: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_o_proj_bias"} loc("p93.659"), %arg94: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_o_proj_weight"} loc("p94.663"), %arg95: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_v_proj_bias"} loc("p95.669"), %arg96: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_v_proj_weight"} loc("p96.673"), %arg97: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_mlp_router_bias"} loc("p97.684"), %arg98: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_mlp_router_weight"} loc("p98.691"), %arg99: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_o_proj_bias"} loc("p99.701"), %arg100: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_o_proj_weight"} loc("p100.705"), %arg101: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_v_proj_bias"} loc("p101.711"), %arg102: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_v_proj_weight"} loc("p102.715"), %arg103: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_mlp_router_bias"} loc("p103.726"), %arg104: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_mlp_router_weight"} loc("p104.733"), %arg105: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_o_proj_bias"} loc("p105.743"), %arg106: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_o_proj_weight"} loc("p106.747"), %arg107: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_v_proj_bias"} loc("p107.753"), %arg108: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_v_proj_weight"} loc("p108.757"), %arg109: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_mlp_router_bias"} loc("p109.768"), %arg110: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_mlp_router_weight"} loc("p110.775"), %arg111: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_o_proj_bias"} loc("p111.785"), %arg112: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_o_proj_weight"} loc("p112.789"), %arg113: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_v_proj_bias"} loc("p113.795"), %arg114: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_v_proj_weight"} loc("p114.799"), %arg115: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_mlp_router_bias"} loc("p115.810"), %arg116: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_mlp_router_weight"} loc("p116.817"), %arg117: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_o_proj_bias"} loc("p117.827"), %arg118: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_o_proj_weight"} loc("p118.831"), %arg119: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_v_proj_bias"} loc("p119.837"), %arg120: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_v_proj_weight"} loc("p120.841"), %arg121: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_mlp_router_bias"} loc("p121.852"), %arg122: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_mlp_router_weight"} loc("p122.859"), %arg123: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_o_proj_bias"} loc("p123.869"), %arg124: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_o_proj_weight"} loc("p124.873"), %arg125: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_v_proj_bias"} loc("p125.879"), %arg126: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_v_proj_weight"} loc("p126.883"), %arg127: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_mlp_router_bias"} loc("p127.894"), %arg128: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_mlp_router_weight"} loc("p128.901"), %arg129: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_o_proj_bias"} loc("p129.911"), %arg130: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_o_proj_weight"} loc("p130.915"), %arg131: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_v_proj_bias"} loc("p131.921"), %arg132: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_v_proj_weight"} loc("p132.925"), %arg133: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_mlp_router_bias"} loc("p133.936"), %arg134: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_mlp_router_weight"} loc("p134.943"), %arg135: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_o_proj_bias"} loc("p135.953"), %arg136: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_o_proj_weight"} loc("p136.957"), %arg137: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_v_proj_bias"} loc("p137.963"), %arg138: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_v_proj_weight"} loc("p138.967"), %arg139: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_mlp_router_bias"} loc("p139.978"), %arg140: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_mlp_router_weight"} loc("p140.985"), %arg141: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_o_proj_bias"} loc("p141.995"), %arg142: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_o_proj_weight"} loc("p142.999"), %arg143: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_v_proj_bias"} loc("p143.1005"), %arg144: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_v_proj_weight"} loc("p144.1009"), %arg145: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_mlp_router_bias"} loc("p145.1020"), %arg146: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_mlp_router_weight"} loc("p146.1027"), %arg147: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_o_proj_bias"} loc("p147.1037"), %arg148: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_o_proj_weight"} loc("p148.1041"), %arg149: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_v_proj_bias"} loc("p149.1047"), %arg150: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_v_proj_weight"} loc("p150.1051"), %arg151: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_mlp_router_bias"} loc("p151.1062"), %arg152: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_mlp_router_weight"} loc("p152.1069"), %arg153: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_o_proj_bias"} loc("p153.1079"), %arg154: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_o_proj_weight"} loc("p154.1083"), %arg155: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_v_proj_bias"} loc("p155.1089"), %arg156: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_v_proj_weight"} loc("p156.1093"), %arg157: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_mlp_router_bias"} loc("p157.1104"), %arg158: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_mlp_router_weight"} loc("p158.1111"), %arg159: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_o_proj_bias"} loc("p159.1121"), %arg160: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_o_proj_weight"} loc("p160.1125"), %arg161: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_v_proj_bias"} loc("p161.1131"), %arg162: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_v_proj_weight"} loc("p162.1135"), %arg163: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_mlp_router_bias"} loc("p163.1146"), %arg164: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_mlp_router_weight"} loc("p164.1153"), %arg165: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_o_proj_bias"} loc("p165.1163"), %arg166: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_o_proj_weight"} loc("p166.1167"), %arg167: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_v_proj_bias"} loc("p167.1173"), %arg168: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_v_proj_weight"} loc("p168.1177"), %arg169: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_mlp_router_bias"} loc("p169.1188"), %arg170: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_mlp_router_weight"} loc("p170.1195"), %arg171: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_o_proj_bias"} loc("p171.1205"), %arg172: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_o_proj_weight"} loc("p172.1209"), %arg173: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_v_proj_bias"} loc("p173.1215"), %arg174: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_v_proj_weight"} loc("p174.1219"), %arg175: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_mlp_router_bias"} loc("p175.1230"), %arg176: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_mlp_router_weight"} loc("p176.1237"), %arg177: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_o_proj_bias"} loc("p177.1247"), %arg178: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_o_proj_weight"} loc("p178.1251"), %arg179: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_v_proj_bias"} loc("p179.1257"), %arg180: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_v_proj_weight"} loc("p180.1261"), %arg181: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_mlp_router_bias"} loc("p181.1272"), %arg182: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_mlp_router_weight"} loc("p182.1279"), %arg183: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_o_proj_bias"} loc("p183.1289"), %arg184: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_o_proj_weight"} loc("p184.1293"), %arg185: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_v_proj_bias"} loc("p185.1299"), %arg186: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_v_proj_weight"} loc("p186.1303"), %arg187: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_mlp_router_bias"} loc("p187.1314"), %arg188: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_mlp_router_weight"} loc("p188.1321"), %arg189: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_o_proj_bias"} loc("p189.1331"), %arg190: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_o_proj_weight"} loc("p190.1335"), %arg191: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_v_proj_bias"} loc("p191.1341"), %arg192: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_v_proj_weight"} loc("p192.1345"), %arg193: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_mlp_router_bias"} loc("p193.1356"), %arg194: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_mlp_router_weight"} loc("p194.1363"), %arg195: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_o_proj_bias"} loc("p195.1373"), %arg196: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_o_proj_weight"} loc("p196.1377"), %arg197: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_v_proj_bias"} loc("p197.1383"), %arg198: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_v_proj_weight"} loc("p198.1387"), %arg199: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_mlp_router_bias"} loc("p199.1398"), %arg200: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_mlp_router_weight"} loc("p200.1405"), %arg201: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_o_proj_bias"} loc("p201.1415"), %arg202: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_o_proj_weight"} loc("p202.1419"), %arg203: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_v_proj_bias"} loc("p203.1425"), %arg204: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_v_proj_weight"} loc("p204.1429"), %arg205: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_mlp_router_bias"} loc("p205.1440"), %arg206: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_mlp_router_weight"} loc("p206.1447"), %arg207: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_o_proj_bias"} loc("p207.1457"), %arg208: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_o_proj_weight"} loc("p208.1461"), %arg209: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_v_proj_bias"} loc("p209.1467"), %arg210: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_v_proj_weight"} loc("p210.1471"), %arg211: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_mlp_router_bias"} loc("p211.1482"), %arg212: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_mlp_router_weight"} loc("p212.1489"), %arg213: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_o_proj_bias"} loc("p213.1499"), %arg214: tensor<2880x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_o_proj_weight"} loc("p214.1503"), %arg215: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_v_proj_bias"} loc("p215.1509"), %arg216: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_v_proj_weight"} loc("p216.1513"), %arg217: tensor<1x128xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"} loc("p217.1521"), %arg218: tensor<201088x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p218.1526"), %arg219: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_input_layernorm_weight"} loc("p219.1561"), %arg220: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_sinks"} loc("p220.1584"), %arg221: tensor<1x128xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"} loc("p221.1640"), %arg222: tensor<i1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p222.1663"), %arg223: tensor<32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p223.1704"), %arg224: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_k_proj_bias"} loc("p224.1720"), %arg225: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_k_proj_weight"} loc("p225.1724"), %arg226: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_q_proj_bias"} loc("p226.1774"), %arg227: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_self_attn_q_proj_weight"} loc("p227.1778"), %arg228: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_post_attention_layernorm_weight"} loc("p228.1928"), %arg229: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_mlp_experts_down_proj_bias"} loc("p229.1996"), %arg230: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_mlp_experts_down_proj"} loc("p230.2001"), %arg231: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_mlp_experts_gate_up_proj_bias"} loc("p231.2007"), %arg232: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_0_mlp_experts_gate_up_proj"} loc("p232.2012"), %arg233: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_input_layernorm_weight"} loc("p233.2095"), %arg234: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_sinks"} loc("p234.2118"), %arg235: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_k_proj_bias"} loc("p235.2206"), %arg236: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_k_proj_weight"} loc("p236.2210"), %arg237: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_q_proj_bias"} loc("p237.2255"), %arg238: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_self_attn_q_proj_weight"} loc("p238.2259"), %arg239: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_post_attention_layernorm_weight"} loc("p239.2409"), %arg240: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_mlp_experts_down_proj_bias"} loc("p240.2477"), %arg241: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_mlp_experts_down_proj"} loc("p241.2482"), %arg242: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_mlp_experts_gate_up_proj_bias"} loc("p242.2488"), %arg243: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_1_mlp_experts_gate_up_proj"} loc("p243.2493"), %arg244: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_input_layernorm_weight"} loc("p244.2576"), %arg245: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_sinks"} loc("p245.2599"), %arg246: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_k_proj_bias"} loc("p246.2612"), %arg247: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_k_proj_weight"} loc("p247.2616"), %arg248: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_q_proj_bias"} loc("p248.2661"), %arg249: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_self_attn_q_proj_weight"} loc("p249.2665"), %arg250: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_post_attention_layernorm_weight"} loc("p250.2815"), %arg251: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_mlp_experts_down_proj_bias"} loc("p251.2883"), %arg252: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_mlp_experts_down_proj"} loc("p252.2888"), %arg253: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_mlp_experts_gate_up_proj_bias"} loc("p253.2894"), %arg254: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_2_mlp_experts_gate_up_proj"} loc("p254.2899"), %arg255: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_input_layernorm_weight"} loc("p255.2982"), %arg256: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_sinks"} loc("p256.3005"), %arg257: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_k_proj_bias"} loc("p257.3018"), %arg258: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_k_proj_weight"} loc("p258.3022"), %arg259: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_q_proj_bias"} loc("p259.3067"), %arg260: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_self_attn_q_proj_weight"} loc("p260.3071"), %arg261: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_post_attention_layernorm_weight"} loc("p261.3221"), %arg262: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_mlp_experts_down_proj_bias"} loc("p262.3289"), %arg263: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_mlp_experts_down_proj"} loc("p263.3294"), %arg264: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_mlp_experts_gate_up_proj_bias"} loc("p264.3300"), %arg265: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_3_mlp_experts_gate_up_proj"} loc("p265.3305"), %arg266: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_input_layernorm_weight"} loc("p266.3388"), %arg267: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_sinks"} loc("p267.3411"), %arg268: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_k_proj_bias"} loc("p268.3424"), %arg269: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_k_proj_weight"} loc("p269.3428"), %arg270: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_q_proj_bias"} loc("p270.3473"), %arg271: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_self_attn_q_proj_weight"} loc("p271.3477"), %arg272: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_post_attention_layernorm_weight"} loc("p272.3627"), %arg273: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_mlp_experts_down_proj_bias"} loc("p273.3695"), %arg274: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_mlp_experts_down_proj"} loc("p274.3700"), %arg275: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_mlp_experts_gate_up_proj_bias"} loc("p275.3706"), %arg276: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_4_mlp_experts_gate_up_proj"} loc("p276.3711"), %arg277: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_input_layernorm_weight"} loc("p277.3794"), %arg278: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_sinks"} loc("p278.3817"), %arg279: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_k_proj_bias"} loc("p279.3830"), %arg280: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_k_proj_weight"} loc("p280.3834"), %arg281: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_q_proj_bias"} loc("p281.3879"), %arg282: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_self_attn_q_proj_weight"} loc("p282.3883"), %arg283: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_post_attention_layernorm_weight"} loc("p283.4033"), %arg284: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_mlp_experts_down_proj_bias"} loc("p284.4101"), %arg285: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_mlp_experts_down_proj"} loc("p285.4106"), %arg286: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_mlp_experts_gate_up_proj_bias"} loc("p286.4112"), %arg287: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_5_mlp_experts_gate_up_proj"} loc("p287.4117"), %arg288: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_input_layernorm_weight"} loc("p288.4200"), %arg289: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_sinks"} loc("p289.4223"), %arg290: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_k_proj_bias"} loc("p290.4236"), %arg291: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_k_proj_weight"} loc("p291.4240"), %arg292: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_q_proj_bias"} loc("p292.4285"), %arg293: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_self_attn_q_proj_weight"} loc("p293.4289"), %arg294: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_post_attention_layernorm_weight"} loc("p294.4439"), %arg295: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_mlp_experts_down_proj_bias"} loc("p295.4507"), %arg296: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_mlp_experts_down_proj"} loc("p296.4512"), %arg297: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_mlp_experts_gate_up_proj_bias"} loc("p297.4518"), %arg298: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_6_mlp_experts_gate_up_proj"} loc("p298.4523"), %arg299: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_input_layernorm_weight"} loc("p299.4606"), %arg300: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_sinks"} loc("p300.4629"), %arg301: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_k_proj_bias"} loc("p301.4642"), %arg302: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_k_proj_weight"} loc("p302.4646"), %arg303: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_q_proj_bias"} loc("p303.4691"), %arg304: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_self_attn_q_proj_weight"} loc("p304.4695"), %arg305: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_post_attention_layernorm_weight"} loc("p305.4845"), %arg306: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_mlp_experts_down_proj_bias"} loc("p306.4913"), %arg307: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_mlp_experts_down_proj"} loc("p307.4918"), %arg308: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_mlp_experts_gate_up_proj_bias"} loc("p308.4924"), %arg309: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_7_mlp_experts_gate_up_proj"} loc("p309.4929"), %arg310: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_input_layernorm_weight"} loc("p310.5012"), %arg311: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_sinks"} loc("p311.5035"), %arg312: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_k_proj_bias"} loc("p312.5048"), %arg313: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_k_proj_weight"} loc("p313.5052"), %arg314: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_q_proj_bias"} loc("p314.5097"), %arg315: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_self_attn_q_proj_weight"} loc("p315.5101"), %arg316: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_post_attention_layernorm_weight"} loc("p316.5251"), %arg317: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_mlp_experts_down_proj_bias"} loc("p317.5319"), %arg318: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_mlp_experts_down_proj"} loc("p318.5324"), %arg319: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_mlp_experts_gate_up_proj_bias"} loc("p319.5330"), %arg320: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_8_mlp_experts_gate_up_proj"} loc("p320.5335"), %arg321: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_input_layernorm_weight"} loc("p321.5418"), %arg322: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_sinks"} loc("p322.5441"), %arg323: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_k_proj_bias"} loc("p323.5454"), %arg324: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_k_proj_weight"} loc("p324.5458"), %arg325: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_q_proj_bias"} loc("p325.5503"), %arg326: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_self_attn_q_proj_weight"} loc("p326.5507"), %arg327: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_post_attention_layernorm_weight"} loc("p327.5657"), %arg328: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_mlp_experts_down_proj_bias"} loc("p328.5725"), %arg329: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_mlp_experts_down_proj"} loc("p329.5730"), %arg330: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_mlp_experts_gate_up_proj_bias"} loc("p330.5736"), %arg331: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_9_mlp_experts_gate_up_proj"} loc("p331.5741"), %arg332: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_input_layernorm_weight"} loc("p332.5824"), %arg333: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_sinks"} loc("p333.5847"), %arg334: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_k_proj_bias"} loc("p334.5860"), %arg335: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_k_proj_weight"} loc("p335.5864"), %arg336: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_q_proj_bias"} loc("p336.5909"), %arg337: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_self_attn_q_proj_weight"} loc("p337.5913"), %arg338: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_post_attention_layernorm_weight"} loc("p338.6063"), %arg339: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_mlp_experts_down_proj_bias"} loc("p339.6131"), %arg340: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_mlp_experts_down_proj"} loc("p340.6136"), %arg341: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_mlp_experts_gate_up_proj_bias"} loc("p341.6142"), %arg342: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_10_mlp_experts_gate_up_proj"} loc("p342.6147"), %arg343: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_input_layernorm_weight"} loc("p343.6230"), %arg344: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_sinks"} loc("p344.6253"), %arg345: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_k_proj_bias"} loc("p345.6266"), %arg346: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_k_proj_weight"} loc("p346.6270"), %arg347: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_q_proj_bias"} loc("p347.6315"), %arg348: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_self_attn_q_proj_weight"} loc("p348.6319"), %arg349: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_post_attention_layernorm_weight"} loc("p349.6469"), %arg350: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_mlp_experts_down_proj_bias"} loc("p350.6537"), %arg351: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_mlp_experts_down_proj"} loc("p351.6542"), %arg352: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_mlp_experts_gate_up_proj_bias"} loc("p352.6548"), %arg353: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_11_mlp_experts_gate_up_proj"} loc("p353.6553"), %arg354: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_input_layernorm_weight"} loc("p354.6636"), %arg355: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_sinks"} loc("p355.6659"), %arg356: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_k_proj_bias"} loc("p356.6672"), %arg357: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_k_proj_weight"} loc("p357.6676"), %arg358: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_q_proj_bias"} loc("p358.6721"), %arg359: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_self_attn_q_proj_weight"} loc("p359.6725"), %arg360: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_post_attention_layernorm_weight"} loc("p360.6875"), %arg361: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_mlp_experts_down_proj_bias"} loc("p361.6943"), %arg362: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_mlp_experts_down_proj"} loc("p362.6948"), %arg363: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_mlp_experts_gate_up_proj_bias"} loc("p363.6954"), %arg364: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_12_mlp_experts_gate_up_proj"} loc("p364.6959"), %arg365: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_input_layernorm_weight"} loc("p365.7042"), %arg366: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_sinks"} loc("p366.7065"), %arg367: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_k_proj_bias"} loc("p367.7078"), %arg368: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_k_proj_weight"} loc("p368.7082"), %arg369: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_q_proj_bias"} loc("p369.7127"), %arg370: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_self_attn_q_proj_weight"} loc("p370.7131"), %arg371: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_post_attention_layernorm_weight"} loc("p371.7281"), %arg372: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_mlp_experts_down_proj_bias"} loc("p372.7349"), %arg373: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_mlp_experts_down_proj"} loc("p373.7354"), %arg374: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_mlp_experts_gate_up_proj_bias"} loc("p374.7360"), %arg375: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_13_mlp_experts_gate_up_proj"} loc("p375.7365"), %arg376: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_input_layernorm_weight"} loc("p376.7448"), %arg377: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_sinks"} loc("p377.7471"), %arg378: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_k_proj_bias"} loc("p378.7484"), %arg379: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_k_proj_weight"} loc("p379.7488"), %arg380: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_q_proj_bias"} loc("p380.7533"), %arg381: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_self_attn_q_proj_weight"} loc("p381.7537"), %arg382: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_post_attention_layernorm_weight"} loc("p382.7687"), %arg383: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_mlp_experts_down_proj_bias"} loc("p383.7755"), %arg384: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_mlp_experts_down_proj"} loc("p384.7760"), %arg385: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_mlp_experts_gate_up_proj_bias"} loc("p385.7766"), %arg386: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_14_mlp_experts_gate_up_proj"} loc("p386.7771"), %arg387: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_input_layernorm_weight"} loc("p387.7854"), %arg388: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_sinks"} loc("p388.7877"), %arg389: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_k_proj_bias"} loc("p389.7890"), %arg390: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_k_proj_weight"} loc("p390.7894"), %arg391: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_q_proj_bias"} loc("p391.7939"), %arg392: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_self_attn_q_proj_weight"} loc("p392.7943"), %arg393: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_post_attention_layernorm_weight"} loc("p393.8093"), %arg394: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_mlp_experts_down_proj_bias"} loc("p394.8161"), %arg395: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_mlp_experts_down_proj"} loc("p395.8166"), %arg396: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_mlp_experts_gate_up_proj_bias"} loc("p396.8172"), %arg397: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_15_mlp_experts_gate_up_proj"} loc("p397.8177"), %arg398: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_input_layernorm_weight"} loc("p398.8260"), %arg399: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_sinks"} loc("p399.8283"), %arg400: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_k_proj_bias"} loc("p400.8296"), %arg401: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_k_proj_weight"} loc("p401.8300"), %arg402: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_q_proj_bias"} loc("p402.8345"), %arg403: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_self_attn_q_proj_weight"} loc("p403.8349"), %arg404: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_post_attention_layernorm_weight"} loc("p404.8499"), %arg405: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_mlp_experts_down_proj_bias"} loc("p405.8567"), %arg406: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_mlp_experts_down_proj"} loc("p406.8572"), %arg407: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_mlp_experts_gate_up_proj_bias"} loc("p407.8578"), %arg408: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_16_mlp_experts_gate_up_proj"} loc("p408.8583"), %arg409: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_input_layernorm_weight"} loc("p409.8666"), %arg410: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_sinks"} loc("p410.8689"), %arg411: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_k_proj_bias"} loc("p411.8702"), %arg412: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_k_proj_weight"} loc("p412.8706"), %arg413: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_q_proj_bias"} loc("p413.8751"), %arg414: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_self_attn_q_proj_weight"} loc("p414.8755"), %arg415: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_post_attention_layernorm_weight"} loc("p415.8905"), %arg416: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_mlp_experts_down_proj_bias"} loc("p416.8973"), %arg417: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_mlp_experts_down_proj"} loc("p417.8978"), %arg418: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_mlp_experts_gate_up_proj_bias"} loc("p418.8984"), %arg419: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_17_mlp_experts_gate_up_proj"} loc("p419.8989"), %arg420: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_input_layernorm_weight"} loc("p420.9072"), %arg421: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_sinks"} loc("p421.9095"), %arg422: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_k_proj_bias"} loc("p422.9108"), %arg423: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_k_proj_weight"} loc("p423.9112"), %arg424: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_q_proj_bias"} loc("p424.9157"), %arg425: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_self_attn_q_proj_weight"} loc("p425.9161"), %arg426: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_post_attention_layernorm_weight"} loc("p426.9311"), %arg427: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_mlp_experts_down_proj_bias"} loc("p427.9379"), %arg428: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_mlp_experts_down_proj"} loc("p428.9384"), %arg429: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_mlp_experts_gate_up_proj_bias"} loc("p429.9390"), %arg430: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_18_mlp_experts_gate_up_proj"} loc("p430.9395"), %arg431: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_input_layernorm_weight"} loc("p431.9478"), %arg432: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_sinks"} loc("p432.9501"), %arg433: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_k_proj_bias"} loc("p433.9514"), %arg434: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_k_proj_weight"} loc("p434.9518"), %arg435: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_q_proj_bias"} loc("p435.9563"), %arg436: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_self_attn_q_proj_weight"} loc("p436.9567"), %arg437: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_post_attention_layernorm_weight"} loc("p437.9717"), %arg438: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_mlp_experts_down_proj_bias"} loc("p438.9785"), %arg439: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_mlp_experts_down_proj"} loc("p439.9790"), %arg440: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_mlp_experts_gate_up_proj_bias"} loc("p440.9796"), %arg441: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_19_mlp_experts_gate_up_proj"} loc("p441.9801"), %arg442: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_input_layernorm_weight"} loc("p442.9884"), %arg443: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_sinks"} loc("p443.9907"), %arg444: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_k_proj_bias"} loc("p444.9920"), %arg445: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_k_proj_weight"} loc("p445.9924"), %arg446: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_q_proj_bias"} loc("p446.9969"), %arg447: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_self_attn_q_proj_weight"} loc("p447.9973"), %arg448: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_post_attention_layernorm_weight"} loc("p448.10123"), %arg449: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_mlp_experts_down_proj_bias"} loc("p449.10191"), %arg450: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_mlp_experts_down_proj"} loc("p450.10196"), %arg451: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_mlp_experts_gate_up_proj_bias"} loc("p451.10202"), %arg452: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_20_mlp_experts_gate_up_proj"} loc("p452.10207"), %arg453: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_input_layernorm_weight"} loc("p453.10290"), %arg454: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_sinks"} loc("p454.10313"), %arg455: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_k_proj_bias"} loc("p455.10326"), %arg456: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_k_proj_weight"} loc("p456.10330"), %arg457: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_q_proj_bias"} loc("p457.10375"), %arg458: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_self_attn_q_proj_weight"} loc("p458.10379"), %arg459: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_post_attention_layernorm_weight"} loc("p459.10529"), %arg460: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_mlp_experts_down_proj_bias"} loc("p460.10597"), %arg461: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_mlp_experts_down_proj"} loc("p461.10602"), %arg462: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_mlp_experts_gate_up_proj_bias"} loc("p462.10608"), %arg463: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_21_mlp_experts_gate_up_proj"} loc("p463.10613"), %arg464: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_input_layernorm_weight"} loc("p464.10696"), %arg465: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_sinks"} loc("p465.10719"), %arg466: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_k_proj_bias"} loc("p466.10732"), %arg467: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_k_proj_weight"} loc("p467.10736"), %arg468: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_q_proj_bias"} loc("p468.10781"), %arg469: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_self_attn_q_proj_weight"} loc("p469.10785"), %arg470: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_post_attention_layernorm_weight"} loc("p470.10935"), %arg471: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_mlp_experts_down_proj_bias"} loc("p471.11003"), %arg472: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_mlp_experts_down_proj"} loc("p472.11008"), %arg473: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_mlp_experts_gate_up_proj_bias"} loc("p473.11014"), %arg474: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_22_mlp_experts_gate_up_proj"} loc("p474.11019"), %arg475: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_input_layernorm_weight"} loc("p475.11102"), %arg476: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_sinks"} loc("p476.11125"), %arg477: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_k_proj_bias"} loc("p477.11138"), %arg478: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_k_proj_weight"} loc("p478.11142"), %arg479: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_q_proj_bias"} loc("p479.11187"), %arg480: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_self_attn_q_proj_weight"} loc("p480.11191"), %arg481: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_post_attention_layernorm_weight"} loc("p481.11341"), %arg482: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_mlp_experts_down_proj_bias"} loc("p482.11409"), %arg483: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_mlp_experts_down_proj"} loc("p483.11414"), %arg484: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_mlp_experts_gate_up_proj_bias"} loc("p484.11420"), %arg485: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_23_mlp_experts_gate_up_proj"} loc("p485.11425"), %arg486: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_input_layernorm_weight"} loc("p486.11508"), %arg487: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_sinks"} loc("p487.11531"), %arg488: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_k_proj_bias"} loc("p488.11544"), %arg489: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_k_proj_weight"} loc("p489.11548"), %arg490: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_q_proj_bias"} loc("p490.11593"), %arg491: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_self_attn_q_proj_weight"} loc("p491.11597"), %arg492: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_post_attention_layernorm_weight"} loc("p492.11747"), %arg493: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_mlp_experts_down_proj_bias"} loc("p493.11815"), %arg494: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_mlp_experts_down_proj"} loc("p494.11820"), %arg495: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_mlp_experts_gate_up_proj_bias"} loc("p495.11826"), %arg496: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_24_mlp_experts_gate_up_proj"} loc("p496.11831"), %arg497: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_input_layernorm_weight"} loc("p497.11914"), %arg498: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_sinks"} loc("p498.11937"), %arg499: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_k_proj_bias"} loc("p499.11950"), %arg500: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_k_proj_weight"} loc("p500.11954"), %arg501: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_q_proj_bias"} loc("p501.11999"), %arg502: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_self_attn_q_proj_weight"} loc("p502.12003"), %arg503: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_post_attention_layernorm_weight"} loc("p503.12153"), %arg504: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_mlp_experts_down_proj_bias"} loc("p504.12221"), %arg505: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_mlp_experts_down_proj"} loc("p505.12226"), %arg506: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_mlp_experts_gate_up_proj_bias"} loc("p506.12232"), %arg507: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_25_mlp_experts_gate_up_proj"} loc("p507.12237"), %arg508: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_input_layernorm_weight"} loc("p508.12320"), %arg509: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_sinks"} loc("p509.12343"), %arg510: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_k_proj_bias"} loc("p510.12356"), %arg511: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_k_proj_weight"} loc("p511.12360"), %arg512: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_q_proj_bias"} loc("p512.12405"), %arg513: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_self_attn_q_proj_weight"} loc("p513.12409"), %arg514: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_post_attention_layernorm_weight"} loc("p514.12559"), %arg515: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_mlp_experts_down_proj_bias"} loc("p515.12627"), %arg516: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_mlp_experts_down_proj"} loc("p516.12632"), %arg517: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_mlp_experts_gate_up_proj_bias"} loc("p517.12638"), %arg518: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_26_mlp_experts_gate_up_proj"} loc("p518.12643"), %arg519: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_input_layernorm_weight"} loc("p519.12726"), %arg520: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_sinks"} loc("p520.12749"), %arg521: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_k_proj_bias"} loc("p521.12762"), %arg522: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_k_proj_weight"} loc("p522.12766"), %arg523: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_q_proj_bias"} loc("p523.12811"), %arg524: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_self_attn_q_proj_weight"} loc("p524.12815"), %arg525: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_post_attention_layernorm_weight"} loc("p525.12965"), %arg526: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_mlp_experts_down_proj_bias"} loc("p526.13033"), %arg527: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_mlp_experts_down_proj"} loc("p527.13038"), %arg528: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_mlp_experts_gate_up_proj_bias"} loc("p528.13044"), %arg529: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_27_mlp_experts_gate_up_proj"} loc("p529.13049"), %arg530: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_input_layernorm_weight"} loc("p530.13132"), %arg531: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_sinks"} loc("p531.13155"), %arg532: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_k_proj_bias"} loc("p532.13168"), %arg533: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_k_proj_weight"} loc("p533.13172"), %arg534: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_q_proj_bias"} loc("p534.13217"), %arg535: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_self_attn_q_proj_weight"} loc("p535.13221"), %arg536: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_post_attention_layernorm_weight"} loc("p536.13371"), %arg537: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_mlp_experts_down_proj_bias"} loc("p537.13439"), %arg538: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_mlp_experts_down_proj"} loc("p538.13444"), %arg539: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_mlp_experts_gate_up_proj_bias"} loc("p539.13450"), %arg540: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_28_mlp_experts_gate_up_proj"} loc("p540.13455"), %arg541: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_input_layernorm_weight"} loc("p541.13538"), %arg542: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_sinks"} loc("p542.13561"), %arg543: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_k_proj_bias"} loc("p543.13574"), %arg544: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_k_proj_weight"} loc("p544.13578"), %arg545: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_q_proj_bias"} loc("p545.13623"), %arg546: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_self_attn_q_proj_weight"} loc("p546.13627"), %arg547: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_post_attention_layernorm_weight"} loc("p547.13777"), %arg548: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_mlp_experts_down_proj_bias"} loc("p548.13845"), %arg549: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_mlp_experts_down_proj"} loc("p549.13850"), %arg550: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_mlp_experts_gate_up_proj_bias"} loc("p550.13856"), %arg551: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_29_mlp_experts_gate_up_proj"} loc("p551.13861"), %arg552: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_input_layernorm_weight"} loc("p552.13944"), %arg553: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_sinks"} loc("p553.13967"), %arg554: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_k_proj_bias"} loc("p554.13980"), %arg555: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_k_proj_weight"} loc("p555.13984"), %arg556: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_q_proj_bias"} loc("p556.14029"), %arg557: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_self_attn_q_proj_weight"} loc("p557.14033"), %arg558: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_post_attention_layernorm_weight"} loc("p558.14183"), %arg559: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_mlp_experts_down_proj_bias"} loc("p559.14251"), %arg560: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_mlp_experts_down_proj"} loc("p560.14256"), %arg561: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_mlp_experts_gate_up_proj_bias"} loc("p561.14262"), %arg562: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_30_mlp_experts_gate_up_proj"} loc("p562.14267"), %arg563: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_input_layernorm_weight"} loc("p563.14350"), %arg564: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_sinks"} loc("p564.14373"), %arg565: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_k_proj_bias"} loc("p565.14386"), %arg566: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_k_proj_weight"} loc("p566.14390"), %arg567: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_q_proj_bias"} loc("p567.14435"), %arg568: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_self_attn_q_proj_weight"} loc("p568.14439"), %arg569: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_post_attention_layernorm_weight"} loc("p569.14589"), %arg570: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_mlp_experts_down_proj_bias"} loc("p570.14657"), %arg571: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_mlp_experts_down_proj"} loc("p571.14662"), %arg572: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_mlp_experts_gate_up_proj_bias"} loc("p572.14668"), %arg573: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_31_mlp_experts_gate_up_proj"} loc("p573.14673"), %arg574: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_input_layernorm_weight"} loc("p574.14756"), %arg575: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_sinks"} loc("p575.14779"), %arg576: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_k_proj_bias"} loc("p576.14792"), %arg577: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_k_proj_weight"} loc("p577.14796"), %arg578: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_q_proj_bias"} loc("p578.14841"), %arg579: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_self_attn_q_proj_weight"} loc("p579.14845"), %arg580: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_post_attention_layernorm_weight"} loc("p580.14995"), %arg581: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_mlp_experts_down_proj_bias"} loc("p581.15063"), %arg582: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_mlp_experts_down_proj"} loc("p582.15068"), %arg583: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_mlp_experts_gate_up_proj_bias"} loc("p583.15074"), %arg584: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_32_mlp_experts_gate_up_proj"} loc("p584.15079"), %arg585: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_input_layernorm_weight"} loc("p585.15162"), %arg586: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_sinks"} loc("p586.15185"), %arg587: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_k_proj_bias"} loc("p587.15198"), %arg588: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_k_proj_weight"} loc("p588.15202"), %arg589: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_q_proj_bias"} loc("p589.15247"), %arg590: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_self_attn_q_proj_weight"} loc("p590.15251"), %arg591: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_post_attention_layernorm_weight"} loc("p591.15401"), %arg592: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_mlp_experts_down_proj_bias"} loc("p592.15469"), %arg593: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_mlp_experts_down_proj"} loc("p593.15474"), %arg594: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_mlp_experts_gate_up_proj_bias"} loc("p594.15480"), %arg595: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_33_mlp_experts_gate_up_proj"} loc("p595.15485"), %arg596: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_input_layernorm_weight"} loc("p596.15568"), %arg597: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_sinks"} loc("p597.15591"), %arg598: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_k_proj_bias"} loc("p598.15604"), %arg599: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_k_proj_weight"} loc("p599.15608"), %arg600: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_q_proj_bias"} loc("p600.15653"), %arg601: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_self_attn_q_proj_weight"} loc("p601.15657"), %arg602: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_post_attention_layernorm_weight"} loc("p602.15807"), %arg603: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_mlp_experts_down_proj_bias"} loc("p603.15875"), %arg604: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_mlp_experts_down_proj"} loc("p604.15880"), %arg605: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_mlp_experts_gate_up_proj_bias"} loc("p605.15886"), %arg606: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_34_mlp_experts_gate_up_proj"} loc("p606.15891"), %arg607: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_input_layernorm_weight"} loc("p607.15974"), %arg608: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_sinks"} loc("p608.15997"), %arg609: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_k_proj_bias"} loc("p609.16010"), %arg610: tensor<512x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_k_proj_weight"} loc("p610.16014"), %arg611: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_q_proj_bias"} loc("p611.16059"), %arg612: tensor<4096x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_self_attn_q_proj_weight"} loc("p612.16063"), %arg613: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_post_attention_layernorm_weight"} loc("p613.16213"), %arg614: tensor<128x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_mlp_experts_down_proj_bias"} loc("p614.16281"), %arg615: tensor<128x2880x2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_mlp_experts_down_proj"} loc("p615.16286"), %arg616: tensor<128x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_mlp_experts_gate_up_proj_bias"} loc("p616.16292"), %arg617: tensor<128x2880x5760xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers_35_mlp_experts_gate_up_proj"} loc("p617.16297"), %arg618: tensor<2880xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"} loc("p618.16380")) -> (tensor<1x128x201088xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73, %arg74, %arg75, %arg76, %arg77, %arg78, %arg79, %arg80, %arg81, %arg82, %arg83, %arg84, %arg85, %arg86, %arg87, %arg88, %arg89, %arg90, %arg91, %arg92, %arg93, %arg94, %arg95, %arg96, %arg97, %arg98, %arg99, %arg100, %arg101, %arg102, %arg103, %arg104, %arg105, %arg106, %arg107, %arg108, %arg109, %arg110, %arg111, %arg112, %arg113, %arg114, %arg115, %arg116, %arg117, %arg118, %arg119, %arg120, %arg121, %arg122, %arg123, %arg124, %arg125, %arg126, %arg127, %arg128, %arg129, %arg130, %arg131, %arg132, %arg133, %arg134, %arg135, %arg136, %arg137, %arg138, %arg139, %arg140, %arg141, %arg142, %arg143, %arg144, %arg145, %arg146, %arg147, %arg148, %arg149, %arg150, %arg151, %arg152, %arg153, %arg154, %arg155, %arg156, %arg157, %arg158, %arg159, %arg160, %arg161, %arg162, %arg163, %arg164, %arg165, %arg166, %arg167, %arg168, %arg169, %arg170, %arg171, %arg172, %arg173, %arg174, %arg175, %arg176, %arg177, %arg178, %arg179, %arg180, %arg181, %arg182, %arg183, %arg184, %arg185, %arg186, %arg187, %arg188, %arg189, %arg190, %arg191, %arg192, %arg193, %arg194, %arg195, %arg196, %arg197, %arg198, %arg199, %arg200, %arg201, %arg202, %arg203, %arg204, %arg205, %arg206, %arg207, %arg208, %arg209, %arg210, %arg211, %arg212, %arg213, %arg214, %arg215, %arg216, %arg217, %arg218, %arg219, %arg220, %arg221, %arg222, %arg223, %arg224, %arg225, %arg226, %arg227, %arg228, %arg229, %arg230, %arg231, %arg232, %arg233, %arg234, %arg235, %arg236, %arg237, %arg238, %arg239, %arg240, %arg241, %arg242, %arg243, %arg244, %arg245, %arg246, %arg247, %arg248, %arg249, %arg250, %arg251, %arg252, %arg253, %arg254, %arg255, %arg256, %arg257, %arg258, %arg259, %arg260, %arg261, %arg262, %arg263, %arg264, %arg265, %arg266, %arg267, %arg268, %arg269, %arg270, %arg271, %arg272, %arg273, %arg274, %arg275, %arg276, %arg277, %arg278, %arg279, %arg280, %arg281, %arg282, %arg283, %arg284, %arg285, %arg286, %arg287, %arg288, %arg289, %arg290, %arg291, %arg292, %arg293, %arg294, %arg295, %arg296, %arg297, %arg298, %arg299, %arg300, %arg301, %arg302, %arg303, %arg304, %arg305, %arg306, %arg307, %arg308, %arg309, %arg310, %arg311, %arg312, %arg313, %arg314, %arg315, %arg316, %arg317, %arg318, %arg319, %arg320, %arg321, %arg322, %arg323, %arg324, %arg325, %arg326, %arg327, %arg328, %arg329, %arg330, %arg331, %arg332, %arg333, %arg334, %arg335, %arg336, %arg337, %arg338, %arg339, %arg340, %arg341, %arg342, %arg343, %arg344, %arg345, %arg346, %arg347, %arg348, %arg349, %arg350, %arg351, %arg352, %arg353, %arg354, %arg355, %arg356, %arg357, %arg358, %arg359, %arg360, %arg361, %arg362, %arg363, %arg364, %arg365, %arg366, %arg367, %arg368, %arg369, %arg370, %arg371, %arg372, %arg373, %arg374, %arg375, %arg376, %arg377, %arg378, %arg379, %arg380, %arg381, %arg382, %arg383, %arg384, %arg385, %arg386, %arg387, %arg388, %arg389, %arg390, %arg391, %arg392, %arg393, %arg394, %arg395, %arg396, %arg397, %arg398, %arg399, %arg400, %arg401, %arg402, %arg403, %arg404, %arg405, %arg406, %arg407, %arg408, %arg409, %arg410, %arg411, %arg412, %arg413, %arg414, %arg415, %arg416, %arg417, %arg418, %arg419, %arg420, %arg421, %arg422, %arg423, %arg424, %arg425, %arg426, %arg427, %arg428, %arg429, %arg430, %arg431, %arg432, %arg433, %arg434, %arg435, %arg436, %arg437, %arg438, %arg439, %arg440, %arg441, %arg442, %arg443, %arg444, %arg445, %arg446, %arg447, %arg448, %arg449, %arg450, %arg451, %arg452, %arg453, %arg454, %arg455, %arg456, %arg457, %arg458, %arg459, %arg460, %arg461, %arg462, %arg463, %arg464, %arg465, %arg466, %arg467, %arg468, %arg469, %arg470, %arg471, %arg472, %arg473, %arg474, %arg475, %arg476, %arg477, %arg478, %arg479, %arg480, %arg481, %arg482, %arg483, %arg484, %arg485, %arg486, %arg487, %arg488, %arg489, %arg490, %arg491, %arg492, %arg493, %arg494, %arg495, %arg496, %arg497, %arg498, %arg499, %arg500, %arg501, %arg502, %arg503, %arg504, %arg505, %arg506, %arg507, %arg508, %arg509, %arg510, %arg511, %arg512, %arg513, %arg514, %arg515, %arg516, %arg517, %arg518, %arg519, %arg520, %arg521, %arg522, %arg523, %arg524, %arg525, %arg526, %arg527, %arg528, %arg529, %arg530, %arg531, %arg532, %arg533, %arg534, %arg535, %arg536, %arg537, %arg538, %arg539, %arg540, %arg541, %arg542, %arg543, %arg544, %arg545, %arg546, %arg547, %arg548, %arg549, %arg550, %arg551, %arg552, %arg553, %arg554, %arg555, %arg556, %arg557, %arg558, %arg559, %arg560, %arg561, %arg562, %arg563, %arg564, %arg565, %arg566, %arg567, %arg568, %arg569, %arg570, %arg571, %arg572, %arg573, %arg574, %arg575, %arg576, %arg577, %arg578, %arg579, %arg580, %arg581, %arg582, %arg583, %arg584, %arg585, %arg586, %arg587, %arg588, %arg589, %arg590, %arg591, %arg592, %arg593, %arg594, %arg595, %arg596, %arg597, %arg598, %arg599, %arg600, %arg601, %arg602, %arg603, %arg604, %arg605, %arg606, %arg607, %arg608, %arg609, %arg610, %arg611, %arg612, %arg613, %arg614, %arg615, %arg616, %arg617, %arg618) in_shardings=[<@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}, {}]>, <@mesh, [{"_axis_0"}]>] out_shardings=[<@mesh, [{}, {}, {"_axis_1"}]>] manual_axes={"_axis_0", "_axis_1"} (%arg619: tensor<50272x360xbf16> loc("p0.1"), %arg620: tensor<128xbf16> loc("p1.12"), %arg621: tensor<128x360xbf16> loc("p2.19"), %arg622: tensor<360xbf16> loc("p3.29"), %arg623: tensor<360x1024xbf16> loc("p4.33"), %arg624: tensor<128xbf16> loc("p5.39"), %arg625: tensor<128x360xbf16> loc("p6.43"), %arg626: tensor<128xbf16> loc("p7.54"), %arg627: tensor<128x360xbf16> loc("p8.61"), %arg628: tensor<360xbf16> loc("p9.71"), %arg629: tensor<360x1024xbf16> loc("p10.75"), %arg630: tensor<128xbf16> loc("p11.81"), %arg631: tensor<128x360xbf16> loc("p12.85"), %arg632: tensor<128xbf16> loc("p13.96"), %arg633: tensor<128x360xbf16> loc("p14.103"), %arg634: tensor<360xbf16> loc("p15.113"), %arg635: tensor<360x1024xbf16> loc("p16.117"), %arg636: tensor<128xbf16> loc("p17.123"), %arg637: tensor<128x360xbf16> loc("p18.127"), %arg638: tensor<128xbf16> loc("p19.138"), %arg639: tensor<128x360xbf16> loc("p20.145"), %arg640: tensor<360xbf16> loc("p21.155"), %arg641: tensor<360x1024xbf16> loc("p22.159"), %arg642: tensor<128xbf16> loc("p23.165"), %arg643: tensor<128x360xbf16> loc("p24.169"), %arg644: tensor<128xbf16> loc("p25.180"), %arg645: tensor<128x360xbf16> loc("p26.187"), %arg646: tensor<360xbf16> loc("p27.197"), %arg647: tensor<360x1024xbf16> loc("p28.201"), %arg648: tensor<128xbf16> loc("p29.207"), %arg649: tensor<128x360xbf16> loc("p30.211"), %arg650: tensor<128xbf16> loc("p31.222"), %arg651: tensor<128x360xbf16> loc("p32.229"), %arg652: tensor<360xbf16> loc("p33.239"), %arg653: tensor<360x1024xbf16> loc("p34.243"), %arg654: tensor<128xbf16> loc("p35.249"), %arg655: tensor<128x360xbf16> loc("p36.253"), %arg656: tensor<128xbf16> loc("p37.264"), %arg657: tensor<128x360xbf16> loc("p38.271"), %arg658: tensor<360xbf16> loc("p39.281"), %arg659: tensor<360x1024xbf16> loc("p40.285"), %arg660: tensor<128xbf16> loc("p41.291"), %arg661: tensor<128x360xbf16> loc("p42.295"), %arg662: tensor<128xbf16> loc("p43.306"), %arg663: tensor<128x360xbf16> loc("p44.313"), %arg664: tensor<360xbf16> loc("p45.323"), %arg665: tensor<360x1024xbf16> loc("p46.327"), %arg666: tensor<128xbf16> loc("p47.333"), %arg667: tensor<128x360xbf16> loc("p48.337"), %arg668: tensor<128xbf16> loc("p49.348"), %arg669: tensor<128x360xbf16> loc("p50.355"), %arg670: tensor<360xbf16> loc("p51.365"), %arg671: tensor<360x1024xbf16> loc("p52.369"), %arg672: tensor<128xbf16> loc("p53.375"), %arg673: tensor<128x360xbf16> loc("p54.379"), %arg674: tensor<128xbf16> loc("p55.390"), %arg675: tensor<128x360xbf16> loc("p56.397"), %arg676: tensor<360xbf16> loc("p57.407"), %arg677: tensor<360x1024xbf16> loc("p58.411"), %arg678: tensor<128xbf16> loc("p59.417"), %arg679: tensor<128x360xbf16> loc("p60.421"), %arg680: tensor<128xbf16> loc("p61.432"), %arg681: tensor<128x360xbf16> loc("p62.439"), %arg682: tensor<360xbf16> loc("p63.449"), %arg683: tensor<360x1024xbf16> loc("p64.453"), %arg684: tensor<128xbf16> loc("p65.459"), %arg685: tensor<128x360xbf16> loc("p66.463"), %arg686: tensor<128xbf16> loc("p67.474"), %arg687: tensor<128x360xbf16> loc("p68.481"), %arg688: tensor<360xbf16> loc("p69.491"), %arg689: tensor<360x1024xbf16> loc("p70.495"), %arg690: tensor<128xbf16> loc("p71.501"), %arg691: tensor<128x360xbf16> loc("p72.505"), %arg692: tensor<128xbf16> loc("p73.516"), %arg693: tensor<128x360xbf16> loc("p74.523"), %arg694: tensor<360xbf16> loc("p75.533"), %arg695: tensor<360x1024xbf16> loc("p76.537"), %arg696: tensor<128xbf16> loc("p77.543"), %arg697: tensor<128x360xbf16> loc("p78.547"), %arg698: tensor<128xbf16> loc("p79.558"), %arg699: tensor<128x360xbf16> loc("p80.565"), %arg700: tensor<360xbf16> loc("p81.575"), %arg701: tensor<360x1024xbf16> loc("p82.579"), %arg702: tensor<128xbf16> loc("p83.585"), %arg703: tensor<128x360xbf16> loc("p84.589"), %arg704: tensor<128xbf16> loc("p85.600"), %arg705: tensor<128x360xbf16> loc("p86.607"), %arg706: tensor<360xbf16> loc("p87.617"), %arg707: tensor<360x1024xbf16> loc("p88.621"), %arg708: tensor<128xbf16> loc("p89.627"), %arg709: tensor<128x360xbf16> loc("p90.631"), %arg710: tensor<128xbf16> loc("p91.642"), %arg711: tensor<128x360xbf16> loc("p92.649"), %arg712: tensor<360xbf16> loc("p93.659"), %arg713: tensor<360x1024xbf16> loc("p94.663"), %arg714: tensor<128xbf16> loc("p95.669"), %arg715: tensor<128x360xbf16> loc("p96.673"), %arg716: tensor<128xbf16> loc("p97.684"), %arg717: tensor<128x360xbf16> loc("p98.691"), %arg718: tensor<360xbf16> loc("p99.701"), %arg719: tensor<360x1024xbf16> loc("p100.705"), %arg720: tensor<128xbf16> loc("p101.711"), %arg721: tensor<128x360xbf16> loc("p102.715"), %arg722: tensor<128xbf16> loc("p103.726"), %arg723: tensor<128x360xbf16> loc("p104.733"), %arg724: tensor<360xbf16> loc("p105.743"), %arg725: tensor<360x1024xbf16> loc("p106.747"), %arg726: tensor<128xbf16> loc("p107.753"), %arg727: tensor<128x360xbf16> loc("p108.757"), %arg728: tensor<128xbf16> loc("p109.768"), %arg729: tensor<128x360xbf16> loc("p110.775"), %arg730: tensor<360xbf16> loc("p111.785"), %arg731: tensor<360x1024xbf16> loc("p112.789"), %arg732: tensor<128xbf16> loc("p113.795"), %arg733: tensor<128x360xbf16> loc("p114.799"), %arg734: tensor<128xbf16> loc("p115.810"), %arg735: tensor<128x360xbf16> loc("p116.817"), %arg736: tensor<360xbf16> loc("p117.827"), %arg737: tensor<360x1024xbf16> loc("p118.831"), %arg738: tensor<128xbf16> loc("p119.837"), %arg739: tensor<128x360xbf16> loc("p120.841"), %arg740: tensor<128xbf16> loc("p121.852"), %arg741: tensor<128x360xbf16> loc("p122.859"), %arg742: tensor<360xbf16> loc("p123.869"), %arg743: tensor<360x1024xbf16> loc("p124.873"), %arg744: tensor<128xbf16> loc("p125.879"), %arg745: tensor<128x360xbf16> loc("p126.883"), %arg746: tensor<128xbf16> loc("p127.894"), %arg747: tensor<128x360xbf16> loc("p128.901"), %arg748: tensor<360xbf16> loc("p129.911"), %arg749: tensor<360x1024xbf16> loc("p130.915"), %arg750: tensor<128xbf16> loc("p131.921"), %arg751: tensor<128x360xbf16> loc("p132.925"), %arg752: tensor<128xbf16> loc("p133.936"), %arg753: tensor<128x360xbf16> loc("p134.943"), %arg754: tensor<360xbf16> loc("p135.953"), %arg755: tensor<360x1024xbf16> loc("p136.957"), %arg756: tensor<128xbf16> loc("p137.963"), %arg757: tensor<128x360xbf16> loc("p138.967"), %arg758: tensor<128xbf16> loc("p139.978"), %arg759: tensor<128x360xbf16> loc("p140.985"), %arg760: tensor<360xbf16> loc("p141.995"), %arg761: tensor<360x1024xbf16> loc("p142.999"), %arg762: tensor<128xbf16> loc("p143.1005"), %arg763: tensor<128x360xbf16> loc("p144.1009"), %arg764: tensor<128xbf16> loc("p145.1020"), %arg765: tensor<128x360xbf16> loc("p146.1027"), %arg766: tensor<360xbf16> loc("p147.1037"), %arg767: tensor<360x1024xbf16> loc("p148.1041"), %arg768: tensor<128xbf16> loc("p149.1047"), %arg769: tensor<128x360xbf16> loc("p150.1051"), %arg770: tensor<128xbf16> loc("p151.1062"), %arg771: tensor<128x360xbf16> loc("p152.1069"), %arg772: tensor<360xbf16> loc("p153.1079"), %arg773: tensor<360x1024xbf16> loc("p154.1083"), %arg774: tensor<128xbf16> loc("p155.1089"), %arg775: tensor<128x360xbf16> loc("p156.1093"), %arg776: tensor<128xbf16> loc("p157.1104"), %arg777: tensor<128x360xbf16> loc("p158.1111"), %arg778: tensor<360xbf16> loc("p159.1121"), %arg779: tensor<360x1024xbf16> loc("p160.1125"), %arg780: tensor<128xbf16> loc("p161.1131"), %arg781: tensor<128x360xbf16> loc("p162.1135"), %arg782: tensor<128xbf16> loc("p163.1146"), %arg783: tensor<128x360xbf16> loc("p164.1153"), %arg784: tensor<360xbf16> loc("p165.1163"), %arg785: tensor<360x1024xbf16> loc("p166.1167"), %arg786: tensor<128xbf16> loc("p167.1173"), %arg787: tensor<128x360xbf16> loc("p168.1177"), %arg788: tensor<128xbf16> loc("p169.1188"), %arg789: tensor<128x360xbf16> loc("p170.1195"), %arg790: tensor<360xbf16> loc("p171.1205"), %arg791: tensor<360x1024xbf16> loc("p172.1209"), %arg792: tensor<128xbf16> loc("p173.1215"), %arg793: tensor<128x360xbf16> loc("p174.1219"), %arg794: tensor<128xbf16> loc("p175.1230"), %arg795: tensor<128x360xbf16> loc("p176.1237"), %arg796: tensor<360xbf16> loc("p177.1247"), %arg797: tensor<360x1024xbf16> loc("p178.1251"), %arg798: tensor<128xbf16> loc("p179.1257"), %arg799: tensor<128x360xbf16> loc("p180.1261"), %arg800: tensor<128xbf16> loc("p181.1272"), %arg801: tensor<128x360xbf16> loc("p182.1279"), %arg802: tensor<360xbf16> loc("p183.1289"), %arg803: tensor<360x1024xbf16> loc("p184.1293"), %arg804: tensor<128xbf16> loc("p185.1299"), %arg805: tensor<128x360xbf16> loc("p186.1303"), %arg806: tensor<128xbf16> loc("p187.1314"), %arg807: tensor<128x360xbf16> loc("p188.1321"), %arg808: tensor<360xbf16> loc("p189.1331"), %arg809: tensor<360x1024xbf16> loc("p190.1335"), %arg810: tensor<128xbf16> loc("p191.1341"), %arg811: tensor<128x360xbf16> loc("p192.1345"), %arg812: tensor<128xbf16> loc("p193.1356"), %arg813: tensor<128x360xbf16> loc("p194.1363"), %arg814: tensor<360xbf16> loc("p195.1373"), %arg815: tensor<360x1024xbf16> loc("p196.1377"), %arg816: tensor<128xbf16> loc("p197.1383"), %arg817: tensor<128x360xbf16> loc("p198.1387"), %arg818: tensor<128xbf16> loc("p199.1398"), %arg819: tensor<128x360xbf16> loc("p200.1405"), %arg820: tensor<360xbf16> loc("p201.1415"), %arg821: tensor<360x1024xbf16> loc("p202.1419"), %arg822: tensor<128xbf16> loc("p203.1425"), %arg823: tensor<128x360xbf16> loc("p204.1429"), %arg824: tensor<128xbf16> loc("p205.1440"), %arg825: tensor<128x360xbf16> loc("p206.1447"), %arg826: tensor<360xbf16> loc("p207.1457"), %arg827: tensor<360x1024xbf16> loc("p208.1461"), %arg828: tensor<128xbf16> loc("p209.1467"), %arg829: tensor<128x360xbf16> loc("p210.1471"), %arg830: tensor<128xbf16> loc("p211.1482"), %arg831: tensor<128x360xbf16> loc("p212.1489"), %arg832: tensor<360xbf16> loc("p213.1499"), %arg833: tensor<360x1024xbf16> loc("p214.1503"), %arg834: tensor<128xbf16> loc("p215.1509"), %arg835: tensor<128x360xbf16> loc("p216.1513"), %arg836: tensor<1x128xi64> loc("p217.1521"), %arg837: tensor<201088x360xbf16> loc("p218.1526"), %arg838: tensor<360xbf16> loc("p219.1561"), %arg839: tensor<64xbf16> loc("p220.1584"), %arg840: tensor<1x128xi64> loc("p221.1640"), %arg841: tensor<i1> loc("p222.1663"), %arg842: tensor<32xf32> loc("p223.1704"), %arg843: tensor<128xbf16> loc("p224.1720"), %arg844: tensor<128x360xbf16> loc("p225.1724"), %arg845: tensor<1024xbf16> loc("p226.1774"), %arg846: tensor<1024x360xbf16> loc("p227.1778"), %arg847: tensor<360xbf16> loc("p228.1928"), %arg848: tensor<32x360xbf16> loc("p229.1996"), %arg849: tensor<32x2880x360xbf16> loc("p230.2001"), %arg850: tensor<32x5760xbf16> loc("p231.2007"), %arg851: tensor<32x360x5760xbf16> loc("p232.2012"), %arg852: tensor<360xbf16> loc("p233.2095"), %arg853: tensor<64xbf16> loc("p234.2118"), %arg854: tensor<128xbf16> loc("p235.2206"), %arg855: tensor<128x360xbf16> loc("p236.2210"), %arg856: tensor<1024xbf16> loc("p237.2255"), %arg857: tensor<1024x360xbf16> loc("p238.2259"), %arg858: tensor<360xbf16> loc("p239.2409"), %arg859: tensor<32x360xbf16> loc("p240.2477"), %arg860: tensor<32x2880x360xbf16> loc("p241.2482"), %arg861: tensor<32x5760xbf16> loc("p242.2488"), %arg862: tensor<32x360x5760xbf16> loc("p243.2493"), %arg863: tensor<360xbf16> loc("p244.2576"), %arg864: tensor<64xbf16> loc("p245.2599"), %arg865: tensor<128xbf16> loc("p246.2612"), %arg866: tensor<128x360xbf16> loc("p247.2616"), %arg867: tensor<1024xbf16> loc("p248.2661"), %arg868: tensor<1024x360xbf16> loc("p249.2665"), %arg869: tensor<360xbf16> loc("p250.2815"), %arg870: tensor<32x360xbf16> loc("p251.2883"), %arg871: tensor<32x2880x360xbf16> loc("p252.2888"), %arg872: tensor<32x5760xbf16> loc("p253.2894"), %arg873: tensor<32x360x5760xbf16> loc("p254.2899"), %arg874: tensor<360xbf16> loc("p255.2982"), %arg875: tensor<64xbf16> loc("p256.3005"), %arg876: tensor<128xbf16> loc("p257.3018"), %arg877: tensor<128x360xbf16> loc("p258.3022"), %arg878: tensor<1024xbf16> loc("p259.3067"), %arg879: tensor<1024x360xbf16> loc("p260.3071"), %arg880: tensor<360xbf16> loc("p261.3221"), %arg881: tensor<32x360xbf16> loc("p262.3289"), %arg882: tensor<32x2880x360xbf16> loc("p263.3294"), %arg883: tensor<32x5760xbf16> loc("p264.3300"), %arg884: tensor<32x360x5760xbf16> loc("p265.3305"), %arg885: tensor<360xbf16> loc("p266.3388"), %arg886: tensor<64xbf16> loc("p267.3411"), %arg887: tensor<128xbf16> loc("p268.3424"), %arg888: tensor<128x360xbf16> loc("p269.3428"), %arg889: tensor<1024xbf16> loc("p270.3473"), %arg890: tensor<1024x360xbf16> loc("p271.3477"), %arg891: tensor<360xbf16> loc("p272.3627"), %arg892: tensor<32x360xbf16> loc("p273.3695"), %arg893: tensor<32x2880x360xbf16> loc("p274.3700"), %arg894: tensor<32x5760xbf16> loc("p275.3706"), %arg895: tensor<32x360x5760xbf16> loc("p276.3711"), %arg896: tensor<360xbf16> loc("p277.3794"), %arg897: tensor<64xbf16> loc("p278.3817"), %arg898: tensor<128xbf16> loc("p279.3830"), %arg899: tensor<128x360xbf16> loc("p280.3834"), %arg900: tensor<1024xbf16> loc("p281.3879"), %arg901: tensor<1024x360xbf16> loc("p282.3883"), %arg902: tensor<360xbf16> loc("p283.4033"), %arg903: tensor<32x360xbf16> loc("p284.4101"), %arg904: tensor<32x2880x360xbf16> loc("p285.4106"), %arg905: tensor<32x5760xbf16> loc("p286.4112"), %arg906: tensor<32x360x5760xbf16> loc("p287.4117"), %arg907: tensor<360xbf16> loc("p288.4200"), %arg908: tensor<64xbf16> loc("p289.4223"), %arg909: tensor<128xbf16> loc("p290.4236"), %arg910: tensor<128x360xbf16> loc("p291.4240"), %arg911: tensor<1024xbf16> loc("p292.4285"), %arg912: tensor<1024x360xbf16> loc("p293.4289"), %arg913: tensor<360xbf16> loc("p294.4439"), %arg914: tensor<32x360xbf16> loc("p295.4507"), %arg915: tensor<32x2880x360xbf16> loc("p296.4512"), %arg916: tensor<32x5760xbf16> loc("p297.4518"), %arg917: tensor<32x360x5760xbf16> loc("p298.4523"), %arg918: tensor<360xbf16> loc("p299.4606"), %arg919: tensor<64xbf16> loc("p300.4629"), %arg920: tensor<128xbf16> loc("p301.4642"), %arg921: tensor<128x360xbf16> loc("p302.4646"), %arg922: tensor<1024xbf16> loc("p303.4691"), %arg923: tensor<1024x360xbf16> loc("p304.4695"), %arg924: tensor<360xbf16> loc("p305.4845"), %arg925: tensor<32x360xbf16> loc("p306.4913"), %arg926: tensor<32x2880x360xbf16> loc("p307.4918"), %arg927: tensor<32x5760xbf16> loc("p308.4924"), %arg928: tensor<32x360x5760xbf16> loc("p309.4929"), %arg929: tensor<360xbf16> loc("p310.5012"), %arg930: tensor<64xbf16> loc("p311.5035"), %arg931: tensor<128xbf16> loc("p312.5048"), %arg932: tensor<128x360xbf16> loc("p313.5052"), %arg933: tensor<1024xbf16> loc("p314.5097"), %arg934: tensor<1024x360xbf16> loc("p315.5101"), %arg935: tensor<360xbf16> loc("p316.5251"), %arg936: tensor<32x360xbf16> loc("p317.5319"), %arg937: tensor<32x2880x360xbf16> loc("p318.5324"), %arg938: tensor<32x5760xbf16> loc("p319.5330"), %arg939: tensor<32x360x5760xbf16> loc("p320.5335"), %arg940: tensor<360xbf16> loc("p321.5418"), %arg941: tensor<64xbf16> loc("p322.5441"), %arg942: tensor<128xbf16> loc("p323.5454"), %arg943: tensor<128x360xbf16> loc("p324.5458"), %arg944: tensor<1024xbf16> loc("p325.5503"), %arg945: tensor<1024x360xbf16> loc("p326.5507"), %arg946: tensor<360xbf16> loc("p327.5657"), %arg947: tensor<32x360xbf16> loc("p328.5725"), %arg948: tensor<32x2880x360xbf16> loc("p329.5730"), %arg949: tensor<32x5760xbf16> loc("p330.5736"), %arg950: tensor<32x360x5760xbf16> loc("p331.5741"), %arg951: tensor<360xbf16> loc("p332.5824"), %arg952: tensor<64xbf16> loc("p333.5847"), %arg953: tensor<128xbf16> loc("p334.5860"), %arg954: tensor<128x360xbf16> loc("p335.5864"), %arg955: tensor<1024xbf16> loc("p336.5909"), %arg956: tensor<1024x360xbf16> loc("p337.5913"), %arg957: tensor<360xbf16> loc("p338.6063"), %arg958: tensor<32x360xbf16> loc("p339.6131"), %arg959: tensor<32x2880x360xbf16> loc("p340.6136"), %arg960: tensor<32x5760xbf16> loc("p341.6142"), %arg961: tensor<32x360x5760xbf16> loc("p342.6147"), %arg962: tensor<360xbf16> loc("p343.6230"), %arg963: tensor<64xbf16> loc("p344.6253"), %arg964: tensor<128xbf16> loc("p345.6266"), %arg965: tensor<128x360xbf16> loc("p346.6270"), %arg966: tensor<1024xbf16> loc("p347.6315"), %arg967: tensor<1024x360xbf16> loc("p348.6319"), %arg968: tensor<360xbf16> loc("p349.6469"), %arg969: tensor<32x360xbf16> loc("p350.6537"), %arg970: tensor<32x2880x360xbf16> loc("p351.6542"), %arg971: tensor<32x5760xbf16> loc("p352.6548"), %arg972: tensor<32x360x5760xbf16> loc("p353.6553"), %arg973: tensor<360xbf16> loc("p354.6636"), %arg974: tensor<64xbf16> loc("p355.6659"), %arg975: tensor<128xbf16> loc("p356.6672"), %arg976: tensor<128x360xbf16> loc("p357.6676"), %arg977: tensor<1024xbf16> loc("p358.6721"), %arg978: tensor<1024x360xbf16> loc("p359.6725"), %arg979: tensor<360xbf16> loc("p360.6875"), %arg980: tensor<32x360xbf16> loc("p361.6943"), %arg981: tensor<32x2880x360xbf16> loc("p362.6948"), %arg982: tensor<32x5760xbf16> loc("p363.6954"), %arg983: tensor<32x360x5760xbf16> loc("p364.6959"), %arg984: tensor<360xbf16> loc("p365.7042"), %arg985: tensor<64xbf16> loc("p366.7065"), %arg986: tensor<128xbf16> loc("p367.7078"), %arg987: tensor<128x360xbf16> loc("p368.7082"), %arg988: tensor<1024xbf16> loc("p369.7127"), %arg989: tensor<1024x360xbf16> loc("p370.7131"), %arg990: tensor<360xbf16> loc("p371.7281"), %arg991: tensor<32x360xbf16> loc("p372.7349"), %arg992: tensor<32x2880x360xbf16> loc("p373.7354"), %arg993: tensor<32x5760xbf16> loc("p374.7360"), %arg994: tensor<32x360x5760xbf16> loc("p375.7365"), %arg995: tensor<360xbf16> loc("p376.7448"), %arg996: tensor<64xbf16> loc("p377.7471"), %arg997: tensor<128xbf16> loc("p378.7484"), %arg998: tensor<128x360xbf16> loc("p379.7488"), %arg999: tensor<1024xbf16> loc("p380.7533"), %arg1000: tensor<1024x360xbf16> loc("p381.7537"), %arg1001: tensor<360xbf16> loc("p382.7687"), %arg1002: tensor<32x360xbf16> loc("p383.7755"), %arg1003: tensor<32x2880x360xbf16> loc("p384.7760"), %arg1004: tensor<32x5760xbf16> loc("p385.7766"), %arg1005: tensor<32x360x5760xbf16> loc("p386.7771"), %arg1006: tensor<360xbf16> loc("p387.7854"), %arg1007: tensor<64xbf16> loc("p388.7877"), %arg1008: tensor<128xbf16> loc("p389.7890"), %arg1009: tensor<128x360xbf16> loc("p390.7894"), %arg1010: tensor<1024xbf16> loc("p391.7939"), %arg1011: tensor<1024x360xbf16> loc("p392.7943"), %arg1012: tensor<360xbf16> loc("p393.8093"), %arg1013: tensor<32x360xbf16> loc("p394.8161"), %arg1014: tensor<32x2880x360xbf16> loc("p395.8166"), %arg1015: tensor<32x5760xbf16> loc("p396.8172"), %arg1016: tensor<32x360x5760xbf16> loc("p397.8177"), %arg1017: tensor<360xbf16> loc("p398.8260"), %arg1018: tensor<64xbf16> loc("p399.8283"), %arg1019: tensor<128xbf16> loc("p400.8296"), %arg1020: tensor<128x360xbf16> loc("p401.8300"), %arg1021: tensor<1024xbf16> loc("p402.8345"), %arg1022: tensor<1024x360xbf16> loc("p403.8349"), %arg1023: tensor<360xbf16> loc("p404.8499"), %arg1024: tensor<32x360xbf16> loc("p405.8567"), %arg1025: tensor<32x2880x360xbf16> loc("p406.8572"), %arg1026: tensor<32x5760xbf16> loc("p407.8578"), %arg1027: tensor<32x360x5760xbf16> loc("p408.8583"), %arg1028: tensor<360xbf16> loc("p409.8666"), %arg1029: tensor<64xbf16> loc("p410.8689"), %arg1030: tensor<128xbf16> loc("p411.8702"), %arg1031: tensor<128x360xbf16> loc("p412.8706"), %arg1032: tensor<1024xbf16> loc("p413.8751"), %arg1033: tensor<1024x360xbf16> loc("p414.8755"), %arg1034: tensor<360xbf16> loc("p415.8905"), %arg1035: tensor<32x360xbf16> loc("p416.8973"), %arg1036: tensor<32x2880x360xbf16> loc("p417.8978"), %arg1037: tensor<32x5760xbf16> loc("p418.8984"), %arg1038: tensor<32x360x5760xbf16> loc("p419.8989"), %arg1039: tensor<360xbf16> loc("p420.9072"), %arg1040: tensor<64xbf16> loc("p421.9095"), %arg1041: tensor<128xbf16> loc("p422.9108"), %arg1042: tensor<128x360xbf16> loc("p423.9112"), %arg1043: tensor<1024xbf16> loc("p424.9157"), %arg1044: tensor<1024x360xbf16> loc("p425.9161"), %arg1045: tensor<360xbf16> loc("p426.9311"), %arg1046: tensor<32x360xbf16> loc("p427.9379"), %arg1047: tensor<32x2880x360xbf16> loc("p428.9384"), %arg1048: tensor<32x5760xbf16> loc("p429.9390"), %arg1049: tensor<32x360x5760xbf16> loc("p430.9395"), %arg1050: tensor<360xbf16> loc("p431.9478"), %arg1051: tensor<64xbf16> loc("p432.9501"), %arg1052: tensor<128xbf16> loc("p433.9514"), %arg1053: tensor<128x360xbf16> loc("p434.9518"), %arg1054: tensor<1024xbf16> loc("p435.9563"), %arg1055: tensor<1024x360xbf16> loc("p436.9567"), %arg1056: tensor<360xbf16> loc("p437.9717"), %arg1057: tensor<32x360xbf16> loc("p438.9785"), %arg1058: tensor<32x2880x360xbf16> loc("p439.9790"), %arg1059: tensor<32x5760xbf16> loc("p440.9796"), %arg1060: tensor<32x360x5760xbf16> loc("p441.9801"), %arg1061: tensor<360xbf16> loc("p442.9884"), %arg1062: tensor<64xbf16> loc("p443.9907"), %arg1063: tensor<128xbf16> loc("p444.9920"), %arg1064: tensor<128x360xbf16> loc("p445.9924"), %arg1065: tensor<1024xbf16> loc("p446.9969"), %arg1066: tensor<1024x360xbf16> loc("p447.9973"), %arg1067: tensor<360xbf16> loc("p448.10123"), %arg1068: tensor<32x360xbf16> loc("p449.10191"), %arg1069: tensor<32x2880x360xbf16> loc("p450.10196"), %arg1070: tensor<32x5760xbf16> loc("p451.10202"), %arg1071: tensor<32x360x5760xbf16> loc("p452.10207"), %arg1072: tensor<360xbf16> loc("p453.10290"), %arg1073: tensor<64xbf16> loc("p454.10313"), %arg1074: tensor<128xbf16> loc("p455.10326"), %arg1075: tensor<128x360xbf16> loc("p456.10330"), %arg1076: tensor<1024xbf16> loc("p457.10375"), %arg1077: tensor<1024x360xbf16> loc("p458.10379"), %arg1078: tensor<360xbf16> loc("p459.10529"), %arg1079: tensor<32x360xbf16> loc("p460.10597"), %arg1080: tensor<32x2880x360xbf16> loc("p461.10602"), %arg1081: tensor<32x5760xbf16> loc("p462.10608"), %arg1082: tensor<32x360x5760xbf16> loc("p463.10613"), %arg1083: tensor<360xbf16> loc("p464.10696"), %arg1084: tensor<64xbf16> loc("p465.10719"), %arg1085: tensor<128xbf16> loc("p466.10732"), %arg1086: tensor<128x360xbf16> loc("p467.10736"), %arg1087: tensor<1024xbf16> loc("p468.10781"), %arg1088: tensor<1024x360xbf16> loc("p469.10785"), %arg1089: tensor<360xbf16> loc("p470.10935"), %arg1090: tensor<32x360xbf16> loc("p471.11003"), %arg1091: tensor<32x2880x360xbf16> loc("p472.11008"), %arg1092: tensor<32x5760xbf16> loc("p473.11014"), %arg1093: tensor<32x360x5760xbf16> loc("p474.11019"), %arg1094: tensor<360xbf16> loc("p475.11102"), %arg1095: tensor<64xbf16> loc("p476.11125"), %arg1096: tensor<128xbf16> loc("p477.11138"), %arg1097: tensor<128x360xbf16> loc("p478.11142"), %arg1098: tensor<1024xbf16> loc("p479.11187"), %arg1099: tensor<1024x360xbf16> loc("p480.11191"), %arg1100: tensor<360xbf16> loc("p481.11341"), %arg1101: tensor<32x360xbf16> loc("p482.11409"), %arg1102: tensor<32x2880x360xbf16> loc("p483.11414"), %arg1103: tensor<32x5760xbf16> loc("p484.11420"), %arg1104: tensor<32x360x5760xbf16> loc("p485.11425"), %arg1105: tensor<360xbf16> loc("p486.11508"), %arg1106: tensor<64xbf16> loc("p487.11531"), %arg1107: tensor<128xbf16> loc("p488.11544"), %arg1108: tensor<128x360xbf16> loc("p489.11548"), %arg1109: tensor<1024xbf16> loc("p490.11593"), %arg1110: tensor<1024x360xbf16> loc("p491.11597"), %arg1111: tensor<360xbf16> loc("p492.11747"), %arg1112: tensor<32x360xbf16> loc("p493.11815"), %arg1113: tensor<32x2880x360xbf16> loc("p494.11820"), %arg1114: tensor<32x5760xbf16> loc("p495.11826"), %arg1115: tensor<32x360x5760xbf16> loc("p496.11831"), %arg1116: tensor<360xbf16> loc("p497.11914"), %arg1117: tensor<64xbf16> loc("p498.11937"), %arg1118: tensor<128xbf16> loc("p499.11950"), %arg1119: tensor<128x360xbf16> loc("p500.11954"), %arg1120: tensor<1024xbf16> loc("p501.11999"), %arg1121: tensor<1024x360xbf16> loc("p502.12003"), %arg1122: tensor<360xbf16> loc("p503.12153"), %arg1123: tensor<32x360xbf16> loc("p504.12221"), %arg1124: tensor<32x2880x360xbf16> loc("p505.12226"), %arg1125: tensor<32x5760xbf16> loc("p506.12232"), %arg1126: tensor<32x360x5760xbf16> loc("p507.12237"), %arg1127: tensor<360xbf16> loc("p508.12320"), %arg1128: tensor<64xbf16> loc("p509.12343"), %arg1129: tensor<128xbf16> loc("p510.12356"), %arg1130: tensor<128x360xbf16> loc("p511.12360"), %arg1131: tensor<1024xbf16> loc("p512.12405"), %arg1132: tensor<1024x360xbf16> loc("p513.12409"), %arg1133: tensor<360xbf16> loc("p514.12559"), %arg1134: tensor<32x360xbf16> loc("p515.12627"), %arg1135: tensor<32x2880x360xbf16> loc("p516.12632"), %arg1136: tensor<32x5760xbf16> loc("p517.12638"), %arg1137: tensor<32x360x5760xbf16> loc("p518.12643"), %arg1138: tensor<360xbf16> loc("p519.12726"), %arg1139: tensor<64xbf16> loc("p520.12749"), %arg1140: tensor<128xbf16> loc("p521.12762"), %arg1141: tensor<128x360xbf16> loc("p522.12766"), %arg1142: tensor<1024xbf16> loc("p523.12811"), %arg1143: tensor<1024x360xbf16> loc("p524.12815"), %arg1144: tensor<360xbf16> loc("p525.12965"), %arg1145: tensor<32x360xbf16> loc("p526.13033"), %arg1146: tensor<32x2880x360xbf16> loc("p527.13038"), %arg1147: tensor<32x5760xbf16> loc("p528.13044"), %arg1148: tensor<32x360x5760xbf16> loc("p529.13049"), %arg1149: tensor<360xbf16> loc("p530.13132"), %arg1150: tensor<64xbf16> loc("p531.13155"), %arg1151: tensor<128xbf16> loc("p532.13168"), %arg1152: tensor<128x360xbf16> loc("p533.13172"), %arg1153: tensor<1024xbf16> loc("p534.13217"), %arg1154: tensor<1024x360xbf16> loc("p535.13221"), %arg1155: tensor<360xbf16> loc("p536.13371"), %arg1156: tensor<32x360xbf16> loc("p537.13439"), %arg1157: tensor<32x2880x360xbf16> loc("p538.13444"), %arg1158: tensor<32x5760xbf16> loc("p539.13450"), %arg1159: tensor<32x360x5760xbf16> loc("p540.13455"), %arg1160: tensor<360xbf16> loc("p541.13538"), %arg1161: tensor<64xbf16> loc("p542.13561"), %arg1162: tensor<128xbf16> loc("p543.13574"), %arg1163: tensor<128x360xbf16> loc("p544.13578"), %arg1164: tensor<1024xbf16> loc("p545.13623"), %arg1165: tensor<1024x360xbf16> loc("p546.13627"), %arg1166: tensor<360xbf16> loc("p547.13777"), %arg1167: tensor<32x360xbf16> loc("p548.13845"), %arg1168: tensor<32x2880x360xbf16> loc("p549.13850"), %arg1169: tensor<32x5760xbf16> loc("p550.13856"), %arg1170: tensor<32x360x5760xbf16> loc("p551.13861"), %arg1171: tensor<360xbf16> loc("p552.13944"), %arg1172: tensor<64xbf16> loc("p553.13967"), %arg1173: tensor<128xbf16> loc("p554.13980"), %arg1174: tensor<128x360xbf16> loc("p555.13984"), %arg1175: tensor<1024xbf16> loc("p556.14029"), %arg1176: tensor<1024x360xbf16> loc("p557.14033"), %arg1177: tensor<360xbf16> loc("p558.14183"), %arg1178: tensor<32x360xbf16> loc("p559.14251"), %arg1179: tensor<32x2880x360xbf16> loc("p560.14256"), %arg1180: tensor<32x5760xbf16> loc("p561.14262"), %arg1181: tensor<32x360x5760xbf16> loc("p562.14267"), %arg1182: tensor<360xbf16> loc("p563.14350"), %arg1183: tensor<64xbf16> loc("p564.14373"), %arg1184: tensor<128xbf16> loc("p565.14386"), %arg1185: tensor<128x360xbf16> loc("p566.14390"), %arg1186: tensor<1024xbf16> loc("p567.14435"), %arg1187: tensor<1024x360xbf16> loc("p568.14439"), %arg1188: tensor<360xbf16> loc("p569.14589"), %arg1189: tensor<32x360xbf16> loc("p570.14657"), %arg1190: tensor<32x2880x360xbf16> loc("p571.14662"), %arg1191: tensor<32x5760xbf16> loc("p572.14668"), %arg1192: tensor<32x360x5760xbf16> loc("p573.14673"), %arg1193: tensor<360xbf16> loc("p574.14756"), %arg1194: tensor<64xbf16> loc("p575.14779"), %arg1195: tensor<128xbf16> loc("p576.14792"), %arg1196: tensor<128x360xbf16> loc("p577.14796"), %arg1197: tensor<1024xbf16> loc("p578.14841"), %arg1198: tensor<1024x360xbf16> loc("p579.14845"), %arg1199: tensor<360xbf16> loc("p580.14995"), %arg1200: tensor<32x360xbf16> loc("p581.15063"), %arg1201: tensor<32x2880x360xbf16> loc("p582.15068"), %arg1202: tensor<32x5760xbf16> loc("p583.15074"), %arg1203: tensor<32x360x5760xbf16> loc("p584.15079"), %arg1204: tensor<360xbf16> loc("p585.15162"), %arg1205: tensor<64xbf16> loc("p586.15185"), %arg1206: tensor<128xbf16> loc("p587.15198"), %arg1207: tensor<128x360xbf16> loc("p588.15202"), %arg1208: tensor<1024xbf16> loc("p589.15247"), %arg1209: tensor<1024x360xbf16> loc("p590.15251"), %arg1210: tensor<360xbf16> loc("p591.15401"), %arg1211: tensor<32x360xbf16> loc("p592.15469"), %arg1212: tensor<32x2880x360xbf16> loc("p593.15474"), %arg1213: tensor<32x5760xbf16> loc("p594.15480"), %arg1214: tensor<32x360x5760xbf16> loc("p595.15485"), %arg1215: tensor<360xbf16> loc("p596.15568"), %arg1216: tensor<64xbf16> loc("p597.15591"), %arg1217: tensor<128xbf16> loc("p598.15604"), %arg1218: tensor<128x360xbf16> loc("p599.15608"), %arg1219: tensor<1024xbf16> loc("p600.15653"), %arg1220: tensor<1024x360xbf16> loc("p601.15657"), %arg1221: tensor<360xbf16> loc("p602.15807"), %arg1222: tensor<32x360xbf16> loc("p603.15875"), %arg1223: tensor<32x2880x360xbf16> loc("p604.15880"), %arg1224: tensor<32x5760xbf16> loc("p605.15886"), %arg1225: tensor<32x360x5760xbf16> loc("p606.15891"), %arg1226: tensor<360xbf16> loc("p607.15974"), %arg1227: tensor<64xbf16> loc("p608.15997"), %arg1228: tensor<128xbf16> loc("p609.16010"), %arg1229: tensor<128x360xbf16> loc("p610.16014"), %arg1230: tensor<1024xbf16> loc("p611.16059"), %arg1231: tensor<1024x360xbf16> loc("p612.16063"), %arg1232: tensor<360xbf16> loc("p613.16213"), %arg1233: tensor<32x360xbf16> loc("p614.16281"), %arg1234: tensor<32x2880x360xbf16> loc("p615.16286"), %arg1235: tensor<32x5760xbf16> loc("p616.16292"), %arg1236: tensor<32x360x5760xbf16> loc("p617.16297"), %arg1237: tensor<360xbf16> loc("p618.16380")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
      %c = stablehlo.constant dense<"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tensor<128xi64> loc(#loc)
      %c_0 = stablehlo.constant dense<0> : tensor<i64> loc(#loc)
      %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
      %cst_2 = stablehlo.constant dense<3.47222231E-4> : tensor<f32> loc(#loc)
      %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<f32> loc(#loc)
      %cst_4 = stablehlo.constant dense<"0x000000000000803F0000004000004040000080400000A0400000C0400000E0400000004100001041000020410000304100004041000050410000604100007041000080410000884100009041000098410000A0410000A8410000B0410000B8410000C0410000C8410000D0410000D8410000E0410000E8410000F0410000F84100000042000004420000084200000C4200001042000014420000184200001C4200002042000024420000284200002C4200003042000034420000384200003C4200004042000044420000484200004C4200005042000054420000584200005C4200006042000064420000684200006C4200007042000074420000784200007C42000080420000824200008442000086420000884200008A4200008C4200008E42000090420000924200009442000096420000984200009A4200009C4200009E420000A0420000A2420000A4420000A6420000A8420000AA420000AC420000AE420000B0420000B2420000B4420000B6420000B8420000BA420000BC420000BE420000C0420000C2420000C4420000C6420000C8420000CA420000CC420000CE420000D0420000D2420000D4420000D6420000D8420000DA420000DC420000DE420000E0420000E2420000E4420000E6420000E8420000EA420000EC420000EE420000F0420000F2420000F4420000F6420000F8420000FA420000FC420000FE42"> : tensor<1x1x128xf32> loc(#loc)
      %cst_5 = stablehlo.constant dense<1.34657359> : tensor<f32> loc(#loc)
      %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16> loc(#loc)
      %c_7 = stablehlo.constant dense<"0x80FFFFFFFFFFFFFF81FFFFFFFFFFFFFF82FFFFFFFFFFFFFF83FFFFFFFFFFFFFF84FFFFFFFFFFFFFF85FFFFFFFFFFFFFF86FFFFFFFFFFFFFF87FFFFFFFFFFFFFF88FFFFFFFFFFFFFF89FFFFFFFFFFFFFF8AFFFFFFFFFFFFFF8BFFFFFFFFFFFFFF8CFFFFFFFFFFFFFF8DFFFFFFFFFFFFFF8EFFFFFFFFFFFFFF8FFFFFFFFFFFFFFF90FFFFFFFFFFFFFF91FFFFFFFFFFFFFF92FFFFFFFFFFFFFF93FFFFFFFFFFFFFF94FFFFFFFFFFFFFF95FFFFFFFFFFFFFF96FFFFFFFFFFFFFF97FFFFFFFFFFFFFF98FFFFFFFFFFFFFF99FFFFFFFFFFFFFF9AFFFFFFFFFFFFFF9BFFFFFFFFFFFFFF9CFFFFFFFFFFFFFF9DFFFFFFFFFFFFFF9EFFFFFFFFFFFFFF9FFFFFFFFFFFFFFFA0FFFFFFFFFFFFFFA1FFFFFFFFFFFFFFA2FFFFFFFFFFFFFFA3FFFFFFFFFFFFFFA4FFFFFFFFFFFFFFA5FFFFFFFFFFFFFFA6FFFFFFFFFFFFFFA7FFFFFFFFFFFFFFA8FFFFFFFFFFFFFFA9FFFFFFFFFFFFFFAAFFFFFFFFFFFFFFABFFFFFFFFFFFFFFACFFFFFFFFFFFFFFADFFFFFFFFFFFFFFAEFFFFFFFFFFFFFFAFFFFFFFFFFFFFFFB0FFFFFFFFFFFFFFB1FFFFFFFFFFFFFFB2FFFFFFFFFFFFFFB3FFFFFFFFFFFFFFB4FFFFFFFFFFFFFFB5FFFFFFFFFFFFFFB6FFFFFFFFFFFFFFB7FFFFFFFFFFFFFFB8FFFFFFFFFFFFFFB9FFFFFFFFFFFFFFBAFFFFFFFFFFFFFFBBFFFFFFFFFFFFFFBCFFFFFFFFFFFFFFBDFFFFFFFFFFFFFFBEFFFFFFFFFFFFFFBFFFFFFFFFFFFFFFC0FFFFFFFFFFFFFFC1FFFFFFFFFFFFFFC2FFFFFFFFFFFFFFC3FFFFFFFFFFFFFFC4FFFFFFFFFFFFFFC5FFFFFFFFFFFFFFC6FFFFFFFFFFFFFFC7FFFFFFFFFFFFFFC8FFFFFFFFFFFFFFC9FFFFFFFFFFFFFFCAFFFFFFFFFFFFFFCBFFFFFFFFFFFFFFCCFFFFFFFFFFFFFFCDFFFFFFFFFFFFFFCEFFFFFFFFFFFFFFCFFFFFFFFFFFFFFFD0FFFFFFFFFFFFFFD1FFFFFFFFFFFFFFD2FFFFFFFFFFFFFFD3FFFFFFFFFFFFFFD4FFFFFFFFFFFFFFD5FFFFFFFFFFFFFFD6FFFFFFFFFFFFFFD7FFFFFFFFFFFFFFD8FFFFFFFFFFFFFFD9FFFFFFFFFFFFFFDAFFFFFFFFFFFFFFDBFFFFFFFFFFFFFFDCFFFFFFFFFFFFFFDDFFFFFFFFFFFFFFDEFFFFFFFFFFFFFFDFFFFFFFFFFFFFFFE0FFFFFFFFFFFFFFE1FFFFFFFFFFFFFFE2FFFFFFFFFFFFFFE3FFFFFFFFFFFFFFE4FFFFFFFFFFFFFFE5FFFFFFFFFFFFFFE6FFFFFFFFFFFFFFE7FFFFFFFFFFFFFFE8FFFFFFFFFFFFFFE9FFFFFFFFFFFFFFEAFFFFFFFFFFFFFFEBFFFFFFFFFFFFFFECFFFFFFFFFFFFFFEDFFFFFFFFFFFFFFEEFFFFFFFFFFFFFFEFFFFFFFFFFFFFFFF0FFFFFFFFFFFFFFF1FFFFFFFFFFFFFFF2FFFFFFFFFFFFFFF3FFFFFFFFFFFFFFF4FFFFFFFFFFFFFFF5FFFFFFFFFFFFFFF6FFFFFFFFFFFFFFF7FFFFFFFFFFFFFFF8FFFFFFFFFFFFFFF9FFFFFFFFFFFFFFFAFFFFFFFFFFFFFFFBFFFFFFFFFFFFFFFCFFFFFFFFFFFFFFFDFFFFFFFFFFFFFFFEFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF"> : tensor<128xi64> loc(#loc)
      %c_8 = stablehlo.constant dense<"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"> : tensor<1x128x2xi64> loc(#loc)
      %cst_9 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16> loc(#loc)
      %cst_10 = stablehlo.constant dense<-7.000000e+00> : tensor<bf16> loc(#loc)
      %cst_11 = stablehlo.constant dense<7.000000e+00> : tensor<bf16> loc(#loc)
      %cst_12 = stablehlo.constant dense<1.000000e+00> : tensor<bf16> loc(#loc)
      %cst_13 = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
      %cst_14 = stablehlo.constant dense<1.703130e+00> : tensor<bf16> loc(#loc)
      %cst_15 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %2 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<1x1x128x128xbf16> loc(#loc)
      %8 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<1x1x128x128xbf16> loc(#loc)
      %9 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %10 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<1x128x32xf32> loc(#loc)
      %11 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %12 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %13 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %14 = stablehlo.broadcast_in_dim %c_0, dims = [] : (tensor<i64>) -> tensor<1x128xi64> loc(#loc)
      %15 = stablehlo.reshape %arg838 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc620)
      %16 = stablehlo.reshape %15 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc621)
      %17 = stablehlo.convert %16 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc622)
      %18 = stablehlo.broadcast_in_dim %17, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc623)
      %19 = stablehlo.reshape %arg837 : (tensor<201088x360xbf16>) -> tensor<1x201088x360xbf16> loc(#loc624)
      %20 = stablehlo.reshape %19 : (tensor<1x201088x360xbf16>) -> tensor<201088x360xbf16> loc(#loc625)
      %21 = stablehlo.reshape %arg836 : (tensor<1x128xi64>) -> tensor<1x1x128xi64> loc(#loc626)
      %22 = stablehlo.reshape %21 : (tensor<1x1x128xi64>) -> tensor<128xi64> loc(#loc627)
      %23 = stablehlo.convert %22 : (tensor<128xi64>) -> tensor<128xui32> loc(#loc628)
      %24 = "stablehlo.gather"(%20, %23) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 360>}> : (tensor<201088x360xbf16>, tensor<128xui32>) -> tensor<128x360xbf16> loc(#loc629)
      %25 = stablehlo.reshape %24 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc630)
      %26 = stablehlo.convert %25 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc631)
      %27 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %28 = stablehlo.power %26, %27 : tensor<1x128x360xf32> loc(#loc632)
      %29 = stablehlo.reduce(%28 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc633)
      %30 = "stablehlo.all_reduce"(%29) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.1542"), %arg1239: tensor<f32> loc("reduce.1542")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc633)
        stablehlo.return %7360 : tensor<f32> loc(#loc633)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc633)
      %31 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %32 = stablehlo.multiply %30, %31 : tensor<1x128xf32> loc(#loc634)
      %33 = stablehlo.reshape %32 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc635)
      %34 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %35 = stablehlo.add %33, %34 : tensor<1x128x1xf32> loc(#loc636)
      %36 = stablehlo.rsqrt %35 : tensor<1x128x1xf32> loc(#loc637)
      %37 = stablehlo.reshape %36 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc638)
      %38 = stablehlo.broadcast_in_dim %37, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc639)
      %39 = stablehlo.multiply %26, %38 : tensor<1x128x360xf32> loc(#loc640)
      %40 = stablehlo.multiply %18, %39 : tensor<1x128x360xf32> loc(#loc641)
      %41 = stablehlo.convert %40 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc642)
      %42 = stablehlo.reshape %41 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc643)
      %43 = stablehlo.reshape %arg846 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc644)
      %44 = stablehlo.reshape %43 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc645)
      %45 = stablehlo.transpose %44, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc646)
      %46 = stablehlo.dot_general %42, %45, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc647)
      %47 = "stablehlo.all_reduce"(%46) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.1784"), %arg1239: tensor<bf16> loc("dot.1784")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc647)
        stablehlo.return %7360 : tensor<bf16> loc(#loc647)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc647)
      %48 = stablehlo.reshape %47 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc648)
      %49 = stablehlo.reshape %arg845 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc649)
      %50 = stablehlo.reshape %49 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc650)
      %51 = stablehlo.broadcast_in_dim %50, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc651)
      %52 = stablehlo.add %48, %51 : tensor<1x128x1024xbf16> loc(#loc652)
      %53 = stablehlo.reshape %52 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc653)
      %54 = stablehlo.transpose %53, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc654)
      %55 = stablehlo.slice %54 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc655)
      %56 = stablehlo.reshape %arg842 : (tensor<32xf32>) -> tensor<1x1x32xf32> loc(#loc656)
      %57 = stablehlo.reshape %56 : (tensor<1x1x32xf32>) -> tensor<1x32x1xf32> loc(#loc657)
      %58 = stablehlo.dot_general %57, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x32x1xf32>, tensor<1x1x128xf32>) -> tensor<1x32x128xf32> loc(#loc658)
      %59 = stablehlo.transpose %58, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,128,32]{1,2,0}"} : (tensor<1x32x128xf32>) -> tensor<1x128x32xf32> loc(#loc659)
      %60 = stablehlo.cosine %59 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,128,32]{1,2,0}"} : tensor<1x128x32xf32> loc(#loc660)
      %61 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<1x128x32xf32> loc(#loc)
      %62 = stablehlo.multiply %60, %61 : tensor<1x128x32xf32> loc(#loc661)
      %63 = stablehlo.convert %62 : (tensor<1x128x32xf32>) -> tensor<1x128x32xbf16> loc(#loc662)
      %64 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x128x32xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc663)
      %65 = stablehlo.multiply %55, %64 : tensor<1x16x128x32xbf16> loc(#loc664)
      %66 = stablehlo.slice %54 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc665)
      %67 = stablehlo.sine %59 {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,128,32]{1,2,0}"} : tensor<1x128x32xf32> loc(#loc666)
      %68 = stablehlo.multiply %67, %10 : tensor<1x128x32xf32> loc(#loc667)
      %69 = stablehlo.convert %68 : (tensor<1x128x32xf32>) -> tensor<1x128x32xbf16> loc(#loc668)
      %70 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x128x32xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc669)
      %71 = stablehlo.multiply %66, %70 : tensor<1x16x128x32xbf16> loc(#loc670)
      %72 = stablehlo.subtract %65, %71 : tensor<1x16x128x32xbf16> loc(#loc671)
      %73 = stablehlo.multiply %66, %64 : tensor<1x16x128x32xbf16> loc(#loc672)
      %74 = stablehlo.multiply %55, %70 : tensor<1x16x128x32xbf16> loc(#loc673)
      %75 = stablehlo.add %73, %74 : tensor<1x16x128x32xbf16> loc(#loc674)
      %76 = stablehlo.concatenate %72, %75, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc675)
      %77 = stablehlo.reshape %arg844 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc676)
      %78 = stablehlo.reshape %77 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc677)
      %79 = stablehlo.transpose %78, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc678)
      %80 = stablehlo.dot_general %42, %79, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc679)
      %81 = "stablehlo.all_reduce"(%80) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.1730"), %arg1239: tensor<bf16> loc("dot.1730")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc679)
        stablehlo.return %7360 : tensor<bf16> loc(#loc679)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc679)
      %82 = stablehlo.reshape %81 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc680)
      %83 = stablehlo.reshape %arg843 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc681)
      %84 = stablehlo.reshape %83 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc682)
      %85 = stablehlo.broadcast_in_dim %84, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc683)
      %86 = stablehlo.add %82, %85 : tensor<1x128x128xbf16> loc(#loc684)
      %87 = stablehlo.reshape %86 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc685)
      %88 = stablehlo.transpose %87, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc686)
      %89 = stablehlo.slice %88 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc687)
      %90 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] : (tensor<1x128x32xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc688)
      %91 = stablehlo.multiply %89, %90 : tensor<1x2x128x32xbf16> loc(#loc689)
      %92 = stablehlo.slice %88 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc690)
      %93 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x128x32xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc691)
      %94 = stablehlo.multiply %92, %93 : tensor<1x2x128x32xbf16> loc(#loc692)
      %95 = stablehlo.subtract %91, %94 : tensor<1x2x128x32xbf16> loc(#loc693)
      %96 = stablehlo.multiply %92, %90 : tensor<1x2x128x32xbf16> loc(#loc694)
      %97 = stablehlo.multiply %89, %93 : tensor<1x2x128x32xbf16> loc(#loc695)
      %98 = stablehlo.add %96, %97 : tensor<1x2x128x32xbf16> loc(#loc696)
      %99 = stablehlo.concatenate %95, %98, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc697)
      %100 = stablehlo.broadcast_in_dim %99, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc698)
      %101 = stablehlo.reshape %100 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc699)
      %102 = stablehlo.transpose %101, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc700)
      %103 = stablehlo.dot_general %76, %102, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc701)
      %104 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %105 = stablehlo.multiply %103, %104 : tensor<1x16x128x128xbf16> loc(#loc702)
      %106 = stablehlo.broadcast_in_dim %arg841, dims = [] : (tensor<i1>) -> tensor<128x128xi1> loc(#loc703)
      %107 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<128xi64>) -> tensor<128x128xi64> loc(#loc704)
      %108 = stablehlo.broadcast_in_dim %c_7, dims = [0] : (tensor<128xi64>) -> tensor<128x128xi64> loc(#loc705)
      %109 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<128xi64>) -> tensor<128x128xi64> loc(#loc704)
      %110 = stablehlo.compare  GT, %109, %108 : (tensor<128x128xi64>, tensor<128x128xi64>) -> tensor<128x128xi1> loc(#loc706)
      %111 = stablehlo.and %106, %110 : tensor<128x128xi1> loc(#loc707)
      %112 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<128xi64>) -> tensor<128x128xi64> loc(#loc708)
      %113 = stablehlo.compare  LE, %107, %112 : (tensor<128x128xi64>, tensor<128x128xi64>) -> tensor<128x128xi1> loc(#loc709)
      %114 = stablehlo.and %111, %113 : tensor<128x128xi1> loc(#loc710)
      %115 = stablehlo.and %106, %114 : tensor<128x128xi1> loc(#loc711)
      %116 = stablehlo.reshape %115 : (tensor<128x128xi1>) -> tensor<1x128x128xi1> loc(#loc712)
      %117 = stablehlo.reshape %arg840 : (tensor<1x128xi64>) -> tensor<1x1x128xi64> loc(#loc713)
      %118 = stablehlo.reshape %117 : (tensor<1x1x128xi64>) -> tensor<1x128xi64> loc(#loc714)
      %119 = stablehlo.compare  NE, %118, %14 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1> loc(#loc715)
      %120 = "stablehlo.gather"(%119, %c_8) <{dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1>}> : (tensor<1x128xi1>, tensor<1x128x2xi64>) -> tensor<1x128xi1> loc(#loc716)
      %121 = stablehlo.broadcast_in_dim %120, dims = [0, 2] : (tensor<1x128xi1>) -> tensor<1x128x128xi1> loc(#loc717)
      %122 = stablehlo.and %116, %121 : tensor<1x128x128xi1> loc(#loc718)
      %123 = stablehlo.reshape %122 : (tensor<1x128x128xi1>) -> tensor<1x1x128x128xi1> loc(#loc719)
      %124 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<1x1x128x128xbf16> loc(#loc)
      %125 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<1x1x128x128xbf16> loc(#loc)
      %126 = stablehlo.select %123, %125, %124 : tensor<1x1x128x128xi1>, tensor<1x1x128x128xbf16> loc(#loc720)
      %127 = stablehlo.reshape %126 : (tensor<1x1x128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc721)
      %128 = stablehlo.broadcast_in_dim %127, dims = [0, 2, 3] : (tensor<1x128x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc722)
      %129 = stablehlo.add %105, %128 : tensor<1x16x128x128xbf16> loc(#loc723)
      %130 = stablehlo.reshape %arg839 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc221)
      %131 = "stablehlo.all_to_all"(%130) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc221)
      %132 = stablehlo.slice %131 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc221)
      %133 = stablehlo.reshape %132 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc221)
      %134 = stablehlo.reshape %133 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc724)
      %135 = stablehlo.reshape %134 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc725)
      %136 = stablehlo.broadcast_in_dim %135, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc726)
      %137 = stablehlo.concatenate %129, %136, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc727)
      %138 = stablehlo.reshape %arg852 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc728)
      %139 = stablehlo.reshape %138 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc729)
      %140 = stablehlo.convert %139 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc730)
      %141 = stablehlo.broadcast_in_dim %140, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc731)
      %142 = stablehlo.reduce(%137 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc732)
      %143 = stablehlo.broadcast_in_dim %142, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc733)
      %144 = stablehlo.subtract %137, %143 : tensor<1x16x128x129xbf16> loc(#loc734)
      %145 = stablehlo.reduce(%144 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc735)
      %146 = stablehlo.broadcast_in_dim %145, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc736)
      %147 = stablehlo.subtract %144, %146 : tensor<1x16x128x129xbf16> loc(#loc737)
      %148 = stablehlo.exponential %147 : tensor<1x16x128x129xbf16> loc(#loc738)
      %149 = stablehlo.reduce(%148 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc739)
      %150 = stablehlo.broadcast_in_dim %149, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc740)
      %151 = stablehlo.divide %148, %150 : tensor<1x16x128x129xbf16> loc(#loc741)
      %152 = stablehlo.slice %151 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc742)
      %153 = stablehlo.reshape %arg835 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc743)
      %154 = stablehlo.reshape %153 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc744)
      %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc745)
      %156 = stablehlo.dot_general %42, %155, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc746)
      %157 = "stablehlo.all_reduce"(%156) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.1570"), %arg1239: tensor<bf16> loc("dot.1570")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc746)
        stablehlo.return %7360 : tensor<bf16> loc(#loc746)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc746)
      %158 = stablehlo.reshape %157 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc747)
      %159 = stablehlo.reshape %arg834 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc748)
      %160 = stablehlo.reshape %159 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc749)
      %161 = stablehlo.broadcast_in_dim %160, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc750)
      %162 = stablehlo.add %158, %161 : tensor<1x128x128xbf16> loc(#loc751)
      %163 = stablehlo.reshape %162 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc752)
      %164 = stablehlo.transpose %163, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc753)
      %165 = stablehlo.broadcast_in_dim %164, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc754)
      %166 = stablehlo.reshape %165 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc755)
      %167 = stablehlo.dot_general %152, %166, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc756)
      %168 = stablehlo.transpose %167, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc757)
      %169 = stablehlo.reshape %168 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc758)
      %170 = stablehlo.reshape %arg833 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc759)
      %171 = stablehlo.reshape %170 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc760)
      %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc761)
      %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc762)
      %174 = "stablehlo.all_reduce"(%173) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.1891"), %arg1239: tensor<bf16> loc("dot.1891")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc762)
        stablehlo.return %7360 : tensor<bf16> loc(#loc762)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc762)
      %175 = stablehlo.reshape %174 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc763)
      %176 = stablehlo.reshape %arg832 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc764)
      %177 = stablehlo.reshape %176 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc765)
      %178 = stablehlo.broadcast_in_dim %177, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc766)
      %179 = stablehlo.add %175, %178 : tensor<1x128x360xbf16> loc(#loc767)
      %180 = stablehlo.add %25, %179 : tensor<1x128x360xbf16> loc(#loc768)
      %181 = stablehlo.reshape %arg847 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc769)
      %182 = stablehlo.reshape %181 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc770)
      %183 = stablehlo.convert %182 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc771)
      %184 = stablehlo.broadcast_in_dim %183, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc772)
      %185 = stablehlo.convert %180 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc773)
      %186 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %187 = stablehlo.power %185, %186 : tensor<1x128x360xf32> loc(#loc774)
      %188 = stablehlo.reduce(%187 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc775)
      %189 = "stablehlo.all_reduce"(%188) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.1909"), %arg1239: tensor<f32> loc("reduce.1909")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc775)
        stablehlo.return %7360 : tensor<f32> loc(#loc775)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc775)
      %190 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %191 = stablehlo.multiply %189, %190 : tensor<1x128xf32> loc(#loc776)
      %192 = stablehlo.reshape %191 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc777)
      %193 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %194 = stablehlo.add %192, %193 : tensor<1x128x1xf32> loc(#loc778)
      %195 = stablehlo.rsqrt %194 : tensor<1x128x1xf32> loc(#loc779)
      %196 = stablehlo.reshape %195 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc780)
      %197 = stablehlo.broadcast_in_dim %196, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc781)
      %198 = stablehlo.multiply %185, %197 : tensor<1x128x360xf32> loc(#loc782)
      %199 = stablehlo.multiply %184, %198 : tensor<1x128x360xf32> loc(#loc783)
      %200 = stablehlo.convert %199 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc784)
      %201 = stablehlo.reshape %200 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc785)
      %202 = stablehlo.broadcast_in_dim %201, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc786)
      %203 = stablehlo.dot_general %202, %arg851, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc787)
      %204 = "stablehlo.all_reduce"(%203) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2021"), %arg1239: tensor<bf16> loc("dot.2021")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc787)
        stablehlo.return %7360 : tensor<bf16> loc(#loc787)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc787)
      %205 = stablehlo.reshape %arg850 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc788)
      %206 = stablehlo.reshape %205 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc789)
      %207 = stablehlo.broadcast_in_dim %206, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc790)
      %208 = stablehlo.add %204, %207 : tensor<32x128x5760xbf16> loc(#loc791)
      %209 = stablehlo.slice %208 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc792)
      %210 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %211 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %212 = stablehlo.clamp %211, %209, %210 : tensor<32x128x2880xbf16> loc(#loc793)
      %213 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %214 = stablehlo.add %212, %213 : tensor<32x128x2880xbf16> loc(#loc794)
      %215 = stablehlo.slice %208 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc795)
      %216 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %217 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %218 = stablehlo.clamp %216, %215, %217 : tensor<32x128x2880xbf16> loc(#loc796)
      %219 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %220 = stablehlo.multiply %218, %219 : tensor<32x128x2880xbf16> loc(#loc797)
      %221 = stablehlo.logistic %220 : tensor<32x128x2880xbf16> loc(#loc798)
      %222 = stablehlo.multiply %218, %221 : tensor<32x128x2880xbf16> loc(#loc799)
      %223 = stablehlo.multiply %214, %222 : tensor<32x128x2880xbf16> loc(#loc800)
      %224 = stablehlo.dot_general %223, %arg849, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc801)
      %225 = stablehlo.reshape %arg848 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc802)
      %226 = stablehlo.reshape %225 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc803)
      %227 = stablehlo.broadcast_in_dim %226, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc804)
      %228 = stablehlo.add %224, %227 : tensor<32x128x360xbf16> loc(#loc805)
      %229 = stablehlo.reshape %228 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc806)
      %230 = stablehlo.iota dim = 0 : tensor<128xi64> loc(#loc807)
      %231 = stablehlo.broadcast_in_dim %230, dims = [0] : (tensor<128xi64>) -> tensor<128x4x1xi64> loc(#loc807)
      %232 = stablehlo.reshape %arg831 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc808)
      %233 = stablehlo.reshape %232 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc809)
      %234 = stablehlo.transpose %233, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc810)
      %235 = stablehlo.dot_general %201, %234, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc811)
      %236 = "stablehlo.all_reduce"(%235) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.1937"), %arg1239: tensor<bf16> loc("dot.1937")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc811)
        stablehlo.return %7360 : tensor<bf16> loc(#loc811)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc811)
      %237 = stablehlo.reshape %arg830 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc812)
      %238 = stablehlo.reshape %237 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc813)
      %239 = stablehlo.broadcast_in_dim %238, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc814)
      %240 = stablehlo.add %236, %239 : tensor<128x128xbf16> loc(#loc815)
      %241 = stablehlo.iota dim = 0 : tensor<128xi32> loc(#loc816)
      %242 = stablehlo.broadcast_in_dim %241, dims = [1] : (tensor<128xi32>) -> tensor<128x128xi32> loc(#loc816)
      %243:2 = "stablehlo.sort"(%240, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.1957"), %arg1239: tensor<bf16> loc("sort.1957"), %arg1240: tensor<i32> loc("sort.1957"), %arg1241: tensor<i32> loc("sort.1957")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc818)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc817)
      %244 = stablehlo.slice %243#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc819)
      %245 = stablehlo.convert %244 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc820)
      %246 = stablehlo.reshape %245 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc821)
      %247 = stablehlo.concatenate %231, %246, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc822)
      %248 = stablehlo.slice %243#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc823)
      %249 = stablehlo.reduce(%248 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc824)
      %250 = stablehlo.broadcast_in_dim %249, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc825)
      %251 = stablehlo.subtract %248, %250 : tensor<128x4xbf16> loc(#loc826)
      %252 = stablehlo.exponential %251 : tensor<128x4xbf16> loc(#loc827)
      %253 = stablehlo.reduce(%252 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc828)
      %254 = stablehlo.broadcast_in_dim %253, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc829)
      %255 = stablehlo.divide %252, %254 : tensor<128x4xbf16> loc(#loc830)
      %256 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %257 = "stablehlo.all_gather"(%256) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %258 = "stablehlo.scatter"(%257, %247, %255) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.1991"), %arg1239: tensor<bf16> loc("scatter.1991")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc831)
      %259 = stablehlo.reshape %258 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc831)
      %260 = "stablehlo.all_to_all"(%259) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc831)
      %261 = stablehlo.slice %260 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc831)
      %262 = stablehlo.reshape %261 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc831)
      %263 = stablehlo.transpose %262, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc832)
      %264 = stablehlo.reshape %263 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc833)
      %265 = stablehlo.broadcast_in_dim %264, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc834)
      %266 = stablehlo.multiply %229, %265 : tensor<32x1x128x360xbf16> loc(#loc835)
      %267 = stablehlo.reduce(%266 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc836)
      %268 = "stablehlo.all_reduce"(%267) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.2063"), %arg1239: tensor<bf16> loc("reduce.2063")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc836)
        stablehlo.return %7360 : tensor<bf16> loc(#loc836)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc836)
      %269 = stablehlo.add %180, %268 : tensor<1x128x360xbf16> loc(#loc837)
      %270 = stablehlo.convert %269 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc838)
      %271 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %272 = stablehlo.power %270, %271 : tensor<1x128x360xf32> loc(#loc839)
      %273 = stablehlo.reduce(%272 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc840)
      %274 = "stablehlo.all_reduce"(%273) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.2076"), %arg1239: tensor<f32> loc("reduce.2076")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc840)
        stablehlo.return %7360 : tensor<f32> loc(#loc840)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc840)
      %275 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %276 = stablehlo.multiply %274, %275 : tensor<1x128xf32> loc(#loc841)
      %277 = stablehlo.reshape %276 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc842)
      %278 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %279 = stablehlo.add %277, %278 : tensor<1x128x1xf32> loc(#loc843)
      %280 = stablehlo.rsqrt %279 : tensor<1x128x1xf32> loc(#loc844)
      %281 = stablehlo.reshape %280 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc845)
      %282 = stablehlo.broadcast_in_dim %281, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc846)
      %283 = stablehlo.multiply %270, %282 : tensor<1x128x360xf32> loc(#loc847)
      %284 = stablehlo.multiply %141, %283 : tensor<1x128x360xf32> loc(#loc848)
      %285 = stablehlo.convert %284 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc849)
      %286 = stablehlo.reshape %285 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc850)
      %287 = stablehlo.reshape %arg857 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc851)
      %288 = stablehlo.reshape %287 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc852)
      %289 = stablehlo.transpose %288, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc853)
      %290 = stablehlo.dot_general %286, %289, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc854)
      %291 = "stablehlo.all_reduce"(%290) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2265"), %arg1239: tensor<bf16> loc("dot.2265")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc854)
        stablehlo.return %7360 : tensor<bf16> loc(#loc854)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc854)
      %292 = stablehlo.reshape %291 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc855)
      %293 = stablehlo.reshape %arg856 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc856)
      %294 = stablehlo.reshape %293 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc857)
      %295 = stablehlo.broadcast_in_dim %294, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc858)
      %296 = stablehlo.add %292, %295 : tensor<1x128x1024xbf16> loc(#loc859)
      %297 = stablehlo.reshape %296 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc860)
      %298 = stablehlo.transpose %297, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc861)
      %299 = stablehlo.slice %298 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc862)
      %300 = stablehlo.multiply %299, %64 : tensor<1x16x128x32xbf16> loc(#loc863)
      %301 = stablehlo.slice %298 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc864)
      %302 = stablehlo.multiply %301, %70 : tensor<1x16x128x32xbf16> loc(#loc865)
      %303 = stablehlo.subtract %300, %302 : tensor<1x16x128x32xbf16> loc(#loc866)
      %304 = stablehlo.multiply %301, %64 : tensor<1x16x128x32xbf16> loc(#loc867)
      %305 = stablehlo.multiply %299, %70 : tensor<1x16x128x32xbf16> loc(#loc868)
      %306 = stablehlo.add %304, %305 : tensor<1x16x128x32xbf16> loc(#loc869)
      %307 = stablehlo.concatenate %303, %306, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc870)
      %308 = stablehlo.reshape %arg855 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc871)
      %309 = stablehlo.reshape %308 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc872)
      %310 = stablehlo.transpose %309, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc873)
      %311 = stablehlo.dot_general %286, %310, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc874)
      %312 = "stablehlo.all_reduce"(%311) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2216"), %arg1239: tensor<bf16> loc("dot.2216")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc874)
        stablehlo.return %7360 : tensor<bf16> loc(#loc874)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc874)
      %313 = stablehlo.reshape %312 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc875)
      %314 = stablehlo.reshape %arg854 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc876)
      %315 = stablehlo.reshape %314 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc877)
      %316 = stablehlo.broadcast_in_dim %315, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc878)
      %317 = stablehlo.add %313, %316 : tensor<1x128x128xbf16> loc(#loc879)
      %318 = stablehlo.reshape %317 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc880)
      %319 = stablehlo.transpose %318, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc881)
      %320 = stablehlo.slice %319 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc882)
      %321 = stablehlo.multiply %320, %90 : tensor<1x2x128x32xbf16> loc(#loc883)
      %322 = stablehlo.slice %319 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc884)
      %323 = stablehlo.multiply %322, %93 : tensor<1x2x128x32xbf16> loc(#loc885)
      %324 = stablehlo.subtract %321, %323 : tensor<1x2x128x32xbf16> loc(#loc886)
      %325 = stablehlo.multiply %322, %90 : tensor<1x2x128x32xbf16> loc(#loc887)
      %326 = stablehlo.multiply %320, %93 : tensor<1x2x128x32xbf16> loc(#loc888)
      %327 = stablehlo.add %325, %326 : tensor<1x2x128x32xbf16> loc(#loc889)
      %328 = stablehlo.concatenate %324, %327, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc890)
      %329 = stablehlo.broadcast_in_dim %328, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc891)
      %330 = stablehlo.reshape %329 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc892)
      %331 = stablehlo.transpose %330, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc893)
      %332 = stablehlo.dot_general %307, %331, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc894)
      %333 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %334 = stablehlo.multiply %332, %333 : tensor<1x16x128x128xbf16> loc(#loc895)
      %335 = stablehlo.and %106, %113 : tensor<128x128xi1> loc(#loc896)
      %336 = stablehlo.reshape %335 : (tensor<128x128xi1>) -> tensor<1x128x128xi1> loc(#loc897)
      %337 = stablehlo.and %336, %121 : tensor<1x128x128xi1> loc(#loc898)
      %338 = stablehlo.reshape %337 : (tensor<1x128x128xi1>) -> tensor<1x1x128x128xi1> loc(#loc899)
      %339 = stablehlo.select %338, %8, %7 : tensor<1x1x128x128xi1>, tensor<1x1x128x128xbf16> loc(#loc900)
      %340 = stablehlo.reshape %339 : (tensor<1x1x128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc901)
      %341 = stablehlo.broadcast_in_dim %340, dims = [0, 2, 3] : (tensor<1x128x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc902)
      %342 = stablehlo.add %334, %341 : tensor<1x16x128x128xbf16> loc(#loc903)
      %343 = stablehlo.reshape %arg853 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc235)
      %344 = "stablehlo.all_to_all"(%343) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc235)
      %345 = stablehlo.slice %344 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc235)
      %346 = stablehlo.reshape %345 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc235)
      %347 = stablehlo.reshape %346 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc904)
      %348 = stablehlo.reshape %347 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc905)
      %349 = stablehlo.broadcast_in_dim %348, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc906)
      %350 = stablehlo.concatenate %342, %349, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc907)
      %351 = stablehlo.reshape %arg863 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc908)
      %352 = stablehlo.reshape %351 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc909)
      %353 = stablehlo.convert %352 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc910)
      %354 = stablehlo.broadcast_in_dim %353, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc911)
      %355 = stablehlo.reduce(%350 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc912)
      %356 = stablehlo.broadcast_in_dim %355, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc913)
      %357 = stablehlo.subtract %350, %356 : tensor<1x16x128x129xbf16> loc(#loc914)
      %358 = stablehlo.reduce(%357 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc915)
      %359 = stablehlo.broadcast_in_dim %358, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc916)
      %360 = stablehlo.subtract %357, %359 : tensor<1x16x128x129xbf16> loc(#loc917)
      %361 = stablehlo.exponential %360 : tensor<1x16x128x129xbf16> loc(#loc918)
      %362 = stablehlo.reduce(%361 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc919)
      %363 = stablehlo.broadcast_in_dim %362, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc920)
      %364 = stablehlo.divide %361, %363 : tensor<1x16x128x129xbf16> loc(#loc921)
      %365 = stablehlo.slice %364 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc922)
      %366 = stablehlo.reshape %arg829 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc923)
      %367 = stablehlo.reshape %366 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc924)
      %368 = stablehlo.transpose %367, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc925)
      %369 = stablehlo.dot_general %286, %368, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc926)
      %370 = "stablehlo.all_reduce"(%369) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2104"), %arg1239: tensor<bf16> loc("dot.2104")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc926)
        stablehlo.return %7360 : tensor<bf16> loc(#loc926)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc926)
      %371 = stablehlo.reshape %370 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc927)
      %372 = stablehlo.reshape %arg828 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc928)
      %373 = stablehlo.reshape %372 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc929)
      %374 = stablehlo.broadcast_in_dim %373, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc930)
      %375 = stablehlo.add %371, %374 : tensor<1x128x128xbf16> loc(#loc931)
      %376 = stablehlo.reshape %375 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc932)
      %377 = stablehlo.transpose %376, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc933)
      %378 = stablehlo.broadcast_in_dim %377, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc934)
      %379 = stablehlo.reshape %378 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc935)
      %380 = stablehlo.dot_general %365, %379, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc936)
      %381 = stablehlo.transpose %380, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc937)
      %382 = stablehlo.reshape %381 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc938)
      %383 = stablehlo.reshape %arg827 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc939)
      %384 = stablehlo.reshape %383 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc940)
      %385 = stablehlo.transpose %384, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc941)
      %386 = stablehlo.dot_general %382, %385, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc942)
      %387 = "stablehlo.all_reduce"(%386) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2372"), %arg1239: tensor<bf16> loc("dot.2372")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc942)
        stablehlo.return %7360 : tensor<bf16> loc(#loc942)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc942)
      %388 = stablehlo.reshape %387 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc943)
      %389 = stablehlo.reshape %arg826 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc944)
      %390 = stablehlo.reshape %389 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc945)
      %391 = stablehlo.broadcast_in_dim %390, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc946)
      %392 = stablehlo.add %388, %391 : tensor<1x128x360xbf16> loc(#loc947)
      %393 = stablehlo.add %269, %392 : tensor<1x128x360xbf16> loc(#loc948)
      %394 = stablehlo.reshape %arg858 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc949)
      %395 = stablehlo.reshape %394 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc950)
      %396 = stablehlo.convert %395 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc951)
      %397 = stablehlo.broadcast_in_dim %396, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc952)
      %398 = stablehlo.convert %393 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc953)
      %399 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %400 = stablehlo.power %398, %399 : tensor<1x128x360xf32> loc(#loc954)
      %401 = stablehlo.reduce(%400 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc955)
      %402 = "stablehlo.all_reduce"(%401) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.2390"), %arg1239: tensor<f32> loc("reduce.2390")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc955)
        stablehlo.return %7360 : tensor<f32> loc(#loc955)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc955)
      %403 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %404 = stablehlo.multiply %402, %403 : tensor<1x128xf32> loc(#loc956)
      %405 = stablehlo.reshape %404 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc957)
      %406 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %407 = stablehlo.add %405, %406 : tensor<1x128x1xf32> loc(#loc958)
      %408 = stablehlo.rsqrt %407 : tensor<1x128x1xf32> loc(#loc959)
      %409 = stablehlo.reshape %408 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc960)
      %410 = stablehlo.broadcast_in_dim %409, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc961)
      %411 = stablehlo.multiply %398, %410 : tensor<1x128x360xf32> loc(#loc962)
      %412 = stablehlo.multiply %397, %411 : tensor<1x128x360xf32> loc(#loc963)
      %413 = stablehlo.convert %412 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc964)
      %414 = stablehlo.reshape %413 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc965)
      %415 = stablehlo.broadcast_in_dim %414, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc966)
      %416 = stablehlo.dot_general %415, %arg862, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc967)
      %417 = "stablehlo.all_reduce"(%416) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2502"), %arg1239: tensor<bf16> loc("dot.2502")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc967)
        stablehlo.return %7360 : tensor<bf16> loc(#loc967)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc967)
      %418 = stablehlo.reshape %arg861 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc968)
      %419 = stablehlo.reshape %418 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc969)
      %420 = stablehlo.broadcast_in_dim %419, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc970)
      %421 = stablehlo.add %417, %420 : tensor<32x128x5760xbf16> loc(#loc971)
      %422 = stablehlo.slice %421 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc972)
      %423 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %424 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %425 = stablehlo.clamp %424, %422, %423 : tensor<32x128x2880xbf16> loc(#loc973)
      %426 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %427 = stablehlo.add %425, %426 : tensor<32x128x2880xbf16> loc(#loc974)
      %428 = stablehlo.slice %421 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc975)
      %429 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %430 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %431 = stablehlo.clamp %429, %428, %430 : tensor<32x128x2880xbf16> loc(#loc976)
      %432 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %433 = stablehlo.multiply %431, %432 : tensor<32x128x2880xbf16> loc(#loc977)
      %434 = stablehlo.logistic %433 : tensor<32x128x2880xbf16> loc(#loc978)
      %435 = stablehlo.multiply %431, %434 : tensor<32x128x2880xbf16> loc(#loc979)
      %436 = stablehlo.multiply %427, %435 : tensor<32x128x2880xbf16> loc(#loc980)
      %437 = stablehlo.dot_general %436, %arg860, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc981)
      %438 = stablehlo.reshape %arg859 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc982)
      %439 = stablehlo.reshape %438 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc983)
      %440 = stablehlo.broadcast_in_dim %439, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc984)
      %441 = stablehlo.add %437, %440 : tensor<32x128x360xbf16> loc(#loc985)
      %442 = stablehlo.reshape %441 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc986)
      %443 = stablehlo.reshape %arg825 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc987)
      %444 = stablehlo.reshape %443 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc988)
      %445 = stablehlo.transpose %444, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc989)
      %446 = stablehlo.dot_general %414, %445, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc990)
      %447 = "stablehlo.all_reduce"(%446) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2418"), %arg1239: tensor<bf16> loc("dot.2418")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc990)
        stablehlo.return %7360 : tensor<bf16> loc(#loc990)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc990)
      %448 = stablehlo.reshape %arg824 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc991)
      %449 = stablehlo.reshape %448 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc992)
      %450 = stablehlo.broadcast_in_dim %449, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc993)
      %451 = stablehlo.add %447, %450 : tensor<128x128xbf16> loc(#loc994)
      %452:2 = "stablehlo.sort"(%451, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.2438"), %arg1239: tensor<bf16> loc("sort.2438"), %arg1240: tensor<i32> loc("sort.2438"), %arg1241: tensor<i32> loc("sort.2438")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc996)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc995)
      %453 = stablehlo.slice %452#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc997)
      %454 = stablehlo.convert %453 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc998)
      %455 = stablehlo.reshape %454 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc999)
      %456 = stablehlo.concatenate %231, %455, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc1000)
      %457 = stablehlo.slice %452#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc1001)
      %458 = stablehlo.reduce(%457 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1002)
      %459 = stablehlo.broadcast_in_dim %458, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1003)
      %460 = stablehlo.subtract %457, %459 : tensor<128x4xbf16> loc(#loc1004)
      %461 = stablehlo.exponential %460 : tensor<128x4xbf16> loc(#loc1005)
      %462 = stablehlo.reduce(%461 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1006)
      %463 = stablehlo.broadcast_in_dim %462, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1007)
      %464 = stablehlo.divide %461, %463 : tensor<128x4xbf16> loc(#loc1008)
      %465 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %466 = "stablehlo.all_gather"(%465) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %467 = "stablehlo.scatter"(%466, %456, %464) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.2472"), %arg1239: tensor<bf16> loc("scatter.2472")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc1009)
      %468 = stablehlo.reshape %467 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc1009)
      %469 = "stablehlo.all_to_all"(%468) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc1009)
      %470 = stablehlo.slice %469 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc1009)
      %471 = stablehlo.reshape %470 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc1009)
      %472 = stablehlo.transpose %471, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc1010)
      %473 = stablehlo.reshape %472 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc1011)
      %474 = stablehlo.broadcast_in_dim %473, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1012)
      %475 = stablehlo.multiply %442, %474 : tensor<32x1x128x360xbf16> loc(#loc1013)
      %476 = stablehlo.reduce(%475 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc1014)
      %477 = "stablehlo.all_reduce"(%476) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.2544"), %arg1239: tensor<bf16> loc("reduce.2544")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1014)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1014)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1014)
      %478 = stablehlo.add %393, %477 : tensor<1x128x360xbf16> loc(#loc1015)
      %479 = stablehlo.convert %478 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1016)
      %480 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %481 = stablehlo.power %479, %480 : tensor<1x128x360xf32> loc(#loc1017)
      %482 = stablehlo.reduce(%481 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1018)
      %483 = "stablehlo.all_reduce"(%482) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.2557"), %arg1239: tensor<f32> loc("reduce.2557")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1018)
        stablehlo.return %7360 : tensor<f32> loc(#loc1018)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1018)
      %484 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %485 = stablehlo.multiply %483, %484 : tensor<1x128xf32> loc(#loc1019)
      %486 = stablehlo.reshape %485 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1020)
      %487 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %488 = stablehlo.add %486, %487 : tensor<1x128x1xf32> loc(#loc1021)
      %489 = stablehlo.rsqrt %488 : tensor<1x128x1xf32> loc(#loc1022)
      %490 = stablehlo.reshape %489 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1023)
      %491 = stablehlo.broadcast_in_dim %490, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1024)
      %492 = stablehlo.multiply %479, %491 : tensor<1x128x360xf32> loc(#loc1025)
      %493 = stablehlo.multiply %354, %492 : tensor<1x128x360xf32> loc(#loc1026)
      %494 = stablehlo.convert %493 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1027)
      %495 = stablehlo.reshape %494 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1028)
      %496 = stablehlo.reshape %arg868 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc1029)
      %497 = stablehlo.reshape %496 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc1030)
      %498 = stablehlo.transpose %497, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc1031)
      %499 = stablehlo.dot_general %495, %498, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1032)
      %500 = "stablehlo.all_reduce"(%499) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2671"), %arg1239: tensor<bf16> loc("dot.2671")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1032)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1032)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1032)
      %501 = stablehlo.reshape %500 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1033)
      %502 = stablehlo.reshape %arg867 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1034)
      %503 = stablehlo.reshape %502 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc1035)
      %504 = stablehlo.broadcast_in_dim %503, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1036)
      %505 = stablehlo.add %501, %504 : tensor<1x128x1024xbf16> loc(#loc1037)
      %506 = stablehlo.reshape %505 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1038)
      %507 = stablehlo.transpose %506, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1039)
      %508 = stablehlo.slice %507 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1040)
      %509 = stablehlo.multiply %508, %64 : tensor<1x16x128x32xbf16> loc(#loc1041)
      %510 = stablehlo.slice %507 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1042)
      %511 = stablehlo.multiply %510, %70 : tensor<1x16x128x32xbf16> loc(#loc1043)
      %512 = stablehlo.subtract %509, %511 : tensor<1x16x128x32xbf16> loc(#loc1044)
      %513 = stablehlo.multiply %510, %64 : tensor<1x16x128x32xbf16> loc(#loc1045)
      %514 = stablehlo.multiply %508, %70 : tensor<1x16x128x32xbf16> loc(#loc1046)
      %515 = stablehlo.add %513, %514 : tensor<1x16x128x32xbf16> loc(#loc1047)
      %516 = stablehlo.concatenate %512, %515, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1048)
      %517 = stablehlo.reshape %arg866 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1049)
      %518 = stablehlo.reshape %517 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1050)
      %519 = stablehlo.transpose %518, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1051)
      %520 = stablehlo.dot_general %495, %519, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1052)
      %521 = "stablehlo.all_reduce"(%520) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2622"), %arg1239: tensor<bf16> loc("dot.2622")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1052)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1052)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1052)
      %522 = stablehlo.reshape %521 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1053)
      %523 = stablehlo.reshape %arg865 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1054)
      %524 = stablehlo.reshape %523 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1055)
      %525 = stablehlo.broadcast_in_dim %524, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1056)
      %526 = stablehlo.add %522, %525 : tensor<1x128x128xbf16> loc(#loc1057)
      %527 = stablehlo.reshape %526 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1058)
      %528 = stablehlo.transpose %527, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1059)
      %529 = stablehlo.slice %528 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1060)
      %530 = stablehlo.multiply %529, %90 : tensor<1x2x128x32xbf16> loc(#loc1061)
      %531 = stablehlo.slice %528 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1062)
      %532 = stablehlo.multiply %531, %93 : tensor<1x2x128x32xbf16> loc(#loc1063)
      %533 = stablehlo.subtract %530, %532 : tensor<1x2x128x32xbf16> loc(#loc1064)
      %534 = stablehlo.multiply %531, %90 : tensor<1x2x128x32xbf16> loc(#loc1065)
      %535 = stablehlo.multiply %529, %93 : tensor<1x2x128x32xbf16> loc(#loc1066)
      %536 = stablehlo.add %534, %535 : tensor<1x2x128x32xbf16> loc(#loc1067)
      %537 = stablehlo.concatenate %533, %536, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1068)
      %538 = stablehlo.broadcast_in_dim %537, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1069)
      %539 = stablehlo.reshape %538 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1070)
      %540 = stablehlo.transpose %539, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc1071)
      %541 = stablehlo.dot_general %516, %540, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1072)
      %542 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %543 = stablehlo.multiply %541, %542 : tensor<1x16x128x128xbf16> loc(#loc1073)
      %544 = stablehlo.add %543, %128 : tensor<1x16x128x128xbf16> loc(#loc1074)
      %545 = stablehlo.reshape %arg864 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc246)
      %546 = "stablehlo.all_to_all"(%545) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc246)
      %547 = stablehlo.slice %546 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc246)
      %548 = stablehlo.reshape %547 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc246)
      %549 = stablehlo.reshape %548 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1075)
      %550 = stablehlo.reshape %549 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc1076)
      %551 = stablehlo.broadcast_in_dim %550, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc1077)
      %552 = stablehlo.concatenate %544, %551, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1078)
      %553 = stablehlo.reshape %arg874 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1079)
      %554 = stablehlo.reshape %553 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1080)
      %555 = stablehlo.convert %554 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1081)
      %556 = stablehlo.broadcast_in_dim %555, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1082)
      %557 = stablehlo.reduce(%552 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1083)
      %558 = stablehlo.broadcast_in_dim %557, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1084)
      %559 = stablehlo.subtract %552, %558 : tensor<1x16x128x129xbf16> loc(#loc1085)
      %560 = stablehlo.reduce(%559 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1086)
      %561 = stablehlo.broadcast_in_dim %560, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1087)
      %562 = stablehlo.subtract %559, %561 : tensor<1x16x128x129xbf16> loc(#loc1088)
      %563 = stablehlo.exponential %562 : tensor<1x16x128x129xbf16> loc(#loc1089)
      %564 = stablehlo.reduce(%563 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1090)
      %565 = stablehlo.broadcast_in_dim %564, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1091)
      %566 = stablehlo.divide %563, %565 : tensor<1x16x128x129xbf16> loc(#loc1092)
      %567 = stablehlo.slice %566 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1093)
      %568 = stablehlo.reshape %arg823 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1094)
      %569 = stablehlo.reshape %568 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1095)
      %570 = stablehlo.transpose %569, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1096)
      %571 = stablehlo.dot_general %495, %570, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1097)
      %572 = "stablehlo.all_reduce"(%571) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2585"), %arg1239: tensor<bf16> loc("dot.2585")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1097)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1097)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1097)
      %573 = stablehlo.reshape %572 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1098)
      %574 = stablehlo.reshape %arg822 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1099)
      %575 = stablehlo.reshape %574 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1100)
      %576 = stablehlo.broadcast_in_dim %575, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1101)
      %577 = stablehlo.add %573, %576 : tensor<1x128x128xbf16> loc(#loc1102)
      %578 = stablehlo.reshape %577 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1103)
      %579 = stablehlo.transpose %578, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1104)
      %580 = stablehlo.broadcast_in_dim %579, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1105)
      %581 = stablehlo.reshape %580 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1106)
      %582 = stablehlo.dot_general %567, %581, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1107)
      %583 = stablehlo.transpose %582, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1108)
      %584 = stablehlo.reshape %583 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc1109)
      %585 = stablehlo.reshape %arg821 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc1110)
      %586 = stablehlo.reshape %585 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc1111)
      %587 = stablehlo.transpose %586, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc1112)
      %588 = stablehlo.dot_general %584, %587, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc1113)
      %589 = "stablehlo.all_reduce"(%588) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2778"), %arg1239: tensor<bf16> loc("dot.2778")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1113)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1113)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1113)
      %590 = stablehlo.reshape %589 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1114)
      %591 = stablehlo.reshape %arg820 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1115)
      %592 = stablehlo.reshape %591 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1116)
      %593 = stablehlo.broadcast_in_dim %592, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1117)
      %594 = stablehlo.add %590, %593 : tensor<1x128x360xbf16> loc(#loc1118)
      %595 = stablehlo.add %478, %594 : tensor<1x128x360xbf16> loc(#loc1119)
      %596 = stablehlo.reshape %arg869 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1120)
      %597 = stablehlo.reshape %596 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1121)
      %598 = stablehlo.convert %597 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1122)
      %599 = stablehlo.broadcast_in_dim %598, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1123)
      %600 = stablehlo.convert %595 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1124)
      %601 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %602 = stablehlo.power %600, %601 : tensor<1x128x360xf32> loc(#loc1125)
      %603 = stablehlo.reduce(%602 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1126)
      %604 = "stablehlo.all_reduce"(%603) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.2796"), %arg1239: tensor<f32> loc("reduce.2796")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1126)
        stablehlo.return %7360 : tensor<f32> loc(#loc1126)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1126)
      %605 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %606 = stablehlo.multiply %604, %605 : tensor<1x128xf32> loc(#loc1127)
      %607 = stablehlo.reshape %606 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1128)
      %608 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %609 = stablehlo.add %607, %608 : tensor<1x128x1xf32> loc(#loc1129)
      %610 = stablehlo.rsqrt %609 : tensor<1x128x1xf32> loc(#loc1130)
      %611 = stablehlo.reshape %610 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1131)
      %612 = stablehlo.broadcast_in_dim %611, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1132)
      %613 = stablehlo.multiply %600, %612 : tensor<1x128x360xf32> loc(#loc1133)
      %614 = stablehlo.multiply %599, %613 : tensor<1x128x360xf32> loc(#loc1134)
      %615 = stablehlo.convert %614 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1135)
      %616 = stablehlo.reshape %615 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1136)
      %617 = stablehlo.broadcast_in_dim %616, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1137)
      %618 = stablehlo.dot_general %617, %arg873, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1138)
      %619 = "stablehlo.all_reduce"(%618) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2908"), %arg1239: tensor<bf16> loc("dot.2908")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1138)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1138)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1138)
      %620 = stablehlo.reshape %arg872 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc1139)
      %621 = stablehlo.reshape %620 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc1140)
      %622 = stablehlo.broadcast_in_dim %621, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1141)
      %623 = stablehlo.add %619, %622 : tensor<32x128x5760xbf16> loc(#loc1142)
      %624 = stablehlo.slice %623 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1143)
      %625 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %626 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %627 = stablehlo.clamp %626, %624, %625 : tensor<32x128x2880xbf16> loc(#loc1144)
      %628 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %629 = stablehlo.add %627, %628 : tensor<32x128x2880xbf16> loc(#loc1145)
      %630 = stablehlo.slice %623 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1146)
      %631 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %632 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %633 = stablehlo.clamp %631, %630, %632 : tensor<32x128x2880xbf16> loc(#loc1147)
      %634 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %635 = stablehlo.multiply %633, %634 : tensor<32x128x2880xbf16> loc(#loc1148)
      %636 = stablehlo.logistic %635 : tensor<32x128x2880xbf16> loc(#loc1149)
      %637 = stablehlo.multiply %633, %636 : tensor<32x128x2880xbf16> loc(#loc1150)
      %638 = stablehlo.multiply %629, %637 : tensor<32x128x2880xbf16> loc(#loc1151)
      %639 = stablehlo.dot_general %638, %arg871, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1152)
      %640 = stablehlo.reshape %arg870 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc1153)
      %641 = stablehlo.reshape %640 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc1154)
      %642 = stablehlo.broadcast_in_dim %641, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1155)
      %643 = stablehlo.add %639, %642 : tensor<32x128x360xbf16> loc(#loc1156)
      %644 = stablehlo.reshape %643 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1157)
      %645 = stablehlo.reshape %arg819 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1158)
      %646 = stablehlo.reshape %645 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1159)
      %647 = stablehlo.transpose %646, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1160)
      %648 = stablehlo.dot_general %616, %647, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1161)
      %649 = "stablehlo.all_reduce"(%648) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2824"), %arg1239: tensor<bf16> loc("dot.2824")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1161)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1161)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1161)
      %650 = stablehlo.reshape %arg818 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1162)
      %651 = stablehlo.reshape %650 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1163)
      %652 = stablehlo.broadcast_in_dim %651, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc1164)
      %653 = stablehlo.add %649, %652 : tensor<128x128xbf16> loc(#loc1165)
      %654:2 = "stablehlo.sort"(%653, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.2844"), %arg1239: tensor<bf16> loc("sort.2844"), %arg1240: tensor<i32> loc("sort.2844"), %arg1241: tensor<i32> loc("sort.2844")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1167)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc1166)
      %655 = stablehlo.slice %654#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc1168)
      %656 = stablehlo.convert %655 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc1169)
      %657 = stablehlo.reshape %656 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc1170)
      %658 = stablehlo.concatenate %231, %657, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc1171)
      %659 = stablehlo.slice %654#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc1172)
      %660 = stablehlo.reduce(%659 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1173)
      %661 = stablehlo.broadcast_in_dim %660, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1174)
      %662 = stablehlo.subtract %659, %661 : tensor<128x4xbf16> loc(#loc1175)
      %663 = stablehlo.exponential %662 : tensor<128x4xbf16> loc(#loc1176)
      %664 = stablehlo.reduce(%663 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1177)
      %665 = stablehlo.broadcast_in_dim %664, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1178)
      %666 = stablehlo.divide %663, %665 : tensor<128x4xbf16> loc(#loc1179)
      %667 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %668 = "stablehlo.all_gather"(%667) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %669 = "stablehlo.scatter"(%668, %658, %666) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.2878"), %arg1239: tensor<bf16> loc("scatter.2878")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc1180)
      %670 = stablehlo.reshape %669 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc1180)
      %671 = "stablehlo.all_to_all"(%670) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc1180)
      %672 = stablehlo.slice %671 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc1180)
      %673 = stablehlo.reshape %672 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc1180)
      %674 = stablehlo.transpose %673, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc1181)
      %675 = stablehlo.reshape %674 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc1182)
      %676 = stablehlo.broadcast_in_dim %675, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1183)
      %677 = stablehlo.multiply %644, %676 : tensor<32x1x128x360xbf16> loc(#loc1184)
      %678 = stablehlo.reduce(%677 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc1185)
      %679 = "stablehlo.all_reduce"(%678) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.2950"), %arg1239: tensor<bf16> loc("reduce.2950")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1185)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1185)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1185)
      %680 = stablehlo.add %595, %679 : tensor<1x128x360xbf16> loc(#loc1186)
      %681 = stablehlo.convert %680 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1187)
      %682 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %683 = stablehlo.power %681, %682 : tensor<1x128x360xf32> loc(#loc1188)
      %684 = stablehlo.reduce(%683 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1189)
      %685 = "stablehlo.all_reduce"(%684) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.2963"), %arg1239: tensor<f32> loc("reduce.2963")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1189)
        stablehlo.return %7360 : tensor<f32> loc(#loc1189)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1189)
      %686 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %687 = stablehlo.multiply %685, %686 : tensor<1x128xf32> loc(#loc1190)
      %688 = stablehlo.reshape %687 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1191)
      %689 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %690 = stablehlo.add %688, %689 : tensor<1x128x1xf32> loc(#loc1192)
      %691 = stablehlo.rsqrt %690 : tensor<1x128x1xf32> loc(#loc1193)
      %692 = stablehlo.reshape %691 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1194)
      %693 = stablehlo.broadcast_in_dim %692, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1195)
      %694 = stablehlo.multiply %681, %693 : tensor<1x128x360xf32> loc(#loc1196)
      %695 = stablehlo.multiply %556, %694 : tensor<1x128x360xf32> loc(#loc1197)
      %696 = stablehlo.convert %695 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1198)
      %697 = stablehlo.reshape %696 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1199)
      %698 = stablehlo.reshape %arg879 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc1200)
      %699 = stablehlo.reshape %698 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc1201)
      %700 = stablehlo.transpose %699, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc1202)
      %701 = stablehlo.dot_general %697, %700, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1203)
      %702 = "stablehlo.all_reduce"(%701) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3077"), %arg1239: tensor<bf16> loc("dot.3077")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1203)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1203)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1203)
      %703 = stablehlo.reshape %702 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1204)
      %704 = stablehlo.reshape %arg878 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1205)
      %705 = stablehlo.reshape %704 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc1206)
      %706 = stablehlo.broadcast_in_dim %705, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1207)
      %707 = stablehlo.add %703, %706 : tensor<1x128x1024xbf16> loc(#loc1208)
      %708 = stablehlo.reshape %707 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1209)
      %709 = stablehlo.transpose %708, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1210)
      %710 = stablehlo.slice %709 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1211)
      %711 = stablehlo.multiply %710, %64 : tensor<1x16x128x32xbf16> loc(#loc1212)
      %712 = stablehlo.slice %709 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1213)
      %713 = stablehlo.multiply %712, %70 : tensor<1x16x128x32xbf16> loc(#loc1214)
      %714 = stablehlo.subtract %711, %713 : tensor<1x16x128x32xbf16> loc(#loc1215)
      %715 = stablehlo.multiply %712, %64 : tensor<1x16x128x32xbf16> loc(#loc1216)
      %716 = stablehlo.multiply %710, %70 : tensor<1x16x128x32xbf16> loc(#loc1217)
      %717 = stablehlo.add %715, %716 : tensor<1x16x128x32xbf16> loc(#loc1218)
      %718 = stablehlo.concatenate %714, %717, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1219)
      %719 = stablehlo.reshape %arg877 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1220)
      %720 = stablehlo.reshape %719 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1221)
      %721 = stablehlo.transpose %720, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1222)
      %722 = stablehlo.dot_general %697, %721, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1223)
      %723 = "stablehlo.all_reduce"(%722) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3028"), %arg1239: tensor<bf16> loc("dot.3028")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1223)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1223)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1223)
      %724 = stablehlo.reshape %723 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1224)
      %725 = stablehlo.reshape %arg876 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1225)
      %726 = stablehlo.reshape %725 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1226)
      %727 = stablehlo.broadcast_in_dim %726, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1227)
      %728 = stablehlo.add %724, %727 : tensor<1x128x128xbf16> loc(#loc1228)
      %729 = stablehlo.reshape %728 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1229)
      %730 = stablehlo.transpose %729, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1230)
      %731 = stablehlo.slice %730 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1231)
      %732 = stablehlo.multiply %731, %90 : tensor<1x2x128x32xbf16> loc(#loc1232)
      %733 = stablehlo.slice %730 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1233)
      %734 = stablehlo.multiply %733, %93 : tensor<1x2x128x32xbf16> loc(#loc1234)
      %735 = stablehlo.subtract %732, %734 : tensor<1x2x128x32xbf16> loc(#loc1235)
      %736 = stablehlo.multiply %733, %90 : tensor<1x2x128x32xbf16> loc(#loc1236)
      %737 = stablehlo.multiply %731, %93 : tensor<1x2x128x32xbf16> loc(#loc1237)
      %738 = stablehlo.add %736, %737 : tensor<1x2x128x32xbf16> loc(#loc1238)
      %739 = stablehlo.concatenate %735, %738, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1239)
      %740 = stablehlo.broadcast_in_dim %739, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1240)
      %741 = stablehlo.reshape %740 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1241)
      %742 = stablehlo.transpose %741, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc1242)
      %743 = stablehlo.dot_general %718, %742, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1243)
      %744 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %745 = stablehlo.multiply %743, %744 : tensor<1x16x128x128xbf16> loc(#loc1244)
      %746 = stablehlo.add %745, %341 : tensor<1x16x128x128xbf16> loc(#loc1245)
      %747 = stablehlo.reshape %arg875 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc257)
      %748 = "stablehlo.all_to_all"(%747) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc257)
      %749 = stablehlo.slice %748 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc257)
      %750 = stablehlo.reshape %749 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc257)
      %751 = stablehlo.reshape %750 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1246)
      %752 = stablehlo.reshape %751 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc1247)
      %753 = stablehlo.broadcast_in_dim %752, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc1248)
      %754 = stablehlo.concatenate %746, %753, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1249)
      %755 = stablehlo.reshape %arg885 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1250)
      %756 = stablehlo.reshape %755 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1251)
      %757 = stablehlo.convert %756 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1252)
      %758 = stablehlo.broadcast_in_dim %757, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1253)
      %759 = stablehlo.reduce(%754 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1254)
      %760 = stablehlo.broadcast_in_dim %759, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1255)
      %761 = stablehlo.subtract %754, %760 : tensor<1x16x128x129xbf16> loc(#loc1256)
      %762 = stablehlo.reduce(%761 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1257)
      %763 = stablehlo.broadcast_in_dim %762, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1258)
      %764 = stablehlo.subtract %761, %763 : tensor<1x16x128x129xbf16> loc(#loc1259)
      %765 = stablehlo.exponential %764 : tensor<1x16x128x129xbf16> loc(#loc1260)
      %766 = stablehlo.reduce(%765 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1261)
      %767 = stablehlo.broadcast_in_dim %766, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1262)
      %768 = stablehlo.divide %765, %767 : tensor<1x16x128x129xbf16> loc(#loc1263)
      %769 = stablehlo.slice %768 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1264)
      %770 = stablehlo.reshape %arg817 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1265)
      %771 = stablehlo.reshape %770 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1266)
      %772 = stablehlo.transpose %771, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1267)
      %773 = stablehlo.dot_general %697, %772, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1268)
      %774 = "stablehlo.all_reduce"(%773) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.2991"), %arg1239: tensor<bf16> loc("dot.2991")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1268)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1268)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1268)
      %775 = stablehlo.reshape %774 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1269)
      %776 = stablehlo.reshape %arg816 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1270)
      %777 = stablehlo.reshape %776 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1271)
      %778 = stablehlo.broadcast_in_dim %777, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1272)
      %779 = stablehlo.add %775, %778 : tensor<1x128x128xbf16> loc(#loc1273)
      %780 = stablehlo.reshape %779 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1274)
      %781 = stablehlo.transpose %780, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1275)
      %782 = stablehlo.broadcast_in_dim %781, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1276)
      %783 = stablehlo.reshape %782 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1277)
      %784 = stablehlo.dot_general %769, %783, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1278)
      %785 = stablehlo.transpose %784, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1279)
      %786 = stablehlo.reshape %785 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc1280)
      %787 = stablehlo.reshape %arg815 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc1281)
      %788 = stablehlo.reshape %787 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc1282)
      %789 = stablehlo.transpose %788, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc1283)
      %790 = stablehlo.dot_general %786, %789, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc1284)
      %791 = "stablehlo.all_reduce"(%790) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3184"), %arg1239: tensor<bf16> loc("dot.3184")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1284)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1284)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1284)
      %792 = stablehlo.reshape %791 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1285)
      %793 = stablehlo.reshape %arg814 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1286)
      %794 = stablehlo.reshape %793 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1287)
      %795 = stablehlo.broadcast_in_dim %794, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1288)
      %796 = stablehlo.add %792, %795 : tensor<1x128x360xbf16> loc(#loc1289)
      %797 = stablehlo.add %680, %796 : tensor<1x128x360xbf16> loc(#loc1290)
      %798 = stablehlo.reshape %arg880 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1291)
      %799 = stablehlo.reshape %798 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1292)
      %800 = stablehlo.convert %799 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1293)
      %801 = stablehlo.broadcast_in_dim %800, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1294)
      %802 = stablehlo.convert %797 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1295)
      %803 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %804 = stablehlo.power %802, %803 : tensor<1x128x360xf32> loc(#loc1296)
      %805 = stablehlo.reduce(%804 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1297)
      %806 = "stablehlo.all_reduce"(%805) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.3202"), %arg1239: tensor<f32> loc("reduce.3202")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1297)
        stablehlo.return %7360 : tensor<f32> loc(#loc1297)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1297)
      %807 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %808 = stablehlo.multiply %806, %807 : tensor<1x128xf32> loc(#loc1298)
      %809 = stablehlo.reshape %808 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1299)
      %810 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %811 = stablehlo.add %809, %810 : tensor<1x128x1xf32> loc(#loc1300)
      %812 = stablehlo.rsqrt %811 : tensor<1x128x1xf32> loc(#loc1301)
      %813 = stablehlo.reshape %812 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1302)
      %814 = stablehlo.broadcast_in_dim %813, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1303)
      %815 = stablehlo.multiply %802, %814 : tensor<1x128x360xf32> loc(#loc1304)
      %816 = stablehlo.multiply %801, %815 : tensor<1x128x360xf32> loc(#loc1305)
      %817 = stablehlo.convert %816 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1306)
      %818 = stablehlo.reshape %817 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1307)
      %819 = stablehlo.broadcast_in_dim %818, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1308)
      %820 = stablehlo.dot_general %819, %arg884, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1309)
      %821 = "stablehlo.all_reduce"(%820) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3314"), %arg1239: tensor<bf16> loc("dot.3314")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1309)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1309)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1309)
      %822 = stablehlo.reshape %arg883 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc1310)
      %823 = stablehlo.reshape %822 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc1311)
      %824 = stablehlo.broadcast_in_dim %823, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1312)
      %825 = stablehlo.add %821, %824 : tensor<32x128x5760xbf16> loc(#loc1313)
      %826 = stablehlo.slice %825 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1314)
      %827 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %828 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %829 = stablehlo.clamp %828, %826, %827 : tensor<32x128x2880xbf16> loc(#loc1315)
      %830 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %831 = stablehlo.add %829, %830 : tensor<32x128x2880xbf16> loc(#loc1316)
      %832 = stablehlo.slice %825 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1317)
      %833 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %834 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %835 = stablehlo.clamp %833, %832, %834 : tensor<32x128x2880xbf16> loc(#loc1318)
      %836 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %837 = stablehlo.multiply %835, %836 : tensor<32x128x2880xbf16> loc(#loc1319)
      %838 = stablehlo.logistic %837 : tensor<32x128x2880xbf16> loc(#loc1320)
      %839 = stablehlo.multiply %835, %838 : tensor<32x128x2880xbf16> loc(#loc1321)
      %840 = stablehlo.multiply %831, %839 : tensor<32x128x2880xbf16> loc(#loc1322)
      %841 = stablehlo.dot_general %840, %arg882, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1323)
      %842 = stablehlo.reshape %arg881 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc1324)
      %843 = stablehlo.reshape %842 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc1325)
      %844 = stablehlo.broadcast_in_dim %843, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1326)
      %845 = stablehlo.add %841, %844 : tensor<32x128x360xbf16> loc(#loc1327)
      %846 = stablehlo.reshape %845 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1328)
      %847 = stablehlo.reshape %arg813 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1329)
      %848 = stablehlo.reshape %847 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1330)
      %849 = stablehlo.transpose %848, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1331)
      %850 = stablehlo.dot_general %818, %849, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1332)
      %851 = "stablehlo.all_reduce"(%850) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3230"), %arg1239: tensor<bf16> loc("dot.3230")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1332)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1332)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1332)
      %852 = stablehlo.reshape %arg812 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1333)
      %853 = stablehlo.reshape %852 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1334)
      %854 = stablehlo.broadcast_in_dim %853, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc1335)
      %855 = stablehlo.add %851, %854 : tensor<128x128xbf16> loc(#loc1336)
      %856:2 = "stablehlo.sort"(%855, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.3250"), %arg1239: tensor<bf16> loc("sort.3250"), %arg1240: tensor<i32> loc("sort.3250"), %arg1241: tensor<i32> loc("sort.3250")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1338)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc1337)
      %857 = stablehlo.slice %856#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc1339)
      %858 = stablehlo.convert %857 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc1340)
      %859 = stablehlo.reshape %858 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc1341)
      %860 = stablehlo.concatenate %231, %859, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc1342)
      %861 = stablehlo.slice %856#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc1343)
      %862 = stablehlo.reduce(%861 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1344)
      %863 = stablehlo.broadcast_in_dim %862, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1345)
      %864 = stablehlo.subtract %861, %863 : tensor<128x4xbf16> loc(#loc1346)
      %865 = stablehlo.exponential %864 : tensor<128x4xbf16> loc(#loc1347)
      %866 = stablehlo.reduce(%865 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1348)
      %867 = stablehlo.broadcast_in_dim %866, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1349)
      %868 = stablehlo.divide %865, %867 : tensor<128x4xbf16> loc(#loc1350)
      %869 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %870 = "stablehlo.all_gather"(%869) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %871 = "stablehlo.scatter"(%870, %860, %868) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.3284"), %arg1239: tensor<bf16> loc("scatter.3284")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc1351)
      %872 = stablehlo.reshape %871 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc1351)
      %873 = "stablehlo.all_to_all"(%872) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc1351)
      %874 = stablehlo.slice %873 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc1351)
      %875 = stablehlo.reshape %874 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc1351)
      %876 = stablehlo.transpose %875, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc1352)
      %877 = stablehlo.reshape %876 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc1353)
      %878 = stablehlo.broadcast_in_dim %877, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1354)
      %879 = stablehlo.multiply %846, %878 : tensor<32x1x128x360xbf16> loc(#loc1355)
      %880 = stablehlo.reduce(%879 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc1356)
      %881 = "stablehlo.all_reduce"(%880) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.3356"), %arg1239: tensor<bf16> loc("reduce.3356")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1356)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1356)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1356)
      %882 = stablehlo.add %797, %881 : tensor<1x128x360xbf16> loc(#loc1357)
      %883 = stablehlo.convert %882 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1358)
      %884 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %885 = stablehlo.power %883, %884 : tensor<1x128x360xf32> loc(#loc1359)
      %886 = stablehlo.reduce(%885 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1360)
      %887 = "stablehlo.all_reduce"(%886) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.3369"), %arg1239: tensor<f32> loc("reduce.3369")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1360)
        stablehlo.return %7360 : tensor<f32> loc(#loc1360)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1360)
      %888 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %889 = stablehlo.multiply %887, %888 : tensor<1x128xf32> loc(#loc1361)
      %890 = stablehlo.reshape %889 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1362)
      %891 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %892 = stablehlo.add %890, %891 : tensor<1x128x1xf32> loc(#loc1363)
      %893 = stablehlo.rsqrt %892 : tensor<1x128x1xf32> loc(#loc1364)
      %894 = stablehlo.reshape %893 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1365)
      %895 = stablehlo.broadcast_in_dim %894, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1366)
      %896 = stablehlo.multiply %883, %895 : tensor<1x128x360xf32> loc(#loc1367)
      %897 = stablehlo.multiply %758, %896 : tensor<1x128x360xf32> loc(#loc1368)
      %898 = stablehlo.convert %897 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1369)
      %899 = stablehlo.reshape %898 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1370)
      %900 = stablehlo.reshape %arg890 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc1371)
      %901 = stablehlo.reshape %900 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc1372)
      %902 = stablehlo.transpose %901, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc1373)
      %903 = stablehlo.dot_general %899, %902, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1374)
      %904 = "stablehlo.all_reduce"(%903) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3483"), %arg1239: tensor<bf16> loc("dot.3483")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1374)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1374)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1374)
      %905 = stablehlo.reshape %904 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1375)
      %906 = stablehlo.reshape %arg889 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1376)
      %907 = stablehlo.reshape %906 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc1377)
      %908 = stablehlo.broadcast_in_dim %907, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1378)
      %909 = stablehlo.add %905, %908 : tensor<1x128x1024xbf16> loc(#loc1379)
      %910 = stablehlo.reshape %909 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1380)
      %911 = stablehlo.transpose %910, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1381)
      %912 = stablehlo.slice %911 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1382)
      %913 = stablehlo.multiply %912, %64 : tensor<1x16x128x32xbf16> loc(#loc1383)
      %914 = stablehlo.slice %911 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1384)
      %915 = stablehlo.multiply %914, %70 : tensor<1x16x128x32xbf16> loc(#loc1385)
      %916 = stablehlo.subtract %913, %915 : tensor<1x16x128x32xbf16> loc(#loc1386)
      %917 = stablehlo.multiply %914, %64 : tensor<1x16x128x32xbf16> loc(#loc1387)
      %918 = stablehlo.multiply %912, %70 : tensor<1x16x128x32xbf16> loc(#loc1388)
      %919 = stablehlo.add %917, %918 : tensor<1x16x128x32xbf16> loc(#loc1389)
      %920 = stablehlo.concatenate %916, %919, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1390)
      %921 = stablehlo.reshape %arg888 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1391)
      %922 = stablehlo.reshape %921 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1392)
      %923 = stablehlo.transpose %922, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1393)
      %924 = stablehlo.dot_general %899, %923, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1394)
      %925 = "stablehlo.all_reduce"(%924) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3434"), %arg1239: tensor<bf16> loc("dot.3434")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1394)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1394)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1394)
      %926 = stablehlo.reshape %925 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1395)
      %927 = stablehlo.reshape %arg887 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1396)
      %928 = stablehlo.reshape %927 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1397)
      %929 = stablehlo.broadcast_in_dim %928, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1398)
      %930 = stablehlo.add %926, %929 : tensor<1x128x128xbf16> loc(#loc1399)
      %931 = stablehlo.reshape %930 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1400)
      %932 = stablehlo.transpose %931, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1401)
      %933 = stablehlo.slice %932 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1402)
      %934 = stablehlo.multiply %933, %90 : tensor<1x2x128x32xbf16> loc(#loc1403)
      %935 = stablehlo.slice %932 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1404)
      %936 = stablehlo.multiply %935, %93 : tensor<1x2x128x32xbf16> loc(#loc1405)
      %937 = stablehlo.subtract %934, %936 : tensor<1x2x128x32xbf16> loc(#loc1406)
      %938 = stablehlo.multiply %935, %90 : tensor<1x2x128x32xbf16> loc(#loc1407)
      %939 = stablehlo.multiply %933, %93 : tensor<1x2x128x32xbf16> loc(#loc1408)
      %940 = stablehlo.add %938, %939 : tensor<1x2x128x32xbf16> loc(#loc1409)
      %941 = stablehlo.concatenate %937, %940, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1410)
      %942 = stablehlo.broadcast_in_dim %941, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1411)
      %943 = stablehlo.reshape %942 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1412)
      %944 = stablehlo.transpose %943, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc1413)
      %945 = stablehlo.dot_general %920, %944, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1414)
      %946 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %947 = stablehlo.multiply %945, %946 : tensor<1x16x128x128xbf16> loc(#loc1415)
      %948 = stablehlo.add %947, %128 : tensor<1x16x128x128xbf16> loc(#loc1416)
      %949 = stablehlo.reshape %arg886 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc268)
      %950 = "stablehlo.all_to_all"(%949) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc268)
      %951 = stablehlo.slice %950 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc268)
      %952 = stablehlo.reshape %951 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc268)
      %953 = stablehlo.reshape %952 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1417)
      %954 = stablehlo.reshape %953 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc1418)
      %955 = stablehlo.broadcast_in_dim %954, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc1419)
      %956 = stablehlo.concatenate %948, %955, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1420)
      %957 = stablehlo.reshape %arg896 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1421)
      %958 = stablehlo.reshape %957 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1422)
      %959 = stablehlo.convert %958 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1423)
      %960 = stablehlo.broadcast_in_dim %959, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1424)
      %961 = stablehlo.reduce(%956 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1425)
      %962 = stablehlo.broadcast_in_dim %961, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1426)
      %963 = stablehlo.subtract %956, %962 : tensor<1x16x128x129xbf16> loc(#loc1427)
      %964 = stablehlo.reduce(%963 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1428)
      %965 = stablehlo.broadcast_in_dim %964, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1429)
      %966 = stablehlo.subtract %963, %965 : tensor<1x16x128x129xbf16> loc(#loc1430)
      %967 = stablehlo.exponential %966 : tensor<1x16x128x129xbf16> loc(#loc1431)
      %968 = stablehlo.reduce(%967 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1432)
      %969 = stablehlo.broadcast_in_dim %968, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1433)
      %970 = stablehlo.divide %967, %969 : tensor<1x16x128x129xbf16> loc(#loc1434)
      %971 = stablehlo.slice %970 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1435)
      %972 = stablehlo.reshape %arg811 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1436)
      %973 = stablehlo.reshape %972 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1437)
      %974 = stablehlo.transpose %973, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1438)
      %975 = stablehlo.dot_general %899, %974, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1439)
      %976 = "stablehlo.all_reduce"(%975) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3397"), %arg1239: tensor<bf16> loc("dot.3397")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1439)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1439)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1439)
      %977 = stablehlo.reshape %976 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1440)
      %978 = stablehlo.reshape %arg810 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1441)
      %979 = stablehlo.reshape %978 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1442)
      %980 = stablehlo.broadcast_in_dim %979, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1443)
      %981 = stablehlo.add %977, %980 : tensor<1x128x128xbf16> loc(#loc1444)
      %982 = stablehlo.reshape %981 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1445)
      %983 = stablehlo.transpose %982, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1446)
      %984 = stablehlo.broadcast_in_dim %983, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1447)
      %985 = stablehlo.reshape %984 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1448)
      %986 = stablehlo.dot_general %971, %985, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1449)
      %987 = stablehlo.transpose %986, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1450)
      %988 = stablehlo.reshape %987 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc1451)
      %989 = stablehlo.reshape %arg809 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc1452)
      %990 = stablehlo.reshape %989 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc1453)
      %991 = stablehlo.transpose %990, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc1454)
      %992 = stablehlo.dot_general %988, %991, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc1455)
      %993 = "stablehlo.all_reduce"(%992) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3590"), %arg1239: tensor<bf16> loc("dot.3590")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1455)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1455)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1455)
      %994 = stablehlo.reshape %993 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1456)
      %995 = stablehlo.reshape %arg808 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1457)
      %996 = stablehlo.reshape %995 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1458)
      %997 = stablehlo.broadcast_in_dim %996, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1459)
      %998 = stablehlo.add %994, %997 : tensor<1x128x360xbf16> loc(#loc1460)
      %999 = stablehlo.add %882, %998 : tensor<1x128x360xbf16> loc(#loc1461)
      %1000 = stablehlo.reshape %arg891 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1462)
      %1001 = stablehlo.reshape %1000 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1463)
      %1002 = stablehlo.convert %1001 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1464)
      %1003 = stablehlo.broadcast_in_dim %1002, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1465)
      %1004 = stablehlo.convert %999 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1466)
      %1005 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1006 = stablehlo.power %1004, %1005 : tensor<1x128x360xf32> loc(#loc1467)
      %1007 = stablehlo.reduce(%1006 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1468)
      %1008 = "stablehlo.all_reduce"(%1007) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.3608"), %arg1239: tensor<f32> loc("reduce.3608")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1468)
        stablehlo.return %7360 : tensor<f32> loc(#loc1468)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1468)
      %1009 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1010 = stablehlo.multiply %1008, %1009 : tensor<1x128xf32> loc(#loc1469)
      %1011 = stablehlo.reshape %1010 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1470)
      %1012 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1013 = stablehlo.add %1011, %1012 : tensor<1x128x1xf32> loc(#loc1471)
      %1014 = stablehlo.rsqrt %1013 : tensor<1x128x1xf32> loc(#loc1472)
      %1015 = stablehlo.reshape %1014 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1473)
      %1016 = stablehlo.broadcast_in_dim %1015, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1474)
      %1017 = stablehlo.multiply %1004, %1016 : tensor<1x128x360xf32> loc(#loc1475)
      %1018 = stablehlo.multiply %1003, %1017 : tensor<1x128x360xf32> loc(#loc1476)
      %1019 = stablehlo.convert %1018 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1477)
      %1020 = stablehlo.reshape %1019 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1478)
      %1021 = stablehlo.broadcast_in_dim %1020, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1479)
      %1022 = stablehlo.dot_general %1021, %arg895, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1480)
      %1023 = "stablehlo.all_reduce"(%1022) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3720"), %arg1239: tensor<bf16> loc("dot.3720")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1480)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1480)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1480)
      %1024 = stablehlo.reshape %arg894 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc1481)
      %1025 = stablehlo.reshape %1024 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc1482)
      %1026 = stablehlo.broadcast_in_dim %1025, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1483)
      %1027 = stablehlo.add %1023, %1026 : tensor<32x128x5760xbf16> loc(#loc1484)
      %1028 = stablehlo.slice %1027 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1485)
      %1029 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1030 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1031 = stablehlo.clamp %1030, %1028, %1029 : tensor<32x128x2880xbf16> loc(#loc1486)
      %1032 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1033 = stablehlo.add %1031, %1032 : tensor<32x128x2880xbf16> loc(#loc1487)
      %1034 = stablehlo.slice %1027 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1488)
      %1035 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1036 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1037 = stablehlo.clamp %1035, %1034, %1036 : tensor<32x128x2880xbf16> loc(#loc1489)
      %1038 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1039 = stablehlo.multiply %1037, %1038 : tensor<32x128x2880xbf16> loc(#loc1490)
      %1040 = stablehlo.logistic %1039 : tensor<32x128x2880xbf16> loc(#loc1491)
      %1041 = stablehlo.multiply %1037, %1040 : tensor<32x128x2880xbf16> loc(#loc1492)
      %1042 = stablehlo.multiply %1033, %1041 : tensor<32x128x2880xbf16> loc(#loc1493)
      %1043 = stablehlo.dot_general %1042, %arg893, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1494)
      %1044 = stablehlo.reshape %arg892 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc1495)
      %1045 = stablehlo.reshape %1044 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc1496)
      %1046 = stablehlo.broadcast_in_dim %1045, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1497)
      %1047 = stablehlo.add %1043, %1046 : tensor<32x128x360xbf16> loc(#loc1498)
      %1048 = stablehlo.reshape %1047 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1499)
      %1049 = stablehlo.reshape %arg807 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1500)
      %1050 = stablehlo.reshape %1049 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1501)
      %1051 = stablehlo.transpose %1050, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1502)
      %1052 = stablehlo.dot_general %1020, %1051, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1503)
      %1053 = "stablehlo.all_reduce"(%1052) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3636"), %arg1239: tensor<bf16> loc("dot.3636")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1503)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1503)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1503)
      %1054 = stablehlo.reshape %arg806 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1504)
      %1055 = stablehlo.reshape %1054 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1505)
      %1056 = stablehlo.broadcast_in_dim %1055, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc1506)
      %1057 = stablehlo.add %1053, %1056 : tensor<128x128xbf16> loc(#loc1507)
      %1058:2 = "stablehlo.sort"(%1057, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.3656"), %arg1239: tensor<bf16> loc("sort.3656"), %arg1240: tensor<i32> loc("sort.3656"), %arg1241: tensor<i32> loc("sort.3656")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1509)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc1508)
      %1059 = stablehlo.slice %1058#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc1510)
      %1060 = stablehlo.convert %1059 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc1511)
      %1061 = stablehlo.reshape %1060 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc1512)
      %1062 = stablehlo.concatenate %231, %1061, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc1513)
      %1063 = stablehlo.slice %1058#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc1514)
      %1064 = stablehlo.reduce(%1063 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1515)
      %1065 = stablehlo.broadcast_in_dim %1064, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1516)
      %1066 = stablehlo.subtract %1063, %1065 : tensor<128x4xbf16> loc(#loc1517)
      %1067 = stablehlo.exponential %1066 : tensor<128x4xbf16> loc(#loc1518)
      %1068 = stablehlo.reduce(%1067 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1519)
      %1069 = stablehlo.broadcast_in_dim %1068, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1520)
      %1070 = stablehlo.divide %1067, %1069 : tensor<128x4xbf16> loc(#loc1521)
      %1071 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %1072 = "stablehlo.all_gather"(%1071) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %1073 = "stablehlo.scatter"(%1072, %1062, %1070) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.3690"), %arg1239: tensor<bf16> loc("scatter.3690")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc1522)
      %1074 = stablehlo.reshape %1073 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc1522)
      %1075 = "stablehlo.all_to_all"(%1074) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc1522)
      %1076 = stablehlo.slice %1075 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc1522)
      %1077 = stablehlo.reshape %1076 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc1522)
      %1078 = stablehlo.transpose %1077, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc1523)
      %1079 = stablehlo.reshape %1078 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc1524)
      %1080 = stablehlo.broadcast_in_dim %1079, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1525)
      %1081 = stablehlo.multiply %1048, %1080 : tensor<32x1x128x360xbf16> loc(#loc1526)
      %1082 = stablehlo.reduce(%1081 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc1527)
      %1083 = "stablehlo.all_reduce"(%1082) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.3762"), %arg1239: tensor<bf16> loc("reduce.3762")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1527)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1527)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1527)
      %1084 = stablehlo.add %999, %1083 : tensor<1x128x360xbf16> loc(#loc1528)
      %1085 = stablehlo.convert %1084 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1529)
      %1086 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1087 = stablehlo.power %1085, %1086 : tensor<1x128x360xf32> loc(#loc1530)
      %1088 = stablehlo.reduce(%1087 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1531)
      %1089 = "stablehlo.all_reduce"(%1088) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.3775"), %arg1239: tensor<f32> loc("reduce.3775")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1531)
        stablehlo.return %7360 : tensor<f32> loc(#loc1531)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1531)
      %1090 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1091 = stablehlo.multiply %1089, %1090 : tensor<1x128xf32> loc(#loc1532)
      %1092 = stablehlo.reshape %1091 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1533)
      %1093 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1094 = stablehlo.add %1092, %1093 : tensor<1x128x1xf32> loc(#loc1534)
      %1095 = stablehlo.rsqrt %1094 : tensor<1x128x1xf32> loc(#loc1535)
      %1096 = stablehlo.reshape %1095 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1536)
      %1097 = stablehlo.broadcast_in_dim %1096, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1537)
      %1098 = stablehlo.multiply %1085, %1097 : tensor<1x128x360xf32> loc(#loc1538)
      %1099 = stablehlo.multiply %960, %1098 : tensor<1x128x360xf32> loc(#loc1539)
      %1100 = stablehlo.convert %1099 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1540)
      %1101 = stablehlo.reshape %1100 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1541)
      %1102 = stablehlo.reshape %arg901 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc1542)
      %1103 = stablehlo.reshape %1102 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc1543)
      %1104 = stablehlo.transpose %1103, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc1544)
      %1105 = stablehlo.dot_general %1101, %1104, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1545)
      %1106 = "stablehlo.all_reduce"(%1105) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3889"), %arg1239: tensor<bf16> loc("dot.3889")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1545)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1545)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1545)
      %1107 = stablehlo.reshape %1106 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1546)
      %1108 = stablehlo.reshape %arg900 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1547)
      %1109 = stablehlo.reshape %1108 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc1548)
      %1110 = stablehlo.broadcast_in_dim %1109, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1549)
      %1111 = stablehlo.add %1107, %1110 : tensor<1x128x1024xbf16> loc(#loc1550)
      %1112 = stablehlo.reshape %1111 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1551)
      %1113 = stablehlo.transpose %1112, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1552)
      %1114 = stablehlo.slice %1113 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1553)
      %1115 = stablehlo.multiply %1114, %64 : tensor<1x16x128x32xbf16> loc(#loc1554)
      %1116 = stablehlo.slice %1113 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1555)
      %1117 = stablehlo.multiply %1116, %70 : tensor<1x16x128x32xbf16> loc(#loc1556)
      %1118 = stablehlo.subtract %1115, %1117 : tensor<1x16x128x32xbf16> loc(#loc1557)
      %1119 = stablehlo.multiply %1116, %64 : tensor<1x16x128x32xbf16> loc(#loc1558)
      %1120 = stablehlo.multiply %1114, %70 : tensor<1x16x128x32xbf16> loc(#loc1559)
      %1121 = stablehlo.add %1119, %1120 : tensor<1x16x128x32xbf16> loc(#loc1560)
      %1122 = stablehlo.concatenate %1118, %1121, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1561)
      %1123 = stablehlo.reshape %arg899 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1562)
      %1124 = stablehlo.reshape %1123 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1563)
      %1125 = stablehlo.transpose %1124, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1564)
      %1126 = stablehlo.dot_general %1101, %1125, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1565)
      %1127 = "stablehlo.all_reduce"(%1126) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3840"), %arg1239: tensor<bf16> loc("dot.3840")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1565)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1565)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1565)
      %1128 = stablehlo.reshape %1127 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1566)
      %1129 = stablehlo.reshape %arg898 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1567)
      %1130 = stablehlo.reshape %1129 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1568)
      %1131 = stablehlo.broadcast_in_dim %1130, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1569)
      %1132 = stablehlo.add %1128, %1131 : tensor<1x128x128xbf16> loc(#loc1570)
      %1133 = stablehlo.reshape %1132 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1571)
      %1134 = stablehlo.transpose %1133, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1572)
      %1135 = stablehlo.slice %1134 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1573)
      %1136 = stablehlo.multiply %1135, %90 : tensor<1x2x128x32xbf16> loc(#loc1574)
      %1137 = stablehlo.slice %1134 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1575)
      %1138 = stablehlo.multiply %1137, %93 : tensor<1x2x128x32xbf16> loc(#loc1576)
      %1139 = stablehlo.subtract %1136, %1138 : tensor<1x2x128x32xbf16> loc(#loc1577)
      %1140 = stablehlo.multiply %1137, %90 : tensor<1x2x128x32xbf16> loc(#loc1578)
      %1141 = stablehlo.multiply %1135, %93 : tensor<1x2x128x32xbf16> loc(#loc1579)
      %1142 = stablehlo.add %1140, %1141 : tensor<1x2x128x32xbf16> loc(#loc1580)
      %1143 = stablehlo.concatenate %1139, %1142, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1581)
      %1144 = stablehlo.broadcast_in_dim %1143, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1582)
      %1145 = stablehlo.reshape %1144 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1583)
      %1146 = stablehlo.transpose %1145, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc1584)
      %1147 = stablehlo.dot_general %1122, %1146, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1585)
      %1148 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %1149 = stablehlo.multiply %1147, %1148 : tensor<1x16x128x128xbf16> loc(#loc1586)
      %1150 = stablehlo.add %1149, %341 : tensor<1x16x128x128xbf16> loc(#loc1587)
      %1151 = stablehlo.reshape %arg897 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc279)
      %1152 = "stablehlo.all_to_all"(%1151) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc279)
      %1153 = stablehlo.slice %1152 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc279)
      %1154 = stablehlo.reshape %1153 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc279)
      %1155 = stablehlo.reshape %1154 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1588)
      %1156 = stablehlo.reshape %1155 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc1589)
      %1157 = stablehlo.broadcast_in_dim %1156, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc1590)
      %1158 = stablehlo.concatenate %1150, %1157, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1591)
      %1159 = stablehlo.reshape %arg907 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1592)
      %1160 = stablehlo.reshape %1159 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1593)
      %1161 = stablehlo.convert %1160 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1594)
      %1162 = stablehlo.broadcast_in_dim %1161, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1595)
      %1163 = stablehlo.reduce(%1158 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1596)
      %1164 = stablehlo.broadcast_in_dim %1163, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1597)
      %1165 = stablehlo.subtract %1158, %1164 : tensor<1x16x128x129xbf16> loc(#loc1598)
      %1166 = stablehlo.reduce(%1165 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1599)
      %1167 = stablehlo.broadcast_in_dim %1166, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1600)
      %1168 = stablehlo.subtract %1165, %1167 : tensor<1x16x128x129xbf16> loc(#loc1601)
      %1169 = stablehlo.exponential %1168 : tensor<1x16x128x129xbf16> loc(#loc1602)
      %1170 = stablehlo.reduce(%1169 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1603)
      %1171 = stablehlo.broadcast_in_dim %1170, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1604)
      %1172 = stablehlo.divide %1169, %1171 : tensor<1x16x128x129xbf16> loc(#loc1605)
      %1173 = stablehlo.slice %1172 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1606)
      %1174 = stablehlo.reshape %arg805 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1607)
      %1175 = stablehlo.reshape %1174 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1608)
      %1176 = stablehlo.transpose %1175, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1609)
      %1177 = stablehlo.dot_general %1101, %1176, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1610)
      %1178 = "stablehlo.all_reduce"(%1177) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3803"), %arg1239: tensor<bf16> loc("dot.3803")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1610)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1610)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1610)
      %1179 = stablehlo.reshape %1178 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1611)
      %1180 = stablehlo.reshape %arg804 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1612)
      %1181 = stablehlo.reshape %1180 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1613)
      %1182 = stablehlo.broadcast_in_dim %1181, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1614)
      %1183 = stablehlo.add %1179, %1182 : tensor<1x128x128xbf16> loc(#loc1615)
      %1184 = stablehlo.reshape %1183 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1616)
      %1185 = stablehlo.transpose %1184, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1617)
      %1186 = stablehlo.broadcast_in_dim %1185, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1618)
      %1187 = stablehlo.reshape %1186 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1619)
      %1188 = stablehlo.dot_general %1173, %1187, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1620)
      %1189 = stablehlo.transpose %1188, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1621)
      %1190 = stablehlo.reshape %1189 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc1622)
      %1191 = stablehlo.reshape %arg803 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc1623)
      %1192 = stablehlo.reshape %1191 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc1624)
      %1193 = stablehlo.transpose %1192, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc1625)
      %1194 = stablehlo.dot_general %1190, %1193, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc1626)
      %1195 = "stablehlo.all_reduce"(%1194) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.3996"), %arg1239: tensor<bf16> loc("dot.3996")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1626)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1626)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1626)
      %1196 = stablehlo.reshape %1195 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1627)
      %1197 = stablehlo.reshape %arg802 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1628)
      %1198 = stablehlo.reshape %1197 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1629)
      %1199 = stablehlo.broadcast_in_dim %1198, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1630)
      %1200 = stablehlo.add %1196, %1199 : tensor<1x128x360xbf16> loc(#loc1631)
      %1201 = stablehlo.add %1084, %1200 : tensor<1x128x360xbf16> loc(#loc1632)
      %1202 = stablehlo.reshape %arg902 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1633)
      %1203 = stablehlo.reshape %1202 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1634)
      %1204 = stablehlo.convert %1203 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1635)
      %1205 = stablehlo.broadcast_in_dim %1204, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1636)
      %1206 = stablehlo.convert %1201 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1637)
      %1207 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1208 = stablehlo.power %1206, %1207 : tensor<1x128x360xf32> loc(#loc1638)
      %1209 = stablehlo.reduce(%1208 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1639)
      %1210 = "stablehlo.all_reduce"(%1209) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.4014"), %arg1239: tensor<f32> loc("reduce.4014")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1639)
        stablehlo.return %7360 : tensor<f32> loc(#loc1639)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1639)
      %1211 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1212 = stablehlo.multiply %1210, %1211 : tensor<1x128xf32> loc(#loc1640)
      %1213 = stablehlo.reshape %1212 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1641)
      %1214 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1215 = stablehlo.add %1213, %1214 : tensor<1x128x1xf32> loc(#loc1642)
      %1216 = stablehlo.rsqrt %1215 : tensor<1x128x1xf32> loc(#loc1643)
      %1217 = stablehlo.reshape %1216 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1644)
      %1218 = stablehlo.broadcast_in_dim %1217, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1645)
      %1219 = stablehlo.multiply %1206, %1218 : tensor<1x128x360xf32> loc(#loc1646)
      %1220 = stablehlo.multiply %1205, %1219 : tensor<1x128x360xf32> loc(#loc1647)
      %1221 = stablehlo.convert %1220 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1648)
      %1222 = stablehlo.reshape %1221 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1649)
      %1223 = stablehlo.broadcast_in_dim %1222, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1650)
      %1224 = stablehlo.dot_general %1223, %arg906, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1651)
      %1225 = "stablehlo.all_reduce"(%1224) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4126"), %arg1239: tensor<bf16> loc("dot.4126")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1651)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1651)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1651)
      %1226 = stablehlo.reshape %arg905 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc1652)
      %1227 = stablehlo.reshape %1226 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc1653)
      %1228 = stablehlo.broadcast_in_dim %1227, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1654)
      %1229 = stablehlo.add %1225, %1228 : tensor<32x128x5760xbf16> loc(#loc1655)
      %1230 = stablehlo.slice %1229 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1656)
      %1231 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1232 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1233 = stablehlo.clamp %1232, %1230, %1231 : tensor<32x128x2880xbf16> loc(#loc1657)
      %1234 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1235 = stablehlo.add %1233, %1234 : tensor<32x128x2880xbf16> loc(#loc1658)
      %1236 = stablehlo.slice %1229 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1659)
      %1237 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1238 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1239 = stablehlo.clamp %1237, %1236, %1238 : tensor<32x128x2880xbf16> loc(#loc1660)
      %1240 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1241 = stablehlo.multiply %1239, %1240 : tensor<32x128x2880xbf16> loc(#loc1661)
      %1242 = stablehlo.logistic %1241 : tensor<32x128x2880xbf16> loc(#loc1662)
      %1243 = stablehlo.multiply %1239, %1242 : tensor<32x128x2880xbf16> loc(#loc1663)
      %1244 = stablehlo.multiply %1235, %1243 : tensor<32x128x2880xbf16> loc(#loc1664)
      %1245 = stablehlo.dot_general %1244, %arg904, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1665)
      %1246 = stablehlo.reshape %arg903 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc1666)
      %1247 = stablehlo.reshape %1246 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc1667)
      %1248 = stablehlo.broadcast_in_dim %1247, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1668)
      %1249 = stablehlo.add %1245, %1248 : tensor<32x128x360xbf16> loc(#loc1669)
      %1250 = stablehlo.reshape %1249 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1670)
      %1251 = stablehlo.reshape %arg801 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1671)
      %1252 = stablehlo.reshape %1251 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1672)
      %1253 = stablehlo.transpose %1252, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1673)
      %1254 = stablehlo.dot_general %1222, %1253, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1674)
      %1255 = "stablehlo.all_reduce"(%1254) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4042"), %arg1239: tensor<bf16> loc("dot.4042")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1674)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1674)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1674)
      %1256 = stablehlo.reshape %arg800 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1675)
      %1257 = stablehlo.reshape %1256 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1676)
      %1258 = stablehlo.broadcast_in_dim %1257, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc1677)
      %1259 = stablehlo.add %1255, %1258 : tensor<128x128xbf16> loc(#loc1678)
      %1260:2 = "stablehlo.sort"(%1259, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.4062"), %arg1239: tensor<bf16> loc("sort.4062"), %arg1240: tensor<i32> loc("sort.4062"), %arg1241: tensor<i32> loc("sort.4062")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1680)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc1679)
      %1261 = stablehlo.slice %1260#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc1681)
      %1262 = stablehlo.convert %1261 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc1682)
      %1263 = stablehlo.reshape %1262 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc1683)
      %1264 = stablehlo.concatenate %231, %1263, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc1684)
      %1265 = stablehlo.slice %1260#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc1685)
      %1266 = stablehlo.reduce(%1265 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1686)
      %1267 = stablehlo.broadcast_in_dim %1266, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1687)
      %1268 = stablehlo.subtract %1265, %1267 : tensor<128x4xbf16> loc(#loc1688)
      %1269 = stablehlo.exponential %1268 : tensor<128x4xbf16> loc(#loc1689)
      %1270 = stablehlo.reduce(%1269 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1690)
      %1271 = stablehlo.broadcast_in_dim %1270, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1691)
      %1272 = stablehlo.divide %1269, %1271 : tensor<128x4xbf16> loc(#loc1692)
      %1273 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %1274 = "stablehlo.all_gather"(%1273) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %1275 = "stablehlo.scatter"(%1274, %1264, %1272) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.4096"), %arg1239: tensor<bf16> loc("scatter.4096")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc1693)
      %1276 = stablehlo.reshape %1275 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc1693)
      %1277 = "stablehlo.all_to_all"(%1276) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc1693)
      %1278 = stablehlo.slice %1277 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc1693)
      %1279 = stablehlo.reshape %1278 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc1693)
      %1280 = stablehlo.transpose %1279, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc1694)
      %1281 = stablehlo.reshape %1280 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc1695)
      %1282 = stablehlo.broadcast_in_dim %1281, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1696)
      %1283 = stablehlo.multiply %1250, %1282 : tensor<32x1x128x360xbf16> loc(#loc1697)
      %1284 = stablehlo.reduce(%1283 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc1698)
      %1285 = "stablehlo.all_reduce"(%1284) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.4168"), %arg1239: tensor<bf16> loc("reduce.4168")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1698)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1698)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1698)
      %1286 = stablehlo.add %1201, %1285 : tensor<1x128x360xbf16> loc(#loc1699)
      %1287 = stablehlo.convert %1286 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1700)
      %1288 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1289 = stablehlo.power %1287, %1288 : tensor<1x128x360xf32> loc(#loc1701)
      %1290 = stablehlo.reduce(%1289 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1702)
      %1291 = "stablehlo.all_reduce"(%1290) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.4181"), %arg1239: tensor<f32> loc("reduce.4181")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1702)
        stablehlo.return %7360 : tensor<f32> loc(#loc1702)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1702)
      %1292 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1293 = stablehlo.multiply %1291, %1292 : tensor<1x128xf32> loc(#loc1703)
      %1294 = stablehlo.reshape %1293 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1704)
      %1295 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1296 = stablehlo.add %1294, %1295 : tensor<1x128x1xf32> loc(#loc1705)
      %1297 = stablehlo.rsqrt %1296 : tensor<1x128x1xf32> loc(#loc1706)
      %1298 = stablehlo.reshape %1297 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1707)
      %1299 = stablehlo.broadcast_in_dim %1298, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1708)
      %1300 = stablehlo.multiply %1287, %1299 : tensor<1x128x360xf32> loc(#loc1709)
      %1301 = stablehlo.multiply %1162, %1300 : tensor<1x128x360xf32> loc(#loc1710)
      %1302 = stablehlo.convert %1301 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1711)
      %1303 = stablehlo.reshape %1302 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1712)
      %1304 = stablehlo.reshape %arg912 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc1713)
      %1305 = stablehlo.reshape %1304 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc1714)
      %1306 = stablehlo.transpose %1305, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc1715)
      %1307 = stablehlo.dot_general %1303, %1306, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1716)
      %1308 = "stablehlo.all_reduce"(%1307) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4295"), %arg1239: tensor<bf16> loc("dot.4295")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1716)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1716)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1716)
      %1309 = stablehlo.reshape %1308 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1717)
      %1310 = stablehlo.reshape %arg911 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1718)
      %1311 = stablehlo.reshape %1310 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc1719)
      %1312 = stablehlo.broadcast_in_dim %1311, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1720)
      %1313 = stablehlo.add %1309, %1312 : tensor<1x128x1024xbf16> loc(#loc1721)
      %1314 = stablehlo.reshape %1313 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1722)
      %1315 = stablehlo.transpose %1314, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1723)
      %1316 = stablehlo.slice %1315 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1724)
      %1317 = stablehlo.multiply %1316, %64 : tensor<1x16x128x32xbf16> loc(#loc1725)
      %1318 = stablehlo.slice %1315 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1726)
      %1319 = stablehlo.multiply %1318, %70 : tensor<1x16x128x32xbf16> loc(#loc1727)
      %1320 = stablehlo.subtract %1317, %1319 : tensor<1x16x128x32xbf16> loc(#loc1728)
      %1321 = stablehlo.multiply %1318, %64 : tensor<1x16x128x32xbf16> loc(#loc1729)
      %1322 = stablehlo.multiply %1316, %70 : tensor<1x16x128x32xbf16> loc(#loc1730)
      %1323 = stablehlo.add %1321, %1322 : tensor<1x16x128x32xbf16> loc(#loc1731)
      %1324 = stablehlo.concatenate %1320, %1323, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1732)
      %1325 = stablehlo.reshape %arg910 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1733)
      %1326 = stablehlo.reshape %1325 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1734)
      %1327 = stablehlo.transpose %1326, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1735)
      %1328 = stablehlo.dot_general %1303, %1327, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1736)
      %1329 = "stablehlo.all_reduce"(%1328) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4246"), %arg1239: tensor<bf16> loc("dot.4246")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1736)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1736)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1736)
      %1330 = stablehlo.reshape %1329 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1737)
      %1331 = stablehlo.reshape %arg909 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1738)
      %1332 = stablehlo.reshape %1331 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1739)
      %1333 = stablehlo.broadcast_in_dim %1332, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1740)
      %1334 = stablehlo.add %1330, %1333 : tensor<1x128x128xbf16> loc(#loc1741)
      %1335 = stablehlo.reshape %1334 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1742)
      %1336 = stablehlo.transpose %1335, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1743)
      %1337 = stablehlo.slice %1336 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1744)
      %1338 = stablehlo.multiply %1337, %90 : tensor<1x2x128x32xbf16> loc(#loc1745)
      %1339 = stablehlo.slice %1336 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1746)
      %1340 = stablehlo.multiply %1339, %93 : tensor<1x2x128x32xbf16> loc(#loc1747)
      %1341 = stablehlo.subtract %1338, %1340 : tensor<1x2x128x32xbf16> loc(#loc1748)
      %1342 = stablehlo.multiply %1339, %90 : tensor<1x2x128x32xbf16> loc(#loc1749)
      %1343 = stablehlo.multiply %1337, %93 : tensor<1x2x128x32xbf16> loc(#loc1750)
      %1344 = stablehlo.add %1342, %1343 : tensor<1x2x128x32xbf16> loc(#loc1751)
      %1345 = stablehlo.concatenate %1341, %1344, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1752)
      %1346 = stablehlo.broadcast_in_dim %1345, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1753)
      %1347 = stablehlo.reshape %1346 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1754)
      %1348 = stablehlo.transpose %1347, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc1755)
      %1349 = stablehlo.dot_general %1324, %1348, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1756)
      %1350 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %1351 = stablehlo.multiply %1349, %1350 : tensor<1x16x128x128xbf16> loc(#loc1757)
      %1352 = stablehlo.add %1351, %128 : tensor<1x16x128x128xbf16> loc(#loc1758)
      %1353 = stablehlo.reshape %arg908 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc290)
      %1354 = "stablehlo.all_to_all"(%1353) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc290)
      %1355 = stablehlo.slice %1354 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc290)
      %1356 = stablehlo.reshape %1355 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc290)
      %1357 = stablehlo.reshape %1356 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1759)
      %1358 = stablehlo.reshape %1357 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc1760)
      %1359 = stablehlo.broadcast_in_dim %1358, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc1761)
      %1360 = stablehlo.concatenate %1352, %1359, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1762)
      %1361 = stablehlo.reshape %arg918 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1763)
      %1362 = stablehlo.reshape %1361 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1764)
      %1363 = stablehlo.convert %1362 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1765)
      %1364 = stablehlo.broadcast_in_dim %1363, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1766)
      %1365 = stablehlo.reduce(%1360 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1767)
      %1366 = stablehlo.broadcast_in_dim %1365, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1768)
      %1367 = stablehlo.subtract %1360, %1366 : tensor<1x16x128x129xbf16> loc(#loc1769)
      %1368 = stablehlo.reduce(%1367 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1770)
      %1369 = stablehlo.broadcast_in_dim %1368, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1771)
      %1370 = stablehlo.subtract %1367, %1369 : tensor<1x16x128x129xbf16> loc(#loc1772)
      %1371 = stablehlo.exponential %1370 : tensor<1x16x128x129xbf16> loc(#loc1773)
      %1372 = stablehlo.reduce(%1371 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1774)
      %1373 = stablehlo.broadcast_in_dim %1372, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1775)
      %1374 = stablehlo.divide %1371, %1373 : tensor<1x16x128x129xbf16> loc(#loc1776)
      %1375 = stablehlo.slice %1374 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1777)
      %1376 = stablehlo.reshape %arg799 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1778)
      %1377 = stablehlo.reshape %1376 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1779)
      %1378 = stablehlo.transpose %1377, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1780)
      %1379 = stablehlo.dot_general %1303, %1378, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1781)
      %1380 = "stablehlo.all_reduce"(%1379) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4209"), %arg1239: tensor<bf16> loc("dot.4209")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1781)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1781)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1781)
      %1381 = stablehlo.reshape %1380 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1782)
      %1382 = stablehlo.reshape %arg798 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1783)
      %1383 = stablehlo.reshape %1382 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1784)
      %1384 = stablehlo.broadcast_in_dim %1383, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1785)
      %1385 = stablehlo.add %1381, %1384 : tensor<1x128x128xbf16> loc(#loc1786)
      %1386 = stablehlo.reshape %1385 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1787)
      %1387 = stablehlo.transpose %1386, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1788)
      %1388 = stablehlo.broadcast_in_dim %1387, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1789)
      %1389 = stablehlo.reshape %1388 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1790)
      %1390 = stablehlo.dot_general %1375, %1389, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1791)
      %1391 = stablehlo.transpose %1390, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1792)
      %1392 = stablehlo.reshape %1391 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc1793)
      %1393 = stablehlo.reshape %arg797 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc1794)
      %1394 = stablehlo.reshape %1393 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc1795)
      %1395 = stablehlo.transpose %1394, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc1796)
      %1396 = stablehlo.dot_general %1392, %1395, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc1797)
      %1397 = "stablehlo.all_reduce"(%1396) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4402"), %arg1239: tensor<bf16> loc("dot.4402")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1797)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1797)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1797)
      %1398 = stablehlo.reshape %1397 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1798)
      %1399 = stablehlo.reshape %arg796 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1799)
      %1400 = stablehlo.reshape %1399 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1800)
      %1401 = stablehlo.broadcast_in_dim %1400, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1801)
      %1402 = stablehlo.add %1398, %1401 : tensor<1x128x360xbf16> loc(#loc1802)
      %1403 = stablehlo.add %1286, %1402 : tensor<1x128x360xbf16> loc(#loc1803)
      %1404 = stablehlo.reshape %arg913 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1804)
      %1405 = stablehlo.reshape %1404 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1805)
      %1406 = stablehlo.convert %1405 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1806)
      %1407 = stablehlo.broadcast_in_dim %1406, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1807)
      %1408 = stablehlo.convert %1403 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1808)
      %1409 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1410 = stablehlo.power %1408, %1409 : tensor<1x128x360xf32> loc(#loc1809)
      %1411 = stablehlo.reduce(%1410 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1810)
      %1412 = "stablehlo.all_reduce"(%1411) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.4420"), %arg1239: tensor<f32> loc("reduce.4420")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1810)
        stablehlo.return %7360 : tensor<f32> loc(#loc1810)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1810)
      %1413 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1414 = stablehlo.multiply %1412, %1413 : tensor<1x128xf32> loc(#loc1811)
      %1415 = stablehlo.reshape %1414 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1812)
      %1416 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1417 = stablehlo.add %1415, %1416 : tensor<1x128x1xf32> loc(#loc1813)
      %1418 = stablehlo.rsqrt %1417 : tensor<1x128x1xf32> loc(#loc1814)
      %1419 = stablehlo.reshape %1418 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1815)
      %1420 = stablehlo.broadcast_in_dim %1419, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1816)
      %1421 = stablehlo.multiply %1408, %1420 : tensor<1x128x360xf32> loc(#loc1817)
      %1422 = stablehlo.multiply %1407, %1421 : tensor<1x128x360xf32> loc(#loc1818)
      %1423 = stablehlo.convert %1422 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1819)
      %1424 = stablehlo.reshape %1423 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1820)
      %1425 = stablehlo.broadcast_in_dim %1424, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1821)
      %1426 = stablehlo.dot_general %1425, %arg917, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1822)
      %1427 = "stablehlo.all_reduce"(%1426) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4532"), %arg1239: tensor<bf16> loc("dot.4532")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1822)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1822)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1822)
      %1428 = stablehlo.reshape %arg916 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc1823)
      %1429 = stablehlo.reshape %1428 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc1824)
      %1430 = stablehlo.broadcast_in_dim %1429, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1825)
      %1431 = stablehlo.add %1427, %1430 : tensor<32x128x5760xbf16> loc(#loc1826)
      %1432 = stablehlo.slice %1431 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1827)
      %1433 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1434 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1435 = stablehlo.clamp %1434, %1432, %1433 : tensor<32x128x2880xbf16> loc(#loc1828)
      %1436 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1437 = stablehlo.add %1435, %1436 : tensor<32x128x2880xbf16> loc(#loc1829)
      %1438 = stablehlo.slice %1431 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1830)
      %1439 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1440 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1441 = stablehlo.clamp %1439, %1438, %1440 : tensor<32x128x2880xbf16> loc(#loc1831)
      %1442 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1443 = stablehlo.multiply %1441, %1442 : tensor<32x128x2880xbf16> loc(#loc1832)
      %1444 = stablehlo.logistic %1443 : tensor<32x128x2880xbf16> loc(#loc1833)
      %1445 = stablehlo.multiply %1441, %1444 : tensor<32x128x2880xbf16> loc(#loc1834)
      %1446 = stablehlo.multiply %1437, %1445 : tensor<32x128x2880xbf16> loc(#loc1835)
      %1447 = stablehlo.dot_general %1446, %arg915, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1836)
      %1448 = stablehlo.reshape %arg914 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc1837)
      %1449 = stablehlo.reshape %1448 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc1838)
      %1450 = stablehlo.broadcast_in_dim %1449, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1839)
      %1451 = stablehlo.add %1447, %1450 : tensor<32x128x360xbf16> loc(#loc1840)
      %1452 = stablehlo.reshape %1451 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1841)
      %1453 = stablehlo.reshape %arg795 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1842)
      %1454 = stablehlo.reshape %1453 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1843)
      %1455 = stablehlo.transpose %1454, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1844)
      %1456 = stablehlo.dot_general %1424, %1455, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1845)
      %1457 = "stablehlo.all_reduce"(%1456) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4448"), %arg1239: tensor<bf16> loc("dot.4448")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1845)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1845)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1845)
      %1458 = stablehlo.reshape %arg794 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1846)
      %1459 = stablehlo.reshape %1458 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1847)
      %1460 = stablehlo.broadcast_in_dim %1459, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc1848)
      %1461 = stablehlo.add %1457, %1460 : tensor<128x128xbf16> loc(#loc1849)
      %1462:2 = "stablehlo.sort"(%1461, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.4468"), %arg1239: tensor<bf16> loc("sort.4468"), %arg1240: tensor<i32> loc("sort.4468"), %arg1241: tensor<i32> loc("sort.4468")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc1851)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc1850)
      %1463 = stablehlo.slice %1462#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc1852)
      %1464 = stablehlo.convert %1463 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc1853)
      %1465 = stablehlo.reshape %1464 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc1854)
      %1466 = stablehlo.concatenate %231, %1465, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc1855)
      %1467 = stablehlo.slice %1462#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc1856)
      %1468 = stablehlo.reduce(%1467 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1857)
      %1469 = stablehlo.broadcast_in_dim %1468, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1858)
      %1470 = stablehlo.subtract %1467, %1469 : tensor<128x4xbf16> loc(#loc1859)
      %1471 = stablehlo.exponential %1470 : tensor<128x4xbf16> loc(#loc1860)
      %1472 = stablehlo.reduce(%1471 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc1861)
      %1473 = stablehlo.broadcast_in_dim %1472, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc1862)
      %1474 = stablehlo.divide %1471, %1473 : tensor<128x4xbf16> loc(#loc1863)
      %1475 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %1476 = "stablehlo.all_gather"(%1475) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %1477 = "stablehlo.scatter"(%1476, %1466, %1474) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.4502"), %arg1239: tensor<bf16> loc("scatter.4502")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc1864)
      %1478 = stablehlo.reshape %1477 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc1864)
      %1479 = "stablehlo.all_to_all"(%1478) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc1864)
      %1480 = stablehlo.slice %1479 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc1864)
      %1481 = stablehlo.reshape %1480 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc1864)
      %1482 = stablehlo.transpose %1481, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc1865)
      %1483 = stablehlo.reshape %1482 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc1866)
      %1484 = stablehlo.broadcast_in_dim %1483, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc1867)
      %1485 = stablehlo.multiply %1452, %1484 : tensor<32x1x128x360xbf16> loc(#loc1868)
      %1486 = stablehlo.reduce(%1485 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc1869)
      %1487 = "stablehlo.all_reduce"(%1486) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.4574"), %arg1239: tensor<bf16> loc("reduce.4574")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1869)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1869)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1869)
      %1488 = stablehlo.add %1403, %1487 : tensor<1x128x360xbf16> loc(#loc1870)
      %1489 = stablehlo.convert %1488 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1871)
      %1490 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1491 = stablehlo.power %1489, %1490 : tensor<1x128x360xf32> loc(#loc1872)
      %1492 = stablehlo.reduce(%1491 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1873)
      %1493 = "stablehlo.all_reduce"(%1492) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.4587"), %arg1239: tensor<f32> loc("reduce.4587")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1873)
        stablehlo.return %7360 : tensor<f32> loc(#loc1873)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1873)
      %1494 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1495 = stablehlo.multiply %1493, %1494 : tensor<1x128xf32> loc(#loc1874)
      %1496 = stablehlo.reshape %1495 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1875)
      %1497 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1498 = stablehlo.add %1496, %1497 : tensor<1x128x1xf32> loc(#loc1876)
      %1499 = stablehlo.rsqrt %1498 : tensor<1x128x1xf32> loc(#loc1877)
      %1500 = stablehlo.reshape %1499 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1878)
      %1501 = stablehlo.broadcast_in_dim %1500, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1879)
      %1502 = stablehlo.multiply %1489, %1501 : tensor<1x128x360xf32> loc(#loc1880)
      %1503 = stablehlo.multiply %1364, %1502 : tensor<1x128x360xf32> loc(#loc1881)
      %1504 = stablehlo.convert %1503 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1882)
      %1505 = stablehlo.reshape %1504 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1883)
      %1506 = stablehlo.reshape %arg923 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc1884)
      %1507 = stablehlo.reshape %1506 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc1885)
      %1508 = stablehlo.transpose %1507, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc1886)
      %1509 = stablehlo.dot_general %1505, %1508, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1887)
      %1510 = "stablehlo.all_reduce"(%1509) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4701"), %arg1239: tensor<bf16> loc("dot.4701")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1887)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1887)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc1887)
      %1511 = stablehlo.reshape %1510 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1888)
      %1512 = stablehlo.reshape %arg922 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc1889)
      %1513 = stablehlo.reshape %1512 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc1890)
      %1514 = stablehlo.broadcast_in_dim %1513, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc1891)
      %1515 = stablehlo.add %1511, %1514 : tensor<1x128x1024xbf16> loc(#loc1892)
      %1516 = stablehlo.reshape %1515 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1893)
      %1517 = stablehlo.transpose %1516, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1894)
      %1518 = stablehlo.slice %1517 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1895)
      %1519 = stablehlo.multiply %1518, %64 : tensor<1x16x128x32xbf16> loc(#loc1896)
      %1520 = stablehlo.slice %1517 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc1897)
      %1521 = stablehlo.multiply %1520, %70 : tensor<1x16x128x32xbf16> loc(#loc1898)
      %1522 = stablehlo.subtract %1519, %1521 : tensor<1x16x128x32xbf16> loc(#loc1899)
      %1523 = stablehlo.multiply %1520, %64 : tensor<1x16x128x32xbf16> loc(#loc1900)
      %1524 = stablehlo.multiply %1518, %70 : tensor<1x16x128x32xbf16> loc(#loc1901)
      %1525 = stablehlo.add %1523, %1524 : tensor<1x16x128x32xbf16> loc(#loc1902)
      %1526 = stablehlo.concatenate %1522, %1525, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1903)
      %1527 = stablehlo.reshape %arg921 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1904)
      %1528 = stablehlo.reshape %1527 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1905)
      %1529 = stablehlo.transpose %1528, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1906)
      %1530 = stablehlo.dot_general %1505, %1529, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1907)
      %1531 = "stablehlo.all_reduce"(%1530) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4652"), %arg1239: tensor<bf16> loc("dot.4652")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1907)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1907)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1907)
      %1532 = stablehlo.reshape %1531 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1908)
      %1533 = stablehlo.reshape %arg920 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1909)
      %1534 = stablehlo.reshape %1533 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1910)
      %1535 = stablehlo.broadcast_in_dim %1534, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1911)
      %1536 = stablehlo.add %1532, %1535 : tensor<1x128x128xbf16> loc(#loc1912)
      %1537 = stablehlo.reshape %1536 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1913)
      %1538 = stablehlo.transpose %1537, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1914)
      %1539 = stablehlo.slice %1538 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1915)
      %1540 = stablehlo.multiply %1539, %90 : tensor<1x2x128x32xbf16> loc(#loc1916)
      %1541 = stablehlo.slice %1538 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc1917)
      %1542 = stablehlo.multiply %1541, %93 : tensor<1x2x128x32xbf16> loc(#loc1918)
      %1543 = stablehlo.subtract %1540, %1542 : tensor<1x2x128x32xbf16> loc(#loc1919)
      %1544 = stablehlo.multiply %1541, %90 : tensor<1x2x128x32xbf16> loc(#loc1920)
      %1545 = stablehlo.multiply %1539, %93 : tensor<1x2x128x32xbf16> loc(#loc1921)
      %1546 = stablehlo.add %1544, %1545 : tensor<1x2x128x32xbf16> loc(#loc1922)
      %1547 = stablehlo.concatenate %1543, %1546, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1923)
      %1548 = stablehlo.broadcast_in_dim %1547, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1924)
      %1549 = stablehlo.reshape %1548 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1925)
      %1550 = stablehlo.transpose %1549, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc1926)
      %1551 = stablehlo.dot_general %1526, %1550, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1927)
      %1552 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %1553 = stablehlo.multiply %1551, %1552 : tensor<1x16x128x128xbf16> loc(#loc1928)
      %1554 = stablehlo.add %1553, %341 : tensor<1x16x128x128xbf16> loc(#loc1929)
      %1555 = stablehlo.reshape %arg919 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc301)
      %1556 = "stablehlo.all_to_all"(%1555) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc301)
      %1557 = stablehlo.slice %1556 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc301)
      %1558 = stablehlo.reshape %1557 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc301)
      %1559 = stablehlo.reshape %1558 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc1930)
      %1560 = stablehlo.reshape %1559 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc1931)
      %1561 = stablehlo.broadcast_in_dim %1560, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc1932)
      %1562 = stablehlo.concatenate %1554, %1561, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1933)
      %1563 = stablehlo.reshape %arg929 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1934)
      %1564 = stablehlo.reshape %1563 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1935)
      %1565 = stablehlo.convert %1564 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1936)
      %1566 = stablehlo.broadcast_in_dim %1565, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1937)
      %1567 = stablehlo.reduce(%1562 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1938)
      %1568 = stablehlo.broadcast_in_dim %1567, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1939)
      %1569 = stablehlo.subtract %1562, %1568 : tensor<1x16x128x129xbf16> loc(#loc1940)
      %1570 = stablehlo.reduce(%1569 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1941)
      %1571 = stablehlo.broadcast_in_dim %1570, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1942)
      %1572 = stablehlo.subtract %1569, %1571 : tensor<1x16x128x129xbf16> loc(#loc1943)
      %1573 = stablehlo.exponential %1572 : tensor<1x16x128x129xbf16> loc(#loc1944)
      %1574 = stablehlo.reduce(%1573 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc1945)
      %1575 = stablehlo.broadcast_in_dim %1574, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc1946)
      %1576 = stablehlo.divide %1573, %1575 : tensor<1x16x128x129xbf16> loc(#loc1947)
      %1577 = stablehlo.slice %1576 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc1948)
      %1578 = stablehlo.reshape %arg793 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1949)
      %1579 = stablehlo.reshape %1578 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1950)
      %1580 = stablehlo.transpose %1579, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc1951)
      %1581 = stablehlo.dot_general %1505, %1580, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc1952)
      %1582 = "stablehlo.all_reduce"(%1581) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4615"), %arg1239: tensor<bf16> loc("dot.4615")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1952)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1952)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc1952)
      %1583 = stablehlo.reshape %1582 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1953)
      %1584 = stablehlo.reshape %arg792 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1954)
      %1585 = stablehlo.reshape %1584 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1955)
      %1586 = stablehlo.broadcast_in_dim %1585, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc1956)
      %1587 = stablehlo.add %1583, %1586 : tensor<1x128x128xbf16> loc(#loc1957)
      %1588 = stablehlo.reshape %1587 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc1958)
      %1589 = stablehlo.transpose %1588, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc1959)
      %1590 = stablehlo.broadcast_in_dim %1589, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc1960)
      %1591 = stablehlo.reshape %1590 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1961)
      %1592 = stablehlo.dot_general %1577, %1591, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc1962)
      %1593 = stablehlo.transpose %1592, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc1963)
      %1594 = stablehlo.reshape %1593 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc1964)
      %1595 = stablehlo.reshape %arg791 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc1965)
      %1596 = stablehlo.reshape %1595 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc1966)
      %1597 = stablehlo.transpose %1596, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc1967)
      %1598 = stablehlo.dot_general %1594, %1597, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc1968)
      %1599 = "stablehlo.all_reduce"(%1598) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4808"), %arg1239: tensor<bf16> loc("dot.4808")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1968)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1968)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1968)
      %1600 = stablehlo.reshape %1599 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1969)
      %1601 = stablehlo.reshape %arg790 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1970)
      %1602 = stablehlo.reshape %1601 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1971)
      %1603 = stablehlo.broadcast_in_dim %1602, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc1972)
      %1604 = stablehlo.add %1600, %1603 : tensor<1x128x360xbf16> loc(#loc1973)
      %1605 = stablehlo.add %1488, %1604 : tensor<1x128x360xbf16> loc(#loc1974)
      %1606 = stablehlo.reshape %arg924 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc1975)
      %1607 = stablehlo.reshape %1606 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc1976)
      %1608 = stablehlo.convert %1607 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc1977)
      %1609 = stablehlo.broadcast_in_dim %1608, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc1978)
      %1610 = stablehlo.convert %1605 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc1979)
      %1611 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1612 = stablehlo.power %1610, %1611 : tensor<1x128x360xf32> loc(#loc1980)
      %1613 = stablehlo.reduce(%1612 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc1981)
      %1614 = "stablehlo.all_reduce"(%1613) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.4826"), %arg1239: tensor<f32> loc("reduce.4826")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc1981)
        stablehlo.return %7360 : tensor<f32> loc(#loc1981)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc1981)
      %1615 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1616 = stablehlo.multiply %1614, %1615 : tensor<1x128xf32> loc(#loc1982)
      %1617 = stablehlo.reshape %1616 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc1983)
      %1618 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1619 = stablehlo.add %1617, %1618 : tensor<1x128x1xf32> loc(#loc1984)
      %1620 = stablehlo.rsqrt %1619 : tensor<1x128x1xf32> loc(#loc1985)
      %1621 = stablehlo.reshape %1620 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc1986)
      %1622 = stablehlo.broadcast_in_dim %1621, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc1987)
      %1623 = stablehlo.multiply %1610, %1622 : tensor<1x128x360xf32> loc(#loc1988)
      %1624 = stablehlo.multiply %1609, %1623 : tensor<1x128x360xf32> loc(#loc1989)
      %1625 = stablehlo.convert %1624 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc1990)
      %1626 = stablehlo.reshape %1625 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc1991)
      %1627 = stablehlo.broadcast_in_dim %1626, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc1992)
      %1628 = stablehlo.dot_general %1627, %arg928, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1993)
      %1629 = "stablehlo.all_reduce"(%1628) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4938"), %arg1239: tensor<bf16> loc("dot.4938")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc1993)
        stablehlo.return %7360 : tensor<bf16> loc(#loc1993)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1993)
      %1630 = stablehlo.reshape %arg927 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc1994)
      %1631 = stablehlo.reshape %1630 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc1995)
      %1632 = stablehlo.broadcast_in_dim %1631, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc1996)
      %1633 = stablehlo.add %1629, %1632 : tensor<32x128x5760xbf16> loc(#loc1997)
      %1634 = stablehlo.slice %1633 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc1998)
      %1635 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1636 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1637 = stablehlo.clamp %1636, %1634, %1635 : tensor<32x128x2880xbf16> loc(#loc1999)
      %1638 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1639 = stablehlo.add %1637, %1638 : tensor<32x128x2880xbf16> loc(#loc2000)
      %1640 = stablehlo.slice %1633 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2001)
      %1641 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1642 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1643 = stablehlo.clamp %1641, %1640, %1642 : tensor<32x128x2880xbf16> loc(#loc2002)
      %1644 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1645 = stablehlo.multiply %1643, %1644 : tensor<32x128x2880xbf16> loc(#loc2003)
      %1646 = stablehlo.logistic %1645 : tensor<32x128x2880xbf16> loc(#loc2004)
      %1647 = stablehlo.multiply %1643, %1646 : tensor<32x128x2880xbf16> loc(#loc2005)
      %1648 = stablehlo.multiply %1639, %1647 : tensor<32x128x2880xbf16> loc(#loc2006)
      %1649 = stablehlo.dot_general %1648, %arg926, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2007)
      %1650 = stablehlo.reshape %arg925 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc2008)
      %1651 = stablehlo.reshape %1650 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc2009)
      %1652 = stablehlo.broadcast_in_dim %1651, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2010)
      %1653 = stablehlo.add %1649, %1652 : tensor<32x128x360xbf16> loc(#loc2011)
      %1654 = stablehlo.reshape %1653 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2012)
      %1655 = stablehlo.reshape %arg789 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2013)
      %1656 = stablehlo.reshape %1655 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2014)
      %1657 = stablehlo.transpose %1656, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2015)
      %1658 = stablehlo.dot_general %1626, %1657, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2016)
      %1659 = "stablehlo.all_reduce"(%1658) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.4854"), %arg1239: tensor<bf16> loc("dot.4854")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2016)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2016)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2016)
      %1660 = stablehlo.reshape %arg788 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2017)
      %1661 = stablehlo.reshape %1660 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2018)
      %1662 = stablehlo.broadcast_in_dim %1661, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc2019)
      %1663 = stablehlo.add %1659, %1662 : tensor<128x128xbf16> loc(#loc2020)
      %1664:2 = "stablehlo.sort"(%1663, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.4874"), %arg1239: tensor<bf16> loc("sort.4874"), %arg1240: tensor<i32> loc("sort.4874"), %arg1241: tensor<i32> loc("sort.4874")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2022)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc2021)
      %1665 = stablehlo.slice %1664#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc2023)
      %1666 = stablehlo.convert %1665 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc2024)
      %1667 = stablehlo.reshape %1666 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc2025)
      %1668 = stablehlo.concatenate %231, %1667, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc2026)
      %1669 = stablehlo.slice %1664#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc2027)
      %1670 = stablehlo.reduce(%1669 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2028)
      %1671 = stablehlo.broadcast_in_dim %1670, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2029)
      %1672 = stablehlo.subtract %1669, %1671 : tensor<128x4xbf16> loc(#loc2030)
      %1673 = stablehlo.exponential %1672 : tensor<128x4xbf16> loc(#loc2031)
      %1674 = stablehlo.reduce(%1673 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2032)
      %1675 = stablehlo.broadcast_in_dim %1674, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2033)
      %1676 = stablehlo.divide %1673, %1675 : tensor<128x4xbf16> loc(#loc2034)
      %1677 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %1678 = "stablehlo.all_gather"(%1677) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %1679 = "stablehlo.scatter"(%1678, %1668, %1676) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.4908"), %arg1239: tensor<bf16> loc("scatter.4908")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc2035)
      %1680 = stablehlo.reshape %1679 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc2035)
      %1681 = "stablehlo.all_to_all"(%1680) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc2035)
      %1682 = stablehlo.slice %1681 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc2035)
      %1683 = stablehlo.reshape %1682 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc2035)
      %1684 = stablehlo.transpose %1683, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc2036)
      %1685 = stablehlo.reshape %1684 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc2037)
      %1686 = stablehlo.broadcast_in_dim %1685, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2038)
      %1687 = stablehlo.multiply %1654, %1686 : tensor<32x1x128x360xbf16> loc(#loc2039)
      %1688 = stablehlo.reduce(%1687 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc2040)
      %1689 = "stablehlo.all_reduce"(%1688) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.4980"), %arg1239: tensor<bf16> loc("reduce.4980")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2040)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2040)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2040)
      %1690 = stablehlo.add %1605, %1689 : tensor<1x128x360xbf16> loc(#loc2041)
      %1691 = stablehlo.convert %1690 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2042)
      %1692 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1693 = stablehlo.power %1691, %1692 : tensor<1x128x360xf32> loc(#loc2043)
      %1694 = stablehlo.reduce(%1693 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2044)
      %1695 = "stablehlo.all_reduce"(%1694) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.4993"), %arg1239: tensor<f32> loc("reduce.4993")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2044)
        stablehlo.return %7360 : tensor<f32> loc(#loc2044)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2044)
      %1696 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1697 = stablehlo.multiply %1695, %1696 : tensor<1x128xf32> loc(#loc2045)
      %1698 = stablehlo.reshape %1697 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2046)
      %1699 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1700 = stablehlo.add %1698, %1699 : tensor<1x128x1xf32> loc(#loc2047)
      %1701 = stablehlo.rsqrt %1700 : tensor<1x128x1xf32> loc(#loc2048)
      %1702 = stablehlo.reshape %1701 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2049)
      %1703 = stablehlo.broadcast_in_dim %1702, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2050)
      %1704 = stablehlo.multiply %1691, %1703 : tensor<1x128x360xf32> loc(#loc2051)
      %1705 = stablehlo.multiply %1566, %1704 : tensor<1x128x360xf32> loc(#loc2052)
      %1706 = stablehlo.convert %1705 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2053)
      %1707 = stablehlo.reshape %1706 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2054)
      %1708 = stablehlo.reshape %arg934 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc2055)
      %1709 = stablehlo.reshape %1708 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc2056)
      %1710 = stablehlo.transpose %1709, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc2057)
      %1711 = stablehlo.dot_general %1707, %1710, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2058)
      %1712 = "stablehlo.all_reduce"(%1711) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5107"), %arg1239: tensor<bf16> loc("dot.5107")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2058)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2058)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2058)
      %1713 = stablehlo.reshape %1712 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2059)
      %1714 = stablehlo.reshape %arg933 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2060)
      %1715 = stablehlo.reshape %1714 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc2061)
      %1716 = stablehlo.broadcast_in_dim %1715, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2062)
      %1717 = stablehlo.add %1713, %1716 : tensor<1x128x1024xbf16> loc(#loc2063)
      %1718 = stablehlo.reshape %1717 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2064)
      %1719 = stablehlo.transpose %1718, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2065)
      %1720 = stablehlo.slice %1719 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2066)
      %1721 = stablehlo.multiply %1720, %64 : tensor<1x16x128x32xbf16> loc(#loc2067)
      %1722 = stablehlo.slice %1719 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2068)
      %1723 = stablehlo.multiply %1722, %70 : tensor<1x16x128x32xbf16> loc(#loc2069)
      %1724 = stablehlo.subtract %1721, %1723 : tensor<1x16x128x32xbf16> loc(#loc2070)
      %1725 = stablehlo.multiply %1722, %64 : tensor<1x16x128x32xbf16> loc(#loc2071)
      %1726 = stablehlo.multiply %1720, %70 : tensor<1x16x128x32xbf16> loc(#loc2072)
      %1727 = stablehlo.add %1725, %1726 : tensor<1x16x128x32xbf16> loc(#loc2073)
      %1728 = stablehlo.concatenate %1724, %1727, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2074)
      %1729 = stablehlo.reshape %arg932 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2075)
      %1730 = stablehlo.reshape %1729 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2076)
      %1731 = stablehlo.transpose %1730, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2077)
      %1732 = stablehlo.dot_general %1707, %1731, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2078)
      %1733 = "stablehlo.all_reduce"(%1732) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5058"), %arg1239: tensor<bf16> loc("dot.5058")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2078)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2078)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2078)
      %1734 = stablehlo.reshape %1733 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2079)
      %1735 = stablehlo.reshape %arg931 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2080)
      %1736 = stablehlo.reshape %1735 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2081)
      %1737 = stablehlo.broadcast_in_dim %1736, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2082)
      %1738 = stablehlo.add %1734, %1737 : tensor<1x128x128xbf16> loc(#loc2083)
      %1739 = stablehlo.reshape %1738 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2084)
      %1740 = stablehlo.transpose %1739, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2085)
      %1741 = stablehlo.slice %1740 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2086)
      %1742 = stablehlo.multiply %1741, %90 : tensor<1x2x128x32xbf16> loc(#loc2087)
      %1743 = stablehlo.slice %1740 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2088)
      %1744 = stablehlo.multiply %1743, %93 : tensor<1x2x128x32xbf16> loc(#loc2089)
      %1745 = stablehlo.subtract %1742, %1744 : tensor<1x2x128x32xbf16> loc(#loc2090)
      %1746 = stablehlo.multiply %1743, %90 : tensor<1x2x128x32xbf16> loc(#loc2091)
      %1747 = stablehlo.multiply %1741, %93 : tensor<1x2x128x32xbf16> loc(#loc2092)
      %1748 = stablehlo.add %1746, %1747 : tensor<1x2x128x32xbf16> loc(#loc2093)
      %1749 = stablehlo.concatenate %1745, %1748, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2094)
      %1750 = stablehlo.broadcast_in_dim %1749, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2095)
      %1751 = stablehlo.reshape %1750 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2096)
      %1752 = stablehlo.transpose %1751, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc2097)
      %1753 = stablehlo.dot_general %1728, %1752, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2098)
      %1754 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %1755 = stablehlo.multiply %1753, %1754 : tensor<1x16x128x128xbf16> loc(#loc2099)
      %1756 = stablehlo.add %1755, %128 : tensor<1x16x128x128xbf16> loc(#loc2100)
      %1757 = stablehlo.reshape %arg930 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc312)
      %1758 = "stablehlo.all_to_all"(%1757) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc312)
      %1759 = stablehlo.slice %1758 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc312)
      %1760 = stablehlo.reshape %1759 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc312)
      %1761 = stablehlo.reshape %1760 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2101)
      %1762 = stablehlo.reshape %1761 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc2102)
      %1763 = stablehlo.broadcast_in_dim %1762, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc2103)
      %1764 = stablehlo.concatenate %1756, %1763, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2104)
      %1765 = stablehlo.reshape %arg940 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2105)
      %1766 = stablehlo.reshape %1765 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2106)
      %1767 = stablehlo.convert %1766 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2107)
      %1768 = stablehlo.broadcast_in_dim %1767, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2108)
      %1769 = stablehlo.reduce(%1764 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2109)
      %1770 = stablehlo.broadcast_in_dim %1769, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2110)
      %1771 = stablehlo.subtract %1764, %1770 : tensor<1x16x128x129xbf16> loc(#loc2111)
      %1772 = stablehlo.reduce(%1771 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2112)
      %1773 = stablehlo.broadcast_in_dim %1772, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2113)
      %1774 = stablehlo.subtract %1771, %1773 : tensor<1x16x128x129xbf16> loc(#loc2114)
      %1775 = stablehlo.exponential %1774 : tensor<1x16x128x129xbf16> loc(#loc2115)
      %1776 = stablehlo.reduce(%1775 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2116)
      %1777 = stablehlo.broadcast_in_dim %1776, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2117)
      %1778 = stablehlo.divide %1775, %1777 : tensor<1x16x128x129xbf16> loc(#loc2118)
      %1779 = stablehlo.slice %1778 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2119)
      %1780 = stablehlo.reshape %arg787 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2120)
      %1781 = stablehlo.reshape %1780 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2121)
      %1782 = stablehlo.transpose %1781, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2122)
      %1783 = stablehlo.dot_general %1707, %1782, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2123)
      %1784 = "stablehlo.all_reduce"(%1783) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5021"), %arg1239: tensor<bf16> loc("dot.5021")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2123)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2123)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2123)
      %1785 = stablehlo.reshape %1784 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2124)
      %1786 = stablehlo.reshape %arg786 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2125)
      %1787 = stablehlo.reshape %1786 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2126)
      %1788 = stablehlo.broadcast_in_dim %1787, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2127)
      %1789 = stablehlo.add %1785, %1788 : tensor<1x128x128xbf16> loc(#loc2128)
      %1790 = stablehlo.reshape %1789 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2129)
      %1791 = stablehlo.transpose %1790, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2130)
      %1792 = stablehlo.broadcast_in_dim %1791, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2131)
      %1793 = stablehlo.reshape %1792 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2132)
      %1794 = stablehlo.dot_general %1779, %1793, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2133)
      %1795 = stablehlo.transpose %1794, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2134)
      %1796 = stablehlo.reshape %1795 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc2135)
      %1797 = stablehlo.reshape %arg785 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc2136)
      %1798 = stablehlo.reshape %1797 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc2137)
      %1799 = stablehlo.transpose %1798, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc2138)
      %1800 = stablehlo.dot_general %1796, %1799, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc2139)
      %1801 = "stablehlo.all_reduce"(%1800) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5214"), %arg1239: tensor<bf16> loc("dot.5214")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2139)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2139)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2139)
      %1802 = stablehlo.reshape %1801 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2140)
      %1803 = stablehlo.reshape %arg784 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2141)
      %1804 = stablehlo.reshape %1803 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2142)
      %1805 = stablehlo.broadcast_in_dim %1804, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2143)
      %1806 = stablehlo.add %1802, %1805 : tensor<1x128x360xbf16> loc(#loc2144)
      %1807 = stablehlo.add %1690, %1806 : tensor<1x128x360xbf16> loc(#loc2145)
      %1808 = stablehlo.reshape %arg935 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2146)
      %1809 = stablehlo.reshape %1808 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2147)
      %1810 = stablehlo.convert %1809 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2148)
      %1811 = stablehlo.broadcast_in_dim %1810, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2149)
      %1812 = stablehlo.convert %1807 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2150)
      %1813 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1814 = stablehlo.power %1812, %1813 : tensor<1x128x360xf32> loc(#loc2151)
      %1815 = stablehlo.reduce(%1814 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2152)
      %1816 = "stablehlo.all_reduce"(%1815) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.5232"), %arg1239: tensor<f32> loc("reduce.5232")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2152)
        stablehlo.return %7360 : tensor<f32> loc(#loc2152)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2152)
      %1817 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1818 = stablehlo.multiply %1816, %1817 : tensor<1x128xf32> loc(#loc2153)
      %1819 = stablehlo.reshape %1818 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2154)
      %1820 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1821 = stablehlo.add %1819, %1820 : tensor<1x128x1xf32> loc(#loc2155)
      %1822 = stablehlo.rsqrt %1821 : tensor<1x128x1xf32> loc(#loc2156)
      %1823 = stablehlo.reshape %1822 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2157)
      %1824 = stablehlo.broadcast_in_dim %1823, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2158)
      %1825 = stablehlo.multiply %1812, %1824 : tensor<1x128x360xf32> loc(#loc2159)
      %1826 = stablehlo.multiply %1811, %1825 : tensor<1x128x360xf32> loc(#loc2160)
      %1827 = stablehlo.convert %1826 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2161)
      %1828 = stablehlo.reshape %1827 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2162)
      %1829 = stablehlo.broadcast_in_dim %1828, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2163)
      %1830 = stablehlo.dot_general %1829, %arg939, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2164)
      %1831 = "stablehlo.all_reduce"(%1830) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5344"), %arg1239: tensor<bf16> loc("dot.5344")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2164)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2164)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2164)
      %1832 = stablehlo.reshape %arg938 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc2165)
      %1833 = stablehlo.reshape %1832 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc2166)
      %1834 = stablehlo.broadcast_in_dim %1833, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2167)
      %1835 = stablehlo.add %1831, %1834 : tensor<32x128x5760xbf16> loc(#loc2168)
      %1836 = stablehlo.slice %1835 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2169)
      %1837 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1838 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1839 = stablehlo.clamp %1838, %1836, %1837 : tensor<32x128x2880xbf16> loc(#loc2170)
      %1840 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1841 = stablehlo.add %1839, %1840 : tensor<32x128x2880xbf16> loc(#loc2171)
      %1842 = stablehlo.slice %1835 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2172)
      %1843 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1844 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1845 = stablehlo.clamp %1843, %1842, %1844 : tensor<32x128x2880xbf16> loc(#loc2173)
      %1846 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %1847 = stablehlo.multiply %1845, %1846 : tensor<32x128x2880xbf16> loc(#loc2174)
      %1848 = stablehlo.logistic %1847 : tensor<32x128x2880xbf16> loc(#loc2175)
      %1849 = stablehlo.multiply %1845, %1848 : tensor<32x128x2880xbf16> loc(#loc2176)
      %1850 = stablehlo.multiply %1841, %1849 : tensor<32x128x2880xbf16> loc(#loc2177)
      %1851 = stablehlo.dot_general %1850, %arg937, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2178)
      %1852 = stablehlo.reshape %arg936 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc2179)
      %1853 = stablehlo.reshape %1852 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc2180)
      %1854 = stablehlo.broadcast_in_dim %1853, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2181)
      %1855 = stablehlo.add %1851, %1854 : tensor<32x128x360xbf16> loc(#loc2182)
      %1856 = stablehlo.reshape %1855 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2183)
      %1857 = stablehlo.reshape %arg783 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2184)
      %1858 = stablehlo.reshape %1857 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2185)
      %1859 = stablehlo.transpose %1858, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2186)
      %1860 = stablehlo.dot_general %1828, %1859, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2187)
      %1861 = "stablehlo.all_reduce"(%1860) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5260"), %arg1239: tensor<bf16> loc("dot.5260")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2187)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2187)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2187)
      %1862 = stablehlo.reshape %arg782 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2188)
      %1863 = stablehlo.reshape %1862 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2189)
      %1864 = stablehlo.broadcast_in_dim %1863, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc2190)
      %1865 = stablehlo.add %1861, %1864 : tensor<128x128xbf16> loc(#loc2191)
      %1866:2 = "stablehlo.sort"(%1865, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.5280"), %arg1239: tensor<bf16> loc("sort.5280"), %arg1240: tensor<i32> loc("sort.5280"), %arg1241: tensor<i32> loc("sort.5280")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2193)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc2192)
      %1867 = stablehlo.slice %1866#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc2194)
      %1868 = stablehlo.convert %1867 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc2195)
      %1869 = stablehlo.reshape %1868 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc2196)
      %1870 = stablehlo.concatenate %231, %1869, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc2197)
      %1871 = stablehlo.slice %1866#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc2198)
      %1872 = stablehlo.reduce(%1871 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2199)
      %1873 = stablehlo.broadcast_in_dim %1872, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2200)
      %1874 = stablehlo.subtract %1871, %1873 : tensor<128x4xbf16> loc(#loc2201)
      %1875 = stablehlo.exponential %1874 : tensor<128x4xbf16> loc(#loc2202)
      %1876 = stablehlo.reduce(%1875 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2203)
      %1877 = stablehlo.broadcast_in_dim %1876, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2204)
      %1878 = stablehlo.divide %1875, %1877 : tensor<128x4xbf16> loc(#loc2205)
      %1879 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %1880 = "stablehlo.all_gather"(%1879) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %1881 = "stablehlo.scatter"(%1880, %1870, %1878) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.5314"), %arg1239: tensor<bf16> loc("scatter.5314")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc2206)
      %1882 = stablehlo.reshape %1881 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc2206)
      %1883 = "stablehlo.all_to_all"(%1882) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc2206)
      %1884 = stablehlo.slice %1883 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc2206)
      %1885 = stablehlo.reshape %1884 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc2206)
      %1886 = stablehlo.transpose %1885, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc2207)
      %1887 = stablehlo.reshape %1886 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc2208)
      %1888 = stablehlo.broadcast_in_dim %1887, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2209)
      %1889 = stablehlo.multiply %1856, %1888 : tensor<32x1x128x360xbf16> loc(#loc2210)
      %1890 = stablehlo.reduce(%1889 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc2211)
      %1891 = "stablehlo.all_reduce"(%1890) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.5386"), %arg1239: tensor<bf16> loc("reduce.5386")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2211)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2211)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2211)
      %1892 = stablehlo.add %1807, %1891 : tensor<1x128x360xbf16> loc(#loc2212)
      %1893 = stablehlo.convert %1892 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2213)
      %1894 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %1895 = stablehlo.power %1893, %1894 : tensor<1x128x360xf32> loc(#loc2214)
      %1896 = stablehlo.reduce(%1895 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2215)
      %1897 = "stablehlo.all_reduce"(%1896) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.5399"), %arg1239: tensor<f32> loc("reduce.5399")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2215)
        stablehlo.return %7360 : tensor<f32> loc(#loc2215)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2215)
      %1898 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %1899 = stablehlo.multiply %1897, %1898 : tensor<1x128xf32> loc(#loc2216)
      %1900 = stablehlo.reshape %1899 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2217)
      %1901 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %1902 = stablehlo.add %1900, %1901 : tensor<1x128x1xf32> loc(#loc2218)
      %1903 = stablehlo.rsqrt %1902 : tensor<1x128x1xf32> loc(#loc2219)
      %1904 = stablehlo.reshape %1903 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2220)
      %1905 = stablehlo.broadcast_in_dim %1904, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2221)
      %1906 = stablehlo.multiply %1893, %1905 : tensor<1x128x360xf32> loc(#loc2222)
      %1907 = stablehlo.multiply %1768, %1906 : tensor<1x128x360xf32> loc(#loc2223)
      %1908 = stablehlo.convert %1907 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2224)
      %1909 = stablehlo.reshape %1908 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2225)
      %1910 = stablehlo.reshape %arg945 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc2226)
      %1911 = stablehlo.reshape %1910 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc2227)
      %1912 = stablehlo.transpose %1911, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc2228)
      %1913 = stablehlo.dot_general %1909, %1912, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2229)
      %1914 = "stablehlo.all_reduce"(%1913) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5513"), %arg1239: tensor<bf16> loc("dot.5513")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2229)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2229)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2229)
      %1915 = stablehlo.reshape %1914 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2230)
      %1916 = stablehlo.reshape %arg944 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2231)
      %1917 = stablehlo.reshape %1916 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc2232)
      %1918 = stablehlo.broadcast_in_dim %1917, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2233)
      %1919 = stablehlo.add %1915, %1918 : tensor<1x128x1024xbf16> loc(#loc2234)
      %1920 = stablehlo.reshape %1919 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2235)
      %1921 = stablehlo.transpose %1920, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2236)
      %1922 = stablehlo.slice %1921 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2237)
      %1923 = stablehlo.multiply %1922, %64 : tensor<1x16x128x32xbf16> loc(#loc2238)
      %1924 = stablehlo.slice %1921 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2239)
      %1925 = stablehlo.multiply %1924, %70 : tensor<1x16x128x32xbf16> loc(#loc2240)
      %1926 = stablehlo.subtract %1923, %1925 : tensor<1x16x128x32xbf16> loc(#loc2241)
      %1927 = stablehlo.multiply %1924, %64 : tensor<1x16x128x32xbf16> loc(#loc2242)
      %1928 = stablehlo.multiply %1922, %70 : tensor<1x16x128x32xbf16> loc(#loc2243)
      %1929 = stablehlo.add %1927, %1928 : tensor<1x16x128x32xbf16> loc(#loc2244)
      %1930 = stablehlo.concatenate %1926, %1929, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2245)
      %1931 = stablehlo.reshape %arg943 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2246)
      %1932 = stablehlo.reshape %1931 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2247)
      %1933 = stablehlo.transpose %1932, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2248)
      %1934 = stablehlo.dot_general %1909, %1933, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2249)
      %1935 = "stablehlo.all_reduce"(%1934) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5464"), %arg1239: tensor<bf16> loc("dot.5464")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2249)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2249)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2249)
      %1936 = stablehlo.reshape %1935 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2250)
      %1937 = stablehlo.reshape %arg942 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2251)
      %1938 = stablehlo.reshape %1937 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2252)
      %1939 = stablehlo.broadcast_in_dim %1938, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2253)
      %1940 = stablehlo.add %1936, %1939 : tensor<1x128x128xbf16> loc(#loc2254)
      %1941 = stablehlo.reshape %1940 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2255)
      %1942 = stablehlo.transpose %1941, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2256)
      %1943 = stablehlo.slice %1942 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2257)
      %1944 = stablehlo.multiply %1943, %90 : tensor<1x2x128x32xbf16> loc(#loc2258)
      %1945 = stablehlo.slice %1942 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2259)
      %1946 = stablehlo.multiply %1945, %93 : tensor<1x2x128x32xbf16> loc(#loc2260)
      %1947 = stablehlo.subtract %1944, %1946 : tensor<1x2x128x32xbf16> loc(#loc2261)
      %1948 = stablehlo.multiply %1945, %90 : tensor<1x2x128x32xbf16> loc(#loc2262)
      %1949 = stablehlo.multiply %1943, %93 : tensor<1x2x128x32xbf16> loc(#loc2263)
      %1950 = stablehlo.add %1948, %1949 : tensor<1x2x128x32xbf16> loc(#loc2264)
      %1951 = stablehlo.concatenate %1947, %1950, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2265)
      %1952 = stablehlo.broadcast_in_dim %1951, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2266)
      %1953 = stablehlo.reshape %1952 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2267)
      %1954 = stablehlo.transpose %1953, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc2268)
      %1955 = stablehlo.dot_general %1930, %1954, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2269)
      %1956 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %1957 = stablehlo.multiply %1955, %1956 : tensor<1x16x128x128xbf16> loc(#loc2270)
      %1958 = stablehlo.add %1957, %341 : tensor<1x16x128x128xbf16> loc(#loc2271)
      %1959 = stablehlo.reshape %arg941 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc323)
      %1960 = "stablehlo.all_to_all"(%1959) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc323)
      %1961 = stablehlo.slice %1960 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc323)
      %1962 = stablehlo.reshape %1961 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc323)
      %1963 = stablehlo.reshape %1962 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2272)
      %1964 = stablehlo.reshape %1963 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc2273)
      %1965 = stablehlo.broadcast_in_dim %1964, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc2274)
      %1966 = stablehlo.concatenate %1958, %1965, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2275)
      %1967 = stablehlo.reshape %arg951 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2276)
      %1968 = stablehlo.reshape %1967 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2277)
      %1969 = stablehlo.convert %1968 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2278)
      %1970 = stablehlo.broadcast_in_dim %1969, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2279)
      %1971 = stablehlo.reduce(%1966 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2280)
      %1972 = stablehlo.broadcast_in_dim %1971, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2281)
      %1973 = stablehlo.subtract %1966, %1972 : tensor<1x16x128x129xbf16> loc(#loc2282)
      %1974 = stablehlo.reduce(%1973 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2283)
      %1975 = stablehlo.broadcast_in_dim %1974, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2284)
      %1976 = stablehlo.subtract %1973, %1975 : tensor<1x16x128x129xbf16> loc(#loc2285)
      %1977 = stablehlo.exponential %1976 : tensor<1x16x128x129xbf16> loc(#loc2286)
      %1978 = stablehlo.reduce(%1977 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2287)
      %1979 = stablehlo.broadcast_in_dim %1978, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2288)
      %1980 = stablehlo.divide %1977, %1979 : tensor<1x16x128x129xbf16> loc(#loc2289)
      %1981 = stablehlo.slice %1980 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2290)
      %1982 = stablehlo.reshape %arg781 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2291)
      %1983 = stablehlo.reshape %1982 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2292)
      %1984 = stablehlo.transpose %1983, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2293)
      %1985 = stablehlo.dot_general %1909, %1984, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2294)
      %1986 = "stablehlo.all_reduce"(%1985) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5427"), %arg1239: tensor<bf16> loc("dot.5427")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2294)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2294)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2294)
      %1987 = stablehlo.reshape %1986 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2295)
      %1988 = stablehlo.reshape %arg780 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2296)
      %1989 = stablehlo.reshape %1988 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2297)
      %1990 = stablehlo.broadcast_in_dim %1989, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2298)
      %1991 = stablehlo.add %1987, %1990 : tensor<1x128x128xbf16> loc(#loc2299)
      %1992 = stablehlo.reshape %1991 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2300)
      %1993 = stablehlo.transpose %1992, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2301)
      %1994 = stablehlo.broadcast_in_dim %1993, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2302)
      %1995 = stablehlo.reshape %1994 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2303)
      %1996 = stablehlo.dot_general %1981, %1995, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2304)
      %1997 = stablehlo.transpose %1996, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2305)
      %1998 = stablehlo.reshape %1997 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc2306)
      %1999 = stablehlo.reshape %arg779 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc2307)
      %2000 = stablehlo.reshape %1999 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc2308)
      %2001 = stablehlo.transpose %2000, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc2309)
      %2002 = stablehlo.dot_general %1998, %2001, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc2310)
      %2003 = "stablehlo.all_reduce"(%2002) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5620"), %arg1239: tensor<bf16> loc("dot.5620")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2310)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2310)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2310)
      %2004 = stablehlo.reshape %2003 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2311)
      %2005 = stablehlo.reshape %arg778 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2312)
      %2006 = stablehlo.reshape %2005 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2313)
      %2007 = stablehlo.broadcast_in_dim %2006, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2314)
      %2008 = stablehlo.add %2004, %2007 : tensor<1x128x360xbf16> loc(#loc2315)
      %2009 = stablehlo.add %1892, %2008 : tensor<1x128x360xbf16> loc(#loc2316)
      %2010 = stablehlo.reshape %arg946 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2317)
      %2011 = stablehlo.reshape %2010 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2318)
      %2012 = stablehlo.convert %2011 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2319)
      %2013 = stablehlo.broadcast_in_dim %2012, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2320)
      %2014 = stablehlo.convert %2009 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2321)
      %2015 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2016 = stablehlo.power %2014, %2015 : tensor<1x128x360xf32> loc(#loc2322)
      %2017 = stablehlo.reduce(%2016 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2323)
      %2018 = "stablehlo.all_reduce"(%2017) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.5638"), %arg1239: tensor<f32> loc("reduce.5638")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2323)
        stablehlo.return %7360 : tensor<f32> loc(#loc2323)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2323)
      %2019 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2020 = stablehlo.multiply %2018, %2019 : tensor<1x128xf32> loc(#loc2324)
      %2021 = stablehlo.reshape %2020 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2325)
      %2022 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2023 = stablehlo.add %2021, %2022 : tensor<1x128x1xf32> loc(#loc2326)
      %2024 = stablehlo.rsqrt %2023 : tensor<1x128x1xf32> loc(#loc2327)
      %2025 = stablehlo.reshape %2024 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2328)
      %2026 = stablehlo.broadcast_in_dim %2025, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2329)
      %2027 = stablehlo.multiply %2014, %2026 : tensor<1x128x360xf32> loc(#loc2330)
      %2028 = stablehlo.multiply %2013, %2027 : tensor<1x128x360xf32> loc(#loc2331)
      %2029 = stablehlo.convert %2028 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2332)
      %2030 = stablehlo.reshape %2029 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2333)
      %2031 = stablehlo.broadcast_in_dim %2030, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2334)
      %2032 = stablehlo.dot_general %2031, %arg950, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2335)
      %2033 = "stablehlo.all_reduce"(%2032) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5750"), %arg1239: tensor<bf16> loc("dot.5750")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2335)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2335)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2335)
      %2034 = stablehlo.reshape %arg949 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc2336)
      %2035 = stablehlo.reshape %2034 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc2337)
      %2036 = stablehlo.broadcast_in_dim %2035, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2338)
      %2037 = stablehlo.add %2033, %2036 : tensor<32x128x5760xbf16> loc(#loc2339)
      %2038 = stablehlo.slice %2037 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2340)
      %2039 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2040 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2041 = stablehlo.clamp %2040, %2038, %2039 : tensor<32x128x2880xbf16> loc(#loc2341)
      %2042 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2043 = stablehlo.add %2041, %2042 : tensor<32x128x2880xbf16> loc(#loc2342)
      %2044 = stablehlo.slice %2037 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2343)
      %2045 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2046 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2047 = stablehlo.clamp %2045, %2044, %2046 : tensor<32x128x2880xbf16> loc(#loc2344)
      %2048 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2049 = stablehlo.multiply %2047, %2048 : tensor<32x128x2880xbf16> loc(#loc2345)
      %2050 = stablehlo.logistic %2049 : tensor<32x128x2880xbf16> loc(#loc2346)
      %2051 = stablehlo.multiply %2047, %2050 : tensor<32x128x2880xbf16> loc(#loc2347)
      %2052 = stablehlo.multiply %2043, %2051 : tensor<32x128x2880xbf16> loc(#loc2348)
      %2053 = stablehlo.dot_general %2052, %arg948, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2349)
      %2054 = stablehlo.reshape %arg947 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc2350)
      %2055 = stablehlo.reshape %2054 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc2351)
      %2056 = stablehlo.broadcast_in_dim %2055, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2352)
      %2057 = stablehlo.add %2053, %2056 : tensor<32x128x360xbf16> loc(#loc2353)
      %2058 = stablehlo.reshape %2057 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2354)
      %2059 = stablehlo.reshape %arg777 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2355)
      %2060 = stablehlo.reshape %2059 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2356)
      %2061 = stablehlo.transpose %2060, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2357)
      %2062 = stablehlo.dot_general %2030, %2061, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2358)
      %2063 = "stablehlo.all_reduce"(%2062) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5666"), %arg1239: tensor<bf16> loc("dot.5666")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2358)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2358)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2358)
      %2064 = stablehlo.reshape %arg776 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2359)
      %2065 = stablehlo.reshape %2064 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2360)
      %2066 = stablehlo.broadcast_in_dim %2065, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc2361)
      %2067 = stablehlo.add %2063, %2066 : tensor<128x128xbf16> loc(#loc2362)
      %2068:2 = "stablehlo.sort"(%2067, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.5686"), %arg1239: tensor<bf16> loc("sort.5686"), %arg1240: tensor<i32> loc("sort.5686"), %arg1241: tensor<i32> loc("sort.5686")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2364)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc2363)
      %2069 = stablehlo.slice %2068#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc2365)
      %2070 = stablehlo.convert %2069 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc2366)
      %2071 = stablehlo.reshape %2070 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc2367)
      %2072 = stablehlo.concatenate %231, %2071, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc2368)
      %2073 = stablehlo.slice %2068#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc2369)
      %2074 = stablehlo.reduce(%2073 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2370)
      %2075 = stablehlo.broadcast_in_dim %2074, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2371)
      %2076 = stablehlo.subtract %2073, %2075 : tensor<128x4xbf16> loc(#loc2372)
      %2077 = stablehlo.exponential %2076 : tensor<128x4xbf16> loc(#loc2373)
      %2078 = stablehlo.reduce(%2077 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2374)
      %2079 = stablehlo.broadcast_in_dim %2078, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2375)
      %2080 = stablehlo.divide %2077, %2079 : tensor<128x4xbf16> loc(#loc2376)
      %2081 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %2082 = "stablehlo.all_gather"(%2081) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %2083 = "stablehlo.scatter"(%2082, %2072, %2080) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.5720"), %arg1239: tensor<bf16> loc("scatter.5720")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc2377)
      %2084 = stablehlo.reshape %2083 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc2377)
      %2085 = "stablehlo.all_to_all"(%2084) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc2377)
      %2086 = stablehlo.slice %2085 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc2377)
      %2087 = stablehlo.reshape %2086 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc2377)
      %2088 = stablehlo.transpose %2087, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc2378)
      %2089 = stablehlo.reshape %2088 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc2379)
      %2090 = stablehlo.broadcast_in_dim %2089, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2380)
      %2091 = stablehlo.multiply %2058, %2090 : tensor<32x1x128x360xbf16> loc(#loc2381)
      %2092 = stablehlo.reduce(%2091 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc2382)
      %2093 = "stablehlo.all_reduce"(%2092) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.5792"), %arg1239: tensor<bf16> loc("reduce.5792")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2382)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2382)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2382)
      %2094 = stablehlo.add %2009, %2093 : tensor<1x128x360xbf16> loc(#loc2383)
      %2095 = stablehlo.convert %2094 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2384)
      %2096 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2097 = stablehlo.power %2095, %2096 : tensor<1x128x360xf32> loc(#loc2385)
      %2098 = stablehlo.reduce(%2097 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2386)
      %2099 = "stablehlo.all_reduce"(%2098) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.5805"), %arg1239: tensor<f32> loc("reduce.5805")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2386)
        stablehlo.return %7360 : tensor<f32> loc(#loc2386)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2386)
      %2100 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2101 = stablehlo.multiply %2099, %2100 : tensor<1x128xf32> loc(#loc2387)
      %2102 = stablehlo.reshape %2101 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2388)
      %2103 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2104 = stablehlo.add %2102, %2103 : tensor<1x128x1xf32> loc(#loc2389)
      %2105 = stablehlo.rsqrt %2104 : tensor<1x128x1xf32> loc(#loc2390)
      %2106 = stablehlo.reshape %2105 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2391)
      %2107 = stablehlo.broadcast_in_dim %2106, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2392)
      %2108 = stablehlo.multiply %2095, %2107 : tensor<1x128x360xf32> loc(#loc2393)
      %2109 = stablehlo.multiply %1970, %2108 : tensor<1x128x360xf32> loc(#loc2394)
      %2110 = stablehlo.convert %2109 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2395)
      %2111 = stablehlo.reshape %2110 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2396)
      %2112 = stablehlo.reshape %arg956 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc2397)
      %2113 = stablehlo.reshape %2112 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc2398)
      %2114 = stablehlo.transpose %2113, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc2399)
      %2115 = stablehlo.dot_general %2111, %2114, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2400)
      %2116 = "stablehlo.all_reduce"(%2115) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5919"), %arg1239: tensor<bf16> loc("dot.5919")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2400)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2400)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2400)
      %2117 = stablehlo.reshape %2116 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2401)
      %2118 = stablehlo.reshape %arg955 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2402)
      %2119 = stablehlo.reshape %2118 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc2403)
      %2120 = stablehlo.broadcast_in_dim %2119, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2404)
      %2121 = stablehlo.add %2117, %2120 : tensor<1x128x1024xbf16> loc(#loc2405)
      %2122 = stablehlo.reshape %2121 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2406)
      %2123 = stablehlo.transpose %2122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2407)
      %2124 = stablehlo.slice %2123 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2408)
      %2125 = stablehlo.multiply %2124, %64 : tensor<1x16x128x32xbf16> loc(#loc2409)
      %2126 = stablehlo.slice %2123 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2410)
      %2127 = stablehlo.multiply %2126, %70 : tensor<1x16x128x32xbf16> loc(#loc2411)
      %2128 = stablehlo.subtract %2125, %2127 : tensor<1x16x128x32xbf16> loc(#loc2412)
      %2129 = stablehlo.multiply %2126, %64 : tensor<1x16x128x32xbf16> loc(#loc2413)
      %2130 = stablehlo.multiply %2124, %70 : tensor<1x16x128x32xbf16> loc(#loc2414)
      %2131 = stablehlo.add %2129, %2130 : tensor<1x16x128x32xbf16> loc(#loc2415)
      %2132 = stablehlo.concatenate %2128, %2131, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2416)
      %2133 = stablehlo.reshape %arg954 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2417)
      %2134 = stablehlo.reshape %2133 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2418)
      %2135 = stablehlo.transpose %2134, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2419)
      %2136 = stablehlo.dot_general %2111, %2135, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2420)
      %2137 = "stablehlo.all_reduce"(%2136) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5870"), %arg1239: tensor<bf16> loc("dot.5870")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2420)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2420)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2420)
      %2138 = stablehlo.reshape %2137 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2421)
      %2139 = stablehlo.reshape %arg953 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2422)
      %2140 = stablehlo.reshape %2139 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2423)
      %2141 = stablehlo.broadcast_in_dim %2140, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2424)
      %2142 = stablehlo.add %2138, %2141 : tensor<1x128x128xbf16> loc(#loc2425)
      %2143 = stablehlo.reshape %2142 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2426)
      %2144 = stablehlo.transpose %2143, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2427)
      %2145 = stablehlo.slice %2144 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2428)
      %2146 = stablehlo.multiply %2145, %90 : tensor<1x2x128x32xbf16> loc(#loc2429)
      %2147 = stablehlo.slice %2144 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2430)
      %2148 = stablehlo.multiply %2147, %93 : tensor<1x2x128x32xbf16> loc(#loc2431)
      %2149 = stablehlo.subtract %2146, %2148 : tensor<1x2x128x32xbf16> loc(#loc2432)
      %2150 = stablehlo.multiply %2147, %90 : tensor<1x2x128x32xbf16> loc(#loc2433)
      %2151 = stablehlo.multiply %2145, %93 : tensor<1x2x128x32xbf16> loc(#loc2434)
      %2152 = stablehlo.add %2150, %2151 : tensor<1x2x128x32xbf16> loc(#loc2435)
      %2153 = stablehlo.concatenate %2149, %2152, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2436)
      %2154 = stablehlo.broadcast_in_dim %2153, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2437)
      %2155 = stablehlo.reshape %2154 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2438)
      %2156 = stablehlo.transpose %2155, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc2439)
      %2157 = stablehlo.dot_general %2132, %2156, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2440)
      %2158 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %2159 = stablehlo.multiply %2157, %2158 : tensor<1x16x128x128xbf16> loc(#loc2441)
      %2160 = stablehlo.add %2159, %128 : tensor<1x16x128x128xbf16> loc(#loc2442)
      %2161 = stablehlo.reshape %arg952 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc334)
      %2162 = "stablehlo.all_to_all"(%2161) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc334)
      %2163 = stablehlo.slice %2162 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc334)
      %2164 = stablehlo.reshape %2163 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc334)
      %2165 = stablehlo.reshape %2164 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2443)
      %2166 = stablehlo.reshape %2165 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc2444)
      %2167 = stablehlo.broadcast_in_dim %2166, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc2445)
      %2168 = stablehlo.concatenate %2160, %2167, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2446)
      %2169 = stablehlo.reshape %arg962 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2447)
      %2170 = stablehlo.reshape %2169 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2448)
      %2171 = stablehlo.convert %2170 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2449)
      %2172 = stablehlo.broadcast_in_dim %2171, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2450)
      %2173 = stablehlo.reduce(%2168 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2451)
      %2174 = stablehlo.broadcast_in_dim %2173, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2452)
      %2175 = stablehlo.subtract %2168, %2174 : tensor<1x16x128x129xbf16> loc(#loc2453)
      %2176 = stablehlo.reduce(%2175 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2454)
      %2177 = stablehlo.broadcast_in_dim %2176, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2455)
      %2178 = stablehlo.subtract %2175, %2177 : tensor<1x16x128x129xbf16> loc(#loc2456)
      %2179 = stablehlo.exponential %2178 : tensor<1x16x128x129xbf16> loc(#loc2457)
      %2180 = stablehlo.reduce(%2179 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2458)
      %2181 = stablehlo.broadcast_in_dim %2180, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2459)
      %2182 = stablehlo.divide %2179, %2181 : tensor<1x16x128x129xbf16> loc(#loc2460)
      %2183 = stablehlo.slice %2182 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2461)
      %2184 = stablehlo.reshape %arg775 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2462)
      %2185 = stablehlo.reshape %2184 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2463)
      %2186 = stablehlo.transpose %2185, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2464)
      %2187 = stablehlo.dot_general %2111, %2186, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2465)
      %2188 = "stablehlo.all_reduce"(%2187) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.5833"), %arg1239: tensor<bf16> loc("dot.5833")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2465)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2465)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2465)
      %2189 = stablehlo.reshape %2188 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2466)
      %2190 = stablehlo.reshape %arg774 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2467)
      %2191 = stablehlo.reshape %2190 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2468)
      %2192 = stablehlo.broadcast_in_dim %2191, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2469)
      %2193 = stablehlo.add %2189, %2192 : tensor<1x128x128xbf16> loc(#loc2470)
      %2194 = stablehlo.reshape %2193 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2471)
      %2195 = stablehlo.transpose %2194, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2472)
      %2196 = stablehlo.broadcast_in_dim %2195, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2473)
      %2197 = stablehlo.reshape %2196 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2474)
      %2198 = stablehlo.dot_general %2183, %2197, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2475)
      %2199 = stablehlo.transpose %2198, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2476)
      %2200 = stablehlo.reshape %2199 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc2477)
      %2201 = stablehlo.reshape %arg773 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc2478)
      %2202 = stablehlo.reshape %2201 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc2479)
      %2203 = stablehlo.transpose %2202, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc2480)
      %2204 = stablehlo.dot_general %2200, %2203, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc2481)
      %2205 = "stablehlo.all_reduce"(%2204) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6026"), %arg1239: tensor<bf16> loc("dot.6026")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2481)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2481)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2481)
      %2206 = stablehlo.reshape %2205 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2482)
      %2207 = stablehlo.reshape %arg772 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2483)
      %2208 = stablehlo.reshape %2207 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2484)
      %2209 = stablehlo.broadcast_in_dim %2208, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2485)
      %2210 = stablehlo.add %2206, %2209 : tensor<1x128x360xbf16> loc(#loc2486)
      %2211 = stablehlo.add %2094, %2210 : tensor<1x128x360xbf16> loc(#loc2487)
      %2212 = stablehlo.reshape %arg957 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2488)
      %2213 = stablehlo.reshape %2212 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2489)
      %2214 = stablehlo.convert %2213 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2490)
      %2215 = stablehlo.broadcast_in_dim %2214, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2491)
      %2216 = stablehlo.convert %2211 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2492)
      %2217 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2218 = stablehlo.power %2216, %2217 : tensor<1x128x360xf32> loc(#loc2493)
      %2219 = stablehlo.reduce(%2218 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2494)
      %2220 = "stablehlo.all_reduce"(%2219) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.6044"), %arg1239: tensor<f32> loc("reduce.6044")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2494)
        stablehlo.return %7360 : tensor<f32> loc(#loc2494)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2494)
      %2221 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2222 = stablehlo.multiply %2220, %2221 : tensor<1x128xf32> loc(#loc2495)
      %2223 = stablehlo.reshape %2222 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2496)
      %2224 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2225 = stablehlo.add %2223, %2224 : tensor<1x128x1xf32> loc(#loc2497)
      %2226 = stablehlo.rsqrt %2225 : tensor<1x128x1xf32> loc(#loc2498)
      %2227 = stablehlo.reshape %2226 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2499)
      %2228 = stablehlo.broadcast_in_dim %2227, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2500)
      %2229 = stablehlo.multiply %2216, %2228 : tensor<1x128x360xf32> loc(#loc2501)
      %2230 = stablehlo.multiply %2215, %2229 : tensor<1x128x360xf32> loc(#loc2502)
      %2231 = stablehlo.convert %2230 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2503)
      %2232 = stablehlo.reshape %2231 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2504)
      %2233 = stablehlo.broadcast_in_dim %2232, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2505)
      %2234 = stablehlo.dot_general %2233, %arg961, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2506)
      %2235 = "stablehlo.all_reduce"(%2234) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6156"), %arg1239: tensor<bf16> loc("dot.6156")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2506)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2506)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2506)
      %2236 = stablehlo.reshape %arg960 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc2507)
      %2237 = stablehlo.reshape %2236 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc2508)
      %2238 = stablehlo.broadcast_in_dim %2237, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2509)
      %2239 = stablehlo.add %2235, %2238 : tensor<32x128x5760xbf16> loc(#loc2510)
      %2240 = stablehlo.slice %2239 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2511)
      %2241 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2242 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2243 = stablehlo.clamp %2242, %2240, %2241 : tensor<32x128x2880xbf16> loc(#loc2512)
      %2244 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2245 = stablehlo.add %2243, %2244 : tensor<32x128x2880xbf16> loc(#loc2513)
      %2246 = stablehlo.slice %2239 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2514)
      %2247 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2248 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2249 = stablehlo.clamp %2247, %2246, %2248 : tensor<32x128x2880xbf16> loc(#loc2515)
      %2250 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2251 = stablehlo.multiply %2249, %2250 : tensor<32x128x2880xbf16> loc(#loc2516)
      %2252 = stablehlo.logistic %2251 : tensor<32x128x2880xbf16> loc(#loc2517)
      %2253 = stablehlo.multiply %2249, %2252 : tensor<32x128x2880xbf16> loc(#loc2518)
      %2254 = stablehlo.multiply %2245, %2253 : tensor<32x128x2880xbf16> loc(#loc2519)
      %2255 = stablehlo.dot_general %2254, %arg959, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2520)
      %2256 = stablehlo.reshape %arg958 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc2521)
      %2257 = stablehlo.reshape %2256 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc2522)
      %2258 = stablehlo.broadcast_in_dim %2257, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2523)
      %2259 = stablehlo.add %2255, %2258 : tensor<32x128x360xbf16> loc(#loc2524)
      %2260 = stablehlo.reshape %2259 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2525)
      %2261 = stablehlo.reshape %arg771 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2526)
      %2262 = stablehlo.reshape %2261 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2527)
      %2263 = stablehlo.transpose %2262, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2528)
      %2264 = stablehlo.dot_general %2232, %2263, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2529)
      %2265 = "stablehlo.all_reduce"(%2264) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6072"), %arg1239: tensor<bf16> loc("dot.6072")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2529)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2529)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2529)
      %2266 = stablehlo.reshape %arg770 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2530)
      %2267 = stablehlo.reshape %2266 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2531)
      %2268 = stablehlo.broadcast_in_dim %2267, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc2532)
      %2269 = stablehlo.add %2265, %2268 : tensor<128x128xbf16> loc(#loc2533)
      %2270:2 = "stablehlo.sort"(%2269, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.6092"), %arg1239: tensor<bf16> loc("sort.6092"), %arg1240: tensor<i32> loc("sort.6092"), %arg1241: tensor<i32> loc("sort.6092")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2535)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc2534)
      %2271 = stablehlo.slice %2270#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc2536)
      %2272 = stablehlo.convert %2271 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc2537)
      %2273 = stablehlo.reshape %2272 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc2538)
      %2274 = stablehlo.concatenate %231, %2273, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc2539)
      %2275 = stablehlo.slice %2270#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc2540)
      %2276 = stablehlo.reduce(%2275 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2541)
      %2277 = stablehlo.broadcast_in_dim %2276, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2542)
      %2278 = stablehlo.subtract %2275, %2277 : tensor<128x4xbf16> loc(#loc2543)
      %2279 = stablehlo.exponential %2278 : tensor<128x4xbf16> loc(#loc2544)
      %2280 = stablehlo.reduce(%2279 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2545)
      %2281 = stablehlo.broadcast_in_dim %2280, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2546)
      %2282 = stablehlo.divide %2279, %2281 : tensor<128x4xbf16> loc(#loc2547)
      %2283 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %2284 = "stablehlo.all_gather"(%2283) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %2285 = "stablehlo.scatter"(%2284, %2274, %2282) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.6126"), %arg1239: tensor<bf16> loc("scatter.6126")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc2548)
      %2286 = stablehlo.reshape %2285 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc2548)
      %2287 = "stablehlo.all_to_all"(%2286) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc2548)
      %2288 = stablehlo.slice %2287 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc2548)
      %2289 = stablehlo.reshape %2288 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc2548)
      %2290 = stablehlo.transpose %2289, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc2549)
      %2291 = stablehlo.reshape %2290 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc2550)
      %2292 = stablehlo.broadcast_in_dim %2291, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2551)
      %2293 = stablehlo.multiply %2260, %2292 : tensor<32x1x128x360xbf16> loc(#loc2552)
      %2294 = stablehlo.reduce(%2293 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc2553)
      %2295 = "stablehlo.all_reduce"(%2294) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.6198"), %arg1239: tensor<bf16> loc("reduce.6198")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2553)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2553)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2553)
      %2296 = stablehlo.add %2211, %2295 : tensor<1x128x360xbf16> loc(#loc2554)
      %2297 = stablehlo.convert %2296 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2555)
      %2298 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2299 = stablehlo.power %2297, %2298 : tensor<1x128x360xf32> loc(#loc2556)
      %2300 = stablehlo.reduce(%2299 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2557)
      %2301 = "stablehlo.all_reduce"(%2300) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.6211"), %arg1239: tensor<f32> loc("reduce.6211")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2557)
        stablehlo.return %7360 : tensor<f32> loc(#loc2557)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2557)
      %2302 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2303 = stablehlo.multiply %2301, %2302 : tensor<1x128xf32> loc(#loc2558)
      %2304 = stablehlo.reshape %2303 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2559)
      %2305 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2306 = stablehlo.add %2304, %2305 : tensor<1x128x1xf32> loc(#loc2560)
      %2307 = stablehlo.rsqrt %2306 : tensor<1x128x1xf32> loc(#loc2561)
      %2308 = stablehlo.reshape %2307 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2562)
      %2309 = stablehlo.broadcast_in_dim %2308, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2563)
      %2310 = stablehlo.multiply %2297, %2309 : tensor<1x128x360xf32> loc(#loc2564)
      %2311 = stablehlo.multiply %2172, %2310 : tensor<1x128x360xf32> loc(#loc2565)
      %2312 = stablehlo.convert %2311 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2566)
      %2313 = stablehlo.reshape %2312 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2567)
      %2314 = stablehlo.reshape %arg967 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc2568)
      %2315 = stablehlo.reshape %2314 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc2569)
      %2316 = stablehlo.transpose %2315, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc2570)
      %2317 = stablehlo.dot_general %2313, %2316, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2571)
      %2318 = "stablehlo.all_reduce"(%2317) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6325"), %arg1239: tensor<bf16> loc("dot.6325")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2571)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2571)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2571)
      %2319 = stablehlo.reshape %2318 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2572)
      %2320 = stablehlo.reshape %arg966 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2573)
      %2321 = stablehlo.reshape %2320 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc2574)
      %2322 = stablehlo.broadcast_in_dim %2321, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2575)
      %2323 = stablehlo.add %2319, %2322 : tensor<1x128x1024xbf16> loc(#loc2576)
      %2324 = stablehlo.reshape %2323 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2577)
      %2325 = stablehlo.transpose %2324, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2578)
      %2326 = stablehlo.slice %2325 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2579)
      %2327 = stablehlo.multiply %2326, %64 : tensor<1x16x128x32xbf16> loc(#loc2580)
      %2328 = stablehlo.slice %2325 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2581)
      %2329 = stablehlo.multiply %2328, %70 : tensor<1x16x128x32xbf16> loc(#loc2582)
      %2330 = stablehlo.subtract %2327, %2329 : tensor<1x16x128x32xbf16> loc(#loc2583)
      %2331 = stablehlo.multiply %2328, %64 : tensor<1x16x128x32xbf16> loc(#loc2584)
      %2332 = stablehlo.multiply %2326, %70 : tensor<1x16x128x32xbf16> loc(#loc2585)
      %2333 = stablehlo.add %2331, %2332 : tensor<1x16x128x32xbf16> loc(#loc2586)
      %2334 = stablehlo.concatenate %2330, %2333, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2587)
      %2335 = stablehlo.reshape %arg965 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2588)
      %2336 = stablehlo.reshape %2335 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2589)
      %2337 = stablehlo.transpose %2336, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2590)
      %2338 = stablehlo.dot_general %2313, %2337, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2591)
      %2339 = "stablehlo.all_reduce"(%2338) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6276"), %arg1239: tensor<bf16> loc("dot.6276")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2591)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2591)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2591)
      %2340 = stablehlo.reshape %2339 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2592)
      %2341 = stablehlo.reshape %arg964 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2593)
      %2342 = stablehlo.reshape %2341 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2594)
      %2343 = stablehlo.broadcast_in_dim %2342, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2595)
      %2344 = stablehlo.add %2340, %2343 : tensor<1x128x128xbf16> loc(#loc2596)
      %2345 = stablehlo.reshape %2344 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2597)
      %2346 = stablehlo.transpose %2345, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2598)
      %2347 = stablehlo.slice %2346 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2599)
      %2348 = stablehlo.multiply %2347, %90 : tensor<1x2x128x32xbf16> loc(#loc2600)
      %2349 = stablehlo.slice %2346 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2601)
      %2350 = stablehlo.multiply %2349, %93 : tensor<1x2x128x32xbf16> loc(#loc2602)
      %2351 = stablehlo.subtract %2348, %2350 : tensor<1x2x128x32xbf16> loc(#loc2603)
      %2352 = stablehlo.multiply %2349, %90 : tensor<1x2x128x32xbf16> loc(#loc2604)
      %2353 = stablehlo.multiply %2347, %93 : tensor<1x2x128x32xbf16> loc(#loc2605)
      %2354 = stablehlo.add %2352, %2353 : tensor<1x2x128x32xbf16> loc(#loc2606)
      %2355 = stablehlo.concatenate %2351, %2354, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2607)
      %2356 = stablehlo.broadcast_in_dim %2355, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2608)
      %2357 = stablehlo.reshape %2356 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2609)
      %2358 = stablehlo.transpose %2357, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc2610)
      %2359 = stablehlo.dot_general %2334, %2358, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2611)
      %2360 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %2361 = stablehlo.multiply %2359, %2360 : tensor<1x16x128x128xbf16> loc(#loc2612)
      %2362 = stablehlo.add %2361, %341 : tensor<1x16x128x128xbf16> loc(#loc2613)
      %2363 = stablehlo.reshape %arg963 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc345)
      %2364 = "stablehlo.all_to_all"(%2363) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc345)
      %2365 = stablehlo.slice %2364 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc345)
      %2366 = stablehlo.reshape %2365 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc345)
      %2367 = stablehlo.reshape %2366 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2614)
      %2368 = stablehlo.reshape %2367 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc2615)
      %2369 = stablehlo.broadcast_in_dim %2368, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc2616)
      %2370 = stablehlo.concatenate %2362, %2369, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2617)
      %2371 = stablehlo.reshape %arg973 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2618)
      %2372 = stablehlo.reshape %2371 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2619)
      %2373 = stablehlo.convert %2372 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2620)
      %2374 = stablehlo.broadcast_in_dim %2373, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2621)
      %2375 = stablehlo.reduce(%2370 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2622)
      %2376 = stablehlo.broadcast_in_dim %2375, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2623)
      %2377 = stablehlo.subtract %2370, %2376 : tensor<1x16x128x129xbf16> loc(#loc2624)
      %2378 = stablehlo.reduce(%2377 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2625)
      %2379 = stablehlo.broadcast_in_dim %2378, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2626)
      %2380 = stablehlo.subtract %2377, %2379 : tensor<1x16x128x129xbf16> loc(#loc2627)
      %2381 = stablehlo.exponential %2380 : tensor<1x16x128x129xbf16> loc(#loc2628)
      %2382 = stablehlo.reduce(%2381 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2629)
      %2383 = stablehlo.broadcast_in_dim %2382, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2630)
      %2384 = stablehlo.divide %2381, %2383 : tensor<1x16x128x129xbf16> loc(#loc2631)
      %2385 = stablehlo.slice %2384 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2632)
      %2386 = stablehlo.reshape %arg769 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2633)
      %2387 = stablehlo.reshape %2386 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2634)
      %2388 = stablehlo.transpose %2387, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2635)
      %2389 = stablehlo.dot_general %2313, %2388, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2636)
      %2390 = "stablehlo.all_reduce"(%2389) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6239"), %arg1239: tensor<bf16> loc("dot.6239")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2636)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2636)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2636)
      %2391 = stablehlo.reshape %2390 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2637)
      %2392 = stablehlo.reshape %arg768 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2638)
      %2393 = stablehlo.reshape %2392 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2639)
      %2394 = stablehlo.broadcast_in_dim %2393, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2640)
      %2395 = stablehlo.add %2391, %2394 : tensor<1x128x128xbf16> loc(#loc2641)
      %2396 = stablehlo.reshape %2395 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2642)
      %2397 = stablehlo.transpose %2396, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2643)
      %2398 = stablehlo.broadcast_in_dim %2397, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2644)
      %2399 = stablehlo.reshape %2398 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2645)
      %2400 = stablehlo.dot_general %2385, %2399, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2646)
      %2401 = stablehlo.transpose %2400, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2647)
      %2402 = stablehlo.reshape %2401 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc2648)
      %2403 = stablehlo.reshape %arg767 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc2649)
      %2404 = stablehlo.reshape %2403 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc2650)
      %2405 = stablehlo.transpose %2404, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc2651)
      %2406 = stablehlo.dot_general %2402, %2405, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc2652)
      %2407 = "stablehlo.all_reduce"(%2406) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6432"), %arg1239: tensor<bf16> loc("dot.6432")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2652)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2652)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2652)
      %2408 = stablehlo.reshape %2407 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2653)
      %2409 = stablehlo.reshape %arg766 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2654)
      %2410 = stablehlo.reshape %2409 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2655)
      %2411 = stablehlo.broadcast_in_dim %2410, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2656)
      %2412 = stablehlo.add %2408, %2411 : tensor<1x128x360xbf16> loc(#loc2657)
      %2413 = stablehlo.add %2296, %2412 : tensor<1x128x360xbf16> loc(#loc2658)
      %2414 = stablehlo.reshape %arg968 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2659)
      %2415 = stablehlo.reshape %2414 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2660)
      %2416 = stablehlo.convert %2415 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2661)
      %2417 = stablehlo.broadcast_in_dim %2416, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2662)
      %2418 = stablehlo.convert %2413 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2663)
      %2419 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2420 = stablehlo.power %2418, %2419 : tensor<1x128x360xf32> loc(#loc2664)
      %2421 = stablehlo.reduce(%2420 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2665)
      %2422 = "stablehlo.all_reduce"(%2421) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.6450"), %arg1239: tensor<f32> loc("reduce.6450")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2665)
        stablehlo.return %7360 : tensor<f32> loc(#loc2665)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2665)
      %2423 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2424 = stablehlo.multiply %2422, %2423 : tensor<1x128xf32> loc(#loc2666)
      %2425 = stablehlo.reshape %2424 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2667)
      %2426 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2427 = stablehlo.add %2425, %2426 : tensor<1x128x1xf32> loc(#loc2668)
      %2428 = stablehlo.rsqrt %2427 : tensor<1x128x1xf32> loc(#loc2669)
      %2429 = stablehlo.reshape %2428 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2670)
      %2430 = stablehlo.broadcast_in_dim %2429, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2671)
      %2431 = stablehlo.multiply %2418, %2430 : tensor<1x128x360xf32> loc(#loc2672)
      %2432 = stablehlo.multiply %2417, %2431 : tensor<1x128x360xf32> loc(#loc2673)
      %2433 = stablehlo.convert %2432 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2674)
      %2434 = stablehlo.reshape %2433 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2675)
      %2435 = stablehlo.broadcast_in_dim %2434, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2676)
      %2436 = stablehlo.dot_general %2435, %arg972, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2677)
      %2437 = "stablehlo.all_reduce"(%2436) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6562"), %arg1239: tensor<bf16> loc("dot.6562")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2677)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2677)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2677)
      %2438 = stablehlo.reshape %arg971 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc2678)
      %2439 = stablehlo.reshape %2438 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc2679)
      %2440 = stablehlo.broadcast_in_dim %2439, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2680)
      %2441 = stablehlo.add %2437, %2440 : tensor<32x128x5760xbf16> loc(#loc2681)
      %2442 = stablehlo.slice %2441 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2682)
      %2443 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2444 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2445 = stablehlo.clamp %2444, %2442, %2443 : tensor<32x128x2880xbf16> loc(#loc2683)
      %2446 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2447 = stablehlo.add %2445, %2446 : tensor<32x128x2880xbf16> loc(#loc2684)
      %2448 = stablehlo.slice %2441 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2685)
      %2449 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2450 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2451 = stablehlo.clamp %2449, %2448, %2450 : tensor<32x128x2880xbf16> loc(#loc2686)
      %2452 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2453 = stablehlo.multiply %2451, %2452 : tensor<32x128x2880xbf16> loc(#loc2687)
      %2454 = stablehlo.logistic %2453 : tensor<32x128x2880xbf16> loc(#loc2688)
      %2455 = stablehlo.multiply %2451, %2454 : tensor<32x128x2880xbf16> loc(#loc2689)
      %2456 = stablehlo.multiply %2447, %2455 : tensor<32x128x2880xbf16> loc(#loc2690)
      %2457 = stablehlo.dot_general %2456, %arg970, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2691)
      %2458 = stablehlo.reshape %arg969 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc2692)
      %2459 = stablehlo.reshape %2458 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc2693)
      %2460 = stablehlo.broadcast_in_dim %2459, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2694)
      %2461 = stablehlo.add %2457, %2460 : tensor<32x128x360xbf16> loc(#loc2695)
      %2462 = stablehlo.reshape %2461 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2696)
      %2463 = stablehlo.reshape %arg765 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2697)
      %2464 = stablehlo.reshape %2463 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2698)
      %2465 = stablehlo.transpose %2464, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2699)
      %2466 = stablehlo.dot_general %2434, %2465, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2700)
      %2467 = "stablehlo.all_reduce"(%2466) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6478"), %arg1239: tensor<bf16> loc("dot.6478")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2700)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2700)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2700)
      %2468 = stablehlo.reshape %arg764 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2701)
      %2469 = stablehlo.reshape %2468 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2702)
      %2470 = stablehlo.broadcast_in_dim %2469, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc2703)
      %2471 = stablehlo.add %2467, %2470 : tensor<128x128xbf16> loc(#loc2704)
      %2472:2 = "stablehlo.sort"(%2471, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.6498"), %arg1239: tensor<bf16> loc("sort.6498"), %arg1240: tensor<i32> loc("sort.6498"), %arg1241: tensor<i32> loc("sort.6498")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2706)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc2705)
      %2473 = stablehlo.slice %2472#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc2707)
      %2474 = stablehlo.convert %2473 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc2708)
      %2475 = stablehlo.reshape %2474 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc2709)
      %2476 = stablehlo.concatenate %231, %2475, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc2710)
      %2477 = stablehlo.slice %2472#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc2711)
      %2478 = stablehlo.reduce(%2477 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2712)
      %2479 = stablehlo.broadcast_in_dim %2478, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2713)
      %2480 = stablehlo.subtract %2477, %2479 : tensor<128x4xbf16> loc(#loc2714)
      %2481 = stablehlo.exponential %2480 : tensor<128x4xbf16> loc(#loc2715)
      %2482 = stablehlo.reduce(%2481 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2716)
      %2483 = stablehlo.broadcast_in_dim %2482, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2717)
      %2484 = stablehlo.divide %2481, %2483 : tensor<128x4xbf16> loc(#loc2718)
      %2485 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %2486 = "stablehlo.all_gather"(%2485) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %2487 = "stablehlo.scatter"(%2486, %2476, %2484) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.6532"), %arg1239: tensor<bf16> loc("scatter.6532")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc2719)
      %2488 = stablehlo.reshape %2487 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc2719)
      %2489 = "stablehlo.all_to_all"(%2488) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc2719)
      %2490 = stablehlo.slice %2489 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc2719)
      %2491 = stablehlo.reshape %2490 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc2719)
      %2492 = stablehlo.transpose %2491, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc2720)
      %2493 = stablehlo.reshape %2492 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc2721)
      %2494 = stablehlo.broadcast_in_dim %2493, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2722)
      %2495 = stablehlo.multiply %2462, %2494 : tensor<32x1x128x360xbf16> loc(#loc2723)
      %2496 = stablehlo.reduce(%2495 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc2724)
      %2497 = "stablehlo.all_reduce"(%2496) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.6604"), %arg1239: tensor<bf16> loc("reduce.6604")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2724)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2724)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2724)
      %2498 = stablehlo.add %2413, %2497 : tensor<1x128x360xbf16> loc(#loc2725)
      %2499 = stablehlo.convert %2498 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2726)
      %2500 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2501 = stablehlo.power %2499, %2500 : tensor<1x128x360xf32> loc(#loc2727)
      %2502 = stablehlo.reduce(%2501 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2728)
      %2503 = "stablehlo.all_reduce"(%2502) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.6617"), %arg1239: tensor<f32> loc("reduce.6617")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2728)
        stablehlo.return %7360 : tensor<f32> loc(#loc2728)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2728)
      %2504 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2505 = stablehlo.multiply %2503, %2504 : tensor<1x128xf32> loc(#loc2729)
      %2506 = stablehlo.reshape %2505 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2730)
      %2507 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2508 = stablehlo.add %2506, %2507 : tensor<1x128x1xf32> loc(#loc2731)
      %2509 = stablehlo.rsqrt %2508 : tensor<1x128x1xf32> loc(#loc2732)
      %2510 = stablehlo.reshape %2509 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2733)
      %2511 = stablehlo.broadcast_in_dim %2510, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2734)
      %2512 = stablehlo.multiply %2499, %2511 : tensor<1x128x360xf32> loc(#loc2735)
      %2513 = stablehlo.multiply %2374, %2512 : tensor<1x128x360xf32> loc(#loc2736)
      %2514 = stablehlo.convert %2513 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2737)
      %2515 = stablehlo.reshape %2514 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2738)
      %2516 = stablehlo.reshape %arg978 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc2739)
      %2517 = stablehlo.reshape %2516 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc2740)
      %2518 = stablehlo.transpose %2517, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc2741)
      %2519 = stablehlo.dot_general %2515, %2518, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2742)
      %2520 = "stablehlo.all_reduce"(%2519) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6731"), %arg1239: tensor<bf16> loc("dot.6731")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2742)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2742)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2742)
      %2521 = stablehlo.reshape %2520 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2743)
      %2522 = stablehlo.reshape %arg977 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2744)
      %2523 = stablehlo.reshape %2522 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc2745)
      %2524 = stablehlo.broadcast_in_dim %2523, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2746)
      %2525 = stablehlo.add %2521, %2524 : tensor<1x128x1024xbf16> loc(#loc2747)
      %2526 = stablehlo.reshape %2525 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2748)
      %2527 = stablehlo.transpose %2526, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2749)
      %2528 = stablehlo.slice %2527 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2750)
      %2529 = stablehlo.multiply %2528, %64 : tensor<1x16x128x32xbf16> loc(#loc2751)
      %2530 = stablehlo.slice %2527 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2752)
      %2531 = stablehlo.multiply %2530, %70 : tensor<1x16x128x32xbf16> loc(#loc2753)
      %2532 = stablehlo.subtract %2529, %2531 : tensor<1x16x128x32xbf16> loc(#loc2754)
      %2533 = stablehlo.multiply %2530, %64 : tensor<1x16x128x32xbf16> loc(#loc2755)
      %2534 = stablehlo.multiply %2528, %70 : tensor<1x16x128x32xbf16> loc(#loc2756)
      %2535 = stablehlo.add %2533, %2534 : tensor<1x16x128x32xbf16> loc(#loc2757)
      %2536 = stablehlo.concatenate %2532, %2535, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2758)
      %2537 = stablehlo.reshape %arg976 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2759)
      %2538 = stablehlo.reshape %2537 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2760)
      %2539 = stablehlo.transpose %2538, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2761)
      %2540 = stablehlo.dot_general %2515, %2539, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2762)
      %2541 = "stablehlo.all_reduce"(%2540) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6682"), %arg1239: tensor<bf16> loc("dot.6682")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2762)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2762)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2762)
      %2542 = stablehlo.reshape %2541 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2763)
      %2543 = stablehlo.reshape %arg975 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2764)
      %2544 = stablehlo.reshape %2543 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2765)
      %2545 = stablehlo.broadcast_in_dim %2544, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2766)
      %2546 = stablehlo.add %2542, %2545 : tensor<1x128x128xbf16> loc(#loc2767)
      %2547 = stablehlo.reshape %2546 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2768)
      %2548 = stablehlo.transpose %2547, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2769)
      %2549 = stablehlo.slice %2548 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2770)
      %2550 = stablehlo.multiply %2549, %90 : tensor<1x2x128x32xbf16> loc(#loc2771)
      %2551 = stablehlo.slice %2548 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2772)
      %2552 = stablehlo.multiply %2551, %93 : tensor<1x2x128x32xbf16> loc(#loc2773)
      %2553 = stablehlo.subtract %2550, %2552 : tensor<1x2x128x32xbf16> loc(#loc2774)
      %2554 = stablehlo.multiply %2551, %90 : tensor<1x2x128x32xbf16> loc(#loc2775)
      %2555 = stablehlo.multiply %2549, %93 : tensor<1x2x128x32xbf16> loc(#loc2776)
      %2556 = stablehlo.add %2554, %2555 : tensor<1x2x128x32xbf16> loc(#loc2777)
      %2557 = stablehlo.concatenate %2553, %2556, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2778)
      %2558 = stablehlo.broadcast_in_dim %2557, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2779)
      %2559 = stablehlo.reshape %2558 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2780)
      %2560 = stablehlo.transpose %2559, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc2781)
      %2561 = stablehlo.dot_general %2536, %2560, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2782)
      %2562 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %2563 = stablehlo.multiply %2561, %2562 : tensor<1x16x128x128xbf16> loc(#loc2783)
      %2564 = stablehlo.add %2563, %128 : tensor<1x16x128x128xbf16> loc(#loc2784)
      %2565 = stablehlo.reshape %arg974 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc356)
      %2566 = "stablehlo.all_to_all"(%2565) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc356)
      %2567 = stablehlo.slice %2566 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc356)
      %2568 = stablehlo.reshape %2567 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc356)
      %2569 = stablehlo.reshape %2568 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2785)
      %2570 = stablehlo.reshape %2569 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc2786)
      %2571 = stablehlo.broadcast_in_dim %2570, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc2787)
      %2572 = stablehlo.concatenate %2564, %2571, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2788)
      %2573 = stablehlo.reshape %arg984 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2789)
      %2574 = stablehlo.reshape %2573 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2790)
      %2575 = stablehlo.convert %2574 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2791)
      %2576 = stablehlo.broadcast_in_dim %2575, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2792)
      %2577 = stablehlo.reduce(%2572 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2793)
      %2578 = stablehlo.broadcast_in_dim %2577, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2794)
      %2579 = stablehlo.subtract %2572, %2578 : tensor<1x16x128x129xbf16> loc(#loc2795)
      %2580 = stablehlo.reduce(%2579 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2796)
      %2581 = stablehlo.broadcast_in_dim %2580, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2797)
      %2582 = stablehlo.subtract %2579, %2581 : tensor<1x16x128x129xbf16> loc(#loc2798)
      %2583 = stablehlo.exponential %2582 : tensor<1x16x128x129xbf16> loc(#loc2799)
      %2584 = stablehlo.reduce(%2583 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2800)
      %2585 = stablehlo.broadcast_in_dim %2584, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2801)
      %2586 = stablehlo.divide %2583, %2585 : tensor<1x16x128x129xbf16> loc(#loc2802)
      %2587 = stablehlo.slice %2586 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2803)
      %2588 = stablehlo.reshape %arg763 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2804)
      %2589 = stablehlo.reshape %2588 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2805)
      %2590 = stablehlo.transpose %2589, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2806)
      %2591 = stablehlo.dot_general %2515, %2590, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2807)
      %2592 = "stablehlo.all_reduce"(%2591) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6645"), %arg1239: tensor<bf16> loc("dot.6645")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2807)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2807)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2807)
      %2593 = stablehlo.reshape %2592 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2808)
      %2594 = stablehlo.reshape %arg762 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2809)
      %2595 = stablehlo.reshape %2594 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2810)
      %2596 = stablehlo.broadcast_in_dim %2595, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2811)
      %2597 = stablehlo.add %2593, %2596 : tensor<1x128x128xbf16> loc(#loc2812)
      %2598 = stablehlo.reshape %2597 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2813)
      %2599 = stablehlo.transpose %2598, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2814)
      %2600 = stablehlo.broadcast_in_dim %2599, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2815)
      %2601 = stablehlo.reshape %2600 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2816)
      %2602 = stablehlo.dot_general %2587, %2601, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2817)
      %2603 = stablehlo.transpose %2602, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2818)
      %2604 = stablehlo.reshape %2603 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc2819)
      %2605 = stablehlo.reshape %arg761 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc2820)
      %2606 = stablehlo.reshape %2605 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc2821)
      %2607 = stablehlo.transpose %2606, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc2822)
      %2608 = stablehlo.dot_general %2604, %2607, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc2823)
      %2609 = "stablehlo.all_reduce"(%2608) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6838"), %arg1239: tensor<bf16> loc("dot.6838")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2823)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2823)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2823)
      %2610 = stablehlo.reshape %2609 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2824)
      %2611 = stablehlo.reshape %arg760 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2825)
      %2612 = stablehlo.reshape %2611 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2826)
      %2613 = stablehlo.broadcast_in_dim %2612, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2827)
      %2614 = stablehlo.add %2610, %2613 : tensor<1x128x360xbf16> loc(#loc2828)
      %2615 = stablehlo.add %2498, %2614 : tensor<1x128x360xbf16> loc(#loc2829)
      %2616 = stablehlo.reshape %arg979 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2830)
      %2617 = stablehlo.reshape %2616 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2831)
      %2618 = stablehlo.convert %2617 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2832)
      %2619 = stablehlo.broadcast_in_dim %2618, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2833)
      %2620 = stablehlo.convert %2615 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2834)
      %2621 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2622 = stablehlo.power %2620, %2621 : tensor<1x128x360xf32> loc(#loc2835)
      %2623 = stablehlo.reduce(%2622 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2836)
      %2624 = "stablehlo.all_reduce"(%2623) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.6856"), %arg1239: tensor<f32> loc("reduce.6856")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2836)
        stablehlo.return %7360 : tensor<f32> loc(#loc2836)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2836)
      %2625 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2626 = stablehlo.multiply %2624, %2625 : tensor<1x128xf32> loc(#loc2837)
      %2627 = stablehlo.reshape %2626 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2838)
      %2628 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2629 = stablehlo.add %2627, %2628 : tensor<1x128x1xf32> loc(#loc2839)
      %2630 = stablehlo.rsqrt %2629 : tensor<1x128x1xf32> loc(#loc2840)
      %2631 = stablehlo.reshape %2630 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2841)
      %2632 = stablehlo.broadcast_in_dim %2631, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2842)
      %2633 = stablehlo.multiply %2620, %2632 : tensor<1x128x360xf32> loc(#loc2843)
      %2634 = stablehlo.multiply %2619, %2633 : tensor<1x128x360xf32> loc(#loc2844)
      %2635 = stablehlo.convert %2634 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2845)
      %2636 = stablehlo.reshape %2635 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2846)
      %2637 = stablehlo.broadcast_in_dim %2636, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2847)
      %2638 = stablehlo.dot_general %2637, %arg983, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2848)
      %2639 = "stablehlo.all_reduce"(%2638) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6968"), %arg1239: tensor<bf16> loc("dot.6968")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2848)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2848)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2848)
      %2640 = stablehlo.reshape %arg982 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc2849)
      %2641 = stablehlo.reshape %2640 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc2850)
      %2642 = stablehlo.broadcast_in_dim %2641, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc2851)
      %2643 = stablehlo.add %2639, %2642 : tensor<32x128x5760xbf16> loc(#loc2852)
      %2644 = stablehlo.slice %2643 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2853)
      %2645 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2646 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2647 = stablehlo.clamp %2646, %2644, %2645 : tensor<32x128x2880xbf16> loc(#loc2854)
      %2648 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2649 = stablehlo.add %2647, %2648 : tensor<32x128x2880xbf16> loc(#loc2855)
      %2650 = stablehlo.slice %2643 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc2856)
      %2651 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2652 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2653 = stablehlo.clamp %2651, %2650, %2652 : tensor<32x128x2880xbf16> loc(#loc2857)
      %2654 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2655 = stablehlo.multiply %2653, %2654 : tensor<32x128x2880xbf16> loc(#loc2858)
      %2656 = stablehlo.logistic %2655 : tensor<32x128x2880xbf16> loc(#loc2859)
      %2657 = stablehlo.multiply %2653, %2656 : tensor<32x128x2880xbf16> loc(#loc2860)
      %2658 = stablehlo.multiply %2649, %2657 : tensor<32x128x2880xbf16> loc(#loc2861)
      %2659 = stablehlo.dot_general %2658, %arg981, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2862)
      %2660 = stablehlo.reshape %arg980 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc2863)
      %2661 = stablehlo.reshape %2660 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc2864)
      %2662 = stablehlo.broadcast_in_dim %2661, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc2865)
      %2663 = stablehlo.add %2659, %2662 : tensor<32x128x360xbf16> loc(#loc2866)
      %2664 = stablehlo.reshape %2663 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2867)
      %2665 = stablehlo.reshape %arg759 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2868)
      %2666 = stablehlo.reshape %2665 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2869)
      %2667 = stablehlo.transpose %2666, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2870)
      %2668 = stablehlo.dot_general %2636, %2667, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2871)
      %2669 = "stablehlo.all_reduce"(%2668) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.6884"), %arg1239: tensor<bf16> loc("dot.6884")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2871)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2871)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2871)
      %2670 = stablehlo.reshape %arg758 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2872)
      %2671 = stablehlo.reshape %2670 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2873)
      %2672 = stablehlo.broadcast_in_dim %2671, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc2874)
      %2673 = stablehlo.add %2669, %2672 : tensor<128x128xbf16> loc(#loc2875)
      %2674:2 = "stablehlo.sort"(%2673, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.6904"), %arg1239: tensor<bf16> loc("sort.6904"), %arg1240: tensor<i32> loc("sort.6904"), %arg1241: tensor<i32> loc("sort.6904")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc2877)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc2876)
      %2675 = stablehlo.slice %2674#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc2878)
      %2676 = stablehlo.convert %2675 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc2879)
      %2677 = stablehlo.reshape %2676 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc2880)
      %2678 = stablehlo.concatenate %231, %2677, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc2881)
      %2679 = stablehlo.slice %2674#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc2882)
      %2680 = stablehlo.reduce(%2679 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2883)
      %2681 = stablehlo.broadcast_in_dim %2680, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2884)
      %2682 = stablehlo.subtract %2679, %2681 : tensor<128x4xbf16> loc(#loc2885)
      %2683 = stablehlo.exponential %2682 : tensor<128x4xbf16> loc(#loc2886)
      %2684 = stablehlo.reduce(%2683 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc2887)
      %2685 = stablehlo.broadcast_in_dim %2684, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc2888)
      %2686 = stablehlo.divide %2683, %2685 : tensor<128x4xbf16> loc(#loc2889)
      %2687 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %2688 = "stablehlo.all_gather"(%2687) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %2689 = "stablehlo.scatter"(%2688, %2678, %2686) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.6938"), %arg1239: tensor<bf16> loc("scatter.6938")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc2890)
      %2690 = stablehlo.reshape %2689 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc2890)
      %2691 = "stablehlo.all_to_all"(%2690) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc2890)
      %2692 = stablehlo.slice %2691 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc2890)
      %2693 = stablehlo.reshape %2692 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc2890)
      %2694 = stablehlo.transpose %2693, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc2891)
      %2695 = stablehlo.reshape %2694 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc2892)
      %2696 = stablehlo.broadcast_in_dim %2695, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc2893)
      %2697 = stablehlo.multiply %2664, %2696 : tensor<32x1x128x360xbf16> loc(#loc2894)
      %2698 = stablehlo.reduce(%2697 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc2895)
      %2699 = "stablehlo.all_reduce"(%2698) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.7010"), %arg1239: tensor<bf16> loc("reduce.7010")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2895)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2895)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2895)
      %2700 = stablehlo.add %2615, %2699 : tensor<1x128x360xbf16> loc(#loc2896)
      %2701 = stablehlo.convert %2700 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc2897)
      %2702 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2703 = stablehlo.power %2701, %2702 : tensor<1x128x360xf32> loc(#loc2898)
      %2704 = stablehlo.reduce(%2703 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc2899)
      %2705 = "stablehlo.all_reduce"(%2704) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.7023"), %arg1239: tensor<f32> loc("reduce.7023")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc2899)
        stablehlo.return %7360 : tensor<f32> loc(#loc2899)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc2899)
      %2706 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2707 = stablehlo.multiply %2705, %2706 : tensor<1x128xf32> loc(#loc2900)
      %2708 = stablehlo.reshape %2707 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc2901)
      %2709 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2710 = stablehlo.add %2708, %2709 : tensor<1x128x1xf32> loc(#loc2902)
      %2711 = stablehlo.rsqrt %2710 : tensor<1x128x1xf32> loc(#loc2903)
      %2712 = stablehlo.reshape %2711 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc2904)
      %2713 = stablehlo.broadcast_in_dim %2712, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc2905)
      %2714 = stablehlo.multiply %2701, %2713 : tensor<1x128x360xf32> loc(#loc2906)
      %2715 = stablehlo.multiply %2576, %2714 : tensor<1x128x360xf32> loc(#loc2907)
      %2716 = stablehlo.convert %2715 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc2908)
      %2717 = stablehlo.reshape %2716 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2909)
      %2718 = stablehlo.reshape %arg989 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc2910)
      %2719 = stablehlo.reshape %2718 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc2911)
      %2720 = stablehlo.transpose %2719, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc2912)
      %2721 = stablehlo.dot_general %2717, %2720, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2913)
      %2722 = "stablehlo.all_reduce"(%2721) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7137"), %arg1239: tensor<bf16> loc("dot.7137")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2913)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2913)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc2913)
      %2723 = stablehlo.reshape %2722 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2914)
      %2724 = stablehlo.reshape %arg988 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc2915)
      %2725 = stablehlo.reshape %2724 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc2916)
      %2726 = stablehlo.broadcast_in_dim %2725, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc2917)
      %2727 = stablehlo.add %2723, %2726 : tensor<1x128x1024xbf16> loc(#loc2918)
      %2728 = stablehlo.reshape %2727 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2919)
      %2729 = stablehlo.transpose %2728, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2920)
      %2730 = stablehlo.slice %2729 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2921)
      %2731 = stablehlo.multiply %2730, %64 : tensor<1x16x128x32xbf16> loc(#loc2922)
      %2732 = stablehlo.slice %2729 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc2923)
      %2733 = stablehlo.multiply %2732, %70 : tensor<1x16x128x32xbf16> loc(#loc2924)
      %2734 = stablehlo.subtract %2731, %2733 : tensor<1x16x128x32xbf16> loc(#loc2925)
      %2735 = stablehlo.multiply %2732, %64 : tensor<1x16x128x32xbf16> loc(#loc2926)
      %2736 = stablehlo.multiply %2730, %70 : tensor<1x16x128x32xbf16> loc(#loc2927)
      %2737 = stablehlo.add %2735, %2736 : tensor<1x16x128x32xbf16> loc(#loc2928)
      %2738 = stablehlo.concatenate %2734, %2737, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2929)
      %2739 = stablehlo.reshape %arg987 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2930)
      %2740 = stablehlo.reshape %2739 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2931)
      %2741 = stablehlo.transpose %2740, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2932)
      %2742 = stablehlo.dot_general %2717, %2741, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2933)
      %2743 = "stablehlo.all_reduce"(%2742) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7088"), %arg1239: tensor<bf16> loc("dot.7088")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2933)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2933)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2933)
      %2744 = stablehlo.reshape %2743 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2934)
      %2745 = stablehlo.reshape %arg986 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2935)
      %2746 = stablehlo.reshape %2745 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2936)
      %2747 = stablehlo.broadcast_in_dim %2746, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2937)
      %2748 = stablehlo.add %2744, %2747 : tensor<1x128x128xbf16> loc(#loc2938)
      %2749 = stablehlo.reshape %2748 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2939)
      %2750 = stablehlo.transpose %2749, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2940)
      %2751 = stablehlo.slice %2750 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2941)
      %2752 = stablehlo.multiply %2751, %90 : tensor<1x2x128x32xbf16> loc(#loc2942)
      %2753 = stablehlo.slice %2750 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc2943)
      %2754 = stablehlo.multiply %2753, %93 : tensor<1x2x128x32xbf16> loc(#loc2944)
      %2755 = stablehlo.subtract %2752, %2754 : tensor<1x2x128x32xbf16> loc(#loc2945)
      %2756 = stablehlo.multiply %2753, %90 : tensor<1x2x128x32xbf16> loc(#loc2946)
      %2757 = stablehlo.multiply %2751, %93 : tensor<1x2x128x32xbf16> loc(#loc2947)
      %2758 = stablehlo.add %2756, %2757 : tensor<1x2x128x32xbf16> loc(#loc2948)
      %2759 = stablehlo.concatenate %2755, %2758, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2949)
      %2760 = stablehlo.broadcast_in_dim %2759, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2950)
      %2761 = stablehlo.reshape %2760 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2951)
      %2762 = stablehlo.transpose %2761, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc2952)
      %2763 = stablehlo.dot_general %2738, %2762, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2953)
      %2764 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %2765 = stablehlo.multiply %2763, %2764 : tensor<1x16x128x128xbf16> loc(#loc2954)
      %2766 = stablehlo.add %2765, %341 : tensor<1x16x128x128xbf16> loc(#loc2955)
      %2767 = stablehlo.reshape %arg985 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc367)
      %2768 = "stablehlo.all_to_all"(%2767) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc367)
      %2769 = stablehlo.slice %2768 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc367)
      %2770 = stablehlo.reshape %2769 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc367)
      %2771 = stablehlo.reshape %2770 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc2956)
      %2772 = stablehlo.reshape %2771 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc2957)
      %2773 = stablehlo.broadcast_in_dim %2772, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc2958)
      %2774 = stablehlo.concatenate %2766, %2773, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2959)
      %2775 = stablehlo.reshape %arg995 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2960)
      %2776 = stablehlo.reshape %2775 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2961)
      %2777 = stablehlo.convert %2776 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc2962)
      %2778 = stablehlo.broadcast_in_dim %2777, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc2963)
      %2779 = stablehlo.reduce(%2774 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2964)
      %2780 = stablehlo.broadcast_in_dim %2779, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2965)
      %2781 = stablehlo.subtract %2774, %2780 : tensor<1x16x128x129xbf16> loc(#loc2966)
      %2782 = stablehlo.reduce(%2781 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2967)
      %2783 = stablehlo.broadcast_in_dim %2782, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2968)
      %2784 = stablehlo.subtract %2781, %2783 : tensor<1x16x128x129xbf16> loc(#loc2969)
      %2785 = stablehlo.exponential %2784 : tensor<1x16x128x129xbf16> loc(#loc2970)
      %2786 = stablehlo.reduce(%2785 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc2971)
      %2787 = stablehlo.broadcast_in_dim %2786, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc2972)
      %2788 = stablehlo.divide %2785, %2787 : tensor<1x16x128x129xbf16> loc(#loc2973)
      %2789 = stablehlo.slice %2788 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc2974)
      %2790 = stablehlo.reshape %arg757 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2975)
      %2791 = stablehlo.reshape %2790 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2976)
      %2792 = stablehlo.transpose %2791, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc2977)
      %2793 = stablehlo.dot_general %2717, %2792, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc2978)
      %2794 = "stablehlo.all_reduce"(%2793) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7051"), %arg1239: tensor<bf16> loc("dot.7051")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2978)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2978)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc2978)
      %2795 = stablehlo.reshape %2794 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2979)
      %2796 = stablehlo.reshape %arg756 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2980)
      %2797 = stablehlo.reshape %2796 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2981)
      %2798 = stablehlo.broadcast_in_dim %2797, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc2982)
      %2799 = stablehlo.add %2795, %2798 : tensor<1x128x128xbf16> loc(#loc2983)
      %2800 = stablehlo.reshape %2799 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc2984)
      %2801 = stablehlo.transpose %2800, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc2985)
      %2802 = stablehlo.broadcast_in_dim %2801, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc2986)
      %2803 = stablehlo.reshape %2802 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2987)
      %2804 = stablehlo.dot_general %2789, %2803, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc2988)
      %2805 = stablehlo.transpose %2804, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc2989)
      %2806 = stablehlo.reshape %2805 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc2990)
      %2807 = stablehlo.reshape %arg755 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc2991)
      %2808 = stablehlo.reshape %2807 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc2992)
      %2809 = stablehlo.transpose %2808, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc2993)
      %2810 = stablehlo.dot_general %2806, %2809, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc2994)
      %2811 = "stablehlo.all_reduce"(%2810) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7244"), %arg1239: tensor<bf16> loc("dot.7244")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc2994)
        stablehlo.return %7360 : tensor<bf16> loc(#loc2994)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc2994)
      %2812 = stablehlo.reshape %2811 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2995)
      %2813 = stablehlo.reshape %arg754 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc2996)
      %2814 = stablehlo.reshape %2813 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc2997)
      %2815 = stablehlo.broadcast_in_dim %2814, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc2998)
      %2816 = stablehlo.add %2812, %2815 : tensor<1x128x360xbf16> loc(#loc2999)
      %2817 = stablehlo.add %2700, %2816 : tensor<1x128x360xbf16> loc(#loc3000)
      %2818 = stablehlo.reshape %arg990 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3001)
      %2819 = stablehlo.reshape %2818 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3002)
      %2820 = stablehlo.convert %2819 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3003)
      %2821 = stablehlo.broadcast_in_dim %2820, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3004)
      %2822 = stablehlo.convert %2817 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3005)
      %2823 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2824 = stablehlo.power %2822, %2823 : tensor<1x128x360xf32> loc(#loc3006)
      %2825 = stablehlo.reduce(%2824 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3007)
      %2826 = "stablehlo.all_reduce"(%2825) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.7262"), %arg1239: tensor<f32> loc("reduce.7262")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3007)
        stablehlo.return %7360 : tensor<f32> loc(#loc3007)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3007)
      %2827 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2828 = stablehlo.multiply %2826, %2827 : tensor<1x128xf32> loc(#loc3008)
      %2829 = stablehlo.reshape %2828 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3009)
      %2830 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2831 = stablehlo.add %2829, %2830 : tensor<1x128x1xf32> loc(#loc3010)
      %2832 = stablehlo.rsqrt %2831 : tensor<1x128x1xf32> loc(#loc3011)
      %2833 = stablehlo.reshape %2832 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3012)
      %2834 = stablehlo.broadcast_in_dim %2833, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3013)
      %2835 = stablehlo.multiply %2822, %2834 : tensor<1x128x360xf32> loc(#loc3014)
      %2836 = stablehlo.multiply %2821, %2835 : tensor<1x128x360xf32> loc(#loc3015)
      %2837 = stablehlo.convert %2836 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3016)
      %2838 = stablehlo.reshape %2837 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3017)
      %2839 = stablehlo.broadcast_in_dim %2838, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3018)
      %2840 = stablehlo.dot_general %2839, %arg994, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3019)
      %2841 = "stablehlo.all_reduce"(%2840) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7374"), %arg1239: tensor<bf16> loc("dot.7374")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3019)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3019)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3019)
      %2842 = stablehlo.reshape %arg993 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc3020)
      %2843 = stablehlo.reshape %2842 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc3021)
      %2844 = stablehlo.broadcast_in_dim %2843, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3022)
      %2845 = stablehlo.add %2841, %2844 : tensor<32x128x5760xbf16> loc(#loc3023)
      %2846 = stablehlo.slice %2845 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3024)
      %2847 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2848 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2849 = stablehlo.clamp %2848, %2846, %2847 : tensor<32x128x2880xbf16> loc(#loc3025)
      %2850 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2851 = stablehlo.add %2849, %2850 : tensor<32x128x2880xbf16> loc(#loc3026)
      %2852 = stablehlo.slice %2845 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3027)
      %2853 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2854 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2855 = stablehlo.clamp %2853, %2852, %2854 : tensor<32x128x2880xbf16> loc(#loc3028)
      %2856 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %2857 = stablehlo.multiply %2855, %2856 : tensor<32x128x2880xbf16> loc(#loc3029)
      %2858 = stablehlo.logistic %2857 : tensor<32x128x2880xbf16> loc(#loc3030)
      %2859 = stablehlo.multiply %2855, %2858 : tensor<32x128x2880xbf16> loc(#loc3031)
      %2860 = stablehlo.multiply %2851, %2859 : tensor<32x128x2880xbf16> loc(#loc3032)
      %2861 = stablehlo.dot_general %2860, %arg992, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3033)
      %2862 = stablehlo.reshape %arg991 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc3034)
      %2863 = stablehlo.reshape %2862 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc3035)
      %2864 = stablehlo.broadcast_in_dim %2863, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3036)
      %2865 = stablehlo.add %2861, %2864 : tensor<32x128x360xbf16> loc(#loc3037)
      %2866 = stablehlo.reshape %2865 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3038)
      %2867 = stablehlo.reshape %arg753 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3039)
      %2868 = stablehlo.reshape %2867 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3040)
      %2869 = stablehlo.transpose %2868, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3041)
      %2870 = stablehlo.dot_general %2838, %2869, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3042)
      %2871 = "stablehlo.all_reduce"(%2870) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7290"), %arg1239: tensor<bf16> loc("dot.7290")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3042)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3042)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3042)
      %2872 = stablehlo.reshape %arg752 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3043)
      %2873 = stablehlo.reshape %2872 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3044)
      %2874 = stablehlo.broadcast_in_dim %2873, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc3045)
      %2875 = stablehlo.add %2871, %2874 : tensor<128x128xbf16> loc(#loc3046)
      %2876:2 = "stablehlo.sort"(%2875, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.7310"), %arg1239: tensor<bf16> loc("sort.7310"), %arg1240: tensor<i32> loc("sort.7310"), %arg1241: tensor<i32> loc("sort.7310")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3048)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc3047)
      %2877 = stablehlo.slice %2876#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc3049)
      %2878 = stablehlo.convert %2877 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc3050)
      %2879 = stablehlo.reshape %2878 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc3051)
      %2880 = stablehlo.concatenate %231, %2879, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc3052)
      %2881 = stablehlo.slice %2876#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc3053)
      %2882 = stablehlo.reduce(%2881 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3054)
      %2883 = stablehlo.broadcast_in_dim %2882, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3055)
      %2884 = stablehlo.subtract %2881, %2883 : tensor<128x4xbf16> loc(#loc3056)
      %2885 = stablehlo.exponential %2884 : tensor<128x4xbf16> loc(#loc3057)
      %2886 = stablehlo.reduce(%2885 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3058)
      %2887 = stablehlo.broadcast_in_dim %2886, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3059)
      %2888 = stablehlo.divide %2885, %2887 : tensor<128x4xbf16> loc(#loc3060)
      %2889 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %2890 = "stablehlo.all_gather"(%2889) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %2891 = "stablehlo.scatter"(%2890, %2880, %2888) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.7344"), %arg1239: tensor<bf16> loc("scatter.7344")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc3061)
      %2892 = stablehlo.reshape %2891 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc3061)
      %2893 = "stablehlo.all_to_all"(%2892) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc3061)
      %2894 = stablehlo.slice %2893 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc3061)
      %2895 = stablehlo.reshape %2894 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc3061)
      %2896 = stablehlo.transpose %2895, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc3062)
      %2897 = stablehlo.reshape %2896 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc3063)
      %2898 = stablehlo.broadcast_in_dim %2897, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3064)
      %2899 = stablehlo.multiply %2866, %2898 : tensor<32x1x128x360xbf16> loc(#loc3065)
      %2900 = stablehlo.reduce(%2899 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc3066)
      %2901 = "stablehlo.all_reduce"(%2900) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.7416"), %arg1239: tensor<bf16> loc("reduce.7416")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3066)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3066)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3066)
      %2902 = stablehlo.add %2817, %2901 : tensor<1x128x360xbf16> loc(#loc3067)
      %2903 = stablehlo.convert %2902 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3068)
      %2904 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %2905 = stablehlo.power %2903, %2904 : tensor<1x128x360xf32> loc(#loc3069)
      %2906 = stablehlo.reduce(%2905 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3070)
      %2907 = "stablehlo.all_reduce"(%2906) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.7429"), %arg1239: tensor<f32> loc("reduce.7429")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3070)
        stablehlo.return %7360 : tensor<f32> loc(#loc3070)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3070)
      %2908 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %2909 = stablehlo.multiply %2907, %2908 : tensor<1x128xf32> loc(#loc3071)
      %2910 = stablehlo.reshape %2909 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3072)
      %2911 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %2912 = stablehlo.add %2910, %2911 : tensor<1x128x1xf32> loc(#loc3073)
      %2913 = stablehlo.rsqrt %2912 : tensor<1x128x1xf32> loc(#loc3074)
      %2914 = stablehlo.reshape %2913 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3075)
      %2915 = stablehlo.broadcast_in_dim %2914, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3076)
      %2916 = stablehlo.multiply %2903, %2915 : tensor<1x128x360xf32> loc(#loc3077)
      %2917 = stablehlo.multiply %2778, %2916 : tensor<1x128x360xf32> loc(#loc3078)
      %2918 = stablehlo.convert %2917 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3079)
      %2919 = stablehlo.reshape %2918 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3080)
      %2920 = stablehlo.reshape %arg1000 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc3081)
      %2921 = stablehlo.reshape %2920 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc3082)
      %2922 = stablehlo.transpose %2921, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc3083)
      %2923 = stablehlo.dot_general %2919, %2922, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3084)
      %2924 = "stablehlo.all_reduce"(%2923) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7543"), %arg1239: tensor<bf16> loc("dot.7543")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3084)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3084)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3084)
      %2925 = stablehlo.reshape %2924 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3085)
      %2926 = stablehlo.reshape %arg999 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3086)
      %2927 = stablehlo.reshape %2926 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc3087)
      %2928 = stablehlo.broadcast_in_dim %2927, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3088)
      %2929 = stablehlo.add %2925, %2928 : tensor<1x128x1024xbf16> loc(#loc3089)
      %2930 = stablehlo.reshape %2929 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3090)
      %2931 = stablehlo.transpose %2930, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3091)
      %2932 = stablehlo.slice %2931 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3092)
      %2933 = stablehlo.multiply %2932, %64 : tensor<1x16x128x32xbf16> loc(#loc3093)
      %2934 = stablehlo.slice %2931 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3094)
      %2935 = stablehlo.multiply %2934, %70 : tensor<1x16x128x32xbf16> loc(#loc3095)
      %2936 = stablehlo.subtract %2933, %2935 : tensor<1x16x128x32xbf16> loc(#loc3096)
      %2937 = stablehlo.multiply %2934, %64 : tensor<1x16x128x32xbf16> loc(#loc3097)
      %2938 = stablehlo.multiply %2932, %70 : tensor<1x16x128x32xbf16> loc(#loc3098)
      %2939 = stablehlo.add %2937, %2938 : tensor<1x16x128x32xbf16> loc(#loc3099)
      %2940 = stablehlo.concatenate %2936, %2939, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3100)
      %2941 = stablehlo.reshape %arg998 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3101)
      %2942 = stablehlo.reshape %2941 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3102)
      %2943 = stablehlo.transpose %2942, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3103)
      %2944 = stablehlo.dot_general %2919, %2943, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3104)
      %2945 = "stablehlo.all_reduce"(%2944) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7494"), %arg1239: tensor<bf16> loc("dot.7494")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3104)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3104)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3104)
      %2946 = stablehlo.reshape %2945 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3105)
      %2947 = stablehlo.reshape %arg997 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3106)
      %2948 = stablehlo.reshape %2947 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3107)
      %2949 = stablehlo.broadcast_in_dim %2948, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3108)
      %2950 = stablehlo.add %2946, %2949 : tensor<1x128x128xbf16> loc(#loc3109)
      %2951 = stablehlo.reshape %2950 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3110)
      %2952 = stablehlo.transpose %2951, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3111)
      %2953 = stablehlo.slice %2952 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3112)
      %2954 = stablehlo.multiply %2953, %90 : tensor<1x2x128x32xbf16> loc(#loc3113)
      %2955 = stablehlo.slice %2952 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3114)
      %2956 = stablehlo.multiply %2955, %93 : tensor<1x2x128x32xbf16> loc(#loc3115)
      %2957 = stablehlo.subtract %2954, %2956 : tensor<1x2x128x32xbf16> loc(#loc3116)
      %2958 = stablehlo.multiply %2955, %90 : tensor<1x2x128x32xbf16> loc(#loc3117)
      %2959 = stablehlo.multiply %2953, %93 : tensor<1x2x128x32xbf16> loc(#loc3118)
      %2960 = stablehlo.add %2958, %2959 : tensor<1x2x128x32xbf16> loc(#loc3119)
      %2961 = stablehlo.concatenate %2957, %2960, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3120)
      %2962 = stablehlo.broadcast_in_dim %2961, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3121)
      %2963 = stablehlo.reshape %2962 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3122)
      %2964 = stablehlo.transpose %2963, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc3123)
      %2965 = stablehlo.dot_general %2940, %2964, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3124)
      %2966 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %2967 = stablehlo.multiply %2965, %2966 : tensor<1x16x128x128xbf16> loc(#loc3125)
      %2968 = stablehlo.add %2967, %128 : tensor<1x16x128x128xbf16> loc(#loc3126)
      %2969 = stablehlo.reshape %arg996 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc378)
      %2970 = "stablehlo.all_to_all"(%2969) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc378)
      %2971 = stablehlo.slice %2970 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc378)
      %2972 = stablehlo.reshape %2971 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc378)
      %2973 = stablehlo.reshape %2972 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3127)
      %2974 = stablehlo.reshape %2973 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc3128)
      %2975 = stablehlo.broadcast_in_dim %2974, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc3129)
      %2976 = stablehlo.concatenate %2968, %2975, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3130)
      %2977 = stablehlo.reshape %arg1006 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3131)
      %2978 = stablehlo.reshape %2977 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3132)
      %2979 = stablehlo.convert %2978 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3133)
      %2980 = stablehlo.broadcast_in_dim %2979, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3134)
      %2981 = stablehlo.reduce(%2976 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3135)
      %2982 = stablehlo.broadcast_in_dim %2981, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3136)
      %2983 = stablehlo.subtract %2976, %2982 : tensor<1x16x128x129xbf16> loc(#loc3137)
      %2984 = stablehlo.reduce(%2983 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3138)
      %2985 = stablehlo.broadcast_in_dim %2984, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3139)
      %2986 = stablehlo.subtract %2983, %2985 : tensor<1x16x128x129xbf16> loc(#loc3140)
      %2987 = stablehlo.exponential %2986 : tensor<1x16x128x129xbf16> loc(#loc3141)
      %2988 = stablehlo.reduce(%2987 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3142)
      %2989 = stablehlo.broadcast_in_dim %2988, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3143)
      %2990 = stablehlo.divide %2987, %2989 : tensor<1x16x128x129xbf16> loc(#loc3144)
      %2991 = stablehlo.slice %2990 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3145)
      %2992 = stablehlo.reshape %arg751 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3146)
      %2993 = stablehlo.reshape %2992 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3147)
      %2994 = stablehlo.transpose %2993, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3148)
      %2995 = stablehlo.dot_general %2919, %2994, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3149)
      %2996 = "stablehlo.all_reduce"(%2995) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7457"), %arg1239: tensor<bf16> loc("dot.7457")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3149)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3149)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3149)
      %2997 = stablehlo.reshape %2996 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3150)
      %2998 = stablehlo.reshape %arg750 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3151)
      %2999 = stablehlo.reshape %2998 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3152)
      %3000 = stablehlo.broadcast_in_dim %2999, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3153)
      %3001 = stablehlo.add %2997, %3000 : tensor<1x128x128xbf16> loc(#loc3154)
      %3002 = stablehlo.reshape %3001 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3155)
      %3003 = stablehlo.transpose %3002, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3156)
      %3004 = stablehlo.broadcast_in_dim %3003, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3157)
      %3005 = stablehlo.reshape %3004 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3158)
      %3006 = stablehlo.dot_general %2991, %3005, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3159)
      %3007 = stablehlo.transpose %3006, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3160)
      %3008 = stablehlo.reshape %3007 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc3161)
      %3009 = stablehlo.reshape %arg749 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc3162)
      %3010 = stablehlo.reshape %3009 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc3163)
      %3011 = stablehlo.transpose %3010, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc3164)
      %3012 = stablehlo.dot_general %3008, %3011, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc3165)
      %3013 = "stablehlo.all_reduce"(%3012) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7650"), %arg1239: tensor<bf16> loc("dot.7650")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3165)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3165)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3165)
      %3014 = stablehlo.reshape %3013 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3166)
      %3015 = stablehlo.reshape %arg748 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3167)
      %3016 = stablehlo.reshape %3015 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3168)
      %3017 = stablehlo.broadcast_in_dim %3016, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3169)
      %3018 = stablehlo.add %3014, %3017 : tensor<1x128x360xbf16> loc(#loc3170)
      %3019 = stablehlo.add %2902, %3018 : tensor<1x128x360xbf16> loc(#loc3171)
      %3020 = stablehlo.reshape %arg1001 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3172)
      %3021 = stablehlo.reshape %3020 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3173)
      %3022 = stablehlo.convert %3021 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3174)
      %3023 = stablehlo.broadcast_in_dim %3022, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3175)
      %3024 = stablehlo.convert %3019 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3176)
      %3025 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3026 = stablehlo.power %3024, %3025 : tensor<1x128x360xf32> loc(#loc3177)
      %3027 = stablehlo.reduce(%3026 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3178)
      %3028 = "stablehlo.all_reduce"(%3027) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.7668"), %arg1239: tensor<f32> loc("reduce.7668")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3178)
        stablehlo.return %7360 : tensor<f32> loc(#loc3178)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3178)
      %3029 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3030 = stablehlo.multiply %3028, %3029 : tensor<1x128xf32> loc(#loc3179)
      %3031 = stablehlo.reshape %3030 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3180)
      %3032 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3033 = stablehlo.add %3031, %3032 : tensor<1x128x1xf32> loc(#loc3181)
      %3034 = stablehlo.rsqrt %3033 : tensor<1x128x1xf32> loc(#loc3182)
      %3035 = stablehlo.reshape %3034 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3183)
      %3036 = stablehlo.broadcast_in_dim %3035, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3184)
      %3037 = stablehlo.multiply %3024, %3036 : tensor<1x128x360xf32> loc(#loc3185)
      %3038 = stablehlo.multiply %3023, %3037 : tensor<1x128x360xf32> loc(#loc3186)
      %3039 = stablehlo.convert %3038 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3187)
      %3040 = stablehlo.reshape %3039 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3188)
      %3041 = stablehlo.broadcast_in_dim %3040, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3189)
      %3042 = stablehlo.dot_general %3041, %arg1005, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3190)
      %3043 = "stablehlo.all_reduce"(%3042) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7780"), %arg1239: tensor<bf16> loc("dot.7780")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3190)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3190)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3190)
      %3044 = stablehlo.reshape %arg1004 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc3191)
      %3045 = stablehlo.reshape %3044 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc3192)
      %3046 = stablehlo.broadcast_in_dim %3045, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3193)
      %3047 = stablehlo.add %3043, %3046 : tensor<32x128x5760xbf16> loc(#loc3194)
      %3048 = stablehlo.slice %3047 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3195)
      %3049 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3050 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3051 = stablehlo.clamp %3050, %3048, %3049 : tensor<32x128x2880xbf16> loc(#loc3196)
      %3052 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3053 = stablehlo.add %3051, %3052 : tensor<32x128x2880xbf16> loc(#loc3197)
      %3054 = stablehlo.slice %3047 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3198)
      %3055 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3056 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3057 = stablehlo.clamp %3055, %3054, %3056 : tensor<32x128x2880xbf16> loc(#loc3199)
      %3058 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3059 = stablehlo.multiply %3057, %3058 : tensor<32x128x2880xbf16> loc(#loc3200)
      %3060 = stablehlo.logistic %3059 : tensor<32x128x2880xbf16> loc(#loc3201)
      %3061 = stablehlo.multiply %3057, %3060 : tensor<32x128x2880xbf16> loc(#loc3202)
      %3062 = stablehlo.multiply %3053, %3061 : tensor<32x128x2880xbf16> loc(#loc3203)
      %3063 = stablehlo.dot_general %3062, %arg1003, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3204)
      %3064 = stablehlo.reshape %arg1002 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc3205)
      %3065 = stablehlo.reshape %3064 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc3206)
      %3066 = stablehlo.broadcast_in_dim %3065, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3207)
      %3067 = stablehlo.add %3063, %3066 : tensor<32x128x360xbf16> loc(#loc3208)
      %3068 = stablehlo.reshape %3067 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3209)
      %3069 = stablehlo.reshape %arg747 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3210)
      %3070 = stablehlo.reshape %3069 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3211)
      %3071 = stablehlo.transpose %3070, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3212)
      %3072 = stablehlo.dot_general %3040, %3071, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3213)
      %3073 = "stablehlo.all_reduce"(%3072) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7696"), %arg1239: tensor<bf16> loc("dot.7696")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3213)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3213)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3213)
      %3074 = stablehlo.reshape %arg746 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3214)
      %3075 = stablehlo.reshape %3074 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3215)
      %3076 = stablehlo.broadcast_in_dim %3075, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc3216)
      %3077 = stablehlo.add %3073, %3076 : tensor<128x128xbf16> loc(#loc3217)
      %3078:2 = "stablehlo.sort"(%3077, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.7716"), %arg1239: tensor<bf16> loc("sort.7716"), %arg1240: tensor<i32> loc("sort.7716"), %arg1241: tensor<i32> loc("sort.7716")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3219)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc3218)
      %3079 = stablehlo.slice %3078#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc3220)
      %3080 = stablehlo.convert %3079 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc3221)
      %3081 = stablehlo.reshape %3080 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc3222)
      %3082 = stablehlo.concatenate %231, %3081, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc3223)
      %3083 = stablehlo.slice %3078#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc3224)
      %3084 = stablehlo.reduce(%3083 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3225)
      %3085 = stablehlo.broadcast_in_dim %3084, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3226)
      %3086 = stablehlo.subtract %3083, %3085 : tensor<128x4xbf16> loc(#loc3227)
      %3087 = stablehlo.exponential %3086 : tensor<128x4xbf16> loc(#loc3228)
      %3088 = stablehlo.reduce(%3087 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3229)
      %3089 = stablehlo.broadcast_in_dim %3088, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3230)
      %3090 = stablehlo.divide %3087, %3089 : tensor<128x4xbf16> loc(#loc3231)
      %3091 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %3092 = "stablehlo.all_gather"(%3091) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %3093 = "stablehlo.scatter"(%3092, %3082, %3090) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.7750"), %arg1239: tensor<bf16> loc("scatter.7750")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc3232)
      %3094 = stablehlo.reshape %3093 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc3232)
      %3095 = "stablehlo.all_to_all"(%3094) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc3232)
      %3096 = stablehlo.slice %3095 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc3232)
      %3097 = stablehlo.reshape %3096 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc3232)
      %3098 = stablehlo.transpose %3097, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc3233)
      %3099 = stablehlo.reshape %3098 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc3234)
      %3100 = stablehlo.broadcast_in_dim %3099, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3235)
      %3101 = stablehlo.multiply %3068, %3100 : tensor<32x1x128x360xbf16> loc(#loc3236)
      %3102 = stablehlo.reduce(%3101 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc3237)
      %3103 = "stablehlo.all_reduce"(%3102) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.7822"), %arg1239: tensor<bf16> loc("reduce.7822")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3237)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3237)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3237)
      %3104 = stablehlo.add %3019, %3103 : tensor<1x128x360xbf16> loc(#loc3238)
      %3105 = stablehlo.convert %3104 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3239)
      %3106 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3107 = stablehlo.power %3105, %3106 : tensor<1x128x360xf32> loc(#loc3240)
      %3108 = stablehlo.reduce(%3107 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3241)
      %3109 = "stablehlo.all_reduce"(%3108) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.7835"), %arg1239: tensor<f32> loc("reduce.7835")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3241)
        stablehlo.return %7360 : tensor<f32> loc(#loc3241)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3241)
      %3110 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3111 = stablehlo.multiply %3109, %3110 : tensor<1x128xf32> loc(#loc3242)
      %3112 = stablehlo.reshape %3111 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3243)
      %3113 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3114 = stablehlo.add %3112, %3113 : tensor<1x128x1xf32> loc(#loc3244)
      %3115 = stablehlo.rsqrt %3114 : tensor<1x128x1xf32> loc(#loc3245)
      %3116 = stablehlo.reshape %3115 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3246)
      %3117 = stablehlo.broadcast_in_dim %3116, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3247)
      %3118 = stablehlo.multiply %3105, %3117 : tensor<1x128x360xf32> loc(#loc3248)
      %3119 = stablehlo.multiply %2980, %3118 : tensor<1x128x360xf32> loc(#loc3249)
      %3120 = stablehlo.convert %3119 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3250)
      %3121 = stablehlo.reshape %3120 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3251)
      %3122 = stablehlo.reshape %arg1011 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc3252)
      %3123 = stablehlo.reshape %3122 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc3253)
      %3124 = stablehlo.transpose %3123, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc3254)
      %3125 = stablehlo.dot_general %3121, %3124, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3255)
      %3126 = "stablehlo.all_reduce"(%3125) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7949"), %arg1239: tensor<bf16> loc("dot.7949")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3255)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3255)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3255)
      %3127 = stablehlo.reshape %3126 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3256)
      %3128 = stablehlo.reshape %arg1010 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3257)
      %3129 = stablehlo.reshape %3128 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc3258)
      %3130 = stablehlo.broadcast_in_dim %3129, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3259)
      %3131 = stablehlo.add %3127, %3130 : tensor<1x128x1024xbf16> loc(#loc3260)
      %3132 = stablehlo.reshape %3131 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3261)
      %3133 = stablehlo.transpose %3132, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3262)
      %3134 = stablehlo.slice %3133 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3263)
      %3135 = stablehlo.multiply %3134, %64 : tensor<1x16x128x32xbf16> loc(#loc3264)
      %3136 = stablehlo.slice %3133 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3265)
      %3137 = stablehlo.multiply %3136, %70 : tensor<1x16x128x32xbf16> loc(#loc3266)
      %3138 = stablehlo.subtract %3135, %3137 : tensor<1x16x128x32xbf16> loc(#loc3267)
      %3139 = stablehlo.multiply %3136, %64 : tensor<1x16x128x32xbf16> loc(#loc3268)
      %3140 = stablehlo.multiply %3134, %70 : tensor<1x16x128x32xbf16> loc(#loc3269)
      %3141 = stablehlo.add %3139, %3140 : tensor<1x16x128x32xbf16> loc(#loc3270)
      %3142 = stablehlo.concatenate %3138, %3141, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3271)
      %3143 = stablehlo.reshape %arg1009 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3272)
      %3144 = stablehlo.reshape %3143 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3273)
      %3145 = stablehlo.transpose %3144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3274)
      %3146 = stablehlo.dot_general %3121, %3145, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3275)
      %3147 = "stablehlo.all_reduce"(%3146) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7900"), %arg1239: tensor<bf16> loc("dot.7900")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3275)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3275)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3275)
      %3148 = stablehlo.reshape %3147 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3276)
      %3149 = stablehlo.reshape %arg1008 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3277)
      %3150 = stablehlo.reshape %3149 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3278)
      %3151 = stablehlo.broadcast_in_dim %3150, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3279)
      %3152 = stablehlo.add %3148, %3151 : tensor<1x128x128xbf16> loc(#loc3280)
      %3153 = stablehlo.reshape %3152 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3281)
      %3154 = stablehlo.transpose %3153, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3282)
      %3155 = stablehlo.slice %3154 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3283)
      %3156 = stablehlo.multiply %3155, %90 : tensor<1x2x128x32xbf16> loc(#loc3284)
      %3157 = stablehlo.slice %3154 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3285)
      %3158 = stablehlo.multiply %3157, %93 : tensor<1x2x128x32xbf16> loc(#loc3286)
      %3159 = stablehlo.subtract %3156, %3158 : tensor<1x2x128x32xbf16> loc(#loc3287)
      %3160 = stablehlo.multiply %3157, %90 : tensor<1x2x128x32xbf16> loc(#loc3288)
      %3161 = stablehlo.multiply %3155, %93 : tensor<1x2x128x32xbf16> loc(#loc3289)
      %3162 = stablehlo.add %3160, %3161 : tensor<1x2x128x32xbf16> loc(#loc3290)
      %3163 = stablehlo.concatenate %3159, %3162, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3291)
      %3164 = stablehlo.broadcast_in_dim %3163, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3292)
      %3165 = stablehlo.reshape %3164 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3293)
      %3166 = stablehlo.transpose %3165, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc3294)
      %3167 = stablehlo.dot_general %3142, %3166, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3295)
      %3168 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %3169 = stablehlo.multiply %3167, %3168 : tensor<1x16x128x128xbf16> loc(#loc3296)
      %3170 = stablehlo.add %3169, %341 : tensor<1x16x128x128xbf16> loc(#loc3297)
      %3171 = stablehlo.reshape %arg1007 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc389)
      %3172 = "stablehlo.all_to_all"(%3171) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc389)
      %3173 = stablehlo.slice %3172 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc389)
      %3174 = stablehlo.reshape %3173 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc389)
      %3175 = stablehlo.reshape %3174 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3298)
      %3176 = stablehlo.reshape %3175 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc3299)
      %3177 = stablehlo.broadcast_in_dim %3176, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc3300)
      %3178 = stablehlo.concatenate %3170, %3177, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3301)
      %3179 = stablehlo.reshape %arg1017 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3302)
      %3180 = stablehlo.reshape %3179 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3303)
      %3181 = stablehlo.convert %3180 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3304)
      %3182 = stablehlo.broadcast_in_dim %3181, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3305)
      %3183 = stablehlo.reduce(%3178 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3306)
      %3184 = stablehlo.broadcast_in_dim %3183, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3307)
      %3185 = stablehlo.subtract %3178, %3184 : tensor<1x16x128x129xbf16> loc(#loc3308)
      %3186 = stablehlo.reduce(%3185 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3309)
      %3187 = stablehlo.broadcast_in_dim %3186, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3310)
      %3188 = stablehlo.subtract %3185, %3187 : tensor<1x16x128x129xbf16> loc(#loc3311)
      %3189 = stablehlo.exponential %3188 : tensor<1x16x128x129xbf16> loc(#loc3312)
      %3190 = stablehlo.reduce(%3189 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3313)
      %3191 = stablehlo.broadcast_in_dim %3190, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3314)
      %3192 = stablehlo.divide %3189, %3191 : tensor<1x16x128x129xbf16> loc(#loc3315)
      %3193 = stablehlo.slice %3192 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3316)
      %3194 = stablehlo.reshape %arg745 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3317)
      %3195 = stablehlo.reshape %3194 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3318)
      %3196 = stablehlo.transpose %3195, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3319)
      %3197 = stablehlo.dot_general %3121, %3196, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3320)
      %3198 = "stablehlo.all_reduce"(%3197) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.7863"), %arg1239: tensor<bf16> loc("dot.7863")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3320)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3320)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3320)
      %3199 = stablehlo.reshape %3198 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3321)
      %3200 = stablehlo.reshape %arg744 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3322)
      %3201 = stablehlo.reshape %3200 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3323)
      %3202 = stablehlo.broadcast_in_dim %3201, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3324)
      %3203 = stablehlo.add %3199, %3202 : tensor<1x128x128xbf16> loc(#loc3325)
      %3204 = stablehlo.reshape %3203 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3326)
      %3205 = stablehlo.transpose %3204, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3327)
      %3206 = stablehlo.broadcast_in_dim %3205, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3328)
      %3207 = stablehlo.reshape %3206 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3329)
      %3208 = stablehlo.dot_general %3193, %3207, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3330)
      %3209 = stablehlo.transpose %3208, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3331)
      %3210 = stablehlo.reshape %3209 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc3332)
      %3211 = stablehlo.reshape %arg743 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc3333)
      %3212 = stablehlo.reshape %3211 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc3334)
      %3213 = stablehlo.transpose %3212, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc3335)
      %3214 = stablehlo.dot_general %3210, %3213, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc3336)
      %3215 = "stablehlo.all_reduce"(%3214) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8056"), %arg1239: tensor<bf16> loc("dot.8056")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3336)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3336)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3336)
      %3216 = stablehlo.reshape %3215 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3337)
      %3217 = stablehlo.reshape %arg742 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3338)
      %3218 = stablehlo.reshape %3217 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3339)
      %3219 = stablehlo.broadcast_in_dim %3218, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3340)
      %3220 = stablehlo.add %3216, %3219 : tensor<1x128x360xbf16> loc(#loc3341)
      %3221 = stablehlo.add %3104, %3220 : tensor<1x128x360xbf16> loc(#loc3342)
      %3222 = stablehlo.reshape %arg1012 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3343)
      %3223 = stablehlo.reshape %3222 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3344)
      %3224 = stablehlo.convert %3223 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3345)
      %3225 = stablehlo.broadcast_in_dim %3224, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3346)
      %3226 = stablehlo.convert %3221 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3347)
      %3227 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3228 = stablehlo.power %3226, %3227 : tensor<1x128x360xf32> loc(#loc3348)
      %3229 = stablehlo.reduce(%3228 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3349)
      %3230 = "stablehlo.all_reduce"(%3229) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.8074"), %arg1239: tensor<f32> loc("reduce.8074")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3349)
        stablehlo.return %7360 : tensor<f32> loc(#loc3349)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3349)
      %3231 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3232 = stablehlo.multiply %3230, %3231 : tensor<1x128xf32> loc(#loc3350)
      %3233 = stablehlo.reshape %3232 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3351)
      %3234 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3235 = stablehlo.add %3233, %3234 : tensor<1x128x1xf32> loc(#loc3352)
      %3236 = stablehlo.rsqrt %3235 : tensor<1x128x1xf32> loc(#loc3353)
      %3237 = stablehlo.reshape %3236 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3354)
      %3238 = stablehlo.broadcast_in_dim %3237, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3355)
      %3239 = stablehlo.multiply %3226, %3238 : tensor<1x128x360xf32> loc(#loc3356)
      %3240 = stablehlo.multiply %3225, %3239 : tensor<1x128x360xf32> loc(#loc3357)
      %3241 = stablehlo.convert %3240 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3358)
      %3242 = stablehlo.reshape %3241 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3359)
      %3243 = stablehlo.broadcast_in_dim %3242, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3360)
      %3244 = stablehlo.dot_general %3243, %arg1016, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3361)
      %3245 = "stablehlo.all_reduce"(%3244) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8186"), %arg1239: tensor<bf16> loc("dot.8186")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3361)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3361)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3361)
      %3246 = stablehlo.reshape %arg1015 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc3362)
      %3247 = stablehlo.reshape %3246 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc3363)
      %3248 = stablehlo.broadcast_in_dim %3247, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3364)
      %3249 = stablehlo.add %3245, %3248 : tensor<32x128x5760xbf16> loc(#loc3365)
      %3250 = stablehlo.slice %3249 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3366)
      %3251 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3252 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3253 = stablehlo.clamp %3252, %3250, %3251 : tensor<32x128x2880xbf16> loc(#loc3367)
      %3254 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3255 = stablehlo.add %3253, %3254 : tensor<32x128x2880xbf16> loc(#loc3368)
      %3256 = stablehlo.slice %3249 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3369)
      %3257 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3258 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3259 = stablehlo.clamp %3257, %3256, %3258 : tensor<32x128x2880xbf16> loc(#loc3370)
      %3260 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3261 = stablehlo.multiply %3259, %3260 : tensor<32x128x2880xbf16> loc(#loc3371)
      %3262 = stablehlo.logistic %3261 : tensor<32x128x2880xbf16> loc(#loc3372)
      %3263 = stablehlo.multiply %3259, %3262 : tensor<32x128x2880xbf16> loc(#loc3373)
      %3264 = stablehlo.multiply %3255, %3263 : tensor<32x128x2880xbf16> loc(#loc3374)
      %3265 = stablehlo.dot_general %3264, %arg1014, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3375)
      %3266 = stablehlo.reshape %arg1013 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc3376)
      %3267 = stablehlo.reshape %3266 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc3377)
      %3268 = stablehlo.broadcast_in_dim %3267, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3378)
      %3269 = stablehlo.add %3265, %3268 : tensor<32x128x360xbf16> loc(#loc3379)
      %3270 = stablehlo.reshape %3269 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3380)
      %3271 = stablehlo.reshape %arg741 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3381)
      %3272 = stablehlo.reshape %3271 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3382)
      %3273 = stablehlo.transpose %3272, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3383)
      %3274 = stablehlo.dot_general %3242, %3273, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3384)
      %3275 = "stablehlo.all_reduce"(%3274) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8102"), %arg1239: tensor<bf16> loc("dot.8102")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3384)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3384)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3384)
      %3276 = stablehlo.reshape %arg740 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3385)
      %3277 = stablehlo.reshape %3276 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3386)
      %3278 = stablehlo.broadcast_in_dim %3277, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc3387)
      %3279 = stablehlo.add %3275, %3278 : tensor<128x128xbf16> loc(#loc3388)
      %3280:2 = "stablehlo.sort"(%3279, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.8122"), %arg1239: tensor<bf16> loc("sort.8122"), %arg1240: tensor<i32> loc("sort.8122"), %arg1241: tensor<i32> loc("sort.8122")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3390)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc3389)
      %3281 = stablehlo.slice %3280#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc3391)
      %3282 = stablehlo.convert %3281 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc3392)
      %3283 = stablehlo.reshape %3282 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc3393)
      %3284 = stablehlo.concatenate %231, %3283, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc3394)
      %3285 = stablehlo.slice %3280#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc3395)
      %3286 = stablehlo.reduce(%3285 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3396)
      %3287 = stablehlo.broadcast_in_dim %3286, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3397)
      %3288 = stablehlo.subtract %3285, %3287 : tensor<128x4xbf16> loc(#loc3398)
      %3289 = stablehlo.exponential %3288 : tensor<128x4xbf16> loc(#loc3399)
      %3290 = stablehlo.reduce(%3289 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3400)
      %3291 = stablehlo.broadcast_in_dim %3290, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3401)
      %3292 = stablehlo.divide %3289, %3291 : tensor<128x4xbf16> loc(#loc3402)
      %3293 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %3294 = "stablehlo.all_gather"(%3293) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %3295 = "stablehlo.scatter"(%3294, %3284, %3292) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.8156"), %arg1239: tensor<bf16> loc("scatter.8156")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc3403)
      %3296 = stablehlo.reshape %3295 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc3403)
      %3297 = "stablehlo.all_to_all"(%3296) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc3403)
      %3298 = stablehlo.slice %3297 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc3403)
      %3299 = stablehlo.reshape %3298 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc3403)
      %3300 = stablehlo.transpose %3299, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc3404)
      %3301 = stablehlo.reshape %3300 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc3405)
      %3302 = stablehlo.broadcast_in_dim %3301, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3406)
      %3303 = stablehlo.multiply %3270, %3302 : tensor<32x1x128x360xbf16> loc(#loc3407)
      %3304 = stablehlo.reduce(%3303 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc3408)
      %3305 = "stablehlo.all_reduce"(%3304) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.8228"), %arg1239: tensor<bf16> loc("reduce.8228")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3408)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3408)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3408)
      %3306 = stablehlo.add %3221, %3305 : tensor<1x128x360xbf16> loc(#loc3409)
      %3307 = stablehlo.convert %3306 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3410)
      %3308 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3309 = stablehlo.power %3307, %3308 : tensor<1x128x360xf32> loc(#loc3411)
      %3310 = stablehlo.reduce(%3309 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3412)
      %3311 = "stablehlo.all_reduce"(%3310) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.8241"), %arg1239: tensor<f32> loc("reduce.8241")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3412)
        stablehlo.return %7360 : tensor<f32> loc(#loc3412)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3412)
      %3312 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3313 = stablehlo.multiply %3311, %3312 : tensor<1x128xf32> loc(#loc3413)
      %3314 = stablehlo.reshape %3313 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3414)
      %3315 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3316 = stablehlo.add %3314, %3315 : tensor<1x128x1xf32> loc(#loc3415)
      %3317 = stablehlo.rsqrt %3316 : tensor<1x128x1xf32> loc(#loc3416)
      %3318 = stablehlo.reshape %3317 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3417)
      %3319 = stablehlo.broadcast_in_dim %3318, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3418)
      %3320 = stablehlo.multiply %3307, %3319 : tensor<1x128x360xf32> loc(#loc3419)
      %3321 = stablehlo.multiply %3182, %3320 : tensor<1x128x360xf32> loc(#loc3420)
      %3322 = stablehlo.convert %3321 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3421)
      %3323 = stablehlo.reshape %3322 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3422)
      %3324 = stablehlo.reshape %arg1022 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc3423)
      %3325 = stablehlo.reshape %3324 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc3424)
      %3326 = stablehlo.transpose %3325, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc3425)
      %3327 = stablehlo.dot_general %3323, %3326, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3426)
      %3328 = "stablehlo.all_reduce"(%3327) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8355"), %arg1239: tensor<bf16> loc("dot.8355")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3426)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3426)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3426)
      %3329 = stablehlo.reshape %3328 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3427)
      %3330 = stablehlo.reshape %arg1021 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3428)
      %3331 = stablehlo.reshape %3330 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc3429)
      %3332 = stablehlo.broadcast_in_dim %3331, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3430)
      %3333 = stablehlo.add %3329, %3332 : tensor<1x128x1024xbf16> loc(#loc3431)
      %3334 = stablehlo.reshape %3333 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3432)
      %3335 = stablehlo.transpose %3334, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3433)
      %3336 = stablehlo.slice %3335 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3434)
      %3337 = stablehlo.multiply %3336, %64 : tensor<1x16x128x32xbf16> loc(#loc3435)
      %3338 = stablehlo.slice %3335 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3436)
      %3339 = stablehlo.multiply %3338, %70 : tensor<1x16x128x32xbf16> loc(#loc3437)
      %3340 = stablehlo.subtract %3337, %3339 : tensor<1x16x128x32xbf16> loc(#loc3438)
      %3341 = stablehlo.multiply %3338, %64 : tensor<1x16x128x32xbf16> loc(#loc3439)
      %3342 = stablehlo.multiply %3336, %70 : tensor<1x16x128x32xbf16> loc(#loc3440)
      %3343 = stablehlo.add %3341, %3342 : tensor<1x16x128x32xbf16> loc(#loc3441)
      %3344 = stablehlo.concatenate %3340, %3343, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3442)
      %3345 = stablehlo.reshape %arg1020 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3443)
      %3346 = stablehlo.reshape %3345 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3444)
      %3347 = stablehlo.transpose %3346, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3445)
      %3348 = stablehlo.dot_general %3323, %3347, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3446)
      %3349 = "stablehlo.all_reduce"(%3348) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8306"), %arg1239: tensor<bf16> loc("dot.8306")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3446)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3446)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3446)
      %3350 = stablehlo.reshape %3349 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3447)
      %3351 = stablehlo.reshape %arg1019 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3448)
      %3352 = stablehlo.reshape %3351 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3449)
      %3353 = stablehlo.broadcast_in_dim %3352, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3450)
      %3354 = stablehlo.add %3350, %3353 : tensor<1x128x128xbf16> loc(#loc3451)
      %3355 = stablehlo.reshape %3354 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3452)
      %3356 = stablehlo.transpose %3355, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3453)
      %3357 = stablehlo.slice %3356 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3454)
      %3358 = stablehlo.multiply %3357, %90 : tensor<1x2x128x32xbf16> loc(#loc3455)
      %3359 = stablehlo.slice %3356 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3456)
      %3360 = stablehlo.multiply %3359, %93 : tensor<1x2x128x32xbf16> loc(#loc3457)
      %3361 = stablehlo.subtract %3358, %3360 : tensor<1x2x128x32xbf16> loc(#loc3458)
      %3362 = stablehlo.multiply %3359, %90 : tensor<1x2x128x32xbf16> loc(#loc3459)
      %3363 = stablehlo.multiply %3357, %93 : tensor<1x2x128x32xbf16> loc(#loc3460)
      %3364 = stablehlo.add %3362, %3363 : tensor<1x2x128x32xbf16> loc(#loc3461)
      %3365 = stablehlo.concatenate %3361, %3364, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3462)
      %3366 = stablehlo.broadcast_in_dim %3365, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3463)
      %3367 = stablehlo.reshape %3366 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3464)
      %3368 = stablehlo.transpose %3367, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc3465)
      %3369 = stablehlo.dot_general %3344, %3368, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3466)
      %3370 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %3371 = stablehlo.multiply %3369, %3370 : tensor<1x16x128x128xbf16> loc(#loc3467)
      %3372 = stablehlo.add %3371, %128 : tensor<1x16x128x128xbf16> loc(#loc3468)
      %3373 = stablehlo.reshape %arg1018 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc400)
      %3374 = "stablehlo.all_to_all"(%3373) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc400)
      %3375 = stablehlo.slice %3374 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc400)
      %3376 = stablehlo.reshape %3375 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc400)
      %3377 = stablehlo.reshape %3376 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3469)
      %3378 = stablehlo.reshape %3377 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc3470)
      %3379 = stablehlo.broadcast_in_dim %3378, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc3471)
      %3380 = stablehlo.concatenate %3372, %3379, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3472)
      %3381 = stablehlo.reshape %arg1028 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3473)
      %3382 = stablehlo.reshape %3381 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3474)
      %3383 = stablehlo.convert %3382 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3475)
      %3384 = stablehlo.broadcast_in_dim %3383, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3476)
      %3385 = stablehlo.reduce(%3380 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3477)
      %3386 = stablehlo.broadcast_in_dim %3385, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3478)
      %3387 = stablehlo.subtract %3380, %3386 : tensor<1x16x128x129xbf16> loc(#loc3479)
      %3388 = stablehlo.reduce(%3387 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3480)
      %3389 = stablehlo.broadcast_in_dim %3388, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3481)
      %3390 = stablehlo.subtract %3387, %3389 : tensor<1x16x128x129xbf16> loc(#loc3482)
      %3391 = stablehlo.exponential %3390 : tensor<1x16x128x129xbf16> loc(#loc3483)
      %3392 = stablehlo.reduce(%3391 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3484)
      %3393 = stablehlo.broadcast_in_dim %3392, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3485)
      %3394 = stablehlo.divide %3391, %3393 : tensor<1x16x128x129xbf16> loc(#loc3486)
      %3395 = stablehlo.slice %3394 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3487)
      %3396 = stablehlo.reshape %arg739 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3488)
      %3397 = stablehlo.reshape %3396 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3489)
      %3398 = stablehlo.transpose %3397, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3490)
      %3399 = stablehlo.dot_general %3323, %3398, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3491)
      %3400 = "stablehlo.all_reduce"(%3399) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8269"), %arg1239: tensor<bf16> loc("dot.8269")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3491)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3491)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3491)
      %3401 = stablehlo.reshape %3400 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3492)
      %3402 = stablehlo.reshape %arg738 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3493)
      %3403 = stablehlo.reshape %3402 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3494)
      %3404 = stablehlo.broadcast_in_dim %3403, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3495)
      %3405 = stablehlo.add %3401, %3404 : tensor<1x128x128xbf16> loc(#loc3496)
      %3406 = stablehlo.reshape %3405 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3497)
      %3407 = stablehlo.transpose %3406, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3498)
      %3408 = stablehlo.broadcast_in_dim %3407, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3499)
      %3409 = stablehlo.reshape %3408 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3500)
      %3410 = stablehlo.dot_general %3395, %3409, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3501)
      %3411 = stablehlo.transpose %3410, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3502)
      %3412 = stablehlo.reshape %3411 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc3503)
      %3413 = stablehlo.reshape %arg737 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc3504)
      %3414 = stablehlo.reshape %3413 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc3505)
      %3415 = stablehlo.transpose %3414, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc3506)
      %3416 = stablehlo.dot_general %3412, %3415, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc3507)
      %3417 = "stablehlo.all_reduce"(%3416) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8462"), %arg1239: tensor<bf16> loc("dot.8462")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3507)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3507)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3507)
      %3418 = stablehlo.reshape %3417 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3508)
      %3419 = stablehlo.reshape %arg736 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3509)
      %3420 = stablehlo.reshape %3419 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3510)
      %3421 = stablehlo.broadcast_in_dim %3420, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3511)
      %3422 = stablehlo.add %3418, %3421 : tensor<1x128x360xbf16> loc(#loc3512)
      %3423 = stablehlo.add %3306, %3422 : tensor<1x128x360xbf16> loc(#loc3513)
      %3424 = stablehlo.reshape %arg1023 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3514)
      %3425 = stablehlo.reshape %3424 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3515)
      %3426 = stablehlo.convert %3425 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3516)
      %3427 = stablehlo.broadcast_in_dim %3426, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3517)
      %3428 = stablehlo.convert %3423 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3518)
      %3429 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3430 = stablehlo.power %3428, %3429 : tensor<1x128x360xf32> loc(#loc3519)
      %3431 = stablehlo.reduce(%3430 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3520)
      %3432 = "stablehlo.all_reduce"(%3431) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.8480"), %arg1239: tensor<f32> loc("reduce.8480")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3520)
        stablehlo.return %7360 : tensor<f32> loc(#loc3520)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3520)
      %3433 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3434 = stablehlo.multiply %3432, %3433 : tensor<1x128xf32> loc(#loc3521)
      %3435 = stablehlo.reshape %3434 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3522)
      %3436 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3437 = stablehlo.add %3435, %3436 : tensor<1x128x1xf32> loc(#loc3523)
      %3438 = stablehlo.rsqrt %3437 : tensor<1x128x1xf32> loc(#loc3524)
      %3439 = stablehlo.reshape %3438 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3525)
      %3440 = stablehlo.broadcast_in_dim %3439, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3526)
      %3441 = stablehlo.multiply %3428, %3440 : tensor<1x128x360xf32> loc(#loc3527)
      %3442 = stablehlo.multiply %3427, %3441 : tensor<1x128x360xf32> loc(#loc3528)
      %3443 = stablehlo.convert %3442 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3529)
      %3444 = stablehlo.reshape %3443 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3530)
      %3445 = stablehlo.broadcast_in_dim %3444, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3531)
      %3446 = stablehlo.dot_general %3445, %arg1027, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3532)
      %3447 = "stablehlo.all_reduce"(%3446) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8592"), %arg1239: tensor<bf16> loc("dot.8592")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3532)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3532)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3532)
      %3448 = stablehlo.reshape %arg1026 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc3533)
      %3449 = stablehlo.reshape %3448 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc3534)
      %3450 = stablehlo.broadcast_in_dim %3449, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3535)
      %3451 = stablehlo.add %3447, %3450 : tensor<32x128x5760xbf16> loc(#loc3536)
      %3452 = stablehlo.slice %3451 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3537)
      %3453 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3454 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3455 = stablehlo.clamp %3454, %3452, %3453 : tensor<32x128x2880xbf16> loc(#loc3538)
      %3456 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3457 = stablehlo.add %3455, %3456 : tensor<32x128x2880xbf16> loc(#loc3539)
      %3458 = stablehlo.slice %3451 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3540)
      %3459 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3460 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3461 = stablehlo.clamp %3459, %3458, %3460 : tensor<32x128x2880xbf16> loc(#loc3541)
      %3462 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3463 = stablehlo.multiply %3461, %3462 : tensor<32x128x2880xbf16> loc(#loc3542)
      %3464 = stablehlo.logistic %3463 : tensor<32x128x2880xbf16> loc(#loc3543)
      %3465 = stablehlo.multiply %3461, %3464 : tensor<32x128x2880xbf16> loc(#loc3544)
      %3466 = stablehlo.multiply %3457, %3465 : tensor<32x128x2880xbf16> loc(#loc3545)
      %3467 = stablehlo.dot_general %3466, %arg1025, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3546)
      %3468 = stablehlo.reshape %arg1024 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc3547)
      %3469 = stablehlo.reshape %3468 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc3548)
      %3470 = stablehlo.broadcast_in_dim %3469, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3549)
      %3471 = stablehlo.add %3467, %3470 : tensor<32x128x360xbf16> loc(#loc3550)
      %3472 = stablehlo.reshape %3471 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3551)
      %3473 = stablehlo.reshape %arg735 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3552)
      %3474 = stablehlo.reshape %3473 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3553)
      %3475 = stablehlo.transpose %3474, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3554)
      %3476 = stablehlo.dot_general %3444, %3475, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3555)
      %3477 = "stablehlo.all_reduce"(%3476) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8508"), %arg1239: tensor<bf16> loc("dot.8508")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3555)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3555)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3555)
      %3478 = stablehlo.reshape %arg734 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3556)
      %3479 = stablehlo.reshape %3478 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3557)
      %3480 = stablehlo.broadcast_in_dim %3479, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc3558)
      %3481 = stablehlo.add %3477, %3480 : tensor<128x128xbf16> loc(#loc3559)
      %3482:2 = "stablehlo.sort"(%3481, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.8528"), %arg1239: tensor<bf16> loc("sort.8528"), %arg1240: tensor<i32> loc("sort.8528"), %arg1241: tensor<i32> loc("sort.8528")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3561)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc3560)
      %3483 = stablehlo.slice %3482#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc3562)
      %3484 = stablehlo.convert %3483 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc3563)
      %3485 = stablehlo.reshape %3484 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc3564)
      %3486 = stablehlo.concatenate %231, %3485, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc3565)
      %3487 = stablehlo.slice %3482#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc3566)
      %3488 = stablehlo.reduce(%3487 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3567)
      %3489 = stablehlo.broadcast_in_dim %3488, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3568)
      %3490 = stablehlo.subtract %3487, %3489 : tensor<128x4xbf16> loc(#loc3569)
      %3491 = stablehlo.exponential %3490 : tensor<128x4xbf16> loc(#loc3570)
      %3492 = stablehlo.reduce(%3491 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3571)
      %3493 = stablehlo.broadcast_in_dim %3492, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3572)
      %3494 = stablehlo.divide %3491, %3493 : tensor<128x4xbf16> loc(#loc3573)
      %3495 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %3496 = "stablehlo.all_gather"(%3495) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %3497 = "stablehlo.scatter"(%3496, %3486, %3494) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.8562"), %arg1239: tensor<bf16> loc("scatter.8562")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc3574)
      %3498 = stablehlo.reshape %3497 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc3574)
      %3499 = "stablehlo.all_to_all"(%3498) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc3574)
      %3500 = stablehlo.slice %3499 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc3574)
      %3501 = stablehlo.reshape %3500 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc3574)
      %3502 = stablehlo.transpose %3501, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc3575)
      %3503 = stablehlo.reshape %3502 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc3576)
      %3504 = stablehlo.broadcast_in_dim %3503, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3577)
      %3505 = stablehlo.multiply %3472, %3504 : tensor<32x1x128x360xbf16> loc(#loc3578)
      %3506 = stablehlo.reduce(%3505 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc3579)
      %3507 = "stablehlo.all_reduce"(%3506) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.8634"), %arg1239: tensor<bf16> loc("reduce.8634")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3579)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3579)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3579)
      %3508 = stablehlo.add %3423, %3507 : tensor<1x128x360xbf16> loc(#loc3580)
      %3509 = stablehlo.convert %3508 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3581)
      %3510 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3511 = stablehlo.power %3509, %3510 : tensor<1x128x360xf32> loc(#loc3582)
      %3512 = stablehlo.reduce(%3511 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3583)
      %3513 = "stablehlo.all_reduce"(%3512) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.8647"), %arg1239: tensor<f32> loc("reduce.8647")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3583)
        stablehlo.return %7360 : tensor<f32> loc(#loc3583)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3583)
      %3514 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3515 = stablehlo.multiply %3513, %3514 : tensor<1x128xf32> loc(#loc3584)
      %3516 = stablehlo.reshape %3515 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3585)
      %3517 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3518 = stablehlo.add %3516, %3517 : tensor<1x128x1xf32> loc(#loc3586)
      %3519 = stablehlo.rsqrt %3518 : tensor<1x128x1xf32> loc(#loc3587)
      %3520 = stablehlo.reshape %3519 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3588)
      %3521 = stablehlo.broadcast_in_dim %3520, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3589)
      %3522 = stablehlo.multiply %3509, %3521 : tensor<1x128x360xf32> loc(#loc3590)
      %3523 = stablehlo.multiply %3384, %3522 : tensor<1x128x360xf32> loc(#loc3591)
      %3524 = stablehlo.convert %3523 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3592)
      %3525 = stablehlo.reshape %3524 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3593)
      %3526 = stablehlo.reshape %arg1033 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc3594)
      %3527 = stablehlo.reshape %3526 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc3595)
      %3528 = stablehlo.transpose %3527, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc3596)
      %3529 = stablehlo.dot_general %3525, %3528, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3597)
      %3530 = "stablehlo.all_reduce"(%3529) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8761"), %arg1239: tensor<bf16> loc("dot.8761")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3597)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3597)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3597)
      %3531 = stablehlo.reshape %3530 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3598)
      %3532 = stablehlo.reshape %arg1032 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3599)
      %3533 = stablehlo.reshape %3532 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc3600)
      %3534 = stablehlo.broadcast_in_dim %3533, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3601)
      %3535 = stablehlo.add %3531, %3534 : tensor<1x128x1024xbf16> loc(#loc3602)
      %3536 = stablehlo.reshape %3535 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3603)
      %3537 = stablehlo.transpose %3536, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3604)
      %3538 = stablehlo.slice %3537 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3605)
      %3539 = stablehlo.multiply %3538, %64 : tensor<1x16x128x32xbf16> loc(#loc3606)
      %3540 = stablehlo.slice %3537 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3607)
      %3541 = stablehlo.multiply %3540, %70 : tensor<1x16x128x32xbf16> loc(#loc3608)
      %3542 = stablehlo.subtract %3539, %3541 : tensor<1x16x128x32xbf16> loc(#loc3609)
      %3543 = stablehlo.multiply %3540, %64 : tensor<1x16x128x32xbf16> loc(#loc3610)
      %3544 = stablehlo.multiply %3538, %70 : tensor<1x16x128x32xbf16> loc(#loc3611)
      %3545 = stablehlo.add %3543, %3544 : tensor<1x16x128x32xbf16> loc(#loc3612)
      %3546 = stablehlo.concatenate %3542, %3545, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3613)
      %3547 = stablehlo.reshape %arg1031 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3614)
      %3548 = stablehlo.reshape %3547 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3615)
      %3549 = stablehlo.transpose %3548, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3616)
      %3550 = stablehlo.dot_general %3525, %3549, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3617)
      %3551 = "stablehlo.all_reduce"(%3550) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8712"), %arg1239: tensor<bf16> loc("dot.8712")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3617)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3617)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3617)
      %3552 = stablehlo.reshape %3551 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3618)
      %3553 = stablehlo.reshape %arg1030 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3619)
      %3554 = stablehlo.reshape %3553 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3620)
      %3555 = stablehlo.broadcast_in_dim %3554, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3621)
      %3556 = stablehlo.add %3552, %3555 : tensor<1x128x128xbf16> loc(#loc3622)
      %3557 = stablehlo.reshape %3556 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3623)
      %3558 = stablehlo.transpose %3557, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3624)
      %3559 = stablehlo.slice %3558 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3625)
      %3560 = stablehlo.multiply %3559, %90 : tensor<1x2x128x32xbf16> loc(#loc3626)
      %3561 = stablehlo.slice %3558 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3627)
      %3562 = stablehlo.multiply %3561, %93 : tensor<1x2x128x32xbf16> loc(#loc3628)
      %3563 = stablehlo.subtract %3560, %3562 : tensor<1x2x128x32xbf16> loc(#loc3629)
      %3564 = stablehlo.multiply %3561, %90 : tensor<1x2x128x32xbf16> loc(#loc3630)
      %3565 = stablehlo.multiply %3559, %93 : tensor<1x2x128x32xbf16> loc(#loc3631)
      %3566 = stablehlo.add %3564, %3565 : tensor<1x2x128x32xbf16> loc(#loc3632)
      %3567 = stablehlo.concatenate %3563, %3566, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3633)
      %3568 = stablehlo.broadcast_in_dim %3567, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3634)
      %3569 = stablehlo.reshape %3568 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3635)
      %3570 = stablehlo.transpose %3569, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc3636)
      %3571 = stablehlo.dot_general %3546, %3570, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3637)
      %3572 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %3573 = stablehlo.multiply %3571, %3572 : tensor<1x16x128x128xbf16> loc(#loc3638)
      %3574 = stablehlo.add %3573, %341 : tensor<1x16x128x128xbf16> loc(#loc3639)
      %3575 = stablehlo.reshape %arg1029 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc411)
      %3576 = "stablehlo.all_to_all"(%3575) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc411)
      %3577 = stablehlo.slice %3576 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc411)
      %3578 = stablehlo.reshape %3577 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc411)
      %3579 = stablehlo.reshape %3578 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3640)
      %3580 = stablehlo.reshape %3579 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc3641)
      %3581 = stablehlo.broadcast_in_dim %3580, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc3642)
      %3582 = stablehlo.concatenate %3574, %3581, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3643)
      %3583 = stablehlo.reshape %arg1039 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3644)
      %3584 = stablehlo.reshape %3583 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3645)
      %3585 = stablehlo.convert %3584 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3646)
      %3586 = stablehlo.broadcast_in_dim %3585, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3647)
      %3587 = stablehlo.reduce(%3582 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3648)
      %3588 = stablehlo.broadcast_in_dim %3587, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3649)
      %3589 = stablehlo.subtract %3582, %3588 : tensor<1x16x128x129xbf16> loc(#loc3650)
      %3590 = stablehlo.reduce(%3589 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3651)
      %3591 = stablehlo.broadcast_in_dim %3590, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3652)
      %3592 = stablehlo.subtract %3589, %3591 : tensor<1x16x128x129xbf16> loc(#loc3653)
      %3593 = stablehlo.exponential %3592 : tensor<1x16x128x129xbf16> loc(#loc3654)
      %3594 = stablehlo.reduce(%3593 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3655)
      %3595 = stablehlo.broadcast_in_dim %3594, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3656)
      %3596 = stablehlo.divide %3593, %3595 : tensor<1x16x128x129xbf16> loc(#loc3657)
      %3597 = stablehlo.slice %3596 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3658)
      %3598 = stablehlo.reshape %arg733 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3659)
      %3599 = stablehlo.reshape %3598 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3660)
      %3600 = stablehlo.transpose %3599, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3661)
      %3601 = stablehlo.dot_general %3525, %3600, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3662)
      %3602 = "stablehlo.all_reduce"(%3601) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8675"), %arg1239: tensor<bf16> loc("dot.8675")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3662)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3662)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3662)
      %3603 = stablehlo.reshape %3602 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3663)
      %3604 = stablehlo.reshape %arg732 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3664)
      %3605 = stablehlo.reshape %3604 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3665)
      %3606 = stablehlo.broadcast_in_dim %3605, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3666)
      %3607 = stablehlo.add %3603, %3606 : tensor<1x128x128xbf16> loc(#loc3667)
      %3608 = stablehlo.reshape %3607 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3668)
      %3609 = stablehlo.transpose %3608, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3669)
      %3610 = stablehlo.broadcast_in_dim %3609, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3670)
      %3611 = stablehlo.reshape %3610 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3671)
      %3612 = stablehlo.dot_general %3597, %3611, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3672)
      %3613 = stablehlo.transpose %3612, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3673)
      %3614 = stablehlo.reshape %3613 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc3674)
      %3615 = stablehlo.reshape %arg731 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc3675)
      %3616 = stablehlo.reshape %3615 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc3676)
      %3617 = stablehlo.transpose %3616, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc3677)
      %3618 = stablehlo.dot_general %3614, %3617, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc3678)
      %3619 = "stablehlo.all_reduce"(%3618) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8868"), %arg1239: tensor<bf16> loc("dot.8868")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3678)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3678)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3678)
      %3620 = stablehlo.reshape %3619 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3679)
      %3621 = stablehlo.reshape %arg730 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3680)
      %3622 = stablehlo.reshape %3621 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3681)
      %3623 = stablehlo.broadcast_in_dim %3622, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3682)
      %3624 = stablehlo.add %3620, %3623 : tensor<1x128x360xbf16> loc(#loc3683)
      %3625 = stablehlo.add %3508, %3624 : tensor<1x128x360xbf16> loc(#loc3684)
      %3626 = stablehlo.reshape %arg1034 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3685)
      %3627 = stablehlo.reshape %3626 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3686)
      %3628 = stablehlo.convert %3627 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3687)
      %3629 = stablehlo.broadcast_in_dim %3628, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3688)
      %3630 = stablehlo.convert %3625 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3689)
      %3631 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3632 = stablehlo.power %3630, %3631 : tensor<1x128x360xf32> loc(#loc3690)
      %3633 = stablehlo.reduce(%3632 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3691)
      %3634 = "stablehlo.all_reduce"(%3633) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.8886"), %arg1239: tensor<f32> loc("reduce.8886")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3691)
        stablehlo.return %7360 : tensor<f32> loc(#loc3691)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3691)
      %3635 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3636 = stablehlo.multiply %3634, %3635 : tensor<1x128xf32> loc(#loc3692)
      %3637 = stablehlo.reshape %3636 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3693)
      %3638 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3639 = stablehlo.add %3637, %3638 : tensor<1x128x1xf32> loc(#loc3694)
      %3640 = stablehlo.rsqrt %3639 : tensor<1x128x1xf32> loc(#loc3695)
      %3641 = stablehlo.reshape %3640 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3696)
      %3642 = stablehlo.broadcast_in_dim %3641, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3697)
      %3643 = stablehlo.multiply %3630, %3642 : tensor<1x128x360xf32> loc(#loc3698)
      %3644 = stablehlo.multiply %3629, %3643 : tensor<1x128x360xf32> loc(#loc3699)
      %3645 = stablehlo.convert %3644 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3700)
      %3646 = stablehlo.reshape %3645 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3701)
      %3647 = stablehlo.broadcast_in_dim %3646, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3702)
      %3648 = stablehlo.dot_general %3647, %arg1038, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3703)
      %3649 = "stablehlo.all_reduce"(%3648) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8998"), %arg1239: tensor<bf16> loc("dot.8998")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3703)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3703)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3703)
      %3650 = stablehlo.reshape %arg1037 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc3704)
      %3651 = stablehlo.reshape %3650 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc3705)
      %3652 = stablehlo.broadcast_in_dim %3651, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3706)
      %3653 = stablehlo.add %3649, %3652 : tensor<32x128x5760xbf16> loc(#loc3707)
      %3654 = stablehlo.slice %3653 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3708)
      %3655 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3656 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3657 = stablehlo.clamp %3656, %3654, %3655 : tensor<32x128x2880xbf16> loc(#loc3709)
      %3658 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3659 = stablehlo.add %3657, %3658 : tensor<32x128x2880xbf16> loc(#loc3710)
      %3660 = stablehlo.slice %3653 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3711)
      %3661 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3662 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3663 = stablehlo.clamp %3661, %3660, %3662 : tensor<32x128x2880xbf16> loc(#loc3712)
      %3664 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3665 = stablehlo.multiply %3663, %3664 : tensor<32x128x2880xbf16> loc(#loc3713)
      %3666 = stablehlo.logistic %3665 : tensor<32x128x2880xbf16> loc(#loc3714)
      %3667 = stablehlo.multiply %3663, %3666 : tensor<32x128x2880xbf16> loc(#loc3715)
      %3668 = stablehlo.multiply %3659, %3667 : tensor<32x128x2880xbf16> loc(#loc3716)
      %3669 = stablehlo.dot_general %3668, %arg1036, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3717)
      %3670 = stablehlo.reshape %arg1035 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc3718)
      %3671 = stablehlo.reshape %3670 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc3719)
      %3672 = stablehlo.broadcast_in_dim %3671, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3720)
      %3673 = stablehlo.add %3669, %3672 : tensor<32x128x360xbf16> loc(#loc3721)
      %3674 = stablehlo.reshape %3673 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3722)
      %3675 = stablehlo.reshape %arg729 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3723)
      %3676 = stablehlo.reshape %3675 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3724)
      %3677 = stablehlo.transpose %3676, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3725)
      %3678 = stablehlo.dot_general %3646, %3677, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3726)
      %3679 = "stablehlo.all_reduce"(%3678) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.8914"), %arg1239: tensor<bf16> loc("dot.8914")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3726)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3726)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3726)
      %3680 = stablehlo.reshape %arg728 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3727)
      %3681 = stablehlo.reshape %3680 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3728)
      %3682 = stablehlo.broadcast_in_dim %3681, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc3729)
      %3683 = stablehlo.add %3679, %3682 : tensor<128x128xbf16> loc(#loc3730)
      %3684:2 = "stablehlo.sort"(%3683, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.8934"), %arg1239: tensor<bf16> loc("sort.8934"), %arg1240: tensor<i32> loc("sort.8934"), %arg1241: tensor<i32> loc("sort.8934")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3732)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc3731)
      %3685 = stablehlo.slice %3684#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc3733)
      %3686 = stablehlo.convert %3685 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc3734)
      %3687 = stablehlo.reshape %3686 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc3735)
      %3688 = stablehlo.concatenate %231, %3687, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc3736)
      %3689 = stablehlo.slice %3684#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc3737)
      %3690 = stablehlo.reduce(%3689 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3738)
      %3691 = stablehlo.broadcast_in_dim %3690, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3739)
      %3692 = stablehlo.subtract %3689, %3691 : tensor<128x4xbf16> loc(#loc3740)
      %3693 = stablehlo.exponential %3692 : tensor<128x4xbf16> loc(#loc3741)
      %3694 = stablehlo.reduce(%3693 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3742)
      %3695 = stablehlo.broadcast_in_dim %3694, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3743)
      %3696 = stablehlo.divide %3693, %3695 : tensor<128x4xbf16> loc(#loc3744)
      %3697 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %3698 = "stablehlo.all_gather"(%3697) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %3699 = "stablehlo.scatter"(%3698, %3688, %3696) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.8968"), %arg1239: tensor<bf16> loc("scatter.8968")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc3745)
      %3700 = stablehlo.reshape %3699 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc3745)
      %3701 = "stablehlo.all_to_all"(%3700) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc3745)
      %3702 = stablehlo.slice %3701 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc3745)
      %3703 = stablehlo.reshape %3702 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc3745)
      %3704 = stablehlo.transpose %3703, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc3746)
      %3705 = stablehlo.reshape %3704 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc3747)
      %3706 = stablehlo.broadcast_in_dim %3705, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3748)
      %3707 = stablehlo.multiply %3674, %3706 : tensor<32x1x128x360xbf16> loc(#loc3749)
      %3708 = stablehlo.reduce(%3707 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc3750)
      %3709 = "stablehlo.all_reduce"(%3708) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.9040"), %arg1239: tensor<bf16> loc("reduce.9040")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3750)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3750)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3750)
      %3710 = stablehlo.add %3625, %3709 : tensor<1x128x360xbf16> loc(#loc3751)
      %3711 = stablehlo.convert %3710 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3752)
      %3712 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3713 = stablehlo.power %3711, %3712 : tensor<1x128x360xf32> loc(#loc3753)
      %3714 = stablehlo.reduce(%3713 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3754)
      %3715 = "stablehlo.all_reduce"(%3714) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.9053"), %arg1239: tensor<f32> loc("reduce.9053")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3754)
        stablehlo.return %7360 : tensor<f32> loc(#loc3754)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3754)
      %3716 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3717 = stablehlo.multiply %3715, %3716 : tensor<1x128xf32> loc(#loc3755)
      %3718 = stablehlo.reshape %3717 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3756)
      %3719 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3720 = stablehlo.add %3718, %3719 : tensor<1x128x1xf32> loc(#loc3757)
      %3721 = stablehlo.rsqrt %3720 : tensor<1x128x1xf32> loc(#loc3758)
      %3722 = stablehlo.reshape %3721 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3759)
      %3723 = stablehlo.broadcast_in_dim %3722, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3760)
      %3724 = stablehlo.multiply %3711, %3723 : tensor<1x128x360xf32> loc(#loc3761)
      %3725 = stablehlo.multiply %3586, %3724 : tensor<1x128x360xf32> loc(#loc3762)
      %3726 = stablehlo.convert %3725 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3763)
      %3727 = stablehlo.reshape %3726 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3764)
      %3728 = stablehlo.reshape %arg1044 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc3765)
      %3729 = stablehlo.reshape %3728 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc3766)
      %3730 = stablehlo.transpose %3729, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc3767)
      %3731 = stablehlo.dot_general %3727, %3730, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3768)
      %3732 = "stablehlo.all_reduce"(%3731) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9167"), %arg1239: tensor<bf16> loc("dot.9167")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3768)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3768)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3768)
      %3733 = stablehlo.reshape %3732 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3769)
      %3734 = stablehlo.reshape %arg1043 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3770)
      %3735 = stablehlo.reshape %3734 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc3771)
      %3736 = stablehlo.broadcast_in_dim %3735, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3772)
      %3737 = stablehlo.add %3733, %3736 : tensor<1x128x1024xbf16> loc(#loc3773)
      %3738 = stablehlo.reshape %3737 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3774)
      %3739 = stablehlo.transpose %3738, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3775)
      %3740 = stablehlo.slice %3739 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3776)
      %3741 = stablehlo.multiply %3740, %64 : tensor<1x16x128x32xbf16> loc(#loc3777)
      %3742 = stablehlo.slice %3739 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3778)
      %3743 = stablehlo.multiply %3742, %70 : tensor<1x16x128x32xbf16> loc(#loc3779)
      %3744 = stablehlo.subtract %3741, %3743 : tensor<1x16x128x32xbf16> loc(#loc3780)
      %3745 = stablehlo.multiply %3742, %64 : tensor<1x16x128x32xbf16> loc(#loc3781)
      %3746 = stablehlo.multiply %3740, %70 : tensor<1x16x128x32xbf16> loc(#loc3782)
      %3747 = stablehlo.add %3745, %3746 : tensor<1x16x128x32xbf16> loc(#loc3783)
      %3748 = stablehlo.concatenate %3744, %3747, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3784)
      %3749 = stablehlo.reshape %arg1042 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3785)
      %3750 = stablehlo.reshape %3749 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3786)
      %3751 = stablehlo.transpose %3750, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3787)
      %3752 = stablehlo.dot_general %3727, %3751, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3788)
      %3753 = "stablehlo.all_reduce"(%3752) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9118"), %arg1239: tensor<bf16> loc("dot.9118")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3788)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3788)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3788)
      %3754 = stablehlo.reshape %3753 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3789)
      %3755 = stablehlo.reshape %arg1041 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3790)
      %3756 = stablehlo.reshape %3755 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3791)
      %3757 = stablehlo.broadcast_in_dim %3756, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3792)
      %3758 = stablehlo.add %3754, %3757 : tensor<1x128x128xbf16> loc(#loc3793)
      %3759 = stablehlo.reshape %3758 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3794)
      %3760 = stablehlo.transpose %3759, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3795)
      %3761 = stablehlo.slice %3760 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3796)
      %3762 = stablehlo.multiply %3761, %90 : tensor<1x2x128x32xbf16> loc(#loc3797)
      %3763 = stablehlo.slice %3760 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3798)
      %3764 = stablehlo.multiply %3763, %93 : tensor<1x2x128x32xbf16> loc(#loc3799)
      %3765 = stablehlo.subtract %3762, %3764 : tensor<1x2x128x32xbf16> loc(#loc3800)
      %3766 = stablehlo.multiply %3763, %90 : tensor<1x2x128x32xbf16> loc(#loc3801)
      %3767 = stablehlo.multiply %3761, %93 : tensor<1x2x128x32xbf16> loc(#loc3802)
      %3768 = stablehlo.add %3766, %3767 : tensor<1x2x128x32xbf16> loc(#loc3803)
      %3769 = stablehlo.concatenate %3765, %3768, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3804)
      %3770 = stablehlo.broadcast_in_dim %3769, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3805)
      %3771 = stablehlo.reshape %3770 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3806)
      %3772 = stablehlo.transpose %3771, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc3807)
      %3773 = stablehlo.dot_general %3748, %3772, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3808)
      %3774 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %3775 = stablehlo.multiply %3773, %3774 : tensor<1x16x128x128xbf16> loc(#loc3809)
      %3776 = stablehlo.add %3775, %128 : tensor<1x16x128x128xbf16> loc(#loc3810)
      %3777 = stablehlo.reshape %arg1040 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc422)
      %3778 = "stablehlo.all_to_all"(%3777) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc422)
      %3779 = stablehlo.slice %3778 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc422)
      %3780 = stablehlo.reshape %3779 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc422)
      %3781 = stablehlo.reshape %3780 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3811)
      %3782 = stablehlo.reshape %3781 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc3812)
      %3783 = stablehlo.broadcast_in_dim %3782, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc3813)
      %3784 = stablehlo.concatenate %3776, %3783, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3814)
      %3785 = stablehlo.reshape %arg1050 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3815)
      %3786 = stablehlo.reshape %3785 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3816)
      %3787 = stablehlo.convert %3786 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3817)
      %3788 = stablehlo.broadcast_in_dim %3787, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3818)
      %3789 = stablehlo.reduce(%3784 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3819)
      %3790 = stablehlo.broadcast_in_dim %3789, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3820)
      %3791 = stablehlo.subtract %3784, %3790 : tensor<1x16x128x129xbf16> loc(#loc3821)
      %3792 = stablehlo.reduce(%3791 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3822)
      %3793 = stablehlo.broadcast_in_dim %3792, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3823)
      %3794 = stablehlo.subtract %3791, %3793 : tensor<1x16x128x129xbf16> loc(#loc3824)
      %3795 = stablehlo.exponential %3794 : tensor<1x16x128x129xbf16> loc(#loc3825)
      %3796 = stablehlo.reduce(%3795 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3826)
      %3797 = stablehlo.broadcast_in_dim %3796, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3827)
      %3798 = stablehlo.divide %3795, %3797 : tensor<1x16x128x129xbf16> loc(#loc3828)
      %3799 = stablehlo.slice %3798 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3829)
      %3800 = stablehlo.reshape %arg727 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3830)
      %3801 = stablehlo.reshape %3800 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3831)
      %3802 = stablehlo.transpose %3801, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3832)
      %3803 = stablehlo.dot_general %3727, %3802, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3833)
      %3804 = "stablehlo.all_reduce"(%3803) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9081"), %arg1239: tensor<bf16> loc("dot.9081")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3833)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3833)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3833)
      %3805 = stablehlo.reshape %3804 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3834)
      %3806 = stablehlo.reshape %arg726 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3835)
      %3807 = stablehlo.reshape %3806 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3836)
      %3808 = stablehlo.broadcast_in_dim %3807, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3837)
      %3809 = stablehlo.add %3805, %3808 : tensor<1x128x128xbf16> loc(#loc3838)
      %3810 = stablehlo.reshape %3809 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3839)
      %3811 = stablehlo.transpose %3810, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3840)
      %3812 = stablehlo.broadcast_in_dim %3811, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3841)
      %3813 = stablehlo.reshape %3812 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3842)
      %3814 = stablehlo.dot_general %3799, %3813, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3843)
      %3815 = stablehlo.transpose %3814, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3844)
      %3816 = stablehlo.reshape %3815 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc3845)
      %3817 = stablehlo.reshape %arg725 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc3846)
      %3818 = stablehlo.reshape %3817 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc3847)
      %3819 = stablehlo.transpose %3818, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc3848)
      %3820 = stablehlo.dot_general %3816, %3819, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc3849)
      %3821 = "stablehlo.all_reduce"(%3820) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9274"), %arg1239: tensor<bf16> loc("dot.9274")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3849)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3849)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3849)
      %3822 = stablehlo.reshape %3821 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3850)
      %3823 = stablehlo.reshape %arg724 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3851)
      %3824 = stablehlo.reshape %3823 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3852)
      %3825 = stablehlo.broadcast_in_dim %3824, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3853)
      %3826 = stablehlo.add %3822, %3825 : tensor<1x128x360xbf16> loc(#loc3854)
      %3827 = stablehlo.add %3710, %3826 : tensor<1x128x360xbf16> loc(#loc3855)
      %3828 = stablehlo.reshape %arg1045 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3856)
      %3829 = stablehlo.reshape %3828 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3857)
      %3830 = stablehlo.convert %3829 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3858)
      %3831 = stablehlo.broadcast_in_dim %3830, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3859)
      %3832 = stablehlo.convert %3827 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3860)
      %3833 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3834 = stablehlo.power %3832, %3833 : tensor<1x128x360xf32> loc(#loc3861)
      %3835 = stablehlo.reduce(%3834 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3862)
      %3836 = "stablehlo.all_reduce"(%3835) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.9292"), %arg1239: tensor<f32> loc("reduce.9292")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3862)
        stablehlo.return %7360 : tensor<f32> loc(#loc3862)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3862)
      %3837 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3838 = stablehlo.multiply %3836, %3837 : tensor<1x128xf32> loc(#loc3863)
      %3839 = stablehlo.reshape %3838 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3864)
      %3840 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3841 = stablehlo.add %3839, %3840 : tensor<1x128x1xf32> loc(#loc3865)
      %3842 = stablehlo.rsqrt %3841 : tensor<1x128x1xf32> loc(#loc3866)
      %3843 = stablehlo.reshape %3842 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3867)
      %3844 = stablehlo.broadcast_in_dim %3843, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3868)
      %3845 = stablehlo.multiply %3832, %3844 : tensor<1x128x360xf32> loc(#loc3869)
      %3846 = stablehlo.multiply %3831, %3845 : tensor<1x128x360xf32> loc(#loc3870)
      %3847 = stablehlo.convert %3846 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3871)
      %3848 = stablehlo.reshape %3847 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3872)
      %3849 = stablehlo.broadcast_in_dim %3848, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3873)
      %3850 = stablehlo.dot_general %3849, %arg1049, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3874)
      %3851 = "stablehlo.all_reduce"(%3850) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9404"), %arg1239: tensor<bf16> loc("dot.9404")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3874)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3874)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3874)
      %3852 = stablehlo.reshape %arg1048 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc3875)
      %3853 = stablehlo.reshape %3852 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc3876)
      %3854 = stablehlo.broadcast_in_dim %3853, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc3877)
      %3855 = stablehlo.add %3851, %3854 : tensor<32x128x5760xbf16> loc(#loc3878)
      %3856 = stablehlo.slice %3855 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3879)
      %3857 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3858 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3859 = stablehlo.clamp %3858, %3856, %3857 : tensor<32x128x2880xbf16> loc(#loc3880)
      %3860 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3861 = stablehlo.add %3859, %3860 : tensor<32x128x2880xbf16> loc(#loc3881)
      %3862 = stablehlo.slice %3855 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc3882)
      %3863 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3864 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3865 = stablehlo.clamp %3863, %3862, %3864 : tensor<32x128x2880xbf16> loc(#loc3883)
      %3866 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %3867 = stablehlo.multiply %3865, %3866 : tensor<32x128x2880xbf16> loc(#loc3884)
      %3868 = stablehlo.logistic %3867 : tensor<32x128x2880xbf16> loc(#loc3885)
      %3869 = stablehlo.multiply %3865, %3868 : tensor<32x128x2880xbf16> loc(#loc3886)
      %3870 = stablehlo.multiply %3861, %3869 : tensor<32x128x2880xbf16> loc(#loc3887)
      %3871 = stablehlo.dot_general %3870, %arg1047, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3888)
      %3872 = stablehlo.reshape %arg1046 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc3889)
      %3873 = stablehlo.reshape %3872 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc3890)
      %3874 = stablehlo.broadcast_in_dim %3873, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc3891)
      %3875 = stablehlo.add %3871, %3874 : tensor<32x128x360xbf16> loc(#loc3892)
      %3876 = stablehlo.reshape %3875 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3893)
      %3877 = stablehlo.reshape %arg723 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3894)
      %3878 = stablehlo.reshape %3877 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3895)
      %3879 = stablehlo.transpose %3878, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3896)
      %3880 = stablehlo.dot_general %3848, %3879, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3897)
      %3881 = "stablehlo.all_reduce"(%3880) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9320"), %arg1239: tensor<bf16> loc("dot.9320")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3897)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3897)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3897)
      %3882 = stablehlo.reshape %arg722 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3898)
      %3883 = stablehlo.reshape %3882 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3899)
      %3884 = stablehlo.broadcast_in_dim %3883, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc3900)
      %3885 = stablehlo.add %3881, %3884 : tensor<128x128xbf16> loc(#loc3901)
      %3886:2 = "stablehlo.sort"(%3885, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.9340"), %arg1239: tensor<bf16> loc("sort.9340"), %arg1240: tensor<i32> loc("sort.9340"), %arg1241: tensor<i32> loc("sort.9340")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc3903)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc3902)
      %3887 = stablehlo.slice %3886#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc3904)
      %3888 = stablehlo.convert %3887 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc3905)
      %3889 = stablehlo.reshape %3888 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc3906)
      %3890 = stablehlo.concatenate %231, %3889, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc3907)
      %3891 = stablehlo.slice %3886#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc3908)
      %3892 = stablehlo.reduce(%3891 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3909)
      %3893 = stablehlo.broadcast_in_dim %3892, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3910)
      %3894 = stablehlo.subtract %3891, %3893 : tensor<128x4xbf16> loc(#loc3911)
      %3895 = stablehlo.exponential %3894 : tensor<128x4xbf16> loc(#loc3912)
      %3896 = stablehlo.reduce(%3895 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc3913)
      %3897 = stablehlo.broadcast_in_dim %3896, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc3914)
      %3898 = stablehlo.divide %3895, %3897 : tensor<128x4xbf16> loc(#loc3915)
      %3899 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %3900 = "stablehlo.all_gather"(%3899) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %3901 = "stablehlo.scatter"(%3900, %3890, %3898) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.9374"), %arg1239: tensor<bf16> loc("scatter.9374")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc3916)
      %3902 = stablehlo.reshape %3901 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc3916)
      %3903 = "stablehlo.all_to_all"(%3902) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc3916)
      %3904 = stablehlo.slice %3903 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc3916)
      %3905 = stablehlo.reshape %3904 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc3916)
      %3906 = stablehlo.transpose %3905, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc3917)
      %3907 = stablehlo.reshape %3906 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc3918)
      %3908 = stablehlo.broadcast_in_dim %3907, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc3919)
      %3909 = stablehlo.multiply %3876, %3908 : tensor<32x1x128x360xbf16> loc(#loc3920)
      %3910 = stablehlo.reduce(%3909 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc3921)
      %3911 = "stablehlo.all_reduce"(%3910) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.9446"), %arg1239: tensor<bf16> loc("reduce.9446")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3921)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3921)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3921)
      %3912 = stablehlo.add %3827, %3911 : tensor<1x128x360xbf16> loc(#loc3922)
      %3913 = stablehlo.convert %3912 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc3923)
      %3914 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %3915 = stablehlo.power %3913, %3914 : tensor<1x128x360xf32> loc(#loc3924)
      %3916 = stablehlo.reduce(%3915 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc3925)
      %3917 = "stablehlo.all_reduce"(%3916) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.9459"), %arg1239: tensor<f32> loc("reduce.9459")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc3925)
        stablehlo.return %7360 : tensor<f32> loc(#loc3925)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc3925)
      %3918 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %3919 = stablehlo.multiply %3917, %3918 : tensor<1x128xf32> loc(#loc3926)
      %3920 = stablehlo.reshape %3919 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc3927)
      %3921 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %3922 = stablehlo.add %3920, %3921 : tensor<1x128x1xf32> loc(#loc3928)
      %3923 = stablehlo.rsqrt %3922 : tensor<1x128x1xf32> loc(#loc3929)
      %3924 = stablehlo.reshape %3923 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc3930)
      %3925 = stablehlo.broadcast_in_dim %3924, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc3931)
      %3926 = stablehlo.multiply %3913, %3925 : tensor<1x128x360xf32> loc(#loc3932)
      %3927 = stablehlo.multiply %3788, %3926 : tensor<1x128x360xf32> loc(#loc3933)
      %3928 = stablehlo.convert %3927 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc3934)
      %3929 = stablehlo.reshape %3928 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3935)
      %3930 = stablehlo.reshape %arg1055 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc3936)
      %3931 = stablehlo.reshape %3930 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc3937)
      %3932 = stablehlo.transpose %3931, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc3938)
      %3933 = stablehlo.dot_general %3929, %3932, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3939)
      %3934 = "stablehlo.all_reduce"(%3933) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9573"), %arg1239: tensor<bf16> loc("dot.9573")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3939)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3939)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc3939)
      %3935 = stablehlo.reshape %3934 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3940)
      %3936 = stablehlo.reshape %arg1054 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc3941)
      %3937 = stablehlo.reshape %3936 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc3942)
      %3938 = stablehlo.broadcast_in_dim %3937, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc3943)
      %3939 = stablehlo.add %3935, %3938 : tensor<1x128x1024xbf16> loc(#loc3944)
      %3940 = stablehlo.reshape %3939 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc3945)
      %3941 = stablehlo.transpose %3940, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3946)
      %3942 = stablehlo.slice %3941 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3947)
      %3943 = stablehlo.multiply %3942, %64 : tensor<1x16x128x32xbf16> loc(#loc3948)
      %3944 = stablehlo.slice %3941 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc3949)
      %3945 = stablehlo.multiply %3944, %70 : tensor<1x16x128x32xbf16> loc(#loc3950)
      %3946 = stablehlo.subtract %3943, %3945 : tensor<1x16x128x32xbf16> loc(#loc3951)
      %3947 = stablehlo.multiply %3944, %64 : tensor<1x16x128x32xbf16> loc(#loc3952)
      %3948 = stablehlo.multiply %3942, %70 : tensor<1x16x128x32xbf16> loc(#loc3953)
      %3949 = stablehlo.add %3947, %3948 : tensor<1x16x128x32xbf16> loc(#loc3954)
      %3950 = stablehlo.concatenate %3946, %3949, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3955)
      %3951 = stablehlo.reshape %arg1053 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc3956)
      %3952 = stablehlo.reshape %3951 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc3957)
      %3953 = stablehlo.transpose %3952, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc3958)
      %3954 = stablehlo.dot_general %3929, %3953, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc3959)
      %3955 = "stablehlo.all_reduce"(%3954) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9524"), %arg1239: tensor<bf16> loc("dot.9524")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc3959)
        stablehlo.return %7360 : tensor<bf16> loc(#loc3959)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc3959)
      %3956 = stablehlo.reshape %3955 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3960)
      %3957 = stablehlo.reshape %arg1052 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3961)
      %3958 = stablehlo.reshape %3957 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3962)
      %3959 = stablehlo.broadcast_in_dim %3958, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc3963)
      %3960 = stablehlo.add %3956, %3959 : tensor<1x128x128xbf16> loc(#loc3964)
      %3961 = stablehlo.reshape %3960 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc3965)
      %3962 = stablehlo.transpose %3961, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3966)
      %3963 = stablehlo.slice %3962 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3967)
      %3964 = stablehlo.multiply %3963, %90 : tensor<1x2x128x32xbf16> loc(#loc3968)
      %3965 = stablehlo.slice %3962 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc3969)
      %3966 = stablehlo.multiply %3965, %93 : tensor<1x2x128x32xbf16> loc(#loc3970)
      %3967 = stablehlo.subtract %3964, %3966 : tensor<1x2x128x32xbf16> loc(#loc3971)
      %3968 = stablehlo.multiply %3965, %90 : tensor<1x2x128x32xbf16> loc(#loc3972)
      %3969 = stablehlo.multiply %3963, %93 : tensor<1x2x128x32xbf16> loc(#loc3973)
      %3970 = stablehlo.add %3968, %3969 : tensor<1x2x128x32xbf16> loc(#loc3974)
      %3971 = stablehlo.concatenate %3967, %3970, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc3975)
      %3972 = stablehlo.broadcast_in_dim %3971, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc3976)
      %3973 = stablehlo.reshape %3972 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc3977)
      %3974 = stablehlo.transpose %3973, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc3978)
      %3975 = stablehlo.dot_general %3950, %3974, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc3979)
      %3976 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %3977 = stablehlo.multiply %3975, %3976 : tensor<1x16x128x128xbf16> loc(#loc3980)
      %3978 = stablehlo.add %3977, %341 : tensor<1x16x128x128xbf16> loc(#loc3981)
      %3979 = stablehlo.reshape %arg1051 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc433)
      %3980 = "stablehlo.all_to_all"(%3979) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc433)
      %3981 = stablehlo.slice %3980 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc433)
      %3982 = stablehlo.reshape %3981 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc433)
      %3983 = stablehlo.reshape %3982 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc3982)
      %3984 = stablehlo.reshape %3983 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc3983)
      %3985 = stablehlo.broadcast_in_dim %3984, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc3984)
      %3986 = stablehlo.concatenate %3978, %3985, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3985)
      %3987 = stablehlo.reshape %arg1061 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc3986)
      %3988 = stablehlo.reshape %3987 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc3987)
      %3989 = stablehlo.convert %3988 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc3988)
      %3990 = stablehlo.broadcast_in_dim %3989, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc3989)
      %3991 = stablehlo.reduce(%3986 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3990)
      %3992 = stablehlo.broadcast_in_dim %3991, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3991)
      %3993 = stablehlo.subtract %3986, %3992 : tensor<1x16x128x129xbf16> loc(#loc3992)
      %3994 = stablehlo.reduce(%3993 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3993)
      %3995 = stablehlo.broadcast_in_dim %3994, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3994)
      %3996 = stablehlo.subtract %3993, %3995 : tensor<1x16x128x129xbf16> loc(#loc3995)
      %3997 = stablehlo.exponential %3996 : tensor<1x16x128x129xbf16> loc(#loc3996)
      %3998 = stablehlo.reduce(%3997 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc3997)
      %3999 = stablehlo.broadcast_in_dim %3998, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc3998)
      %4000 = stablehlo.divide %3997, %3999 : tensor<1x16x128x129xbf16> loc(#loc3999)
      %4001 = stablehlo.slice %4000 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4000)
      %4002 = stablehlo.reshape %arg721 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4001)
      %4003 = stablehlo.reshape %4002 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4002)
      %4004 = stablehlo.transpose %4003, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4003)
      %4005 = stablehlo.dot_general %3929, %4004, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4004)
      %4006 = "stablehlo.all_reduce"(%4005) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9487"), %arg1239: tensor<bf16> loc("dot.9487")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4004)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4004)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4004)
      %4007 = stablehlo.reshape %4006 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4005)
      %4008 = stablehlo.reshape %arg720 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4006)
      %4009 = stablehlo.reshape %4008 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4007)
      %4010 = stablehlo.broadcast_in_dim %4009, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4008)
      %4011 = stablehlo.add %4007, %4010 : tensor<1x128x128xbf16> loc(#loc4009)
      %4012 = stablehlo.reshape %4011 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4010)
      %4013 = stablehlo.transpose %4012, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4011)
      %4014 = stablehlo.broadcast_in_dim %4013, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4012)
      %4015 = stablehlo.reshape %4014 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4013)
      %4016 = stablehlo.dot_general %4001, %4015, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4014)
      %4017 = stablehlo.transpose %4016, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4015)
      %4018 = stablehlo.reshape %4017 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc4016)
      %4019 = stablehlo.reshape %arg719 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc4017)
      %4020 = stablehlo.reshape %4019 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc4018)
      %4021 = stablehlo.transpose %4020, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc4019)
      %4022 = stablehlo.dot_general %4018, %4021, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc4020)
      %4023 = "stablehlo.all_reduce"(%4022) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9680"), %arg1239: tensor<bf16> loc("dot.9680")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4020)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4020)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4020)
      %4024 = stablehlo.reshape %4023 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4021)
      %4025 = stablehlo.reshape %arg718 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4022)
      %4026 = stablehlo.reshape %4025 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4023)
      %4027 = stablehlo.broadcast_in_dim %4026, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4024)
      %4028 = stablehlo.add %4024, %4027 : tensor<1x128x360xbf16> loc(#loc4025)
      %4029 = stablehlo.add %3912, %4028 : tensor<1x128x360xbf16> loc(#loc4026)
      %4030 = stablehlo.reshape %arg1056 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4027)
      %4031 = stablehlo.reshape %4030 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4028)
      %4032 = stablehlo.convert %4031 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4029)
      %4033 = stablehlo.broadcast_in_dim %4032, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4030)
      %4034 = stablehlo.convert %4029 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4031)
      %4035 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4036 = stablehlo.power %4034, %4035 : tensor<1x128x360xf32> loc(#loc4032)
      %4037 = stablehlo.reduce(%4036 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4033)
      %4038 = "stablehlo.all_reduce"(%4037) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.9698"), %arg1239: tensor<f32> loc("reduce.9698")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4033)
        stablehlo.return %7360 : tensor<f32> loc(#loc4033)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4033)
      %4039 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4040 = stablehlo.multiply %4038, %4039 : tensor<1x128xf32> loc(#loc4034)
      %4041 = stablehlo.reshape %4040 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4035)
      %4042 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4043 = stablehlo.add %4041, %4042 : tensor<1x128x1xf32> loc(#loc4036)
      %4044 = stablehlo.rsqrt %4043 : tensor<1x128x1xf32> loc(#loc4037)
      %4045 = stablehlo.reshape %4044 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4038)
      %4046 = stablehlo.broadcast_in_dim %4045, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4039)
      %4047 = stablehlo.multiply %4034, %4046 : tensor<1x128x360xf32> loc(#loc4040)
      %4048 = stablehlo.multiply %4033, %4047 : tensor<1x128x360xf32> loc(#loc4041)
      %4049 = stablehlo.convert %4048 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4042)
      %4050 = stablehlo.reshape %4049 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4043)
      %4051 = stablehlo.broadcast_in_dim %4050, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4044)
      %4052 = stablehlo.dot_general %4051, %arg1060, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4045)
      %4053 = "stablehlo.all_reduce"(%4052) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9810"), %arg1239: tensor<bf16> loc("dot.9810")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4045)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4045)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4045)
      %4054 = stablehlo.reshape %arg1059 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc4046)
      %4055 = stablehlo.reshape %4054 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc4047)
      %4056 = stablehlo.broadcast_in_dim %4055, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4048)
      %4057 = stablehlo.add %4053, %4056 : tensor<32x128x5760xbf16> loc(#loc4049)
      %4058 = stablehlo.slice %4057 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4050)
      %4059 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4060 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4061 = stablehlo.clamp %4060, %4058, %4059 : tensor<32x128x2880xbf16> loc(#loc4051)
      %4062 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4063 = stablehlo.add %4061, %4062 : tensor<32x128x2880xbf16> loc(#loc4052)
      %4064 = stablehlo.slice %4057 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4053)
      %4065 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4066 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4067 = stablehlo.clamp %4065, %4064, %4066 : tensor<32x128x2880xbf16> loc(#loc4054)
      %4068 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4069 = stablehlo.multiply %4067, %4068 : tensor<32x128x2880xbf16> loc(#loc4055)
      %4070 = stablehlo.logistic %4069 : tensor<32x128x2880xbf16> loc(#loc4056)
      %4071 = stablehlo.multiply %4067, %4070 : tensor<32x128x2880xbf16> loc(#loc4057)
      %4072 = stablehlo.multiply %4063, %4071 : tensor<32x128x2880xbf16> loc(#loc4058)
      %4073 = stablehlo.dot_general %4072, %arg1058, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4059)
      %4074 = stablehlo.reshape %arg1057 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc4060)
      %4075 = stablehlo.reshape %4074 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc4061)
      %4076 = stablehlo.broadcast_in_dim %4075, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4062)
      %4077 = stablehlo.add %4073, %4076 : tensor<32x128x360xbf16> loc(#loc4063)
      %4078 = stablehlo.reshape %4077 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4064)
      %4079 = stablehlo.reshape %arg717 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4065)
      %4080 = stablehlo.reshape %4079 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4066)
      %4081 = stablehlo.transpose %4080, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4067)
      %4082 = stablehlo.dot_general %4050, %4081, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4068)
      %4083 = "stablehlo.all_reduce"(%4082) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9726"), %arg1239: tensor<bf16> loc("dot.9726")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4068)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4068)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4068)
      %4084 = stablehlo.reshape %arg716 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4069)
      %4085 = stablehlo.reshape %4084 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4070)
      %4086 = stablehlo.broadcast_in_dim %4085, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc4071)
      %4087 = stablehlo.add %4083, %4086 : tensor<128x128xbf16> loc(#loc4072)
      %4088:2 = "stablehlo.sort"(%4087, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.9746"), %arg1239: tensor<bf16> loc("sort.9746"), %arg1240: tensor<i32> loc("sort.9746"), %arg1241: tensor<i32> loc("sort.9746")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4074)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc4073)
      %4089 = stablehlo.slice %4088#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc4075)
      %4090 = stablehlo.convert %4089 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc4076)
      %4091 = stablehlo.reshape %4090 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc4077)
      %4092 = stablehlo.concatenate %231, %4091, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc4078)
      %4093 = stablehlo.slice %4088#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc4079)
      %4094 = stablehlo.reduce(%4093 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4080)
      %4095 = stablehlo.broadcast_in_dim %4094, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4081)
      %4096 = stablehlo.subtract %4093, %4095 : tensor<128x4xbf16> loc(#loc4082)
      %4097 = stablehlo.exponential %4096 : tensor<128x4xbf16> loc(#loc4083)
      %4098 = stablehlo.reduce(%4097 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4084)
      %4099 = stablehlo.broadcast_in_dim %4098, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4085)
      %4100 = stablehlo.divide %4097, %4099 : tensor<128x4xbf16> loc(#loc4086)
      %4101 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %4102 = "stablehlo.all_gather"(%4101) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %4103 = "stablehlo.scatter"(%4102, %4092, %4100) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.9780"), %arg1239: tensor<bf16> loc("scatter.9780")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc4087)
      %4104 = stablehlo.reshape %4103 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc4087)
      %4105 = "stablehlo.all_to_all"(%4104) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc4087)
      %4106 = stablehlo.slice %4105 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc4087)
      %4107 = stablehlo.reshape %4106 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc4087)
      %4108 = stablehlo.transpose %4107, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc4088)
      %4109 = stablehlo.reshape %4108 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc4089)
      %4110 = stablehlo.broadcast_in_dim %4109, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4090)
      %4111 = stablehlo.multiply %4078, %4110 : tensor<32x1x128x360xbf16> loc(#loc4091)
      %4112 = stablehlo.reduce(%4111 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc4092)
      %4113 = "stablehlo.all_reduce"(%4112) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.9852"), %arg1239: tensor<bf16> loc("reduce.9852")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4092)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4092)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4092)
      %4114 = stablehlo.add %4029, %4113 : tensor<1x128x360xbf16> loc(#loc4093)
      %4115 = stablehlo.convert %4114 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4094)
      %4116 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4117 = stablehlo.power %4115, %4116 : tensor<1x128x360xf32> loc(#loc4095)
      %4118 = stablehlo.reduce(%4117 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4096)
      %4119 = "stablehlo.all_reduce"(%4118) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.9865"), %arg1239: tensor<f32> loc("reduce.9865")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4096)
        stablehlo.return %7360 : tensor<f32> loc(#loc4096)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4096)
      %4120 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4121 = stablehlo.multiply %4119, %4120 : tensor<1x128xf32> loc(#loc4097)
      %4122 = stablehlo.reshape %4121 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4098)
      %4123 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4124 = stablehlo.add %4122, %4123 : tensor<1x128x1xf32> loc(#loc4099)
      %4125 = stablehlo.rsqrt %4124 : tensor<1x128x1xf32> loc(#loc4100)
      %4126 = stablehlo.reshape %4125 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4101)
      %4127 = stablehlo.broadcast_in_dim %4126, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4102)
      %4128 = stablehlo.multiply %4115, %4127 : tensor<1x128x360xf32> loc(#loc4103)
      %4129 = stablehlo.multiply %3990, %4128 : tensor<1x128x360xf32> loc(#loc4104)
      %4130 = stablehlo.convert %4129 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4105)
      %4131 = stablehlo.reshape %4130 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4106)
      %4132 = stablehlo.reshape %arg1066 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc4107)
      %4133 = stablehlo.reshape %4132 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc4108)
      %4134 = stablehlo.transpose %4133, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc4109)
      %4135 = stablehlo.dot_general %4131, %4134, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4110)
      %4136 = "stablehlo.all_reduce"(%4135) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9979"), %arg1239: tensor<bf16> loc("dot.9979")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4110)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4110)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4110)
      %4137 = stablehlo.reshape %4136 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4111)
      %4138 = stablehlo.reshape %arg1065 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4112)
      %4139 = stablehlo.reshape %4138 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc4113)
      %4140 = stablehlo.broadcast_in_dim %4139, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4114)
      %4141 = stablehlo.add %4137, %4140 : tensor<1x128x1024xbf16> loc(#loc4115)
      %4142 = stablehlo.reshape %4141 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4116)
      %4143 = stablehlo.transpose %4142, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4117)
      %4144 = stablehlo.slice %4143 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4118)
      %4145 = stablehlo.multiply %4144, %64 : tensor<1x16x128x32xbf16> loc(#loc4119)
      %4146 = stablehlo.slice %4143 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4120)
      %4147 = stablehlo.multiply %4146, %70 : tensor<1x16x128x32xbf16> loc(#loc4121)
      %4148 = stablehlo.subtract %4145, %4147 : tensor<1x16x128x32xbf16> loc(#loc4122)
      %4149 = stablehlo.multiply %4146, %64 : tensor<1x16x128x32xbf16> loc(#loc4123)
      %4150 = stablehlo.multiply %4144, %70 : tensor<1x16x128x32xbf16> loc(#loc4124)
      %4151 = stablehlo.add %4149, %4150 : tensor<1x16x128x32xbf16> loc(#loc4125)
      %4152 = stablehlo.concatenate %4148, %4151, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4126)
      %4153 = stablehlo.reshape %arg1064 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4127)
      %4154 = stablehlo.reshape %4153 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4128)
      %4155 = stablehlo.transpose %4154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4129)
      %4156 = stablehlo.dot_general %4131, %4155, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4130)
      %4157 = "stablehlo.all_reduce"(%4156) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9930"), %arg1239: tensor<bf16> loc("dot.9930")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4130)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4130)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4130)
      %4158 = stablehlo.reshape %4157 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4131)
      %4159 = stablehlo.reshape %arg1063 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4132)
      %4160 = stablehlo.reshape %4159 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4133)
      %4161 = stablehlo.broadcast_in_dim %4160, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4134)
      %4162 = stablehlo.add %4158, %4161 : tensor<1x128x128xbf16> loc(#loc4135)
      %4163 = stablehlo.reshape %4162 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4136)
      %4164 = stablehlo.transpose %4163, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4137)
      %4165 = stablehlo.slice %4164 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4138)
      %4166 = stablehlo.multiply %4165, %90 : tensor<1x2x128x32xbf16> loc(#loc4139)
      %4167 = stablehlo.slice %4164 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4140)
      %4168 = stablehlo.multiply %4167, %93 : tensor<1x2x128x32xbf16> loc(#loc4141)
      %4169 = stablehlo.subtract %4166, %4168 : tensor<1x2x128x32xbf16> loc(#loc4142)
      %4170 = stablehlo.multiply %4167, %90 : tensor<1x2x128x32xbf16> loc(#loc4143)
      %4171 = stablehlo.multiply %4165, %93 : tensor<1x2x128x32xbf16> loc(#loc4144)
      %4172 = stablehlo.add %4170, %4171 : tensor<1x2x128x32xbf16> loc(#loc4145)
      %4173 = stablehlo.concatenate %4169, %4172, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4146)
      %4174 = stablehlo.broadcast_in_dim %4173, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4147)
      %4175 = stablehlo.reshape %4174 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4148)
      %4176 = stablehlo.transpose %4175, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc4149)
      %4177 = stablehlo.dot_general %4152, %4176, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4150)
      %4178 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %4179 = stablehlo.multiply %4177, %4178 : tensor<1x16x128x128xbf16> loc(#loc4151)
      %4180 = stablehlo.add %4179, %128 : tensor<1x16x128x128xbf16> loc(#loc4152)
      %4181 = stablehlo.reshape %arg1062 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc444)
      %4182 = "stablehlo.all_to_all"(%4181) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc444)
      %4183 = stablehlo.slice %4182 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc444)
      %4184 = stablehlo.reshape %4183 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc444)
      %4185 = stablehlo.reshape %4184 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc4153)
      %4186 = stablehlo.reshape %4185 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4154)
      %4187 = stablehlo.broadcast_in_dim %4186, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc4155)
      %4188 = stablehlo.concatenate %4180, %4187, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4156)
      %4189 = stablehlo.reshape %arg1072 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4157)
      %4190 = stablehlo.reshape %4189 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4158)
      %4191 = stablehlo.convert %4190 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4159)
      %4192 = stablehlo.broadcast_in_dim %4191, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4160)
      %4193 = stablehlo.reduce(%4188 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4161)
      %4194 = stablehlo.broadcast_in_dim %4193, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4162)
      %4195 = stablehlo.subtract %4188, %4194 : tensor<1x16x128x129xbf16> loc(#loc4163)
      %4196 = stablehlo.reduce(%4195 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4164)
      %4197 = stablehlo.broadcast_in_dim %4196, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4165)
      %4198 = stablehlo.subtract %4195, %4197 : tensor<1x16x128x129xbf16> loc(#loc4166)
      %4199 = stablehlo.exponential %4198 : tensor<1x16x128x129xbf16> loc(#loc4167)
      %4200 = stablehlo.reduce(%4199 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4168)
      %4201 = stablehlo.broadcast_in_dim %4200, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4169)
      %4202 = stablehlo.divide %4199, %4201 : tensor<1x16x128x129xbf16> loc(#loc4170)
      %4203 = stablehlo.slice %4202 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4171)
      %4204 = stablehlo.reshape %arg715 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4172)
      %4205 = stablehlo.reshape %4204 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4173)
      %4206 = stablehlo.transpose %4205, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4174)
      %4207 = stablehlo.dot_general %4131, %4206, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4175)
      %4208 = "stablehlo.all_reduce"(%4207) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.9893"), %arg1239: tensor<bf16> loc("dot.9893")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4175)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4175)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4175)
      %4209 = stablehlo.reshape %4208 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4176)
      %4210 = stablehlo.reshape %arg714 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4177)
      %4211 = stablehlo.reshape %4210 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4178)
      %4212 = stablehlo.broadcast_in_dim %4211, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4179)
      %4213 = stablehlo.add %4209, %4212 : tensor<1x128x128xbf16> loc(#loc4180)
      %4214 = stablehlo.reshape %4213 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4181)
      %4215 = stablehlo.transpose %4214, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4182)
      %4216 = stablehlo.broadcast_in_dim %4215, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4183)
      %4217 = stablehlo.reshape %4216 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4184)
      %4218 = stablehlo.dot_general %4203, %4217, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4185)
      %4219 = stablehlo.transpose %4218, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4186)
      %4220 = stablehlo.reshape %4219 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc4187)
      %4221 = stablehlo.reshape %arg713 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc4188)
      %4222 = stablehlo.reshape %4221 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc4189)
      %4223 = stablehlo.transpose %4222, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc4190)
      %4224 = stablehlo.dot_general %4220, %4223, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc4191)
      %4225 = "stablehlo.all_reduce"(%4224) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10086"), %arg1239: tensor<bf16> loc("dot.10086")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4191)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4191)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4191)
      %4226 = stablehlo.reshape %4225 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4192)
      %4227 = stablehlo.reshape %arg712 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4193)
      %4228 = stablehlo.reshape %4227 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4194)
      %4229 = stablehlo.broadcast_in_dim %4228, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4195)
      %4230 = stablehlo.add %4226, %4229 : tensor<1x128x360xbf16> loc(#loc4196)
      %4231 = stablehlo.add %4114, %4230 : tensor<1x128x360xbf16> loc(#loc4197)
      %4232 = stablehlo.reshape %arg1067 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4198)
      %4233 = stablehlo.reshape %4232 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4199)
      %4234 = stablehlo.convert %4233 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4200)
      %4235 = stablehlo.broadcast_in_dim %4234, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4201)
      %4236 = stablehlo.convert %4231 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4202)
      %4237 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4238 = stablehlo.power %4236, %4237 : tensor<1x128x360xf32> loc(#loc4203)
      %4239 = stablehlo.reduce(%4238 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4204)
      %4240 = "stablehlo.all_reduce"(%4239) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.10104"), %arg1239: tensor<f32> loc("reduce.10104")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4204)
        stablehlo.return %7360 : tensor<f32> loc(#loc4204)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4204)
      %4241 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4242 = stablehlo.multiply %4240, %4241 : tensor<1x128xf32> loc(#loc4205)
      %4243 = stablehlo.reshape %4242 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4206)
      %4244 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4245 = stablehlo.add %4243, %4244 : tensor<1x128x1xf32> loc(#loc4207)
      %4246 = stablehlo.rsqrt %4245 : tensor<1x128x1xf32> loc(#loc4208)
      %4247 = stablehlo.reshape %4246 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4209)
      %4248 = stablehlo.broadcast_in_dim %4247, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4210)
      %4249 = stablehlo.multiply %4236, %4248 : tensor<1x128x360xf32> loc(#loc4211)
      %4250 = stablehlo.multiply %4235, %4249 : tensor<1x128x360xf32> loc(#loc4212)
      %4251 = stablehlo.convert %4250 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4213)
      %4252 = stablehlo.reshape %4251 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4214)
      %4253 = stablehlo.broadcast_in_dim %4252, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4215)
      %4254 = stablehlo.dot_general %4253, %arg1071, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4216)
      %4255 = "stablehlo.all_reduce"(%4254) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10216"), %arg1239: tensor<bf16> loc("dot.10216")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4216)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4216)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4216)
      %4256 = stablehlo.reshape %arg1070 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc4217)
      %4257 = stablehlo.reshape %4256 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc4218)
      %4258 = stablehlo.broadcast_in_dim %4257, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4219)
      %4259 = stablehlo.add %4255, %4258 : tensor<32x128x5760xbf16> loc(#loc4220)
      %4260 = stablehlo.slice %4259 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4221)
      %4261 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4262 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4263 = stablehlo.clamp %4262, %4260, %4261 : tensor<32x128x2880xbf16> loc(#loc4222)
      %4264 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4265 = stablehlo.add %4263, %4264 : tensor<32x128x2880xbf16> loc(#loc4223)
      %4266 = stablehlo.slice %4259 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4224)
      %4267 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4268 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4269 = stablehlo.clamp %4267, %4266, %4268 : tensor<32x128x2880xbf16> loc(#loc4225)
      %4270 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4271 = stablehlo.multiply %4269, %4270 : tensor<32x128x2880xbf16> loc(#loc4226)
      %4272 = stablehlo.logistic %4271 : tensor<32x128x2880xbf16> loc(#loc4227)
      %4273 = stablehlo.multiply %4269, %4272 : tensor<32x128x2880xbf16> loc(#loc4228)
      %4274 = stablehlo.multiply %4265, %4273 : tensor<32x128x2880xbf16> loc(#loc4229)
      %4275 = stablehlo.dot_general %4274, %arg1069, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4230)
      %4276 = stablehlo.reshape %arg1068 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc4231)
      %4277 = stablehlo.reshape %4276 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc4232)
      %4278 = stablehlo.broadcast_in_dim %4277, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4233)
      %4279 = stablehlo.add %4275, %4278 : tensor<32x128x360xbf16> loc(#loc4234)
      %4280 = stablehlo.reshape %4279 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4235)
      %4281 = stablehlo.reshape %arg711 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4236)
      %4282 = stablehlo.reshape %4281 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4237)
      %4283 = stablehlo.transpose %4282, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4238)
      %4284 = stablehlo.dot_general %4252, %4283, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4239)
      %4285 = "stablehlo.all_reduce"(%4284) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10132"), %arg1239: tensor<bf16> loc("dot.10132")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4239)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4239)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4239)
      %4286 = stablehlo.reshape %arg710 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4240)
      %4287 = stablehlo.reshape %4286 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4241)
      %4288 = stablehlo.broadcast_in_dim %4287, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc4242)
      %4289 = stablehlo.add %4285, %4288 : tensor<128x128xbf16> loc(#loc4243)
      %4290:2 = "stablehlo.sort"(%4289, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.10152"), %arg1239: tensor<bf16> loc("sort.10152"), %arg1240: tensor<i32> loc("sort.10152"), %arg1241: tensor<i32> loc("sort.10152")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4245)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc4244)
      %4291 = stablehlo.slice %4290#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc4246)
      %4292 = stablehlo.convert %4291 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc4247)
      %4293 = stablehlo.reshape %4292 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc4248)
      %4294 = stablehlo.concatenate %231, %4293, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc4249)
      %4295 = stablehlo.slice %4290#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc4250)
      %4296 = stablehlo.reduce(%4295 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4251)
      %4297 = stablehlo.broadcast_in_dim %4296, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4252)
      %4298 = stablehlo.subtract %4295, %4297 : tensor<128x4xbf16> loc(#loc4253)
      %4299 = stablehlo.exponential %4298 : tensor<128x4xbf16> loc(#loc4254)
      %4300 = stablehlo.reduce(%4299 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4255)
      %4301 = stablehlo.broadcast_in_dim %4300, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4256)
      %4302 = stablehlo.divide %4299, %4301 : tensor<128x4xbf16> loc(#loc4257)
      %4303 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %4304 = "stablehlo.all_gather"(%4303) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %4305 = "stablehlo.scatter"(%4304, %4294, %4302) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.10186"), %arg1239: tensor<bf16> loc("scatter.10186")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc4258)
      %4306 = stablehlo.reshape %4305 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc4258)
      %4307 = "stablehlo.all_to_all"(%4306) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc4258)
      %4308 = stablehlo.slice %4307 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc4258)
      %4309 = stablehlo.reshape %4308 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc4258)
      %4310 = stablehlo.transpose %4309, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc4259)
      %4311 = stablehlo.reshape %4310 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc4260)
      %4312 = stablehlo.broadcast_in_dim %4311, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4261)
      %4313 = stablehlo.multiply %4280, %4312 : tensor<32x1x128x360xbf16> loc(#loc4262)
      %4314 = stablehlo.reduce(%4313 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc4263)
      %4315 = "stablehlo.all_reduce"(%4314) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.10258"), %arg1239: tensor<bf16> loc("reduce.10258")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4263)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4263)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4263)
      %4316 = stablehlo.add %4231, %4315 : tensor<1x128x360xbf16> loc(#loc4264)
      %4317 = stablehlo.convert %4316 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4265)
      %4318 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4319 = stablehlo.power %4317, %4318 : tensor<1x128x360xf32> loc(#loc4266)
      %4320 = stablehlo.reduce(%4319 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4267)
      %4321 = "stablehlo.all_reduce"(%4320) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.10271"), %arg1239: tensor<f32> loc("reduce.10271")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4267)
        stablehlo.return %7360 : tensor<f32> loc(#loc4267)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4267)
      %4322 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4323 = stablehlo.multiply %4321, %4322 : tensor<1x128xf32> loc(#loc4268)
      %4324 = stablehlo.reshape %4323 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4269)
      %4325 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4326 = stablehlo.add %4324, %4325 : tensor<1x128x1xf32> loc(#loc4270)
      %4327 = stablehlo.rsqrt %4326 : tensor<1x128x1xf32> loc(#loc4271)
      %4328 = stablehlo.reshape %4327 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4272)
      %4329 = stablehlo.broadcast_in_dim %4328, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4273)
      %4330 = stablehlo.multiply %4317, %4329 : tensor<1x128x360xf32> loc(#loc4274)
      %4331 = stablehlo.multiply %4192, %4330 : tensor<1x128x360xf32> loc(#loc4275)
      %4332 = stablehlo.convert %4331 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4276)
      %4333 = stablehlo.reshape %4332 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4277)
      %4334 = stablehlo.reshape %arg1077 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc4278)
      %4335 = stablehlo.reshape %4334 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc4279)
      %4336 = stablehlo.transpose %4335, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc4280)
      %4337 = stablehlo.dot_general %4333, %4336, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4281)
      %4338 = "stablehlo.all_reduce"(%4337) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10385"), %arg1239: tensor<bf16> loc("dot.10385")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4281)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4281)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4281)
      %4339 = stablehlo.reshape %4338 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4282)
      %4340 = stablehlo.reshape %arg1076 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4283)
      %4341 = stablehlo.reshape %4340 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc4284)
      %4342 = stablehlo.broadcast_in_dim %4341, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4285)
      %4343 = stablehlo.add %4339, %4342 : tensor<1x128x1024xbf16> loc(#loc4286)
      %4344 = stablehlo.reshape %4343 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4287)
      %4345 = stablehlo.transpose %4344, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4288)
      %4346 = stablehlo.slice %4345 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4289)
      %4347 = stablehlo.multiply %4346, %64 : tensor<1x16x128x32xbf16> loc(#loc4290)
      %4348 = stablehlo.slice %4345 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4291)
      %4349 = stablehlo.multiply %4348, %70 : tensor<1x16x128x32xbf16> loc(#loc4292)
      %4350 = stablehlo.subtract %4347, %4349 : tensor<1x16x128x32xbf16> loc(#loc4293)
      %4351 = stablehlo.multiply %4348, %64 : tensor<1x16x128x32xbf16> loc(#loc4294)
      %4352 = stablehlo.multiply %4346, %70 : tensor<1x16x128x32xbf16> loc(#loc4295)
      %4353 = stablehlo.add %4351, %4352 : tensor<1x16x128x32xbf16> loc(#loc4296)
      %4354 = stablehlo.concatenate %4350, %4353, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4297)
      %4355 = stablehlo.reshape %arg1075 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4298)
      %4356 = stablehlo.reshape %4355 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4299)
      %4357 = stablehlo.transpose %4356, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4300)
      %4358 = stablehlo.dot_general %4333, %4357, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4301)
      %4359 = "stablehlo.all_reduce"(%4358) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10336"), %arg1239: tensor<bf16> loc("dot.10336")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4301)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4301)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4301)
      %4360 = stablehlo.reshape %4359 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4302)
      %4361 = stablehlo.reshape %arg1074 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4303)
      %4362 = stablehlo.reshape %4361 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4304)
      %4363 = stablehlo.broadcast_in_dim %4362, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4305)
      %4364 = stablehlo.add %4360, %4363 : tensor<1x128x128xbf16> loc(#loc4306)
      %4365 = stablehlo.reshape %4364 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4307)
      %4366 = stablehlo.transpose %4365, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4308)
      %4367 = stablehlo.slice %4366 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4309)
      %4368 = stablehlo.multiply %4367, %90 : tensor<1x2x128x32xbf16> loc(#loc4310)
      %4369 = stablehlo.slice %4366 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4311)
      %4370 = stablehlo.multiply %4369, %93 : tensor<1x2x128x32xbf16> loc(#loc4312)
      %4371 = stablehlo.subtract %4368, %4370 : tensor<1x2x128x32xbf16> loc(#loc4313)
      %4372 = stablehlo.multiply %4369, %90 : tensor<1x2x128x32xbf16> loc(#loc4314)
      %4373 = stablehlo.multiply %4367, %93 : tensor<1x2x128x32xbf16> loc(#loc4315)
      %4374 = stablehlo.add %4372, %4373 : tensor<1x2x128x32xbf16> loc(#loc4316)
      %4375 = stablehlo.concatenate %4371, %4374, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4317)
      %4376 = stablehlo.broadcast_in_dim %4375, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4318)
      %4377 = stablehlo.reshape %4376 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4319)
      %4378 = stablehlo.transpose %4377, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc4320)
      %4379 = stablehlo.dot_general %4354, %4378, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4321)
      %4380 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %4381 = stablehlo.multiply %4379, %4380 : tensor<1x16x128x128xbf16> loc(#loc4322)
      %4382 = stablehlo.add %4381, %341 : tensor<1x16x128x128xbf16> loc(#loc4323)
      %4383 = stablehlo.reshape %arg1073 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc455)
      %4384 = "stablehlo.all_to_all"(%4383) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc455)
      %4385 = stablehlo.slice %4384 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc455)
      %4386 = stablehlo.reshape %4385 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc455)
      %4387 = stablehlo.reshape %4386 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc4324)
      %4388 = stablehlo.reshape %4387 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4325)
      %4389 = stablehlo.broadcast_in_dim %4388, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc4326)
      %4390 = stablehlo.concatenate %4382, %4389, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4327)
      %4391 = stablehlo.reshape %arg1083 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4328)
      %4392 = stablehlo.reshape %4391 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4329)
      %4393 = stablehlo.convert %4392 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4330)
      %4394 = stablehlo.broadcast_in_dim %4393, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4331)
      %4395 = stablehlo.reduce(%4390 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4332)
      %4396 = stablehlo.broadcast_in_dim %4395, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4333)
      %4397 = stablehlo.subtract %4390, %4396 : tensor<1x16x128x129xbf16> loc(#loc4334)
      %4398 = stablehlo.reduce(%4397 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4335)
      %4399 = stablehlo.broadcast_in_dim %4398, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4336)
      %4400 = stablehlo.subtract %4397, %4399 : tensor<1x16x128x129xbf16> loc(#loc4337)
      %4401 = stablehlo.exponential %4400 : tensor<1x16x128x129xbf16> loc(#loc4338)
      %4402 = stablehlo.reduce(%4401 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4339)
      %4403 = stablehlo.broadcast_in_dim %4402, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4340)
      %4404 = stablehlo.divide %4401, %4403 : tensor<1x16x128x129xbf16> loc(#loc4341)
      %4405 = stablehlo.slice %4404 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4342)
      %4406 = stablehlo.reshape %arg709 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4343)
      %4407 = stablehlo.reshape %4406 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4344)
      %4408 = stablehlo.transpose %4407, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4345)
      %4409 = stablehlo.dot_general %4333, %4408, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4346)
      %4410 = "stablehlo.all_reduce"(%4409) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10299"), %arg1239: tensor<bf16> loc("dot.10299")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4346)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4346)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4346)
      %4411 = stablehlo.reshape %4410 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4347)
      %4412 = stablehlo.reshape %arg708 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4348)
      %4413 = stablehlo.reshape %4412 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4349)
      %4414 = stablehlo.broadcast_in_dim %4413, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4350)
      %4415 = stablehlo.add %4411, %4414 : tensor<1x128x128xbf16> loc(#loc4351)
      %4416 = stablehlo.reshape %4415 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4352)
      %4417 = stablehlo.transpose %4416, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4353)
      %4418 = stablehlo.broadcast_in_dim %4417, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4354)
      %4419 = stablehlo.reshape %4418 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4355)
      %4420 = stablehlo.dot_general %4405, %4419, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4356)
      %4421 = stablehlo.transpose %4420, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4357)
      %4422 = stablehlo.reshape %4421 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc4358)
      %4423 = stablehlo.reshape %arg707 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc4359)
      %4424 = stablehlo.reshape %4423 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc4360)
      %4425 = stablehlo.transpose %4424, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc4361)
      %4426 = stablehlo.dot_general %4422, %4425, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc4362)
      %4427 = "stablehlo.all_reduce"(%4426) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10492"), %arg1239: tensor<bf16> loc("dot.10492")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4362)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4362)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4362)
      %4428 = stablehlo.reshape %4427 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4363)
      %4429 = stablehlo.reshape %arg706 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4364)
      %4430 = stablehlo.reshape %4429 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4365)
      %4431 = stablehlo.broadcast_in_dim %4430, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4366)
      %4432 = stablehlo.add %4428, %4431 : tensor<1x128x360xbf16> loc(#loc4367)
      %4433 = stablehlo.add %4316, %4432 : tensor<1x128x360xbf16> loc(#loc4368)
      %4434 = stablehlo.reshape %arg1078 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4369)
      %4435 = stablehlo.reshape %4434 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4370)
      %4436 = stablehlo.convert %4435 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4371)
      %4437 = stablehlo.broadcast_in_dim %4436, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4372)
      %4438 = stablehlo.convert %4433 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4373)
      %4439 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4440 = stablehlo.power %4438, %4439 : tensor<1x128x360xf32> loc(#loc4374)
      %4441 = stablehlo.reduce(%4440 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4375)
      %4442 = "stablehlo.all_reduce"(%4441) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.10510"), %arg1239: tensor<f32> loc("reduce.10510")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4375)
        stablehlo.return %7360 : tensor<f32> loc(#loc4375)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4375)
      %4443 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4444 = stablehlo.multiply %4442, %4443 : tensor<1x128xf32> loc(#loc4376)
      %4445 = stablehlo.reshape %4444 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4377)
      %4446 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4447 = stablehlo.add %4445, %4446 : tensor<1x128x1xf32> loc(#loc4378)
      %4448 = stablehlo.rsqrt %4447 : tensor<1x128x1xf32> loc(#loc4379)
      %4449 = stablehlo.reshape %4448 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4380)
      %4450 = stablehlo.broadcast_in_dim %4449, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4381)
      %4451 = stablehlo.multiply %4438, %4450 : tensor<1x128x360xf32> loc(#loc4382)
      %4452 = stablehlo.multiply %4437, %4451 : tensor<1x128x360xf32> loc(#loc4383)
      %4453 = stablehlo.convert %4452 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4384)
      %4454 = stablehlo.reshape %4453 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4385)
      %4455 = stablehlo.broadcast_in_dim %4454, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4386)
      %4456 = stablehlo.dot_general %4455, %arg1082, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4387)
      %4457 = "stablehlo.all_reduce"(%4456) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10622"), %arg1239: tensor<bf16> loc("dot.10622")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4387)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4387)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4387)
      %4458 = stablehlo.reshape %arg1081 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc4388)
      %4459 = stablehlo.reshape %4458 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc4389)
      %4460 = stablehlo.broadcast_in_dim %4459, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4390)
      %4461 = stablehlo.add %4457, %4460 : tensor<32x128x5760xbf16> loc(#loc4391)
      %4462 = stablehlo.slice %4461 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4392)
      %4463 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4464 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4465 = stablehlo.clamp %4464, %4462, %4463 : tensor<32x128x2880xbf16> loc(#loc4393)
      %4466 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4467 = stablehlo.add %4465, %4466 : tensor<32x128x2880xbf16> loc(#loc4394)
      %4468 = stablehlo.slice %4461 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4395)
      %4469 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4470 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4471 = stablehlo.clamp %4469, %4468, %4470 : tensor<32x128x2880xbf16> loc(#loc4396)
      %4472 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4473 = stablehlo.multiply %4471, %4472 : tensor<32x128x2880xbf16> loc(#loc4397)
      %4474 = stablehlo.logistic %4473 : tensor<32x128x2880xbf16> loc(#loc4398)
      %4475 = stablehlo.multiply %4471, %4474 : tensor<32x128x2880xbf16> loc(#loc4399)
      %4476 = stablehlo.multiply %4467, %4475 : tensor<32x128x2880xbf16> loc(#loc4400)
      %4477 = stablehlo.dot_general %4476, %arg1080, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4401)
      %4478 = stablehlo.reshape %arg1079 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc4402)
      %4479 = stablehlo.reshape %4478 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc4403)
      %4480 = stablehlo.broadcast_in_dim %4479, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4404)
      %4481 = stablehlo.add %4477, %4480 : tensor<32x128x360xbf16> loc(#loc4405)
      %4482 = stablehlo.reshape %4481 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4406)
      %4483 = stablehlo.reshape %arg705 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4407)
      %4484 = stablehlo.reshape %4483 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4408)
      %4485 = stablehlo.transpose %4484, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4409)
      %4486 = stablehlo.dot_general %4454, %4485, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4410)
      %4487 = "stablehlo.all_reduce"(%4486) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10538"), %arg1239: tensor<bf16> loc("dot.10538")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4410)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4410)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4410)
      %4488 = stablehlo.reshape %arg704 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4411)
      %4489 = stablehlo.reshape %4488 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4412)
      %4490 = stablehlo.broadcast_in_dim %4489, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc4413)
      %4491 = stablehlo.add %4487, %4490 : tensor<128x128xbf16> loc(#loc4414)
      %4492:2 = "stablehlo.sort"(%4491, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.10558"), %arg1239: tensor<bf16> loc("sort.10558"), %arg1240: tensor<i32> loc("sort.10558"), %arg1241: tensor<i32> loc("sort.10558")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4416)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc4415)
      %4493 = stablehlo.slice %4492#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc4417)
      %4494 = stablehlo.convert %4493 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc4418)
      %4495 = stablehlo.reshape %4494 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc4419)
      %4496 = stablehlo.concatenate %231, %4495, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc4420)
      %4497 = stablehlo.slice %4492#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc4421)
      %4498 = stablehlo.reduce(%4497 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4422)
      %4499 = stablehlo.broadcast_in_dim %4498, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4423)
      %4500 = stablehlo.subtract %4497, %4499 : tensor<128x4xbf16> loc(#loc4424)
      %4501 = stablehlo.exponential %4500 : tensor<128x4xbf16> loc(#loc4425)
      %4502 = stablehlo.reduce(%4501 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4426)
      %4503 = stablehlo.broadcast_in_dim %4502, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4427)
      %4504 = stablehlo.divide %4501, %4503 : tensor<128x4xbf16> loc(#loc4428)
      %4505 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %4506 = "stablehlo.all_gather"(%4505) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %4507 = "stablehlo.scatter"(%4506, %4496, %4504) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.10592"), %arg1239: tensor<bf16> loc("scatter.10592")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc4429)
      %4508 = stablehlo.reshape %4507 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc4429)
      %4509 = "stablehlo.all_to_all"(%4508) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc4429)
      %4510 = stablehlo.slice %4509 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc4429)
      %4511 = stablehlo.reshape %4510 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc4429)
      %4512 = stablehlo.transpose %4511, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc4430)
      %4513 = stablehlo.reshape %4512 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc4431)
      %4514 = stablehlo.broadcast_in_dim %4513, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4432)
      %4515 = stablehlo.multiply %4482, %4514 : tensor<32x1x128x360xbf16> loc(#loc4433)
      %4516 = stablehlo.reduce(%4515 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc4434)
      %4517 = "stablehlo.all_reduce"(%4516) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.10664"), %arg1239: tensor<bf16> loc("reduce.10664")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4434)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4434)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4434)
      %4518 = stablehlo.add %4433, %4517 : tensor<1x128x360xbf16> loc(#loc4435)
      %4519 = stablehlo.convert %4518 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4436)
      %4520 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4521 = stablehlo.power %4519, %4520 : tensor<1x128x360xf32> loc(#loc4437)
      %4522 = stablehlo.reduce(%4521 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4438)
      %4523 = "stablehlo.all_reduce"(%4522) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.10677"), %arg1239: tensor<f32> loc("reduce.10677")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4438)
        stablehlo.return %7360 : tensor<f32> loc(#loc4438)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4438)
      %4524 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4525 = stablehlo.multiply %4523, %4524 : tensor<1x128xf32> loc(#loc4439)
      %4526 = stablehlo.reshape %4525 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4440)
      %4527 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4528 = stablehlo.add %4526, %4527 : tensor<1x128x1xf32> loc(#loc4441)
      %4529 = stablehlo.rsqrt %4528 : tensor<1x128x1xf32> loc(#loc4442)
      %4530 = stablehlo.reshape %4529 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4443)
      %4531 = stablehlo.broadcast_in_dim %4530, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4444)
      %4532 = stablehlo.multiply %4519, %4531 : tensor<1x128x360xf32> loc(#loc4445)
      %4533 = stablehlo.multiply %4394, %4532 : tensor<1x128x360xf32> loc(#loc4446)
      %4534 = stablehlo.convert %4533 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4447)
      %4535 = stablehlo.reshape %4534 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4448)
      %4536 = stablehlo.reshape %arg1088 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc4449)
      %4537 = stablehlo.reshape %4536 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc4450)
      %4538 = stablehlo.transpose %4537, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc4451)
      %4539 = stablehlo.dot_general %4535, %4538, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4452)
      %4540 = "stablehlo.all_reduce"(%4539) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10791"), %arg1239: tensor<bf16> loc("dot.10791")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4452)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4452)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4452)
      %4541 = stablehlo.reshape %4540 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4453)
      %4542 = stablehlo.reshape %arg1087 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4454)
      %4543 = stablehlo.reshape %4542 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc4455)
      %4544 = stablehlo.broadcast_in_dim %4543, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4456)
      %4545 = stablehlo.add %4541, %4544 : tensor<1x128x1024xbf16> loc(#loc4457)
      %4546 = stablehlo.reshape %4545 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4458)
      %4547 = stablehlo.transpose %4546, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4459)
      %4548 = stablehlo.slice %4547 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4460)
      %4549 = stablehlo.multiply %4548, %64 : tensor<1x16x128x32xbf16> loc(#loc4461)
      %4550 = stablehlo.slice %4547 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4462)
      %4551 = stablehlo.multiply %4550, %70 : tensor<1x16x128x32xbf16> loc(#loc4463)
      %4552 = stablehlo.subtract %4549, %4551 : tensor<1x16x128x32xbf16> loc(#loc4464)
      %4553 = stablehlo.multiply %4550, %64 : tensor<1x16x128x32xbf16> loc(#loc4465)
      %4554 = stablehlo.multiply %4548, %70 : tensor<1x16x128x32xbf16> loc(#loc4466)
      %4555 = stablehlo.add %4553, %4554 : tensor<1x16x128x32xbf16> loc(#loc4467)
      %4556 = stablehlo.concatenate %4552, %4555, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4468)
      %4557 = stablehlo.reshape %arg1086 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4469)
      %4558 = stablehlo.reshape %4557 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4470)
      %4559 = stablehlo.transpose %4558, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4471)
      %4560 = stablehlo.dot_general %4535, %4559, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4472)
      %4561 = "stablehlo.all_reduce"(%4560) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10742"), %arg1239: tensor<bf16> loc("dot.10742")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4472)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4472)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4472)
      %4562 = stablehlo.reshape %4561 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4473)
      %4563 = stablehlo.reshape %arg1085 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4474)
      %4564 = stablehlo.reshape %4563 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4475)
      %4565 = stablehlo.broadcast_in_dim %4564, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4476)
      %4566 = stablehlo.add %4562, %4565 : tensor<1x128x128xbf16> loc(#loc4477)
      %4567 = stablehlo.reshape %4566 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4478)
      %4568 = stablehlo.transpose %4567, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4479)
      %4569 = stablehlo.slice %4568 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4480)
      %4570 = stablehlo.multiply %4569, %90 : tensor<1x2x128x32xbf16> loc(#loc4481)
      %4571 = stablehlo.slice %4568 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4482)
      %4572 = stablehlo.multiply %4571, %93 : tensor<1x2x128x32xbf16> loc(#loc4483)
      %4573 = stablehlo.subtract %4570, %4572 : tensor<1x2x128x32xbf16> loc(#loc4484)
      %4574 = stablehlo.multiply %4571, %90 : tensor<1x2x128x32xbf16> loc(#loc4485)
      %4575 = stablehlo.multiply %4569, %93 : tensor<1x2x128x32xbf16> loc(#loc4486)
      %4576 = stablehlo.add %4574, %4575 : tensor<1x2x128x32xbf16> loc(#loc4487)
      %4577 = stablehlo.concatenate %4573, %4576, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4488)
      %4578 = stablehlo.broadcast_in_dim %4577, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4489)
      %4579 = stablehlo.reshape %4578 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4490)
      %4580 = stablehlo.transpose %4579, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc4491)
      %4581 = stablehlo.dot_general %4556, %4580, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4492)
      %4582 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %4583 = stablehlo.multiply %4581, %4582 : tensor<1x16x128x128xbf16> loc(#loc4493)
      %4584 = stablehlo.add %4583, %128 : tensor<1x16x128x128xbf16> loc(#loc4494)
      %4585 = stablehlo.reshape %arg1084 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc466)
      %4586 = "stablehlo.all_to_all"(%4585) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc466)
      %4587 = stablehlo.slice %4586 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc466)
      %4588 = stablehlo.reshape %4587 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc466)
      %4589 = stablehlo.reshape %4588 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc4495)
      %4590 = stablehlo.reshape %4589 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4496)
      %4591 = stablehlo.broadcast_in_dim %4590, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc4497)
      %4592 = stablehlo.concatenate %4584, %4591, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4498)
      %4593 = stablehlo.reshape %arg1094 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4499)
      %4594 = stablehlo.reshape %4593 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4500)
      %4595 = stablehlo.convert %4594 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4501)
      %4596 = stablehlo.broadcast_in_dim %4595, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4502)
      %4597 = stablehlo.reduce(%4592 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4503)
      %4598 = stablehlo.broadcast_in_dim %4597, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4504)
      %4599 = stablehlo.subtract %4592, %4598 : tensor<1x16x128x129xbf16> loc(#loc4505)
      %4600 = stablehlo.reduce(%4599 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4506)
      %4601 = stablehlo.broadcast_in_dim %4600, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4507)
      %4602 = stablehlo.subtract %4599, %4601 : tensor<1x16x128x129xbf16> loc(#loc4508)
      %4603 = stablehlo.exponential %4602 : tensor<1x16x128x129xbf16> loc(#loc4509)
      %4604 = stablehlo.reduce(%4603 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4510)
      %4605 = stablehlo.broadcast_in_dim %4604, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4511)
      %4606 = stablehlo.divide %4603, %4605 : tensor<1x16x128x129xbf16> loc(#loc4512)
      %4607 = stablehlo.slice %4606 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4513)
      %4608 = stablehlo.reshape %arg703 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4514)
      %4609 = stablehlo.reshape %4608 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4515)
      %4610 = stablehlo.transpose %4609, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4516)
      %4611 = stablehlo.dot_general %4535, %4610, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4517)
      %4612 = "stablehlo.all_reduce"(%4611) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10705"), %arg1239: tensor<bf16> loc("dot.10705")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4517)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4517)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4517)
      %4613 = stablehlo.reshape %4612 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4518)
      %4614 = stablehlo.reshape %arg702 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4519)
      %4615 = stablehlo.reshape %4614 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4520)
      %4616 = stablehlo.broadcast_in_dim %4615, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4521)
      %4617 = stablehlo.add %4613, %4616 : tensor<1x128x128xbf16> loc(#loc4522)
      %4618 = stablehlo.reshape %4617 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4523)
      %4619 = stablehlo.transpose %4618, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4524)
      %4620 = stablehlo.broadcast_in_dim %4619, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4525)
      %4621 = stablehlo.reshape %4620 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4526)
      %4622 = stablehlo.dot_general %4607, %4621, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4527)
      %4623 = stablehlo.transpose %4622, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4528)
      %4624 = stablehlo.reshape %4623 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc4529)
      %4625 = stablehlo.reshape %arg701 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc4530)
      %4626 = stablehlo.reshape %4625 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc4531)
      %4627 = stablehlo.transpose %4626, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc4532)
      %4628 = stablehlo.dot_general %4624, %4627, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc4533)
      %4629 = "stablehlo.all_reduce"(%4628) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10898"), %arg1239: tensor<bf16> loc("dot.10898")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4533)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4533)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4533)
      %4630 = stablehlo.reshape %4629 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4534)
      %4631 = stablehlo.reshape %arg700 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4535)
      %4632 = stablehlo.reshape %4631 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4536)
      %4633 = stablehlo.broadcast_in_dim %4632, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4537)
      %4634 = stablehlo.add %4630, %4633 : tensor<1x128x360xbf16> loc(#loc4538)
      %4635 = stablehlo.add %4518, %4634 : tensor<1x128x360xbf16> loc(#loc4539)
      %4636 = stablehlo.reshape %arg1089 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4540)
      %4637 = stablehlo.reshape %4636 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4541)
      %4638 = stablehlo.convert %4637 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4542)
      %4639 = stablehlo.broadcast_in_dim %4638, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4543)
      %4640 = stablehlo.convert %4635 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4544)
      %4641 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4642 = stablehlo.power %4640, %4641 : tensor<1x128x360xf32> loc(#loc4545)
      %4643 = stablehlo.reduce(%4642 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4546)
      %4644 = "stablehlo.all_reduce"(%4643) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.10916"), %arg1239: tensor<f32> loc("reduce.10916")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4546)
        stablehlo.return %7360 : tensor<f32> loc(#loc4546)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4546)
      %4645 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4646 = stablehlo.multiply %4644, %4645 : tensor<1x128xf32> loc(#loc4547)
      %4647 = stablehlo.reshape %4646 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4548)
      %4648 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4649 = stablehlo.add %4647, %4648 : tensor<1x128x1xf32> loc(#loc4549)
      %4650 = stablehlo.rsqrt %4649 : tensor<1x128x1xf32> loc(#loc4550)
      %4651 = stablehlo.reshape %4650 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4551)
      %4652 = stablehlo.broadcast_in_dim %4651, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4552)
      %4653 = stablehlo.multiply %4640, %4652 : tensor<1x128x360xf32> loc(#loc4553)
      %4654 = stablehlo.multiply %4639, %4653 : tensor<1x128x360xf32> loc(#loc4554)
      %4655 = stablehlo.convert %4654 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4555)
      %4656 = stablehlo.reshape %4655 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4556)
      %4657 = stablehlo.broadcast_in_dim %4656, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4557)
      %4658 = stablehlo.dot_general %4657, %arg1093, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4558)
      %4659 = "stablehlo.all_reduce"(%4658) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11028"), %arg1239: tensor<bf16> loc("dot.11028")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4558)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4558)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4558)
      %4660 = stablehlo.reshape %arg1092 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc4559)
      %4661 = stablehlo.reshape %4660 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc4560)
      %4662 = stablehlo.broadcast_in_dim %4661, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4561)
      %4663 = stablehlo.add %4659, %4662 : tensor<32x128x5760xbf16> loc(#loc4562)
      %4664 = stablehlo.slice %4663 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4563)
      %4665 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4666 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4667 = stablehlo.clamp %4666, %4664, %4665 : tensor<32x128x2880xbf16> loc(#loc4564)
      %4668 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4669 = stablehlo.add %4667, %4668 : tensor<32x128x2880xbf16> loc(#loc4565)
      %4670 = stablehlo.slice %4663 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4566)
      %4671 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4672 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4673 = stablehlo.clamp %4671, %4670, %4672 : tensor<32x128x2880xbf16> loc(#loc4567)
      %4674 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4675 = stablehlo.multiply %4673, %4674 : tensor<32x128x2880xbf16> loc(#loc4568)
      %4676 = stablehlo.logistic %4675 : tensor<32x128x2880xbf16> loc(#loc4569)
      %4677 = stablehlo.multiply %4673, %4676 : tensor<32x128x2880xbf16> loc(#loc4570)
      %4678 = stablehlo.multiply %4669, %4677 : tensor<32x128x2880xbf16> loc(#loc4571)
      %4679 = stablehlo.dot_general %4678, %arg1091, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4572)
      %4680 = stablehlo.reshape %arg1090 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc4573)
      %4681 = stablehlo.reshape %4680 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc4574)
      %4682 = stablehlo.broadcast_in_dim %4681, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4575)
      %4683 = stablehlo.add %4679, %4682 : tensor<32x128x360xbf16> loc(#loc4576)
      %4684 = stablehlo.reshape %4683 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4577)
      %4685 = stablehlo.reshape %arg699 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4578)
      %4686 = stablehlo.reshape %4685 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4579)
      %4687 = stablehlo.transpose %4686, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4580)
      %4688 = stablehlo.dot_general %4656, %4687, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4581)
      %4689 = "stablehlo.all_reduce"(%4688) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.10944"), %arg1239: tensor<bf16> loc("dot.10944")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4581)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4581)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4581)
      %4690 = stablehlo.reshape %arg698 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4582)
      %4691 = stablehlo.reshape %4690 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4583)
      %4692 = stablehlo.broadcast_in_dim %4691, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc4584)
      %4693 = stablehlo.add %4689, %4692 : tensor<128x128xbf16> loc(#loc4585)
      %4694:2 = "stablehlo.sort"(%4693, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.10964"), %arg1239: tensor<bf16> loc("sort.10964"), %arg1240: tensor<i32> loc("sort.10964"), %arg1241: tensor<i32> loc("sort.10964")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4587)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc4586)
      %4695 = stablehlo.slice %4694#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc4588)
      %4696 = stablehlo.convert %4695 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc4589)
      %4697 = stablehlo.reshape %4696 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc4590)
      %4698 = stablehlo.concatenate %231, %4697, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc4591)
      %4699 = stablehlo.slice %4694#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc4592)
      %4700 = stablehlo.reduce(%4699 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4593)
      %4701 = stablehlo.broadcast_in_dim %4700, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4594)
      %4702 = stablehlo.subtract %4699, %4701 : tensor<128x4xbf16> loc(#loc4595)
      %4703 = stablehlo.exponential %4702 : tensor<128x4xbf16> loc(#loc4596)
      %4704 = stablehlo.reduce(%4703 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4597)
      %4705 = stablehlo.broadcast_in_dim %4704, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4598)
      %4706 = stablehlo.divide %4703, %4705 : tensor<128x4xbf16> loc(#loc4599)
      %4707 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %4708 = "stablehlo.all_gather"(%4707) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %4709 = "stablehlo.scatter"(%4708, %4698, %4706) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.10998"), %arg1239: tensor<bf16> loc("scatter.10998")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc4600)
      %4710 = stablehlo.reshape %4709 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc4600)
      %4711 = "stablehlo.all_to_all"(%4710) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc4600)
      %4712 = stablehlo.slice %4711 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc4600)
      %4713 = stablehlo.reshape %4712 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc4600)
      %4714 = stablehlo.transpose %4713, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc4601)
      %4715 = stablehlo.reshape %4714 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc4602)
      %4716 = stablehlo.broadcast_in_dim %4715, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4603)
      %4717 = stablehlo.multiply %4684, %4716 : tensor<32x1x128x360xbf16> loc(#loc4604)
      %4718 = stablehlo.reduce(%4717 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc4605)
      %4719 = "stablehlo.all_reduce"(%4718) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.11070"), %arg1239: tensor<bf16> loc("reduce.11070")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4605)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4605)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4605)
      %4720 = stablehlo.add %4635, %4719 : tensor<1x128x360xbf16> loc(#loc4606)
      %4721 = stablehlo.convert %4720 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4607)
      %4722 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4723 = stablehlo.power %4721, %4722 : tensor<1x128x360xf32> loc(#loc4608)
      %4724 = stablehlo.reduce(%4723 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4609)
      %4725 = "stablehlo.all_reduce"(%4724) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.11083"), %arg1239: tensor<f32> loc("reduce.11083")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4609)
        stablehlo.return %7360 : tensor<f32> loc(#loc4609)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4609)
      %4726 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4727 = stablehlo.multiply %4725, %4726 : tensor<1x128xf32> loc(#loc4610)
      %4728 = stablehlo.reshape %4727 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4611)
      %4729 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4730 = stablehlo.add %4728, %4729 : tensor<1x128x1xf32> loc(#loc4612)
      %4731 = stablehlo.rsqrt %4730 : tensor<1x128x1xf32> loc(#loc4613)
      %4732 = stablehlo.reshape %4731 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4614)
      %4733 = stablehlo.broadcast_in_dim %4732, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4615)
      %4734 = stablehlo.multiply %4721, %4733 : tensor<1x128x360xf32> loc(#loc4616)
      %4735 = stablehlo.multiply %4596, %4734 : tensor<1x128x360xf32> loc(#loc4617)
      %4736 = stablehlo.convert %4735 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4618)
      %4737 = stablehlo.reshape %4736 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4619)
      %4738 = stablehlo.reshape %arg1099 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc4620)
      %4739 = stablehlo.reshape %4738 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc4621)
      %4740 = stablehlo.transpose %4739, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc4622)
      %4741 = stablehlo.dot_general %4737, %4740, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4623)
      %4742 = "stablehlo.all_reduce"(%4741) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11197"), %arg1239: tensor<bf16> loc("dot.11197")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4623)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4623)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4623)
      %4743 = stablehlo.reshape %4742 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4624)
      %4744 = stablehlo.reshape %arg1098 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4625)
      %4745 = stablehlo.reshape %4744 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc4626)
      %4746 = stablehlo.broadcast_in_dim %4745, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4627)
      %4747 = stablehlo.add %4743, %4746 : tensor<1x128x1024xbf16> loc(#loc4628)
      %4748 = stablehlo.reshape %4747 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4629)
      %4749 = stablehlo.transpose %4748, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4630)
      %4750 = stablehlo.slice %4749 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4631)
      %4751 = stablehlo.multiply %4750, %64 : tensor<1x16x128x32xbf16> loc(#loc4632)
      %4752 = stablehlo.slice %4749 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4633)
      %4753 = stablehlo.multiply %4752, %70 : tensor<1x16x128x32xbf16> loc(#loc4634)
      %4754 = stablehlo.subtract %4751, %4753 : tensor<1x16x128x32xbf16> loc(#loc4635)
      %4755 = stablehlo.multiply %4752, %64 : tensor<1x16x128x32xbf16> loc(#loc4636)
      %4756 = stablehlo.multiply %4750, %70 : tensor<1x16x128x32xbf16> loc(#loc4637)
      %4757 = stablehlo.add %4755, %4756 : tensor<1x16x128x32xbf16> loc(#loc4638)
      %4758 = stablehlo.concatenate %4754, %4757, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4639)
      %4759 = stablehlo.reshape %arg1097 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4640)
      %4760 = stablehlo.reshape %4759 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4641)
      %4761 = stablehlo.transpose %4760, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4642)
      %4762 = stablehlo.dot_general %4737, %4761, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4643)
      %4763 = "stablehlo.all_reduce"(%4762) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11148"), %arg1239: tensor<bf16> loc("dot.11148")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4643)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4643)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4643)
      %4764 = stablehlo.reshape %4763 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4644)
      %4765 = stablehlo.reshape %arg1096 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4645)
      %4766 = stablehlo.reshape %4765 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4646)
      %4767 = stablehlo.broadcast_in_dim %4766, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4647)
      %4768 = stablehlo.add %4764, %4767 : tensor<1x128x128xbf16> loc(#loc4648)
      %4769 = stablehlo.reshape %4768 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4649)
      %4770 = stablehlo.transpose %4769, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4650)
      %4771 = stablehlo.slice %4770 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4651)
      %4772 = stablehlo.multiply %4771, %90 : tensor<1x2x128x32xbf16> loc(#loc4652)
      %4773 = stablehlo.slice %4770 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4653)
      %4774 = stablehlo.multiply %4773, %93 : tensor<1x2x128x32xbf16> loc(#loc4654)
      %4775 = stablehlo.subtract %4772, %4774 : tensor<1x2x128x32xbf16> loc(#loc4655)
      %4776 = stablehlo.multiply %4773, %90 : tensor<1x2x128x32xbf16> loc(#loc4656)
      %4777 = stablehlo.multiply %4771, %93 : tensor<1x2x128x32xbf16> loc(#loc4657)
      %4778 = stablehlo.add %4776, %4777 : tensor<1x2x128x32xbf16> loc(#loc4658)
      %4779 = stablehlo.concatenate %4775, %4778, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4659)
      %4780 = stablehlo.broadcast_in_dim %4779, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4660)
      %4781 = stablehlo.reshape %4780 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4661)
      %4782 = stablehlo.transpose %4781, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc4662)
      %4783 = stablehlo.dot_general %4758, %4782, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4663)
      %4784 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %4785 = stablehlo.multiply %4783, %4784 : tensor<1x16x128x128xbf16> loc(#loc4664)
      %4786 = stablehlo.add %4785, %341 : tensor<1x16x128x128xbf16> loc(#loc4665)
      %4787 = stablehlo.reshape %arg1095 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc477)
      %4788 = "stablehlo.all_to_all"(%4787) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc477)
      %4789 = stablehlo.slice %4788 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc477)
      %4790 = stablehlo.reshape %4789 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc477)
      %4791 = stablehlo.reshape %4790 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc4666)
      %4792 = stablehlo.reshape %4791 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4667)
      %4793 = stablehlo.broadcast_in_dim %4792, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc4668)
      %4794 = stablehlo.concatenate %4786, %4793, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4669)
      %4795 = stablehlo.reshape %arg1105 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4670)
      %4796 = stablehlo.reshape %4795 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4671)
      %4797 = stablehlo.convert %4796 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4672)
      %4798 = stablehlo.broadcast_in_dim %4797, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4673)
      %4799 = stablehlo.reduce(%4794 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4674)
      %4800 = stablehlo.broadcast_in_dim %4799, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4675)
      %4801 = stablehlo.subtract %4794, %4800 : tensor<1x16x128x129xbf16> loc(#loc4676)
      %4802 = stablehlo.reduce(%4801 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4677)
      %4803 = stablehlo.broadcast_in_dim %4802, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4678)
      %4804 = stablehlo.subtract %4801, %4803 : tensor<1x16x128x129xbf16> loc(#loc4679)
      %4805 = stablehlo.exponential %4804 : tensor<1x16x128x129xbf16> loc(#loc4680)
      %4806 = stablehlo.reduce(%4805 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4681)
      %4807 = stablehlo.broadcast_in_dim %4806, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4682)
      %4808 = stablehlo.divide %4805, %4807 : tensor<1x16x128x129xbf16> loc(#loc4683)
      %4809 = stablehlo.slice %4808 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4684)
      %4810 = stablehlo.reshape %arg697 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4685)
      %4811 = stablehlo.reshape %4810 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4686)
      %4812 = stablehlo.transpose %4811, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4687)
      %4813 = stablehlo.dot_general %4737, %4812, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4688)
      %4814 = "stablehlo.all_reduce"(%4813) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11111"), %arg1239: tensor<bf16> loc("dot.11111")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4688)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4688)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4688)
      %4815 = stablehlo.reshape %4814 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4689)
      %4816 = stablehlo.reshape %arg696 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4690)
      %4817 = stablehlo.reshape %4816 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4691)
      %4818 = stablehlo.broadcast_in_dim %4817, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4692)
      %4819 = stablehlo.add %4815, %4818 : tensor<1x128x128xbf16> loc(#loc4693)
      %4820 = stablehlo.reshape %4819 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4694)
      %4821 = stablehlo.transpose %4820, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4695)
      %4822 = stablehlo.broadcast_in_dim %4821, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4696)
      %4823 = stablehlo.reshape %4822 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4697)
      %4824 = stablehlo.dot_general %4809, %4823, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4698)
      %4825 = stablehlo.transpose %4824, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4699)
      %4826 = stablehlo.reshape %4825 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc4700)
      %4827 = stablehlo.reshape %arg695 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc4701)
      %4828 = stablehlo.reshape %4827 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc4702)
      %4829 = stablehlo.transpose %4828, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc4703)
      %4830 = stablehlo.dot_general %4826, %4829, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc4704)
      %4831 = "stablehlo.all_reduce"(%4830) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11304"), %arg1239: tensor<bf16> loc("dot.11304")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4704)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4704)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4704)
      %4832 = stablehlo.reshape %4831 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4705)
      %4833 = stablehlo.reshape %arg694 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4706)
      %4834 = stablehlo.reshape %4833 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4707)
      %4835 = stablehlo.broadcast_in_dim %4834, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4708)
      %4836 = stablehlo.add %4832, %4835 : tensor<1x128x360xbf16> loc(#loc4709)
      %4837 = stablehlo.add %4720, %4836 : tensor<1x128x360xbf16> loc(#loc4710)
      %4838 = stablehlo.reshape %arg1100 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4711)
      %4839 = stablehlo.reshape %4838 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4712)
      %4840 = stablehlo.convert %4839 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4713)
      %4841 = stablehlo.broadcast_in_dim %4840, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4714)
      %4842 = stablehlo.convert %4837 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4715)
      %4843 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4844 = stablehlo.power %4842, %4843 : tensor<1x128x360xf32> loc(#loc4716)
      %4845 = stablehlo.reduce(%4844 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4717)
      %4846 = "stablehlo.all_reduce"(%4845) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.11322"), %arg1239: tensor<f32> loc("reduce.11322")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4717)
        stablehlo.return %7360 : tensor<f32> loc(#loc4717)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4717)
      %4847 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4848 = stablehlo.multiply %4846, %4847 : tensor<1x128xf32> loc(#loc4718)
      %4849 = stablehlo.reshape %4848 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4719)
      %4850 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4851 = stablehlo.add %4849, %4850 : tensor<1x128x1xf32> loc(#loc4720)
      %4852 = stablehlo.rsqrt %4851 : tensor<1x128x1xf32> loc(#loc4721)
      %4853 = stablehlo.reshape %4852 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4722)
      %4854 = stablehlo.broadcast_in_dim %4853, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4723)
      %4855 = stablehlo.multiply %4842, %4854 : tensor<1x128x360xf32> loc(#loc4724)
      %4856 = stablehlo.multiply %4841, %4855 : tensor<1x128x360xf32> loc(#loc4725)
      %4857 = stablehlo.convert %4856 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4726)
      %4858 = stablehlo.reshape %4857 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4727)
      %4859 = stablehlo.broadcast_in_dim %4858, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4728)
      %4860 = stablehlo.dot_general %4859, %arg1104, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4729)
      %4861 = "stablehlo.all_reduce"(%4860) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11434"), %arg1239: tensor<bf16> loc("dot.11434")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4729)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4729)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4729)
      %4862 = stablehlo.reshape %arg1103 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc4730)
      %4863 = stablehlo.reshape %4862 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc4731)
      %4864 = stablehlo.broadcast_in_dim %4863, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4732)
      %4865 = stablehlo.add %4861, %4864 : tensor<32x128x5760xbf16> loc(#loc4733)
      %4866 = stablehlo.slice %4865 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4734)
      %4867 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4868 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4869 = stablehlo.clamp %4868, %4866, %4867 : tensor<32x128x2880xbf16> loc(#loc4735)
      %4870 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4871 = stablehlo.add %4869, %4870 : tensor<32x128x2880xbf16> loc(#loc4736)
      %4872 = stablehlo.slice %4865 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4737)
      %4873 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4874 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4875 = stablehlo.clamp %4873, %4872, %4874 : tensor<32x128x2880xbf16> loc(#loc4738)
      %4876 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %4877 = stablehlo.multiply %4875, %4876 : tensor<32x128x2880xbf16> loc(#loc4739)
      %4878 = stablehlo.logistic %4877 : tensor<32x128x2880xbf16> loc(#loc4740)
      %4879 = stablehlo.multiply %4875, %4878 : tensor<32x128x2880xbf16> loc(#loc4741)
      %4880 = stablehlo.multiply %4871, %4879 : tensor<32x128x2880xbf16> loc(#loc4742)
      %4881 = stablehlo.dot_general %4880, %arg1102, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4743)
      %4882 = stablehlo.reshape %arg1101 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc4744)
      %4883 = stablehlo.reshape %4882 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc4745)
      %4884 = stablehlo.broadcast_in_dim %4883, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4746)
      %4885 = stablehlo.add %4881, %4884 : tensor<32x128x360xbf16> loc(#loc4747)
      %4886 = stablehlo.reshape %4885 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4748)
      %4887 = stablehlo.reshape %arg693 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4749)
      %4888 = stablehlo.reshape %4887 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4750)
      %4889 = stablehlo.transpose %4888, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4751)
      %4890 = stablehlo.dot_general %4858, %4889, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4752)
      %4891 = "stablehlo.all_reduce"(%4890) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11350"), %arg1239: tensor<bf16> loc("dot.11350")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4752)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4752)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4752)
      %4892 = stablehlo.reshape %arg692 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4753)
      %4893 = stablehlo.reshape %4892 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4754)
      %4894 = stablehlo.broadcast_in_dim %4893, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc4755)
      %4895 = stablehlo.add %4891, %4894 : tensor<128x128xbf16> loc(#loc4756)
      %4896:2 = "stablehlo.sort"(%4895, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.11370"), %arg1239: tensor<bf16> loc("sort.11370"), %arg1240: tensor<i32> loc("sort.11370"), %arg1241: tensor<i32> loc("sort.11370")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4758)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc4757)
      %4897 = stablehlo.slice %4896#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc4759)
      %4898 = stablehlo.convert %4897 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc4760)
      %4899 = stablehlo.reshape %4898 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc4761)
      %4900 = stablehlo.concatenate %231, %4899, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc4762)
      %4901 = stablehlo.slice %4896#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc4763)
      %4902 = stablehlo.reduce(%4901 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4764)
      %4903 = stablehlo.broadcast_in_dim %4902, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4765)
      %4904 = stablehlo.subtract %4901, %4903 : tensor<128x4xbf16> loc(#loc4766)
      %4905 = stablehlo.exponential %4904 : tensor<128x4xbf16> loc(#loc4767)
      %4906 = stablehlo.reduce(%4905 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4768)
      %4907 = stablehlo.broadcast_in_dim %4906, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4769)
      %4908 = stablehlo.divide %4905, %4907 : tensor<128x4xbf16> loc(#loc4770)
      %4909 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %4910 = "stablehlo.all_gather"(%4909) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %4911 = "stablehlo.scatter"(%4910, %4900, %4908) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.11404"), %arg1239: tensor<bf16> loc("scatter.11404")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc4771)
      %4912 = stablehlo.reshape %4911 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc4771)
      %4913 = "stablehlo.all_to_all"(%4912) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc4771)
      %4914 = stablehlo.slice %4913 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc4771)
      %4915 = stablehlo.reshape %4914 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc4771)
      %4916 = stablehlo.transpose %4915, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc4772)
      %4917 = stablehlo.reshape %4916 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc4773)
      %4918 = stablehlo.broadcast_in_dim %4917, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4774)
      %4919 = stablehlo.multiply %4886, %4918 : tensor<32x1x128x360xbf16> loc(#loc4775)
      %4920 = stablehlo.reduce(%4919 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc4776)
      %4921 = "stablehlo.all_reduce"(%4920) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.11476"), %arg1239: tensor<bf16> loc("reduce.11476")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4776)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4776)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4776)
      %4922 = stablehlo.add %4837, %4921 : tensor<1x128x360xbf16> loc(#loc4777)
      %4923 = stablehlo.convert %4922 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4778)
      %4924 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %4925 = stablehlo.power %4923, %4924 : tensor<1x128x360xf32> loc(#loc4779)
      %4926 = stablehlo.reduce(%4925 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4780)
      %4927 = "stablehlo.all_reduce"(%4926) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.11489"), %arg1239: tensor<f32> loc("reduce.11489")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4780)
        stablehlo.return %7360 : tensor<f32> loc(#loc4780)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4780)
      %4928 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %4929 = stablehlo.multiply %4927, %4928 : tensor<1x128xf32> loc(#loc4781)
      %4930 = stablehlo.reshape %4929 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4782)
      %4931 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %4932 = stablehlo.add %4930, %4931 : tensor<1x128x1xf32> loc(#loc4783)
      %4933 = stablehlo.rsqrt %4932 : tensor<1x128x1xf32> loc(#loc4784)
      %4934 = stablehlo.reshape %4933 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4785)
      %4935 = stablehlo.broadcast_in_dim %4934, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4786)
      %4936 = stablehlo.multiply %4923, %4935 : tensor<1x128x360xf32> loc(#loc4787)
      %4937 = stablehlo.multiply %4798, %4936 : tensor<1x128x360xf32> loc(#loc4788)
      %4938 = stablehlo.convert %4937 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4789)
      %4939 = stablehlo.reshape %4938 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4790)
      %4940 = stablehlo.reshape %arg1110 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc4791)
      %4941 = stablehlo.reshape %4940 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc4792)
      %4942 = stablehlo.transpose %4941, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc4793)
      %4943 = stablehlo.dot_general %4939, %4942, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4794)
      %4944 = "stablehlo.all_reduce"(%4943) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11603"), %arg1239: tensor<bf16> loc("dot.11603")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4794)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4794)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4794)
      %4945 = stablehlo.reshape %4944 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4795)
      %4946 = stablehlo.reshape %arg1109 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4796)
      %4947 = stablehlo.reshape %4946 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc4797)
      %4948 = stablehlo.broadcast_in_dim %4947, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4798)
      %4949 = stablehlo.add %4945, %4948 : tensor<1x128x1024xbf16> loc(#loc4799)
      %4950 = stablehlo.reshape %4949 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4800)
      %4951 = stablehlo.transpose %4950, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4801)
      %4952 = stablehlo.slice %4951 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4802)
      %4953 = stablehlo.multiply %4952, %64 : tensor<1x16x128x32xbf16> loc(#loc4803)
      %4954 = stablehlo.slice %4951 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4804)
      %4955 = stablehlo.multiply %4954, %70 : tensor<1x16x128x32xbf16> loc(#loc4805)
      %4956 = stablehlo.subtract %4953, %4955 : tensor<1x16x128x32xbf16> loc(#loc4806)
      %4957 = stablehlo.multiply %4954, %64 : tensor<1x16x128x32xbf16> loc(#loc4807)
      %4958 = stablehlo.multiply %4952, %70 : tensor<1x16x128x32xbf16> loc(#loc4808)
      %4959 = stablehlo.add %4957, %4958 : tensor<1x16x128x32xbf16> loc(#loc4809)
      %4960 = stablehlo.concatenate %4956, %4959, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4810)
      %4961 = stablehlo.reshape %arg1108 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4811)
      %4962 = stablehlo.reshape %4961 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4812)
      %4963 = stablehlo.transpose %4962, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4813)
      %4964 = stablehlo.dot_general %4939, %4963, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4814)
      %4965 = "stablehlo.all_reduce"(%4964) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11554"), %arg1239: tensor<bf16> loc("dot.11554")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4814)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4814)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4814)
      %4966 = stablehlo.reshape %4965 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4815)
      %4967 = stablehlo.reshape %arg1107 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4816)
      %4968 = stablehlo.reshape %4967 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4817)
      %4969 = stablehlo.broadcast_in_dim %4968, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4818)
      %4970 = stablehlo.add %4966, %4969 : tensor<1x128x128xbf16> loc(#loc4819)
      %4971 = stablehlo.reshape %4970 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4820)
      %4972 = stablehlo.transpose %4971, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4821)
      %4973 = stablehlo.slice %4972 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4822)
      %4974 = stablehlo.multiply %4973, %90 : tensor<1x2x128x32xbf16> loc(#loc4823)
      %4975 = stablehlo.slice %4972 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4824)
      %4976 = stablehlo.multiply %4975, %93 : tensor<1x2x128x32xbf16> loc(#loc4825)
      %4977 = stablehlo.subtract %4974, %4976 : tensor<1x2x128x32xbf16> loc(#loc4826)
      %4978 = stablehlo.multiply %4975, %90 : tensor<1x2x128x32xbf16> loc(#loc4827)
      %4979 = stablehlo.multiply %4973, %93 : tensor<1x2x128x32xbf16> loc(#loc4828)
      %4980 = stablehlo.add %4978, %4979 : tensor<1x2x128x32xbf16> loc(#loc4829)
      %4981 = stablehlo.concatenate %4977, %4980, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4830)
      %4982 = stablehlo.broadcast_in_dim %4981, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4831)
      %4983 = stablehlo.reshape %4982 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4832)
      %4984 = stablehlo.transpose %4983, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc4833)
      %4985 = stablehlo.dot_general %4960, %4984, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4834)
      %4986 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %4987 = stablehlo.multiply %4985, %4986 : tensor<1x16x128x128xbf16> loc(#loc4835)
      %4988 = stablehlo.add %4987, %128 : tensor<1x16x128x128xbf16> loc(#loc4836)
      %4989 = stablehlo.reshape %arg1106 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc488)
      %4990 = "stablehlo.all_to_all"(%4989) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc488)
      %4991 = stablehlo.slice %4990 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc488)
      %4992 = stablehlo.reshape %4991 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc488)
      %4993 = stablehlo.reshape %4992 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc4837)
      %4994 = stablehlo.reshape %4993 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc4838)
      %4995 = stablehlo.broadcast_in_dim %4994, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc4839)
      %4996 = stablehlo.concatenate %4988, %4995, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4840)
      %4997 = stablehlo.reshape %arg1116 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4841)
      %4998 = stablehlo.reshape %4997 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4842)
      %4999 = stablehlo.convert %4998 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4843)
      %5000 = stablehlo.broadcast_in_dim %4999, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4844)
      %5001 = stablehlo.reduce(%4996 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4845)
      %5002 = stablehlo.broadcast_in_dim %5001, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4846)
      %5003 = stablehlo.subtract %4996, %5002 : tensor<1x16x128x129xbf16> loc(#loc4847)
      %5004 = stablehlo.reduce(%5003 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4848)
      %5005 = stablehlo.broadcast_in_dim %5004, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4849)
      %5006 = stablehlo.subtract %5003, %5005 : tensor<1x16x128x129xbf16> loc(#loc4850)
      %5007 = stablehlo.exponential %5006 : tensor<1x16x128x129xbf16> loc(#loc4851)
      %5008 = stablehlo.reduce(%5007 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc4852)
      %5009 = stablehlo.broadcast_in_dim %5008, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc4853)
      %5010 = stablehlo.divide %5007, %5009 : tensor<1x16x128x129xbf16> loc(#loc4854)
      %5011 = stablehlo.slice %5010 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc4855)
      %5012 = stablehlo.reshape %arg691 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4856)
      %5013 = stablehlo.reshape %5012 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4857)
      %5014 = stablehlo.transpose %5013, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4858)
      %5015 = stablehlo.dot_general %4939, %5014, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4859)
      %5016 = "stablehlo.all_reduce"(%5015) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11517"), %arg1239: tensor<bf16> loc("dot.11517")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4859)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4859)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4859)
      %5017 = stablehlo.reshape %5016 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4860)
      %5018 = stablehlo.reshape %arg690 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4861)
      %5019 = stablehlo.reshape %5018 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4862)
      %5020 = stablehlo.broadcast_in_dim %5019, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4863)
      %5021 = stablehlo.add %5017, %5020 : tensor<1x128x128xbf16> loc(#loc4864)
      %5022 = stablehlo.reshape %5021 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4865)
      %5023 = stablehlo.transpose %5022, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4866)
      %5024 = stablehlo.broadcast_in_dim %5023, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc4867)
      %5025 = stablehlo.reshape %5024 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4868)
      %5026 = stablehlo.dot_general %5011, %5025, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4869)
      %5027 = stablehlo.transpose %5026, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4870)
      %5028 = stablehlo.reshape %5027 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc4871)
      %5029 = stablehlo.reshape %arg689 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc4872)
      %5030 = stablehlo.reshape %5029 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc4873)
      %5031 = stablehlo.transpose %5030, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc4874)
      %5032 = stablehlo.dot_general %5028, %5031, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc4875)
      %5033 = "stablehlo.all_reduce"(%5032) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11710"), %arg1239: tensor<bf16> loc("dot.11710")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4875)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4875)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4875)
      %5034 = stablehlo.reshape %5033 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4876)
      %5035 = stablehlo.reshape %arg688 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4877)
      %5036 = stablehlo.reshape %5035 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4878)
      %5037 = stablehlo.broadcast_in_dim %5036, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4879)
      %5038 = stablehlo.add %5034, %5037 : tensor<1x128x360xbf16> loc(#loc4880)
      %5039 = stablehlo.add %4922, %5038 : tensor<1x128x360xbf16> loc(#loc4881)
      %5040 = stablehlo.reshape %arg1111 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc4882)
      %5041 = stablehlo.reshape %5040 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc4883)
      %5042 = stablehlo.convert %5041 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc4884)
      %5043 = stablehlo.broadcast_in_dim %5042, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc4885)
      %5044 = stablehlo.convert %5039 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4886)
      %5045 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5046 = stablehlo.power %5044, %5045 : tensor<1x128x360xf32> loc(#loc4887)
      %5047 = stablehlo.reduce(%5046 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4888)
      %5048 = "stablehlo.all_reduce"(%5047) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.11728"), %arg1239: tensor<f32> loc("reduce.11728")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4888)
        stablehlo.return %7360 : tensor<f32> loc(#loc4888)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4888)
      %5049 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5050 = stablehlo.multiply %5048, %5049 : tensor<1x128xf32> loc(#loc4889)
      %5051 = stablehlo.reshape %5050 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4890)
      %5052 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5053 = stablehlo.add %5051, %5052 : tensor<1x128x1xf32> loc(#loc4891)
      %5054 = stablehlo.rsqrt %5053 : tensor<1x128x1xf32> loc(#loc4892)
      %5055 = stablehlo.reshape %5054 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4893)
      %5056 = stablehlo.broadcast_in_dim %5055, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4894)
      %5057 = stablehlo.multiply %5044, %5056 : tensor<1x128x360xf32> loc(#loc4895)
      %5058 = stablehlo.multiply %5043, %5057 : tensor<1x128x360xf32> loc(#loc4896)
      %5059 = stablehlo.convert %5058 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4897)
      %5060 = stablehlo.reshape %5059 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4898)
      %5061 = stablehlo.broadcast_in_dim %5060, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4899)
      %5062 = stablehlo.dot_general %5061, %arg1115, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4900)
      %5063 = "stablehlo.all_reduce"(%5062) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11840"), %arg1239: tensor<bf16> loc("dot.11840")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4900)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4900)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4900)
      %5064 = stablehlo.reshape %arg1114 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc4901)
      %5065 = stablehlo.reshape %5064 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc4902)
      %5066 = stablehlo.broadcast_in_dim %5065, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc4903)
      %5067 = stablehlo.add %5063, %5066 : tensor<32x128x5760xbf16> loc(#loc4904)
      %5068 = stablehlo.slice %5067 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4905)
      %5069 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5070 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5071 = stablehlo.clamp %5070, %5068, %5069 : tensor<32x128x2880xbf16> loc(#loc4906)
      %5072 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5073 = stablehlo.add %5071, %5072 : tensor<32x128x2880xbf16> loc(#loc4907)
      %5074 = stablehlo.slice %5067 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc4908)
      %5075 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5076 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5077 = stablehlo.clamp %5075, %5074, %5076 : tensor<32x128x2880xbf16> loc(#loc4909)
      %5078 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5079 = stablehlo.multiply %5077, %5078 : tensor<32x128x2880xbf16> loc(#loc4910)
      %5080 = stablehlo.logistic %5079 : tensor<32x128x2880xbf16> loc(#loc4911)
      %5081 = stablehlo.multiply %5077, %5080 : tensor<32x128x2880xbf16> loc(#loc4912)
      %5082 = stablehlo.multiply %5073, %5081 : tensor<32x128x2880xbf16> loc(#loc4913)
      %5083 = stablehlo.dot_general %5082, %arg1113, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4914)
      %5084 = stablehlo.reshape %arg1112 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc4915)
      %5085 = stablehlo.reshape %5084 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc4916)
      %5086 = stablehlo.broadcast_in_dim %5085, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc4917)
      %5087 = stablehlo.add %5083, %5086 : tensor<32x128x360xbf16> loc(#loc4918)
      %5088 = stablehlo.reshape %5087 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4919)
      %5089 = stablehlo.reshape %arg687 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4920)
      %5090 = stablehlo.reshape %5089 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4921)
      %5091 = stablehlo.transpose %5090, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4922)
      %5092 = stablehlo.dot_general %5060, %5091, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4923)
      %5093 = "stablehlo.all_reduce"(%5092) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11756"), %arg1239: tensor<bf16> loc("dot.11756")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4923)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4923)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4923)
      %5094 = stablehlo.reshape %arg686 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4924)
      %5095 = stablehlo.reshape %5094 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4925)
      %5096 = stablehlo.broadcast_in_dim %5095, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc4926)
      %5097 = stablehlo.add %5093, %5096 : tensor<128x128xbf16> loc(#loc4927)
      %5098:2 = "stablehlo.sort"(%5097, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.11776"), %arg1239: tensor<bf16> loc("sort.11776"), %arg1240: tensor<i32> loc("sort.11776"), %arg1241: tensor<i32> loc("sort.11776")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc4929)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc4928)
      %5099 = stablehlo.slice %5098#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc4930)
      %5100 = stablehlo.convert %5099 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc4931)
      %5101 = stablehlo.reshape %5100 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc4932)
      %5102 = stablehlo.concatenate %231, %5101, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc4933)
      %5103 = stablehlo.slice %5098#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc4934)
      %5104 = stablehlo.reduce(%5103 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4935)
      %5105 = stablehlo.broadcast_in_dim %5104, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4936)
      %5106 = stablehlo.subtract %5103, %5105 : tensor<128x4xbf16> loc(#loc4937)
      %5107 = stablehlo.exponential %5106 : tensor<128x4xbf16> loc(#loc4938)
      %5108 = stablehlo.reduce(%5107 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc4939)
      %5109 = stablehlo.broadcast_in_dim %5108, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc4940)
      %5110 = stablehlo.divide %5107, %5109 : tensor<128x4xbf16> loc(#loc4941)
      %5111 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %5112 = "stablehlo.all_gather"(%5111) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %5113 = "stablehlo.scatter"(%5112, %5102, %5110) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.11810"), %arg1239: tensor<bf16> loc("scatter.11810")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc4942)
      %5114 = stablehlo.reshape %5113 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc4942)
      %5115 = "stablehlo.all_to_all"(%5114) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc4942)
      %5116 = stablehlo.slice %5115 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc4942)
      %5117 = stablehlo.reshape %5116 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc4942)
      %5118 = stablehlo.transpose %5117, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc4943)
      %5119 = stablehlo.reshape %5118 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc4944)
      %5120 = stablehlo.broadcast_in_dim %5119, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc4945)
      %5121 = stablehlo.multiply %5088, %5120 : tensor<32x1x128x360xbf16> loc(#loc4946)
      %5122 = stablehlo.reduce(%5121 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc4947)
      %5123 = "stablehlo.all_reduce"(%5122) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.11882"), %arg1239: tensor<bf16> loc("reduce.11882")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4947)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4947)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4947)
      %5124 = stablehlo.add %5039, %5123 : tensor<1x128x360xbf16> loc(#loc4948)
      %5125 = stablehlo.convert %5124 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc4949)
      %5126 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5127 = stablehlo.power %5125, %5126 : tensor<1x128x360xf32> loc(#loc4950)
      %5128 = stablehlo.reduce(%5127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc4951)
      %5129 = "stablehlo.all_reduce"(%5128) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.11895"), %arg1239: tensor<f32> loc("reduce.11895")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc4951)
        stablehlo.return %7360 : tensor<f32> loc(#loc4951)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc4951)
      %5130 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5131 = stablehlo.multiply %5129, %5130 : tensor<1x128xf32> loc(#loc4952)
      %5132 = stablehlo.reshape %5131 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc4953)
      %5133 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5134 = stablehlo.add %5132, %5133 : tensor<1x128x1xf32> loc(#loc4954)
      %5135 = stablehlo.rsqrt %5134 : tensor<1x128x1xf32> loc(#loc4955)
      %5136 = stablehlo.reshape %5135 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc4956)
      %5137 = stablehlo.broadcast_in_dim %5136, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc4957)
      %5138 = stablehlo.multiply %5125, %5137 : tensor<1x128x360xf32> loc(#loc4958)
      %5139 = stablehlo.multiply %5000, %5138 : tensor<1x128x360xf32> loc(#loc4959)
      %5140 = stablehlo.convert %5139 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc4960)
      %5141 = stablehlo.reshape %5140 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4961)
      %5142 = stablehlo.reshape %arg1121 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc4962)
      %5143 = stablehlo.reshape %5142 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc4963)
      %5144 = stablehlo.transpose %5143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc4964)
      %5145 = stablehlo.dot_general %5141, %5144, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4965)
      %5146 = "stablehlo.all_reduce"(%5145) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12009"), %arg1239: tensor<bf16> loc("dot.12009")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4965)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4965)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc4965)
      %5147 = stablehlo.reshape %5146 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4966)
      %5148 = stablehlo.reshape %arg1120 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc4967)
      %5149 = stablehlo.reshape %5148 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc4968)
      %5150 = stablehlo.broadcast_in_dim %5149, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc4969)
      %5151 = stablehlo.add %5147, %5150 : tensor<1x128x1024xbf16> loc(#loc4970)
      %5152 = stablehlo.reshape %5151 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc4971)
      %5153 = stablehlo.transpose %5152, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4972)
      %5154 = stablehlo.slice %5153 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4973)
      %5155 = stablehlo.multiply %5154, %64 : tensor<1x16x128x32xbf16> loc(#loc4974)
      %5156 = stablehlo.slice %5153 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc4975)
      %5157 = stablehlo.multiply %5156, %70 : tensor<1x16x128x32xbf16> loc(#loc4976)
      %5158 = stablehlo.subtract %5155, %5157 : tensor<1x16x128x32xbf16> loc(#loc4977)
      %5159 = stablehlo.multiply %5156, %64 : tensor<1x16x128x32xbf16> loc(#loc4978)
      %5160 = stablehlo.multiply %5154, %70 : tensor<1x16x128x32xbf16> loc(#loc4979)
      %5161 = stablehlo.add %5159, %5160 : tensor<1x16x128x32xbf16> loc(#loc4980)
      %5162 = stablehlo.concatenate %5158, %5161, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc4981)
      %5163 = stablehlo.reshape %arg1119 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc4982)
      %5164 = stablehlo.reshape %5163 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc4983)
      %5165 = stablehlo.transpose %5164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc4984)
      %5166 = stablehlo.dot_general %5141, %5165, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc4985)
      %5167 = "stablehlo.all_reduce"(%5166) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11960"), %arg1239: tensor<bf16> loc("dot.11960")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc4985)
        stablehlo.return %7360 : tensor<bf16> loc(#loc4985)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc4985)
      %5168 = stablehlo.reshape %5167 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4986)
      %5169 = stablehlo.reshape %arg1118 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4987)
      %5170 = stablehlo.reshape %5169 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4988)
      %5171 = stablehlo.broadcast_in_dim %5170, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc4989)
      %5172 = stablehlo.add %5168, %5171 : tensor<1x128x128xbf16> loc(#loc4990)
      %5173 = stablehlo.reshape %5172 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc4991)
      %5174 = stablehlo.transpose %5173, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc4992)
      %5175 = stablehlo.slice %5174 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4993)
      %5176 = stablehlo.multiply %5175, %90 : tensor<1x2x128x32xbf16> loc(#loc4994)
      %5177 = stablehlo.slice %5174 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc4995)
      %5178 = stablehlo.multiply %5177, %93 : tensor<1x2x128x32xbf16> loc(#loc4996)
      %5179 = stablehlo.subtract %5176, %5178 : tensor<1x2x128x32xbf16> loc(#loc4997)
      %5180 = stablehlo.multiply %5177, %90 : tensor<1x2x128x32xbf16> loc(#loc4998)
      %5181 = stablehlo.multiply %5175, %93 : tensor<1x2x128x32xbf16> loc(#loc4999)
      %5182 = stablehlo.add %5180, %5181 : tensor<1x2x128x32xbf16> loc(#loc5000)
      %5183 = stablehlo.concatenate %5179, %5182, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5001)
      %5184 = stablehlo.broadcast_in_dim %5183, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5002)
      %5185 = stablehlo.reshape %5184 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5003)
      %5186 = stablehlo.transpose %5185, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc5004)
      %5187 = stablehlo.dot_general %5162, %5186, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5005)
      %5188 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %5189 = stablehlo.multiply %5187, %5188 : tensor<1x16x128x128xbf16> loc(#loc5006)
      %5190 = stablehlo.add %5189, %341 : tensor<1x16x128x128xbf16> loc(#loc5007)
      %5191 = stablehlo.reshape %arg1117 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc499)
      %5192 = "stablehlo.all_to_all"(%5191) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc499)
      %5193 = stablehlo.slice %5192 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc499)
      %5194 = stablehlo.reshape %5193 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc499)
      %5195 = stablehlo.reshape %5194 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc5008)
      %5196 = stablehlo.reshape %5195 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5009)
      %5197 = stablehlo.broadcast_in_dim %5196, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc5010)
      %5198 = stablehlo.concatenate %5190, %5197, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5011)
      %5199 = stablehlo.reshape %arg1127 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5012)
      %5200 = stablehlo.reshape %5199 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5013)
      %5201 = stablehlo.convert %5200 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5014)
      %5202 = stablehlo.broadcast_in_dim %5201, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5015)
      %5203 = stablehlo.reduce(%5198 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5016)
      %5204 = stablehlo.broadcast_in_dim %5203, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5017)
      %5205 = stablehlo.subtract %5198, %5204 : tensor<1x16x128x129xbf16> loc(#loc5018)
      %5206 = stablehlo.reduce(%5205 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5019)
      %5207 = stablehlo.broadcast_in_dim %5206, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5020)
      %5208 = stablehlo.subtract %5205, %5207 : tensor<1x16x128x129xbf16> loc(#loc5021)
      %5209 = stablehlo.exponential %5208 : tensor<1x16x128x129xbf16> loc(#loc5022)
      %5210 = stablehlo.reduce(%5209 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5023)
      %5211 = stablehlo.broadcast_in_dim %5210, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5024)
      %5212 = stablehlo.divide %5209, %5211 : tensor<1x16x128x129xbf16> loc(#loc5025)
      %5213 = stablehlo.slice %5212 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5026)
      %5214 = stablehlo.reshape %arg685 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5027)
      %5215 = stablehlo.reshape %5214 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5028)
      %5216 = stablehlo.transpose %5215, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5029)
      %5217 = stablehlo.dot_general %5141, %5216, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5030)
      %5218 = "stablehlo.all_reduce"(%5217) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.11923"), %arg1239: tensor<bf16> loc("dot.11923")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5030)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5030)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5030)
      %5219 = stablehlo.reshape %5218 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5031)
      %5220 = stablehlo.reshape %arg684 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5032)
      %5221 = stablehlo.reshape %5220 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5033)
      %5222 = stablehlo.broadcast_in_dim %5221, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5034)
      %5223 = stablehlo.add %5219, %5222 : tensor<1x128x128xbf16> loc(#loc5035)
      %5224 = stablehlo.reshape %5223 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5036)
      %5225 = stablehlo.transpose %5224, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5037)
      %5226 = stablehlo.broadcast_in_dim %5225, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5038)
      %5227 = stablehlo.reshape %5226 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5039)
      %5228 = stablehlo.dot_general %5213, %5227, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5040)
      %5229 = stablehlo.transpose %5228, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5041)
      %5230 = stablehlo.reshape %5229 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc5042)
      %5231 = stablehlo.reshape %arg683 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc5043)
      %5232 = stablehlo.reshape %5231 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc5044)
      %5233 = stablehlo.transpose %5232, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc5045)
      %5234 = stablehlo.dot_general %5230, %5233, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc5046)
      %5235 = "stablehlo.all_reduce"(%5234) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12116"), %arg1239: tensor<bf16> loc("dot.12116")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5046)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5046)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5046)
      %5236 = stablehlo.reshape %5235 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5047)
      %5237 = stablehlo.reshape %arg682 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5048)
      %5238 = stablehlo.reshape %5237 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5049)
      %5239 = stablehlo.broadcast_in_dim %5238, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5050)
      %5240 = stablehlo.add %5236, %5239 : tensor<1x128x360xbf16> loc(#loc5051)
      %5241 = stablehlo.add %5124, %5240 : tensor<1x128x360xbf16> loc(#loc5052)
      %5242 = stablehlo.reshape %arg1122 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5053)
      %5243 = stablehlo.reshape %5242 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5054)
      %5244 = stablehlo.convert %5243 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5055)
      %5245 = stablehlo.broadcast_in_dim %5244, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5056)
      %5246 = stablehlo.convert %5241 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5057)
      %5247 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5248 = stablehlo.power %5246, %5247 : tensor<1x128x360xf32> loc(#loc5058)
      %5249 = stablehlo.reduce(%5248 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5059)
      %5250 = "stablehlo.all_reduce"(%5249) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.12134"), %arg1239: tensor<f32> loc("reduce.12134")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5059)
        stablehlo.return %7360 : tensor<f32> loc(#loc5059)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5059)
      %5251 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5252 = stablehlo.multiply %5250, %5251 : tensor<1x128xf32> loc(#loc5060)
      %5253 = stablehlo.reshape %5252 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5061)
      %5254 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5255 = stablehlo.add %5253, %5254 : tensor<1x128x1xf32> loc(#loc5062)
      %5256 = stablehlo.rsqrt %5255 : tensor<1x128x1xf32> loc(#loc5063)
      %5257 = stablehlo.reshape %5256 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5064)
      %5258 = stablehlo.broadcast_in_dim %5257, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5065)
      %5259 = stablehlo.multiply %5246, %5258 : tensor<1x128x360xf32> loc(#loc5066)
      %5260 = stablehlo.multiply %5245, %5259 : tensor<1x128x360xf32> loc(#loc5067)
      %5261 = stablehlo.convert %5260 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5068)
      %5262 = stablehlo.reshape %5261 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5069)
      %5263 = stablehlo.broadcast_in_dim %5262, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5070)
      %5264 = stablehlo.dot_general %5263, %arg1126, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5071)
      %5265 = "stablehlo.all_reduce"(%5264) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12246"), %arg1239: tensor<bf16> loc("dot.12246")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5071)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5071)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5071)
      %5266 = stablehlo.reshape %arg1125 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc5072)
      %5267 = stablehlo.reshape %5266 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc5073)
      %5268 = stablehlo.broadcast_in_dim %5267, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5074)
      %5269 = stablehlo.add %5265, %5268 : tensor<32x128x5760xbf16> loc(#loc5075)
      %5270 = stablehlo.slice %5269 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5076)
      %5271 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5272 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5273 = stablehlo.clamp %5272, %5270, %5271 : tensor<32x128x2880xbf16> loc(#loc5077)
      %5274 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5275 = stablehlo.add %5273, %5274 : tensor<32x128x2880xbf16> loc(#loc5078)
      %5276 = stablehlo.slice %5269 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5079)
      %5277 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5278 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5279 = stablehlo.clamp %5277, %5276, %5278 : tensor<32x128x2880xbf16> loc(#loc5080)
      %5280 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5281 = stablehlo.multiply %5279, %5280 : tensor<32x128x2880xbf16> loc(#loc5081)
      %5282 = stablehlo.logistic %5281 : tensor<32x128x2880xbf16> loc(#loc5082)
      %5283 = stablehlo.multiply %5279, %5282 : tensor<32x128x2880xbf16> loc(#loc5083)
      %5284 = stablehlo.multiply %5275, %5283 : tensor<32x128x2880xbf16> loc(#loc5084)
      %5285 = stablehlo.dot_general %5284, %arg1124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5085)
      %5286 = stablehlo.reshape %arg1123 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc5086)
      %5287 = stablehlo.reshape %5286 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc5087)
      %5288 = stablehlo.broadcast_in_dim %5287, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5088)
      %5289 = stablehlo.add %5285, %5288 : tensor<32x128x360xbf16> loc(#loc5089)
      %5290 = stablehlo.reshape %5289 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5090)
      %5291 = stablehlo.reshape %arg681 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5091)
      %5292 = stablehlo.reshape %5291 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5092)
      %5293 = stablehlo.transpose %5292, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5093)
      %5294 = stablehlo.dot_general %5262, %5293, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5094)
      %5295 = "stablehlo.all_reduce"(%5294) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12162"), %arg1239: tensor<bf16> loc("dot.12162")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5094)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5094)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5094)
      %5296 = stablehlo.reshape %arg680 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5095)
      %5297 = stablehlo.reshape %5296 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5096)
      %5298 = stablehlo.broadcast_in_dim %5297, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc5097)
      %5299 = stablehlo.add %5295, %5298 : tensor<128x128xbf16> loc(#loc5098)
      %5300:2 = "stablehlo.sort"(%5299, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.12182"), %arg1239: tensor<bf16> loc("sort.12182"), %arg1240: tensor<i32> loc("sort.12182"), %arg1241: tensor<i32> loc("sort.12182")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc5100)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc5099)
      %5301 = stablehlo.slice %5300#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc5101)
      %5302 = stablehlo.convert %5301 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc5102)
      %5303 = stablehlo.reshape %5302 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc5103)
      %5304 = stablehlo.concatenate %231, %5303, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc5104)
      %5305 = stablehlo.slice %5300#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc5105)
      %5306 = stablehlo.reduce(%5305 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5106)
      %5307 = stablehlo.broadcast_in_dim %5306, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5107)
      %5308 = stablehlo.subtract %5305, %5307 : tensor<128x4xbf16> loc(#loc5108)
      %5309 = stablehlo.exponential %5308 : tensor<128x4xbf16> loc(#loc5109)
      %5310 = stablehlo.reduce(%5309 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5110)
      %5311 = stablehlo.broadcast_in_dim %5310, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5111)
      %5312 = stablehlo.divide %5309, %5311 : tensor<128x4xbf16> loc(#loc5112)
      %5313 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %5314 = "stablehlo.all_gather"(%5313) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %5315 = "stablehlo.scatter"(%5314, %5304, %5312) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.12216"), %arg1239: tensor<bf16> loc("scatter.12216")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc5113)
      %5316 = stablehlo.reshape %5315 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc5113)
      %5317 = "stablehlo.all_to_all"(%5316) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc5113)
      %5318 = stablehlo.slice %5317 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc5113)
      %5319 = stablehlo.reshape %5318 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc5113)
      %5320 = stablehlo.transpose %5319, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc5114)
      %5321 = stablehlo.reshape %5320 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc5115)
      %5322 = stablehlo.broadcast_in_dim %5321, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5116)
      %5323 = stablehlo.multiply %5290, %5322 : tensor<32x1x128x360xbf16> loc(#loc5117)
      %5324 = stablehlo.reduce(%5323 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc5118)
      %5325 = "stablehlo.all_reduce"(%5324) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.12288"), %arg1239: tensor<bf16> loc("reduce.12288")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5118)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5118)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5118)
      %5326 = stablehlo.add %5241, %5325 : tensor<1x128x360xbf16> loc(#loc5119)
      %5327 = stablehlo.convert %5326 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5120)
      %5328 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5329 = stablehlo.power %5327, %5328 : tensor<1x128x360xf32> loc(#loc5121)
      %5330 = stablehlo.reduce(%5329 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5122)
      %5331 = "stablehlo.all_reduce"(%5330) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.12301"), %arg1239: tensor<f32> loc("reduce.12301")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5122)
        stablehlo.return %7360 : tensor<f32> loc(#loc5122)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5122)
      %5332 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5333 = stablehlo.multiply %5331, %5332 : tensor<1x128xf32> loc(#loc5123)
      %5334 = stablehlo.reshape %5333 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5124)
      %5335 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5336 = stablehlo.add %5334, %5335 : tensor<1x128x1xf32> loc(#loc5125)
      %5337 = stablehlo.rsqrt %5336 : tensor<1x128x1xf32> loc(#loc5126)
      %5338 = stablehlo.reshape %5337 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5127)
      %5339 = stablehlo.broadcast_in_dim %5338, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5128)
      %5340 = stablehlo.multiply %5327, %5339 : tensor<1x128x360xf32> loc(#loc5129)
      %5341 = stablehlo.multiply %5202, %5340 : tensor<1x128x360xf32> loc(#loc5130)
      %5342 = stablehlo.convert %5341 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5131)
      %5343 = stablehlo.reshape %5342 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5132)
      %5344 = stablehlo.reshape %arg1132 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc5133)
      %5345 = stablehlo.reshape %5344 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc5134)
      %5346 = stablehlo.transpose %5345, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc5135)
      %5347 = stablehlo.dot_general %5343, %5346, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5136)
      %5348 = "stablehlo.all_reduce"(%5347) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12415"), %arg1239: tensor<bf16> loc("dot.12415")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5136)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5136)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5136)
      %5349 = stablehlo.reshape %5348 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5137)
      %5350 = stablehlo.reshape %arg1131 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc5138)
      %5351 = stablehlo.reshape %5350 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc5139)
      %5352 = stablehlo.broadcast_in_dim %5351, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5140)
      %5353 = stablehlo.add %5349, %5352 : tensor<1x128x1024xbf16> loc(#loc5141)
      %5354 = stablehlo.reshape %5353 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5142)
      %5355 = stablehlo.transpose %5354, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5143)
      %5356 = stablehlo.slice %5355 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5144)
      %5357 = stablehlo.multiply %5356, %64 : tensor<1x16x128x32xbf16> loc(#loc5145)
      %5358 = stablehlo.slice %5355 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5146)
      %5359 = stablehlo.multiply %5358, %70 : tensor<1x16x128x32xbf16> loc(#loc5147)
      %5360 = stablehlo.subtract %5357, %5359 : tensor<1x16x128x32xbf16> loc(#loc5148)
      %5361 = stablehlo.multiply %5358, %64 : tensor<1x16x128x32xbf16> loc(#loc5149)
      %5362 = stablehlo.multiply %5356, %70 : tensor<1x16x128x32xbf16> loc(#loc5150)
      %5363 = stablehlo.add %5361, %5362 : tensor<1x16x128x32xbf16> loc(#loc5151)
      %5364 = stablehlo.concatenate %5360, %5363, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5152)
      %5365 = stablehlo.reshape %arg1130 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5153)
      %5366 = stablehlo.reshape %5365 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5154)
      %5367 = stablehlo.transpose %5366, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5155)
      %5368 = stablehlo.dot_general %5343, %5367, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5156)
      %5369 = "stablehlo.all_reduce"(%5368) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12366"), %arg1239: tensor<bf16> loc("dot.12366")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5156)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5156)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5156)
      %5370 = stablehlo.reshape %5369 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5157)
      %5371 = stablehlo.reshape %arg1129 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5158)
      %5372 = stablehlo.reshape %5371 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5159)
      %5373 = stablehlo.broadcast_in_dim %5372, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5160)
      %5374 = stablehlo.add %5370, %5373 : tensor<1x128x128xbf16> loc(#loc5161)
      %5375 = stablehlo.reshape %5374 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5162)
      %5376 = stablehlo.transpose %5375, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5163)
      %5377 = stablehlo.slice %5376 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5164)
      %5378 = stablehlo.multiply %5377, %90 : tensor<1x2x128x32xbf16> loc(#loc5165)
      %5379 = stablehlo.slice %5376 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5166)
      %5380 = stablehlo.multiply %5379, %93 : tensor<1x2x128x32xbf16> loc(#loc5167)
      %5381 = stablehlo.subtract %5378, %5380 : tensor<1x2x128x32xbf16> loc(#loc5168)
      %5382 = stablehlo.multiply %5379, %90 : tensor<1x2x128x32xbf16> loc(#loc5169)
      %5383 = stablehlo.multiply %5377, %93 : tensor<1x2x128x32xbf16> loc(#loc5170)
      %5384 = stablehlo.add %5382, %5383 : tensor<1x2x128x32xbf16> loc(#loc5171)
      %5385 = stablehlo.concatenate %5381, %5384, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5172)
      %5386 = stablehlo.broadcast_in_dim %5385, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5173)
      %5387 = stablehlo.reshape %5386 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5174)
      %5388 = stablehlo.transpose %5387, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc5175)
      %5389 = stablehlo.dot_general %5364, %5388, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5176)
      %5390 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %5391 = stablehlo.multiply %5389, %5390 : tensor<1x16x128x128xbf16> loc(#loc5177)
      %5392 = stablehlo.add %5391, %128 : tensor<1x16x128x128xbf16> loc(#loc5178)
      %5393 = stablehlo.reshape %arg1128 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc510)
      %5394 = "stablehlo.all_to_all"(%5393) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc510)
      %5395 = stablehlo.slice %5394 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc510)
      %5396 = stablehlo.reshape %5395 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc510)
      %5397 = stablehlo.reshape %5396 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc5179)
      %5398 = stablehlo.reshape %5397 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5180)
      %5399 = stablehlo.broadcast_in_dim %5398, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc5181)
      %5400 = stablehlo.concatenate %5392, %5399, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5182)
      %5401 = stablehlo.reshape %arg1138 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5183)
      %5402 = stablehlo.reshape %5401 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5184)
      %5403 = stablehlo.convert %5402 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5185)
      %5404 = stablehlo.broadcast_in_dim %5403, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5186)
      %5405 = stablehlo.reduce(%5400 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5187)
      %5406 = stablehlo.broadcast_in_dim %5405, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5188)
      %5407 = stablehlo.subtract %5400, %5406 : tensor<1x16x128x129xbf16> loc(#loc5189)
      %5408 = stablehlo.reduce(%5407 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5190)
      %5409 = stablehlo.broadcast_in_dim %5408, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5191)
      %5410 = stablehlo.subtract %5407, %5409 : tensor<1x16x128x129xbf16> loc(#loc5192)
      %5411 = stablehlo.exponential %5410 : tensor<1x16x128x129xbf16> loc(#loc5193)
      %5412 = stablehlo.reduce(%5411 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5194)
      %5413 = stablehlo.broadcast_in_dim %5412, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5195)
      %5414 = stablehlo.divide %5411, %5413 : tensor<1x16x128x129xbf16> loc(#loc5196)
      %5415 = stablehlo.slice %5414 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5197)
      %5416 = stablehlo.reshape %arg679 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5198)
      %5417 = stablehlo.reshape %5416 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5199)
      %5418 = stablehlo.transpose %5417, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5200)
      %5419 = stablehlo.dot_general %5343, %5418, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5201)
      %5420 = "stablehlo.all_reduce"(%5419) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12329"), %arg1239: tensor<bf16> loc("dot.12329")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5201)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5201)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5201)
      %5421 = stablehlo.reshape %5420 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5202)
      %5422 = stablehlo.reshape %arg678 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5203)
      %5423 = stablehlo.reshape %5422 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5204)
      %5424 = stablehlo.broadcast_in_dim %5423, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5205)
      %5425 = stablehlo.add %5421, %5424 : tensor<1x128x128xbf16> loc(#loc5206)
      %5426 = stablehlo.reshape %5425 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5207)
      %5427 = stablehlo.transpose %5426, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5208)
      %5428 = stablehlo.broadcast_in_dim %5427, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5209)
      %5429 = stablehlo.reshape %5428 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5210)
      %5430 = stablehlo.dot_general %5415, %5429, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5211)
      %5431 = stablehlo.transpose %5430, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5212)
      %5432 = stablehlo.reshape %5431 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc5213)
      %5433 = stablehlo.reshape %arg677 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc5214)
      %5434 = stablehlo.reshape %5433 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc5215)
      %5435 = stablehlo.transpose %5434, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc5216)
      %5436 = stablehlo.dot_general %5432, %5435, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc5217)
      %5437 = "stablehlo.all_reduce"(%5436) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12522"), %arg1239: tensor<bf16> loc("dot.12522")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5217)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5217)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5217)
      %5438 = stablehlo.reshape %5437 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5218)
      %5439 = stablehlo.reshape %arg676 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5219)
      %5440 = stablehlo.reshape %5439 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5220)
      %5441 = stablehlo.broadcast_in_dim %5440, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5221)
      %5442 = stablehlo.add %5438, %5441 : tensor<1x128x360xbf16> loc(#loc5222)
      %5443 = stablehlo.add %5326, %5442 : tensor<1x128x360xbf16> loc(#loc5223)
      %5444 = stablehlo.reshape %arg1133 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5224)
      %5445 = stablehlo.reshape %5444 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5225)
      %5446 = stablehlo.convert %5445 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5226)
      %5447 = stablehlo.broadcast_in_dim %5446, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5227)
      %5448 = stablehlo.convert %5443 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5228)
      %5449 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5450 = stablehlo.power %5448, %5449 : tensor<1x128x360xf32> loc(#loc5229)
      %5451 = stablehlo.reduce(%5450 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5230)
      %5452 = "stablehlo.all_reduce"(%5451) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.12540"), %arg1239: tensor<f32> loc("reduce.12540")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5230)
        stablehlo.return %7360 : tensor<f32> loc(#loc5230)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5230)
      %5453 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5454 = stablehlo.multiply %5452, %5453 : tensor<1x128xf32> loc(#loc5231)
      %5455 = stablehlo.reshape %5454 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5232)
      %5456 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5457 = stablehlo.add %5455, %5456 : tensor<1x128x1xf32> loc(#loc5233)
      %5458 = stablehlo.rsqrt %5457 : tensor<1x128x1xf32> loc(#loc5234)
      %5459 = stablehlo.reshape %5458 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5235)
      %5460 = stablehlo.broadcast_in_dim %5459, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5236)
      %5461 = stablehlo.multiply %5448, %5460 : tensor<1x128x360xf32> loc(#loc5237)
      %5462 = stablehlo.multiply %5447, %5461 : tensor<1x128x360xf32> loc(#loc5238)
      %5463 = stablehlo.convert %5462 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5239)
      %5464 = stablehlo.reshape %5463 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5240)
      %5465 = stablehlo.broadcast_in_dim %5464, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5241)
      %5466 = stablehlo.dot_general %5465, %arg1137, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5242)
      %5467 = "stablehlo.all_reduce"(%5466) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12652"), %arg1239: tensor<bf16> loc("dot.12652")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5242)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5242)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5242)
      %5468 = stablehlo.reshape %arg1136 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc5243)
      %5469 = stablehlo.reshape %5468 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc5244)
      %5470 = stablehlo.broadcast_in_dim %5469, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5245)
      %5471 = stablehlo.add %5467, %5470 : tensor<32x128x5760xbf16> loc(#loc5246)
      %5472 = stablehlo.slice %5471 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5247)
      %5473 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5474 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5475 = stablehlo.clamp %5474, %5472, %5473 : tensor<32x128x2880xbf16> loc(#loc5248)
      %5476 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5477 = stablehlo.add %5475, %5476 : tensor<32x128x2880xbf16> loc(#loc5249)
      %5478 = stablehlo.slice %5471 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5250)
      %5479 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5480 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5481 = stablehlo.clamp %5479, %5478, %5480 : tensor<32x128x2880xbf16> loc(#loc5251)
      %5482 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5483 = stablehlo.multiply %5481, %5482 : tensor<32x128x2880xbf16> loc(#loc5252)
      %5484 = stablehlo.logistic %5483 : tensor<32x128x2880xbf16> loc(#loc5253)
      %5485 = stablehlo.multiply %5481, %5484 : tensor<32x128x2880xbf16> loc(#loc5254)
      %5486 = stablehlo.multiply %5477, %5485 : tensor<32x128x2880xbf16> loc(#loc5255)
      %5487 = stablehlo.dot_general %5486, %arg1135, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5256)
      %5488 = stablehlo.reshape %arg1134 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc5257)
      %5489 = stablehlo.reshape %5488 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc5258)
      %5490 = stablehlo.broadcast_in_dim %5489, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5259)
      %5491 = stablehlo.add %5487, %5490 : tensor<32x128x360xbf16> loc(#loc5260)
      %5492 = stablehlo.reshape %5491 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5261)
      %5493 = stablehlo.reshape %arg675 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5262)
      %5494 = stablehlo.reshape %5493 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5263)
      %5495 = stablehlo.transpose %5494, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5264)
      %5496 = stablehlo.dot_general %5464, %5495, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5265)
      %5497 = "stablehlo.all_reduce"(%5496) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12568"), %arg1239: tensor<bf16> loc("dot.12568")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5265)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5265)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5265)
      %5498 = stablehlo.reshape %arg674 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5266)
      %5499 = stablehlo.reshape %5498 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5267)
      %5500 = stablehlo.broadcast_in_dim %5499, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc5268)
      %5501 = stablehlo.add %5497, %5500 : tensor<128x128xbf16> loc(#loc5269)
      %5502:2 = "stablehlo.sort"(%5501, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.12588"), %arg1239: tensor<bf16> loc("sort.12588"), %arg1240: tensor<i32> loc("sort.12588"), %arg1241: tensor<i32> loc("sort.12588")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc5271)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc5270)
      %5503 = stablehlo.slice %5502#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc5272)
      %5504 = stablehlo.convert %5503 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc5273)
      %5505 = stablehlo.reshape %5504 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc5274)
      %5506 = stablehlo.concatenate %231, %5505, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc5275)
      %5507 = stablehlo.slice %5502#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc5276)
      %5508 = stablehlo.reduce(%5507 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5277)
      %5509 = stablehlo.broadcast_in_dim %5508, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5278)
      %5510 = stablehlo.subtract %5507, %5509 : tensor<128x4xbf16> loc(#loc5279)
      %5511 = stablehlo.exponential %5510 : tensor<128x4xbf16> loc(#loc5280)
      %5512 = stablehlo.reduce(%5511 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5281)
      %5513 = stablehlo.broadcast_in_dim %5512, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5282)
      %5514 = stablehlo.divide %5511, %5513 : tensor<128x4xbf16> loc(#loc5283)
      %5515 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %5516 = "stablehlo.all_gather"(%5515) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %5517 = "stablehlo.scatter"(%5516, %5506, %5514) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.12622"), %arg1239: tensor<bf16> loc("scatter.12622")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc5284)
      %5518 = stablehlo.reshape %5517 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc5284)
      %5519 = "stablehlo.all_to_all"(%5518) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc5284)
      %5520 = stablehlo.slice %5519 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc5284)
      %5521 = stablehlo.reshape %5520 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc5284)
      %5522 = stablehlo.transpose %5521, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc5285)
      %5523 = stablehlo.reshape %5522 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc5286)
      %5524 = stablehlo.broadcast_in_dim %5523, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5287)
      %5525 = stablehlo.multiply %5492, %5524 : tensor<32x1x128x360xbf16> loc(#loc5288)
      %5526 = stablehlo.reduce(%5525 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc5289)
      %5527 = "stablehlo.all_reduce"(%5526) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.12694"), %arg1239: tensor<bf16> loc("reduce.12694")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5289)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5289)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5289)
      %5528 = stablehlo.add %5443, %5527 : tensor<1x128x360xbf16> loc(#loc5290)
      %5529 = stablehlo.convert %5528 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5291)
      %5530 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5531 = stablehlo.power %5529, %5530 : tensor<1x128x360xf32> loc(#loc5292)
      %5532 = stablehlo.reduce(%5531 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5293)
      %5533 = "stablehlo.all_reduce"(%5532) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.12707"), %arg1239: tensor<f32> loc("reduce.12707")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5293)
        stablehlo.return %7360 : tensor<f32> loc(#loc5293)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5293)
      %5534 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5535 = stablehlo.multiply %5533, %5534 : tensor<1x128xf32> loc(#loc5294)
      %5536 = stablehlo.reshape %5535 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5295)
      %5537 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5538 = stablehlo.add %5536, %5537 : tensor<1x128x1xf32> loc(#loc5296)
      %5539 = stablehlo.rsqrt %5538 : tensor<1x128x1xf32> loc(#loc5297)
      %5540 = stablehlo.reshape %5539 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5298)
      %5541 = stablehlo.broadcast_in_dim %5540, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5299)
      %5542 = stablehlo.multiply %5529, %5541 : tensor<1x128x360xf32> loc(#loc5300)
      %5543 = stablehlo.multiply %5404, %5542 : tensor<1x128x360xf32> loc(#loc5301)
      %5544 = stablehlo.convert %5543 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5302)
      %5545 = stablehlo.reshape %5544 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5303)
      %5546 = stablehlo.reshape %arg1143 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc5304)
      %5547 = stablehlo.reshape %5546 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc5305)
      %5548 = stablehlo.transpose %5547, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc5306)
      %5549 = stablehlo.dot_general %5545, %5548, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5307)
      %5550 = "stablehlo.all_reduce"(%5549) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12821"), %arg1239: tensor<bf16> loc("dot.12821")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5307)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5307)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5307)
      %5551 = stablehlo.reshape %5550 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5308)
      %5552 = stablehlo.reshape %arg1142 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc5309)
      %5553 = stablehlo.reshape %5552 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc5310)
      %5554 = stablehlo.broadcast_in_dim %5553, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5311)
      %5555 = stablehlo.add %5551, %5554 : tensor<1x128x1024xbf16> loc(#loc5312)
      %5556 = stablehlo.reshape %5555 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5313)
      %5557 = stablehlo.transpose %5556, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5314)
      %5558 = stablehlo.slice %5557 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5315)
      %5559 = stablehlo.multiply %5558, %64 : tensor<1x16x128x32xbf16> loc(#loc5316)
      %5560 = stablehlo.slice %5557 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5317)
      %5561 = stablehlo.multiply %5560, %70 : tensor<1x16x128x32xbf16> loc(#loc5318)
      %5562 = stablehlo.subtract %5559, %5561 : tensor<1x16x128x32xbf16> loc(#loc5319)
      %5563 = stablehlo.multiply %5560, %64 : tensor<1x16x128x32xbf16> loc(#loc5320)
      %5564 = stablehlo.multiply %5558, %70 : tensor<1x16x128x32xbf16> loc(#loc5321)
      %5565 = stablehlo.add %5563, %5564 : tensor<1x16x128x32xbf16> loc(#loc5322)
      %5566 = stablehlo.concatenate %5562, %5565, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5323)
      %5567 = stablehlo.reshape %arg1141 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5324)
      %5568 = stablehlo.reshape %5567 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5325)
      %5569 = stablehlo.transpose %5568, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5326)
      %5570 = stablehlo.dot_general %5545, %5569, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5327)
      %5571 = "stablehlo.all_reduce"(%5570) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12772"), %arg1239: tensor<bf16> loc("dot.12772")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5327)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5327)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5327)
      %5572 = stablehlo.reshape %5571 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5328)
      %5573 = stablehlo.reshape %arg1140 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5329)
      %5574 = stablehlo.reshape %5573 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5330)
      %5575 = stablehlo.broadcast_in_dim %5574, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5331)
      %5576 = stablehlo.add %5572, %5575 : tensor<1x128x128xbf16> loc(#loc5332)
      %5577 = stablehlo.reshape %5576 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5333)
      %5578 = stablehlo.transpose %5577, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5334)
      %5579 = stablehlo.slice %5578 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5335)
      %5580 = stablehlo.multiply %5579, %90 : tensor<1x2x128x32xbf16> loc(#loc5336)
      %5581 = stablehlo.slice %5578 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5337)
      %5582 = stablehlo.multiply %5581, %93 : tensor<1x2x128x32xbf16> loc(#loc5338)
      %5583 = stablehlo.subtract %5580, %5582 : tensor<1x2x128x32xbf16> loc(#loc5339)
      %5584 = stablehlo.multiply %5581, %90 : tensor<1x2x128x32xbf16> loc(#loc5340)
      %5585 = stablehlo.multiply %5579, %93 : tensor<1x2x128x32xbf16> loc(#loc5341)
      %5586 = stablehlo.add %5584, %5585 : tensor<1x2x128x32xbf16> loc(#loc5342)
      %5587 = stablehlo.concatenate %5583, %5586, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5343)
      %5588 = stablehlo.broadcast_in_dim %5587, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5344)
      %5589 = stablehlo.reshape %5588 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5345)
      %5590 = stablehlo.transpose %5589, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc5346)
      %5591 = stablehlo.dot_general %5566, %5590, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5347)
      %5592 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %5593 = stablehlo.multiply %5591, %5592 : tensor<1x16x128x128xbf16> loc(#loc5348)
      %5594 = stablehlo.add %5593, %341 : tensor<1x16x128x128xbf16> loc(#loc5349)
      %5595 = stablehlo.reshape %arg1139 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc521)
      %5596 = "stablehlo.all_to_all"(%5595) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc521)
      %5597 = stablehlo.slice %5596 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc521)
      %5598 = stablehlo.reshape %5597 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc521)
      %5599 = stablehlo.reshape %5598 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc5350)
      %5600 = stablehlo.reshape %5599 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5351)
      %5601 = stablehlo.broadcast_in_dim %5600, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc5352)
      %5602 = stablehlo.concatenate %5594, %5601, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5353)
      %5603 = stablehlo.reshape %arg1149 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5354)
      %5604 = stablehlo.reshape %5603 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5355)
      %5605 = stablehlo.convert %5604 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5356)
      %5606 = stablehlo.broadcast_in_dim %5605, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5357)
      %5607 = stablehlo.reduce(%5602 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5358)
      %5608 = stablehlo.broadcast_in_dim %5607, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5359)
      %5609 = stablehlo.subtract %5602, %5608 : tensor<1x16x128x129xbf16> loc(#loc5360)
      %5610 = stablehlo.reduce(%5609 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5361)
      %5611 = stablehlo.broadcast_in_dim %5610, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5362)
      %5612 = stablehlo.subtract %5609, %5611 : tensor<1x16x128x129xbf16> loc(#loc5363)
      %5613 = stablehlo.exponential %5612 : tensor<1x16x128x129xbf16> loc(#loc5364)
      %5614 = stablehlo.reduce(%5613 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5365)
      %5615 = stablehlo.broadcast_in_dim %5614, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5366)
      %5616 = stablehlo.divide %5613, %5615 : tensor<1x16x128x129xbf16> loc(#loc5367)
      %5617 = stablehlo.slice %5616 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5368)
      %5618 = stablehlo.reshape %arg673 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5369)
      %5619 = stablehlo.reshape %5618 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5370)
      %5620 = stablehlo.transpose %5619, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5371)
      %5621 = stablehlo.dot_general %5545, %5620, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5372)
      %5622 = "stablehlo.all_reduce"(%5621) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12735"), %arg1239: tensor<bf16> loc("dot.12735")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5372)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5372)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5372)
      %5623 = stablehlo.reshape %5622 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5373)
      %5624 = stablehlo.reshape %arg672 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5374)
      %5625 = stablehlo.reshape %5624 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5375)
      %5626 = stablehlo.broadcast_in_dim %5625, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5376)
      %5627 = stablehlo.add %5623, %5626 : tensor<1x128x128xbf16> loc(#loc5377)
      %5628 = stablehlo.reshape %5627 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5378)
      %5629 = stablehlo.transpose %5628, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5379)
      %5630 = stablehlo.broadcast_in_dim %5629, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5380)
      %5631 = stablehlo.reshape %5630 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5381)
      %5632 = stablehlo.dot_general %5617, %5631, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5382)
      %5633 = stablehlo.transpose %5632, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5383)
      %5634 = stablehlo.reshape %5633 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc5384)
      %5635 = stablehlo.reshape %arg671 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc5385)
      %5636 = stablehlo.reshape %5635 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc5386)
      %5637 = stablehlo.transpose %5636, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc5387)
      %5638 = stablehlo.dot_general %5634, %5637, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc5388)
      %5639 = "stablehlo.all_reduce"(%5638) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12928"), %arg1239: tensor<bf16> loc("dot.12928")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5388)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5388)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5388)
      %5640 = stablehlo.reshape %5639 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5389)
      %5641 = stablehlo.reshape %arg670 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5390)
      %5642 = stablehlo.reshape %5641 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5391)
      %5643 = stablehlo.broadcast_in_dim %5642, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5392)
      %5644 = stablehlo.add %5640, %5643 : tensor<1x128x360xbf16> loc(#loc5393)
      %5645 = stablehlo.add %5528, %5644 : tensor<1x128x360xbf16> loc(#loc5394)
      %5646 = stablehlo.reshape %arg1144 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5395)
      %5647 = stablehlo.reshape %5646 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5396)
      %5648 = stablehlo.convert %5647 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5397)
      %5649 = stablehlo.broadcast_in_dim %5648, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5398)
      %5650 = stablehlo.convert %5645 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5399)
      %5651 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5652 = stablehlo.power %5650, %5651 : tensor<1x128x360xf32> loc(#loc5400)
      %5653 = stablehlo.reduce(%5652 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5401)
      %5654 = "stablehlo.all_reduce"(%5653) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.12946"), %arg1239: tensor<f32> loc("reduce.12946")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5401)
        stablehlo.return %7360 : tensor<f32> loc(#loc5401)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5401)
      %5655 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5656 = stablehlo.multiply %5654, %5655 : tensor<1x128xf32> loc(#loc5402)
      %5657 = stablehlo.reshape %5656 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5403)
      %5658 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5659 = stablehlo.add %5657, %5658 : tensor<1x128x1xf32> loc(#loc5404)
      %5660 = stablehlo.rsqrt %5659 : tensor<1x128x1xf32> loc(#loc5405)
      %5661 = stablehlo.reshape %5660 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5406)
      %5662 = stablehlo.broadcast_in_dim %5661, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5407)
      %5663 = stablehlo.multiply %5650, %5662 : tensor<1x128x360xf32> loc(#loc5408)
      %5664 = stablehlo.multiply %5649, %5663 : tensor<1x128x360xf32> loc(#loc5409)
      %5665 = stablehlo.convert %5664 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5410)
      %5666 = stablehlo.reshape %5665 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5411)
      %5667 = stablehlo.broadcast_in_dim %5666, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5412)
      %5668 = stablehlo.dot_general %5667, %arg1148, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5413)
      %5669 = "stablehlo.all_reduce"(%5668) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13058"), %arg1239: tensor<bf16> loc("dot.13058")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5413)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5413)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5413)
      %5670 = stablehlo.reshape %arg1147 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc5414)
      %5671 = stablehlo.reshape %5670 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc5415)
      %5672 = stablehlo.broadcast_in_dim %5671, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5416)
      %5673 = stablehlo.add %5669, %5672 : tensor<32x128x5760xbf16> loc(#loc5417)
      %5674 = stablehlo.slice %5673 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5418)
      %5675 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5676 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5677 = stablehlo.clamp %5676, %5674, %5675 : tensor<32x128x2880xbf16> loc(#loc5419)
      %5678 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5679 = stablehlo.add %5677, %5678 : tensor<32x128x2880xbf16> loc(#loc5420)
      %5680 = stablehlo.slice %5673 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5421)
      %5681 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5682 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5683 = stablehlo.clamp %5681, %5680, %5682 : tensor<32x128x2880xbf16> loc(#loc5422)
      %5684 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5685 = stablehlo.multiply %5683, %5684 : tensor<32x128x2880xbf16> loc(#loc5423)
      %5686 = stablehlo.logistic %5685 : tensor<32x128x2880xbf16> loc(#loc5424)
      %5687 = stablehlo.multiply %5683, %5686 : tensor<32x128x2880xbf16> loc(#loc5425)
      %5688 = stablehlo.multiply %5679, %5687 : tensor<32x128x2880xbf16> loc(#loc5426)
      %5689 = stablehlo.dot_general %5688, %arg1146, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5427)
      %5690 = stablehlo.reshape %arg1145 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc5428)
      %5691 = stablehlo.reshape %5690 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc5429)
      %5692 = stablehlo.broadcast_in_dim %5691, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5430)
      %5693 = stablehlo.add %5689, %5692 : tensor<32x128x360xbf16> loc(#loc5431)
      %5694 = stablehlo.reshape %5693 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5432)
      %5695 = stablehlo.reshape %arg669 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5433)
      %5696 = stablehlo.reshape %5695 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5434)
      %5697 = stablehlo.transpose %5696, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5435)
      %5698 = stablehlo.dot_general %5666, %5697, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5436)
      %5699 = "stablehlo.all_reduce"(%5698) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.12974"), %arg1239: tensor<bf16> loc("dot.12974")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5436)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5436)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5436)
      %5700 = stablehlo.reshape %arg668 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5437)
      %5701 = stablehlo.reshape %5700 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5438)
      %5702 = stablehlo.broadcast_in_dim %5701, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc5439)
      %5703 = stablehlo.add %5699, %5702 : tensor<128x128xbf16> loc(#loc5440)
      %5704:2 = "stablehlo.sort"(%5703, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.12994"), %arg1239: tensor<bf16> loc("sort.12994"), %arg1240: tensor<i32> loc("sort.12994"), %arg1241: tensor<i32> loc("sort.12994")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc5442)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc5441)
      %5705 = stablehlo.slice %5704#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc5443)
      %5706 = stablehlo.convert %5705 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc5444)
      %5707 = stablehlo.reshape %5706 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc5445)
      %5708 = stablehlo.concatenate %231, %5707, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc5446)
      %5709 = stablehlo.slice %5704#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc5447)
      %5710 = stablehlo.reduce(%5709 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5448)
      %5711 = stablehlo.broadcast_in_dim %5710, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5449)
      %5712 = stablehlo.subtract %5709, %5711 : tensor<128x4xbf16> loc(#loc5450)
      %5713 = stablehlo.exponential %5712 : tensor<128x4xbf16> loc(#loc5451)
      %5714 = stablehlo.reduce(%5713 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5452)
      %5715 = stablehlo.broadcast_in_dim %5714, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5453)
      %5716 = stablehlo.divide %5713, %5715 : tensor<128x4xbf16> loc(#loc5454)
      %5717 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %5718 = "stablehlo.all_gather"(%5717) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %5719 = "stablehlo.scatter"(%5718, %5708, %5716) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.13028"), %arg1239: tensor<bf16> loc("scatter.13028")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc5455)
      %5720 = stablehlo.reshape %5719 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc5455)
      %5721 = "stablehlo.all_to_all"(%5720) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc5455)
      %5722 = stablehlo.slice %5721 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc5455)
      %5723 = stablehlo.reshape %5722 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc5455)
      %5724 = stablehlo.transpose %5723, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc5456)
      %5725 = stablehlo.reshape %5724 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc5457)
      %5726 = stablehlo.broadcast_in_dim %5725, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5458)
      %5727 = stablehlo.multiply %5694, %5726 : tensor<32x1x128x360xbf16> loc(#loc5459)
      %5728 = stablehlo.reduce(%5727 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc5460)
      %5729 = "stablehlo.all_reduce"(%5728) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.13100"), %arg1239: tensor<bf16> loc("reduce.13100")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5460)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5460)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5460)
      %5730 = stablehlo.add %5645, %5729 : tensor<1x128x360xbf16> loc(#loc5461)
      %5731 = stablehlo.convert %5730 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5462)
      %5732 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5733 = stablehlo.power %5731, %5732 : tensor<1x128x360xf32> loc(#loc5463)
      %5734 = stablehlo.reduce(%5733 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5464)
      %5735 = "stablehlo.all_reduce"(%5734) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.13113"), %arg1239: tensor<f32> loc("reduce.13113")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5464)
        stablehlo.return %7360 : tensor<f32> loc(#loc5464)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5464)
      %5736 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5737 = stablehlo.multiply %5735, %5736 : tensor<1x128xf32> loc(#loc5465)
      %5738 = stablehlo.reshape %5737 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5466)
      %5739 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5740 = stablehlo.add %5738, %5739 : tensor<1x128x1xf32> loc(#loc5467)
      %5741 = stablehlo.rsqrt %5740 : tensor<1x128x1xf32> loc(#loc5468)
      %5742 = stablehlo.reshape %5741 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5469)
      %5743 = stablehlo.broadcast_in_dim %5742, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5470)
      %5744 = stablehlo.multiply %5731, %5743 : tensor<1x128x360xf32> loc(#loc5471)
      %5745 = stablehlo.multiply %5606, %5744 : tensor<1x128x360xf32> loc(#loc5472)
      %5746 = stablehlo.convert %5745 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5473)
      %5747 = stablehlo.reshape %5746 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5474)
      %5748 = stablehlo.reshape %arg1154 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc5475)
      %5749 = stablehlo.reshape %5748 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc5476)
      %5750 = stablehlo.transpose %5749, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc5477)
      %5751 = stablehlo.dot_general %5747, %5750, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5478)
      %5752 = "stablehlo.all_reduce"(%5751) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13227"), %arg1239: tensor<bf16> loc("dot.13227")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5478)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5478)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5478)
      %5753 = stablehlo.reshape %5752 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5479)
      %5754 = stablehlo.reshape %arg1153 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc5480)
      %5755 = stablehlo.reshape %5754 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc5481)
      %5756 = stablehlo.broadcast_in_dim %5755, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5482)
      %5757 = stablehlo.add %5753, %5756 : tensor<1x128x1024xbf16> loc(#loc5483)
      %5758 = stablehlo.reshape %5757 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5484)
      %5759 = stablehlo.transpose %5758, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5485)
      %5760 = stablehlo.slice %5759 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5486)
      %5761 = stablehlo.multiply %5760, %64 : tensor<1x16x128x32xbf16> loc(#loc5487)
      %5762 = stablehlo.slice %5759 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5488)
      %5763 = stablehlo.multiply %5762, %70 : tensor<1x16x128x32xbf16> loc(#loc5489)
      %5764 = stablehlo.subtract %5761, %5763 : tensor<1x16x128x32xbf16> loc(#loc5490)
      %5765 = stablehlo.multiply %5762, %64 : tensor<1x16x128x32xbf16> loc(#loc5491)
      %5766 = stablehlo.multiply %5760, %70 : tensor<1x16x128x32xbf16> loc(#loc5492)
      %5767 = stablehlo.add %5765, %5766 : tensor<1x16x128x32xbf16> loc(#loc5493)
      %5768 = stablehlo.concatenate %5764, %5767, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5494)
      %5769 = stablehlo.reshape %arg1152 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5495)
      %5770 = stablehlo.reshape %5769 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5496)
      %5771 = stablehlo.transpose %5770, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5497)
      %5772 = stablehlo.dot_general %5747, %5771, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5498)
      %5773 = "stablehlo.all_reduce"(%5772) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13178"), %arg1239: tensor<bf16> loc("dot.13178")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5498)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5498)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5498)
      %5774 = stablehlo.reshape %5773 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5499)
      %5775 = stablehlo.reshape %arg1151 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5500)
      %5776 = stablehlo.reshape %5775 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5501)
      %5777 = stablehlo.broadcast_in_dim %5776, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5502)
      %5778 = stablehlo.add %5774, %5777 : tensor<1x128x128xbf16> loc(#loc5503)
      %5779 = stablehlo.reshape %5778 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5504)
      %5780 = stablehlo.transpose %5779, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5505)
      %5781 = stablehlo.slice %5780 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5506)
      %5782 = stablehlo.multiply %5781, %90 : tensor<1x2x128x32xbf16> loc(#loc5507)
      %5783 = stablehlo.slice %5780 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5508)
      %5784 = stablehlo.multiply %5783, %93 : tensor<1x2x128x32xbf16> loc(#loc5509)
      %5785 = stablehlo.subtract %5782, %5784 : tensor<1x2x128x32xbf16> loc(#loc5510)
      %5786 = stablehlo.multiply %5783, %90 : tensor<1x2x128x32xbf16> loc(#loc5511)
      %5787 = stablehlo.multiply %5781, %93 : tensor<1x2x128x32xbf16> loc(#loc5512)
      %5788 = stablehlo.add %5786, %5787 : tensor<1x2x128x32xbf16> loc(#loc5513)
      %5789 = stablehlo.concatenate %5785, %5788, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5514)
      %5790 = stablehlo.broadcast_in_dim %5789, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5515)
      %5791 = stablehlo.reshape %5790 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5516)
      %5792 = stablehlo.transpose %5791, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc5517)
      %5793 = stablehlo.dot_general %5768, %5792, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5518)
      %5794 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %5795 = stablehlo.multiply %5793, %5794 : tensor<1x16x128x128xbf16> loc(#loc5519)
      %5796 = stablehlo.add %5795, %128 : tensor<1x16x128x128xbf16> loc(#loc5520)
      %5797 = stablehlo.reshape %arg1150 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc532)
      %5798 = "stablehlo.all_to_all"(%5797) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc532)
      %5799 = stablehlo.slice %5798 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc532)
      %5800 = stablehlo.reshape %5799 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc532)
      %5801 = stablehlo.reshape %5800 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc5521)
      %5802 = stablehlo.reshape %5801 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5522)
      %5803 = stablehlo.broadcast_in_dim %5802, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc5523)
      %5804 = stablehlo.concatenate %5796, %5803, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5524)
      %5805 = stablehlo.reshape %arg1160 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5525)
      %5806 = stablehlo.reshape %5805 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5526)
      %5807 = stablehlo.convert %5806 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5527)
      %5808 = stablehlo.broadcast_in_dim %5807, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5528)
      %5809 = stablehlo.reduce(%5804 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5529)
      %5810 = stablehlo.broadcast_in_dim %5809, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5530)
      %5811 = stablehlo.subtract %5804, %5810 : tensor<1x16x128x129xbf16> loc(#loc5531)
      %5812 = stablehlo.reduce(%5811 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5532)
      %5813 = stablehlo.broadcast_in_dim %5812, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5533)
      %5814 = stablehlo.subtract %5811, %5813 : tensor<1x16x128x129xbf16> loc(#loc5534)
      %5815 = stablehlo.exponential %5814 : tensor<1x16x128x129xbf16> loc(#loc5535)
      %5816 = stablehlo.reduce(%5815 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5536)
      %5817 = stablehlo.broadcast_in_dim %5816, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5537)
      %5818 = stablehlo.divide %5815, %5817 : tensor<1x16x128x129xbf16> loc(#loc5538)
      %5819 = stablehlo.slice %5818 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5539)
      %5820 = stablehlo.reshape %arg667 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5540)
      %5821 = stablehlo.reshape %5820 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5541)
      %5822 = stablehlo.transpose %5821, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5542)
      %5823 = stablehlo.dot_general %5747, %5822, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5543)
      %5824 = "stablehlo.all_reduce"(%5823) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13141"), %arg1239: tensor<bf16> loc("dot.13141")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5543)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5543)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5543)
      %5825 = stablehlo.reshape %5824 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5544)
      %5826 = stablehlo.reshape %arg666 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5545)
      %5827 = stablehlo.reshape %5826 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5546)
      %5828 = stablehlo.broadcast_in_dim %5827, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5547)
      %5829 = stablehlo.add %5825, %5828 : tensor<1x128x128xbf16> loc(#loc5548)
      %5830 = stablehlo.reshape %5829 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5549)
      %5831 = stablehlo.transpose %5830, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5550)
      %5832 = stablehlo.broadcast_in_dim %5831, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5551)
      %5833 = stablehlo.reshape %5832 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5552)
      %5834 = stablehlo.dot_general %5819, %5833, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5553)
      %5835 = stablehlo.transpose %5834, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5554)
      %5836 = stablehlo.reshape %5835 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc5555)
      %5837 = stablehlo.reshape %arg665 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc5556)
      %5838 = stablehlo.reshape %5837 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc5557)
      %5839 = stablehlo.transpose %5838, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc5558)
      %5840 = stablehlo.dot_general %5836, %5839, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc5559)
      %5841 = "stablehlo.all_reduce"(%5840) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13334"), %arg1239: tensor<bf16> loc("dot.13334")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5559)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5559)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5559)
      %5842 = stablehlo.reshape %5841 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5560)
      %5843 = stablehlo.reshape %arg664 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5561)
      %5844 = stablehlo.reshape %5843 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5562)
      %5845 = stablehlo.broadcast_in_dim %5844, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5563)
      %5846 = stablehlo.add %5842, %5845 : tensor<1x128x360xbf16> loc(#loc5564)
      %5847 = stablehlo.add %5730, %5846 : tensor<1x128x360xbf16> loc(#loc5565)
      %5848 = stablehlo.reshape %arg1155 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5566)
      %5849 = stablehlo.reshape %5848 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5567)
      %5850 = stablehlo.convert %5849 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5568)
      %5851 = stablehlo.broadcast_in_dim %5850, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5569)
      %5852 = stablehlo.convert %5847 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5570)
      %5853 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5854 = stablehlo.power %5852, %5853 : tensor<1x128x360xf32> loc(#loc5571)
      %5855 = stablehlo.reduce(%5854 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5572)
      %5856 = "stablehlo.all_reduce"(%5855) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.13352"), %arg1239: tensor<f32> loc("reduce.13352")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5572)
        stablehlo.return %7360 : tensor<f32> loc(#loc5572)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5572)
      %5857 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5858 = stablehlo.multiply %5856, %5857 : tensor<1x128xf32> loc(#loc5573)
      %5859 = stablehlo.reshape %5858 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5574)
      %5860 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5861 = stablehlo.add %5859, %5860 : tensor<1x128x1xf32> loc(#loc5575)
      %5862 = stablehlo.rsqrt %5861 : tensor<1x128x1xf32> loc(#loc5576)
      %5863 = stablehlo.reshape %5862 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5577)
      %5864 = stablehlo.broadcast_in_dim %5863, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5578)
      %5865 = stablehlo.multiply %5852, %5864 : tensor<1x128x360xf32> loc(#loc5579)
      %5866 = stablehlo.multiply %5851, %5865 : tensor<1x128x360xf32> loc(#loc5580)
      %5867 = stablehlo.convert %5866 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5581)
      %5868 = stablehlo.reshape %5867 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5582)
      %5869 = stablehlo.broadcast_in_dim %5868, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5583)
      %5870 = stablehlo.dot_general %5869, %arg1159, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5584)
      %5871 = "stablehlo.all_reduce"(%5870) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13464"), %arg1239: tensor<bf16> loc("dot.13464")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5584)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5584)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5584)
      %5872 = stablehlo.reshape %arg1158 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc5585)
      %5873 = stablehlo.reshape %5872 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc5586)
      %5874 = stablehlo.broadcast_in_dim %5873, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5587)
      %5875 = stablehlo.add %5871, %5874 : tensor<32x128x5760xbf16> loc(#loc5588)
      %5876 = stablehlo.slice %5875 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5589)
      %5877 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5878 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5879 = stablehlo.clamp %5878, %5876, %5877 : tensor<32x128x2880xbf16> loc(#loc5590)
      %5880 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5881 = stablehlo.add %5879, %5880 : tensor<32x128x2880xbf16> loc(#loc5591)
      %5882 = stablehlo.slice %5875 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5592)
      %5883 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5884 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5885 = stablehlo.clamp %5883, %5882, %5884 : tensor<32x128x2880xbf16> loc(#loc5593)
      %5886 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %5887 = stablehlo.multiply %5885, %5886 : tensor<32x128x2880xbf16> loc(#loc5594)
      %5888 = stablehlo.logistic %5887 : tensor<32x128x2880xbf16> loc(#loc5595)
      %5889 = stablehlo.multiply %5885, %5888 : tensor<32x128x2880xbf16> loc(#loc5596)
      %5890 = stablehlo.multiply %5881, %5889 : tensor<32x128x2880xbf16> loc(#loc5597)
      %5891 = stablehlo.dot_general %5890, %arg1157, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5598)
      %5892 = stablehlo.reshape %arg1156 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc5599)
      %5893 = stablehlo.reshape %5892 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc5600)
      %5894 = stablehlo.broadcast_in_dim %5893, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5601)
      %5895 = stablehlo.add %5891, %5894 : tensor<32x128x360xbf16> loc(#loc5602)
      %5896 = stablehlo.reshape %5895 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5603)
      %5897 = stablehlo.reshape %arg663 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5604)
      %5898 = stablehlo.reshape %5897 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5605)
      %5899 = stablehlo.transpose %5898, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5606)
      %5900 = stablehlo.dot_general %5868, %5899, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5607)
      %5901 = "stablehlo.all_reduce"(%5900) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13380"), %arg1239: tensor<bf16> loc("dot.13380")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5607)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5607)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5607)
      %5902 = stablehlo.reshape %arg662 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5608)
      %5903 = stablehlo.reshape %5902 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5609)
      %5904 = stablehlo.broadcast_in_dim %5903, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc5610)
      %5905 = stablehlo.add %5901, %5904 : tensor<128x128xbf16> loc(#loc5611)
      %5906:2 = "stablehlo.sort"(%5905, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.13400"), %arg1239: tensor<bf16> loc("sort.13400"), %arg1240: tensor<i32> loc("sort.13400"), %arg1241: tensor<i32> loc("sort.13400")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc5613)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc5612)
      %5907 = stablehlo.slice %5906#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc5614)
      %5908 = stablehlo.convert %5907 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc5615)
      %5909 = stablehlo.reshape %5908 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc5616)
      %5910 = stablehlo.concatenate %231, %5909, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc5617)
      %5911 = stablehlo.slice %5906#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc5618)
      %5912 = stablehlo.reduce(%5911 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5619)
      %5913 = stablehlo.broadcast_in_dim %5912, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5620)
      %5914 = stablehlo.subtract %5911, %5913 : tensor<128x4xbf16> loc(#loc5621)
      %5915 = stablehlo.exponential %5914 : tensor<128x4xbf16> loc(#loc5622)
      %5916 = stablehlo.reduce(%5915 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5623)
      %5917 = stablehlo.broadcast_in_dim %5916, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5624)
      %5918 = stablehlo.divide %5915, %5917 : tensor<128x4xbf16> loc(#loc5625)
      %5919 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %5920 = "stablehlo.all_gather"(%5919) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %5921 = "stablehlo.scatter"(%5920, %5910, %5918) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.13434"), %arg1239: tensor<bf16> loc("scatter.13434")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc5626)
      %5922 = stablehlo.reshape %5921 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc5626)
      %5923 = "stablehlo.all_to_all"(%5922) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc5626)
      %5924 = stablehlo.slice %5923 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc5626)
      %5925 = stablehlo.reshape %5924 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc5626)
      %5926 = stablehlo.transpose %5925, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc5627)
      %5927 = stablehlo.reshape %5926 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc5628)
      %5928 = stablehlo.broadcast_in_dim %5927, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5629)
      %5929 = stablehlo.multiply %5896, %5928 : tensor<32x1x128x360xbf16> loc(#loc5630)
      %5930 = stablehlo.reduce(%5929 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc5631)
      %5931 = "stablehlo.all_reduce"(%5930) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.13506"), %arg1239: tensor<bf16> loc("reduce.13506")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5631)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5631)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5631)
      %5932 = stablehlo.add %5847, %5931 : tensor<1x128x360xbf16> loc(#loc5632)
      %5933 = stablehlo.convert %5932 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5633)
      %5934 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %5935 = stablehlo.power %5933, %5934 : tensor<1x128x360xf32> loc(#loc5634)
      %5936 = stablehlo.reduce(%5935 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5635)
      %5937 = "stablehlo.all_reduce"(%5936) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.13519"), %arg1239: tensor<f32> loc("reduce.13519")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5635)
        stablehlo.return %7360 : tensor<f32> loc(#loc5635)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5635)
      %5938 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %5939 = stablehlo.multiply %5937, %5938 : tensor<1x128xf32> loc(#loc5636)
      %5940 = stablehlo.reshape %5939 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5637)
      %5941 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %5942 = stablehlo.add %5940, %5941 : tensor<1x128x1xf32> loc(#loc5638)
      %5943 = stablehlo.rsqrt %5942 : tensor<1x128x1xf32> loc(#loc5639)
      %5944 = stablehlo.reshape %5943 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5640)
      %5945 = stablehlo.broadcast_in_dim %5944, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5641)
      %5946 = stablehlo.multiply %5933, %5945 : tensor<1x128x360xf32> loc(#loc5642)
      %5947 = stablehlo.multiply %5808, %5946 : tensor<1x128x360xf32> loc(#loc5643)
      %5948 = stablehlo.convert %5947 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5644)
      %5949 = stablehlo.reshape %5948 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5645)
      %5950 = stablehlo.reshape %arg1165 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc5646)
      %5951 = stablehlo.reshape %5950 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc5647)
      %5952 = stablehlo.transpose %5951, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc5648)
      %5953 = stablehlo.dot_general %5949, %5952, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5649)
      %5954 = "stablehlo.all_reduce"(%5953) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13633"), %arg1239: tensor<bf16> loc("dot.13633")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5649)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5649)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5649)
      %5955 = stablehlo.reshape %5954 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5650)
      %5956 = stablehlo.reshape %arg1164 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc5651)
      %5957 = stablehlo.reshape %5956 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc5652)
      %5958 = stablehlo.broadcast_in_dim %5957, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5653)
      %5959 = stablehlo.add %5955, %5958 : tensor<1x128x1024xbf16> loc(#loc5654)
      %5960 = stablehlo.reshape %5959 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5655)
      %5961 = stablehlo.transpose %5960, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5656)
      %5962 = stablehlo.slice %5961 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5657)
      %5963 = stablehlo.multiply %5962, %64 : tensor<1x16x128x32xbf16> loc(#loc5658)
      %5964 = stablehlo.slice %5961 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5659)
      %5965 = stablehlo.multiply %5964, %70 : tensor<1x16x128x32xbf16> loc(#loc5660)
      %5966 = stablehlo.subtract %5963, %5965 : tensor<1x16x128x32xbf16> loc(#loc5661)
      %5967 = stablehlo.multiply %5964, %64 : tensor<1x16x128x32xbf16> loc(#loc5662)
      %5968 = stablehlo.multiply %5962, %70 : tensor<1x16x128x32xbf16> loc(#loc5663)
      %5969 = stablehlo.add %5967, %5968 : tensor<1x16x128x32xbf16> loc(#loc5664)
      %5970 = stablehlo.concatenate %5966, %5969, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5665)
      %5971 = stablehlo.reshape %arg1163 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5666)
      %5972 = stablehlo.reshape %5971 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5667)
      %5973 = stablehlo.transpose %5972, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5668)
      %5974 = stablehlo.dot_general %5949, %5973, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5669)
      %5975 = "stablehlo.all_reduce"(%5974) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13584"), %arg1239: tensor<bf16> loc("dot.13584")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5669)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5669)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5669)
      %5976 = stablehlo.reshape %5975 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5670)
      %5977 = stablehlo.reshape %arg1162 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5671)
      %5978 = stablehlo.reshape %5977 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5672)
      %5979 = stablehlo.broadcast_in_dim %5978, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5673)
      %5980 = stablehlo.add %5976, %5979 : tensor<1x128x128xbf16> loc(#loc5674)
      %5981 = stablehlo.reshape %5980 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5675)
      %5982 = stablehlo.transpose %5981, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5676)
      %5983 = stablehlo.slice %5982 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5677)
      %5984 = stablehlo.multiply %5983, %90 : tensor<1x2x128x32xbf16> loc(#loc5678)
      %5985 = stablehlo.slice %5982 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5679)
      %5986 = stablehlo.multiply %5985, %93 : tensor<1x2x128x32xbf16> loc(#loc5680)
      %5987 = stablehlo.subtract %5984, %5986 : tensor<1x2x128x32xbf16> loc(#loc5681)
      %5988 = stablehlo.multiply %5985, %90 : tensor<1x2x128x32xbf16> loc(#loc5682)
      %5989 = stablehlo.multiply %5983, %93 : tensor<1x2x128x32xbf16> loc(#loc5683)
      %5990 = stablehlo.add %5988, %5989 : tensor<1x2x128x32xbf16> loc(#loc5684)
      %5991 = stablehlo.concatenate %5987, %5990, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5685)
      %5992 = stablehlo.broadcast_in_dim %5991, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5686)
      %5993 = stablehlo.reshape %5992 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5687)
      %5994 = stablehlo.transpose %5993, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc5688)
      %5995 = stablehlo.dot_general %5970, %5994, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5689)
      %5996 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %5997 = stablehlo.multiply %5995, %5996 : tensor<1x16x128x128xbf16> loc(#loc5690)
      %5998 = stablehlo.add %5997, %341 : tensor<1x16x128x128xbf16> loc(#loc5691)
      %5999 = stablehlo.reshape %arg1161 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc543)
      %6000 = "stablehlo.all_to_all"(%5999) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc543)
      %6001 = stablehlo.slice %6000 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc543)
      %6002 = stablehlo.reshape %6001 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc543)
      %6003 = stablehlo.reshape %6002 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc5692)
      %6004 = stablehlo.reshape %6003 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5693)
      %6005 = stablehlo.broadcast_in_dim %6004, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc5694)
      %6006 = stablehlo.concatenate %5998, %6005, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5695)
      %6007 = stablehlo.reshape %arg1171 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5696)
      %6008 = stablehlo.reshape %6007 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5697)
      %6009 = stablehlo.convert %6008 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5698)
      %6010 = stablehlo.broadcast_in_dim %6009, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5699)
      %6011 = stablehlo.reduce(%6006 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5700)
      %6012 = stablehlo.broadcast_in_dim %6011, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5701)
      %6013 = stablehlo.subtract %6006, %6012 : tensor<1x16x128x129xbf16> loc(#loc5702)
      %6014 = stablehlo.reduce(%6013 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5703)
      %6015 = stablehlo.broadcast_in_dim %6014, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5704)
      %6016 = stablehlo.subtract %6013, %6015 : tensor<1x16x128x129xbf16> loc(#loc5705)
      %6017 = stablehlo.exponential %6016 : tensor<1x16x128x129xbf16> loc(#loc5706)
      %6018 = stablehlo.reduce(%6017 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5707)
      %6019 = stablehlo.broadcast_in_dim %6018, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5708)
      %6020 = stablehlo.divide %6017, %6019 : tensor<1x16x128x129xbf16> loc(#loc5709)
      %6021 = stablehlo.slice %6020 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5710)
      %6022 = stablehlo.reshape %arg661 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5711)
      %6023 = stablehlo.reshape %6022 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5712)
      %6024 = stablehlo.transpose %6023, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5713)
      %6025 = stablehlo.dot_general %5949, %6024, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5714)
      %6026 = "stablehlo.all_reduce"(%6025) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13547"), %arg1239: tensor<bf16> loc("dot.13547")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5714)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5714)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5714)
      %6027 = stablehlo.reshape %6026 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5715)
      %6028 = stablehlo.reshape %arg660 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5716)
      %6029 = stablehlo.reshape %6028 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5717)
      %6030 = stablehlo.broadcast_in_dim %6029, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5718)
      %6031 = stablehlo.add %6027, %6030 : tensor<1x128x128xbf16> loc(#loc5719)
      %6032 = stablehlo.reshape %6031 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5720)
      %6033 = stablehlo.transpose %6032, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5721)
      %6034 = stablehlo.broadcast_in_dim %6033, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5722)
      %6035 = stablehlo.reshape %6034 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5723)
      %6036 = stablehlo.dot_general %6021, %6035, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5724)
      %6037 = stablehlo.transpose %6036, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5725)
      %6038 = stablehlo.reshape %6037 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc5726)
      %6039 = stablehlo.reshape %arg659 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc5727)
      %6040 = stablehlo.reshape %6039 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc5728)
      %6041 = stablehlo.transpose %6040, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc5729)
      %6042 = stablehlo.dot_general %6038, %6041, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc5730)
      %6043 = "stablehlo.all_reduce"(%6042) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13740"), %arg1239: tensor<bf16> loc("dot.13740")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5730)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5730)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5730)
      %6044 = stablehlo.reshape %6043 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5731)
      %6045 = stablehlo.reshape %arg658 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5732)
      %6046 = stablehlo.reshape %6045 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5733)
      %6047 = stablehlo.broadcast_in_dim %6046, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5734)
      %6048 = stablehlo.add %6044, %6047 : tensor<1x128x360xbf16> loc(#loc5735)
      %6049 = stablehlo.add %5932, %6048 : tensor<1x128x360xbf16> loc(#loc5736)
      %6050 = stablehlo.reshape %arg1166 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5737)
      %6051 = stablehlo.reshape %6050 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5738)
      %6052 = stablehlo.convert %6051 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5739)
      %6053 = stablehlo.broadcast_in_dim %6052, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5740)
      %6054 = stablehlo.convert %6049 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5741)
      %6055 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6056 = stablehlo.power %6054, %6055 : tensor<1x128x360xf32> loc(#loc5742)
      %6057 = stablehlo.reduce(%6056 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5743)
      %6058 = "stablehlo.all_reduce"(%6057) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.13758"), %arg1239: tensor<f32> loc("reduce.13758")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5743)
        stablehlo.return %7360 : tensor<f32> loc(#loc5743)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5743)
      %6059 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6060 = stablehlo.multiply %6058, %6059 : tensor<1x128xf32> loc(#loc5744)
      %6061 = stablehlo.reshape %6060 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5745)
      %6062 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6063 = stablehlo.add %6061, %6062 : tensor<1x128x1xf32> loc(#loc5746)
      %6064 = stablehlo.rsqrt %6063 : tensor<1x128x1xf32> loc(#loc5747)
      %6065 = stablehlo.reshape %6064 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5748)
      %6066 = stablehlo.broadcast_in_dim %6065, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5749)
      %6067 = stablehlo.multiply %6054, %6066 : tensor<1x128x360xf32> loc(#loc5750)
      %6068 = stablehlo.multiply %6053, %6067 : tensor<1x128x360xf32> loc(#loc5751)
      %6069 = stablehlo.convert %6068 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5752)
      %6070 = stablehlo.reshape %6069 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5753)
      %6071 = stablehlo.broadcast_in_dim %6070, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5754)
      %6072 = stablehlo.dot_general %6071, %arg1170, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5755)
      %6073 = "stablehlo.all_reduce"(%6072) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13870"), %arg1239: tensor<bf16> loc("dot.13870")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5755)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5755)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5755)
      %6074 = stablehlo.reshape %arg1169 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc5756)
      %6075 = stablehlo.reshape %6074 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc5757)
      %6076 = stablehlo.broadcast_in_dim %6075, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5758)
      %6077 = stablehlo.add %6073, %6076 : tensor<32x128x5760xbf16> loc(#loc5759)
      %6078 = stablehlo.slice %6077 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5760)
      %6079 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6080 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6081 = stablehlo.clamp %6080, %6078, %6079 : tensor<32x128x2880xbf16> loc(#loc5761)
      %6082 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6083 = stablehlo.add %6081, %6082 : tensor<32x128x2880xbf16> loc(#loc5762)
      %6084 = stablehlo.slice %6077 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5763)
      %6085 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6086 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6087 = stablehlo.clamp %6085, %6084, %6086 : tensor<32x128x2880xbf16> loc(#loc5764)
      %6088 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6089 = stablehlo.multiply %6087, %6088 : tensor<32x128x2880xbf16> loc(#loc5765)
      %6090 = stablehlo.logistic %6089 : tensor<32x128x2880xbf16> loc(#loc5766)
      %6091 = stablehlo.multiply %6087, %6090 : tensor<32x128x2880xbf16> loc(#loc5767)
      %6092 = stablehlo.multiply %6083, %6091 : tensor<32x128x2880xbf16> loc(#loc5768)
      %6093 = stablehlo.dot_general %6092, %arg1168, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5769)
      %6094 = stablehlo.reshape %arg1167 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc5770)
      %6095 = stablehlo.reshape %6094 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc5771)
      %6096 = stablehlo.broadcast_in_dim %6095, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5772)
      %6097 = stablehlo.add %6093, %6096 : tensor<32x128x360xbf16> loc(#loc5773)
      %6098 = stablehlo.reshape %6097 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5774)
      %6099 = stablehlo.reshape %arg657 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5775)
      %6100 = stablehlo.reshape %6099 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5776)
      %6101 = stablehlo.transpose %6100, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5777)
      %6102 = stablehlo.dot_general %6070, %6101, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5778)
      %6103 = "stablehlo.all_reduce"(%6102) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13786"), %arg1239: tensor<bf16> loc("dot.13786")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5778)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5778)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5778)
      %6104 = stablehlo.reshape %arg656 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5779)
      %6105 = stablehlo.reshape %6104 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5780)
      %6106 = stablehlo.broadcast_in_dim %6105, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc5781)
      %6107 = stablehlo.add %6103, %6106 : tensor<128x128xbf16> loc(#loc5782)
      %6108:2 = "stablehlo.sort"(%6107, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.13806"), %arg1239: tensor<bf16> loc("sort.13806"), %arg1240: tensor<i32> loc("sort.13806"), %arg1241: tensor<i32> loc("sort.13806")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc5784)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc5783)
      %6109 = stablehlo.slice %6108#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc5785)
      %6110 = stablehlo.convert %6109 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc5786)
      %6111 = stablehlo.reshape %6110 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc5787)
      %6112 = stablehlo.concatenate %231, %6111, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc5788)
      %6113 = stablehlo.slice %6108#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc5789)
      %6114 = stablehlo.reduce(%6113 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5790)
      %6115 = stablehlo.broadcast_in_dim %6114, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5791)
      %6116 = stablehlo.subtract %6113, %6115 : tensor<128x4xbf16> loc(#loc5792)
      %6117 = stablehlo.exponential %6116 : tensor<128x4xbf16> loc(#loc5793)
      %6118 = stablehlo.reduce(%6117 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5794)
      %6119 = stablehlo.broadcast_in_dim %6118, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5795)
      %6120 = stablehlo.divide %6117, %6119 : tensor<128x4xbf16> loc(#loc5796)
      %6121 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %6122 = "stablehlo.all_gather"(%6121) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %6123 = "stablehlo.scatter"(%6122, %6112, %6120) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.13840"), %arg1239: tensor<bf16> loc("scatter.13840")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc5797)
      %6124 = stablehlo.reshape %6123 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc5797)
      %6125 = "stablehlo.all_to_all"(%6124) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc5797)
      %6126 = stablehlo.slice %6125 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc5797)
      %6127 = stablehlo.reshape %6126 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc5797)
      %6128 = stablehlo.transpose %6127, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc5798)
      %6129 = stablehlo.reshape %6128 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc5799)
      %6130 = stablehlo.broadcast_in_dim %6129, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5800)
      %6131 = stablehlo.multiply %6098, %6130 : tensor<32x1x128x360xbf16> loc(#loc5801)
      %6132 = stablehlo.reduce(%6131 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc5802)
      %6133 = "stablehlo.all_reduce"(%6132) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.13912"), %arg1239: tensor<bf16> loc("reduce.13912")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5802)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5802)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5802)
      %6134 = stablehlo.add %6049, %6133 : tensor<1x128x360xbf16> loc(#loc5803)
      %6135 = stablehlo.convert %6134 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5804)
      %6136 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6137 = stablehlo.power %6135, %6136 : tensor<1x128x360xf32> loc(#loc5805)
      %6138 = stablehlo.reduce(%6137 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5806)
      %6139 = "stablehlo.all_reduce"(%6138) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.13925"), %arg1239: tensor<f32> loc("reduce.13925")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5806)
        stablehlo.return %7360 : tensor<f32> loc(#loc5806)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5806)
      %6140 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6141 = stablehlo.multiply %6139, %6140 : tensor<1x128xf32> loc(#loc5807)
      %6142 = stablehlo.reshape %6141 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5808)
      %6143 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6144 = stablehlo.add %6142, %6143 : tensor<1x128x1xf32> loc(#loc5809)
      %6145 = stablehlo.rsqrt %6144 : tensor<1x128x1xf32> loc(#loc5810)
      %6146 = stablehlo.reshape %6145 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5811)
      %6147 = stablehlo.broadcast_in_dim %6146, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5812)
      %6148 = stablehlo.multiply %6135, %6147 : tensor<1x128x360xf32> loc(#loc5813)
      %6149 = stablehlo.multiply %6010, %6148 : tensor<1x128x360xf32> loc(#loc5814)
      %6150 = stablehlo.convert %6149 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5815)
      %6151 = stablehlo.reshape %6150 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5816)
      %6152 = stablehlo.reshape %arg1176 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc5817)
      %6153 = stablehlo.reshape %6152 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc5818)
      %6154 = stablehlo.transpose %6153, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc5819)
      %6155 = stablehlo.dot_general %6151, %6154, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5820)
      %6156 = "stablehlo.all_reduce"(%6155) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14039"), %arg1239: tensor<bf16> loc("dot.14039")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5820)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5820)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5820)
      %6157 = stablehlo.reshape %6156 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5821)
      %6158 = stablehlo.reshape %arg1175 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc5822)
      %6159 = stablehlo.reshape %6158 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc5823)
      %6160 = stablehlo.broadcast_in_dim %6159, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5824)
      %6161 = stablehlo.add %6157, %6160 : tensor<1x128x1024xbf16> loc(#loc5825)
      %6162 = stablehlo.reshape %6161 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5826)
      %6163 = stablehlo.transpose %6162, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5827)
      %6164 = stablehlo.slice %6163 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5828)
      %6165 = stablehlo.multiply %6164, %64 : tensor<1x16x128x32xbf16> loc(#loc5829)
      %6166 = stablehlo.slice %6163 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5830)
      %6167 = stablehlo.multiply %6166, %70 : tensor<1x16x128x32xbf16> loc(#loc5831)
      %6168 = stablehlo.subtract %6165, %6167 : tensor<1x16x128x32xbf16> loc(#loc5832)
      %6169 = stablehlo.multiply %6166, %64 : tensor<1x16x128x32xbf16> loc(#loc5833)
      %6170 = stablehlo.multiply %6164, %70 : tensor<1x16x128x32xbf16> loc(#loc5834)
      %6171 = stablehlo.add %6169, %6170 : tensor<1x16x128x32xbf16> loc(#loc5835)
      %6172 = stablehlo.concatenate %6168, %6171, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5836)
      %6173 = stablehlo.reshape %arg1174 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5837)
      %6174 = stablehlo.reshape %6173 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5838)
      %6175 = stablehlo.transpose %6174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5839)
      %6176 = stablehlo.dot_general %6151, %6175, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5840)
      %6177 = "stablehlo.all_reduce"(%6176) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13990"), %arg1239: tensor<bf16> loc("dot.13990")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5840)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5840)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5840)
      %6178 = stablehlo.reshape %6177 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5841)
      %6179 = stablehlo.reshape %arg1173 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5842)
      %6180 = stablehlo.reshape %6179 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5843)
      %6181 = stablehlo.broadcast_in_dim %6180, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5844)
      %6182 = stablehlo.add %6178, %6181 : tensor<1x128x128xbf16> loc(#loc5845)
      %6183 = stablehlo.reshape %6182 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5846)
      %6184 = stablehlo.transpose %6183, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5847)
      %6185 = stablehlo.slice %6184 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5848)
      %6186 = stablehlo.multiply %6185, %90 : tensor<1x2x128x32xbf16> loc(#loc5849)
      %6187 = stablehlo.slice %6184 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc5850)
      %6188 = stablehlo.multiply %6187, %93 : tensor<1x2x128x32xbf16> loc(#loc5851)
      %6189 = stablehlo.subtract %6186, %6188 : tensor<1x2x128x32xbf16> loc(#loc5852)
      %6190 = stablehlo.multiply %6187, %90 : tensor<1x2x128x32xbf16> loc(#loc5853)
      %6191 = stablehlo.multiply %6185, %93 : tensor<1x2x128x32xbf16> loc(#loc5854)
      %6192 = stablehlo.add %6190, %6191 : tensor<1x2x128x32xbf16> loc(#loc5855)
      %6193 = stablehlo.concatenate %6189, %6192, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5856)
      %6194 = stablehlo.broadcast_in_dim %6193, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5857)
      %6195 = stablehlo.reshape %6194 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5858)
      %6196 = stablehlo.transpose %6195, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc5859)
      %6197 = stablehlo.dot_general %6172, %6196, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5860)
      %6198 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %6199 = stablehlo.multiply %6197, %6198 : tensor<1x16x128x128xbf16> loc(#loc5861)
      %6200 = stablehlo.add %6199, %128 : tensor<1x16x128x128xbf16> loc(#loc5862)
      %6201 = stablehlo.reshape %arg1172 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc554)
      %6202 = "stablehlo.all_to_all"(%6201) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc554)
      %6203 = stablehlo.slice %6202 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc554)
      %6204 = stablehlo.reshape %6203 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc554)
      %6205 = stablehlo.reshape %6204 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc5863)
      %6206 = stablehlo.reshape %6205 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc5864)
      %6207 = stablehlo.broadcast_in_dim %6206, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc5865)
      %6208 = stablehlo.concatenate %6200, %6207, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5866)
      %6209 = stablehlo.reshape %arg1182 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5867)
      %6210 = stablehlo.reshape %6209 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5868)
      %6211 = stablehlo.convert %6210 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5869)
      %6212 = stablehlo.broadcast_in_dim %6211, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5870)
      %6213 = stablehlo.reduce(%6208 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5871)
      %6214 = stablehlo.broadcast_in_dim %6213, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5872)
      %6215 = stablehlo.subtract %6208, %6214 : tensor<1x16x128x129xbf16> loc(#loc5873)
      %6216 = stablehlo.reduce(%6215 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5874)
      %6217 = stablehlo.broadcast_in_dim %6216, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5875)
      %6218 = stablehlo.subtract %6215, %6217 : tensor<1x16x128x129xbf16> loc(#loc5876)
      %6219 = stablehlo.exponential %6218 : tensor<1x16x128x129xbf16> loc(#loc5877)
      %6220 = stablehlo.reduce(%6219 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc5878)
      %6221 = stablehlo.broadcast_in_dim %6220, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc5879)
      %6222 = stablehlo.divide %6219, %6221 : tensor<1x16x128x129xbf16> loc(#loc5880)
      %6223 = stablehlo.slice %6222 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc5881)
      %6224 = stablehlo.reshape %arg655 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5882)
      %6225 = stablehlo.reshape %6224 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5883)
      %6226 = stablehlo.transpose %6225, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5884)
      %6227 = stablehlo.dot_general %6151, %6226, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5885)
      %6228 = "stablehlo.all_reduce"(%6227) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.13953"), %arg1239: tensor<bf16> loc("dot.13953")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5885)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5885)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5885)
      %6229 = stablehlo.reshape %6228 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5886)
      %6230 = stablehlo.reshape %arg654 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5887)
      %6231 = stablehlo.reshape %6230 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5888)
      %6232 = stablehlo.broadcast_in_dim %6231, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc5889)
      %6233 = stablehlo.add %6229, %6232 : tensor<1x128x128xbf16> loc(#loc5890)
      %6234 = stablehlo.reshape %6233 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc5891)
      %6235 = stablehlo.transpose %6234, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc5892)
      %6236 = stablehlo.broadcast_in_dim %6235, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc5893)
      %6237 = stablehlo.reshape %6236 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5894)
      %6238 = stablehlo.dot_general %6223, %6237, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5895)
      %6239 = stablehlo.transpose %6238, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5896)
      %6240 = stablehlo.reshape %6239 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc5897)
      %6241 = stablehlo.reshape %arg653 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc5898)
      %6242 = stablehlo.reshape %6241 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc5899)
      %6243 = stablehlo.transpose %6242, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc5900)
      %6244 = stablehlo.dot_general %6240, %6243, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc5901)
      %6245 = "stablehlo.all_reduce"(%6244) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14146"), %arg1239: tensor<bf16> loc("dot.14146")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5901)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5901)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5901)
      %6246 = stablehlo.reshape %6245 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5902)
      %6247 = stablehlo.reshape %arg652 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5903)
      %6248 = stablehlo.reshape %6247 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5904)
      %6249 = stablehlo.broadcast_in_dim %6248, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5905)
      %6250 = stablehlo.add %6246, %6249 : tensor<1x128x360xbf16> loc(#loc5906)
      %6251 = stablehlo.add %6134, %6250 : tensor<1x128x360xbf16> loc(#loc5907)
      %6252 = stablehlo.reshape %arg1177 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc5908)
      %6253 = stablehlo.reshape %6252 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc5909)
      %6254 = stablehlo.convert %6253 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc5910)
      %6255 = stablehlo.broadcast_in_dim %6254, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc5911)
      %6256 = stablehlo.convert %6251 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5912)
      %6257 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6258 = stablehlo.power %6256, %6257 : tensor<1x128x360xf32> loc(#loc5913)
      %6259 = stablehlo.reduce(%6258 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5914)
      %6260 = "stablehlo.all_reduce"(%6259) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.14164"), %arg1239: tensor<f32> loc("reduce.14164")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5914)
        stablehlo.return %7360 : tensor<f32> loc(#loc5914)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5914)
      %6261 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6262 = stablehlo.multiply %6260, %6261 : tensor<1x128xf32> loc(#loc5915)
      %6263 = stablehlo.reshape %6262 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5916)
      %6264 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6265 = stablehlo.add %6263, %6264 : tensor<1x128x1xf32> loc(#loc5917)
      %6266 = stablehlo.rsqrt %6265 : tensor<1x128x1xf32> loc(#loc5918)
      %6267 = stablehlo.reshape %6266 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5919)
      %6268 = stablehlo.broadcast_in_dim %6267, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5920)
      %6269 = stablehlo.multiply %6256, %6268 : tensor<1x128x360xf32> loc(#loc5921)
      %6270 = stablehlo.multiply %6255, %6269 : tensor<1x128x360xf32> loc(#loc5922)
      %6271 = stablehlo.convert %6270 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5923)
      %6272 = stablehlo.reshape %6271 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5924)
      %6273 = stablehlo.broadcast_in_dim %6272, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5925)
      %6274 = stablehlo.dot_general %6273, %arg1181, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5926)
      %6275 = "stablehlo.all_reduce"(%6274) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14276"), %arg1239: tensor<bf16> loc("dot.14276")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5926)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5926)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5926)
      %6276 = stablehlo.reshape %arg1180 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc5927)
      %6277 = stablehlo.reshape %6276 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc5928)
      %6278 = stablehlo.broadcast_in_dim %6277, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc5929)
      %6279 = stablehlo.add %6275, %6278 : tensor<32x128x5760xbf16> loc(#loc5930)
      %6280 = stablehlo.slice %6279 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5931)
      %6281 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6282 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6283 = stablehlo.clamp %6282, %6280, %6281 : tensor<32x128x2880xbf16> loc(#loc5932)
      %6284 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6285 = stablehlo.add %6283, %6284 : tensor<32x128x2880xbf16> loc(#loc5933)
      %6286 = stablehlo.slice %6279 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc5934)
      %6287 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6288 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6289 = stablehlo.clamp %6287, %6286, %6288 : tensor<32x128x2880xbf16> loc(#loc5935)
      %6290 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6291 = stablehlo.multiply %6289, %6290 : tensor<32x128x2880xbf16> loc(#loc5936)
      %6292 = stablehlo.logistic %6291 : tensor<32x128x2880xbf16> loc(#loc5937)
      %6293 = stablehlo.multiply %6289, %6292 : tensor<32x128x2880xbf16> loc(#loc5938)
      %6294 = stablehlo.multiply %6285, %6293 : tensor<32x128x2880xbf16> loc(#loc5939)
      %6295 = stablehlo.dot_general %6294, %arg1179, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5940)
      %6296 = stablehlo.reshape %arg1178 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc5941)
      %6297 = stablehlo.reshape %6296 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc5942)
      %6298 = stablehlo.broadcast_in_dim %6297, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc5943)
      %6299 = stablehlo.add %6295, %6298 : tensor<32x128x360xbf16> loc(#loc5944)
      %6300 = stablehlo.reshape %6299 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5945)
      %6301 = stablehlo.reshape %arg651 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5946)
      %6302 = stablehlo.reshape %6301 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5947)
      %6303 = stablehlo.transpose %6302, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc5948)
      %6304 = stablehlo.dot_general %6272, %6303, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc5949)
      %6305 = "stablehlo.all_reduce"(%6304) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14192"), %arg1239: tensor<bf16> loc("dot.14192")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5949)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5949)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc5949)
      %6306 = stablehlo.reshape %arg650 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5950)
      %6307 = stablehlo.reshape %6306 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5951)
      %6308 = stablehlo.broadcast_in_dim %6307, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc5952)
      %6309 = stablehlo.add %6305, %6308 : tensor<128x128xbf16> loc(#loc5953)
      %6310:2 = "stablehlo.sort"(%6309, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.14212"), %arg1239: tensor<bf16> loc("sort.14212"), %arg1240: tensor<i32> loc("sort.14212"), %arg1241: tensor<i32> loc("sort.14212")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc5955)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc5954)
      %6311 = stablehlo.slice %6310#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc5956)
      %6312 = stablehlo.convert %6311 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc5957)
      %6313 = stablehlo.reshape %6312 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc5958)
      %6314 = stablehlo.concatenate %231, %6313, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc5959)
      %6315 = stablehlo.slice %6310#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc5960)
      %6316 = stablehlo.reduce(%6315 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5961)
      %6317 = stablehlo.broadcast_in_dim %6316, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5962)
      %6318 = stablehlo.subtract %6315, %6317 : tensor<128x4xbf16> loc(#loc5963)
      %6319 = stablehlo.exponential %6318 : tensor<128x4xbf16> loc(#loc5964)
      %6320 = stablehlo.reduce(%6319 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc5965)
      %6321 = stablehlo.broadcast_in_dim %6320, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc5966)
      %6322 = stablehlo.divide %6319, %6321 : tensor<128x4xbf16> loc(#loc5967)
      %6323 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %6324 = "stablehlo.all_gather"(%6323) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %6325 = "stablehlo.scatter"(%6324, %6314, %6322) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.14246"), %arg1239: tensor<bf16> loc("scatter.14246")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc5968)
      %6326 = stablehlo.reshape %6325 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc5968)
      %6327 = "stablehlo.all_to_all"(%6326) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc5968)
      %6328 = stablehlo.slice %6327 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc5968)
      %6329 = stablehlo.reshape %6328 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc5968)
      %6330 = stablehlo.transpose %6329, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc5969)
      %6331 = stablehlo.reshape %6330 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc5970)
      %6332 = stablehlo.broadcast_in_dim %6331, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc5971)
      %6333 = stablehlo.multiply %6300, %6332 : tensor<32x1x128x360xbf16> loc(#loc5972)
      %6334 = stablehlo.reduce(%6333 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc5973)
      %6335 = "stablehlo.all_reduce"(%6334) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.14318"), %arg1239: tensor<bf16> loc("reduce.14318")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5973)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5973)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc5973)
      %6336 = stablehlo.add %6251, %6335 : tensor<1x128x360xbf16> loc(#loc5974)
      %6337 = stablehlo.convert %6336 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc5975)
      %6338 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6339 = stablehlo.power %6337, %6338 : tensor<1x128x360xf32> loc(#loc5976)
      %6340 = stablehlo.reduce(%6339 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc5977)
      %6341 = "stablehlo.all_reduce"(%6340) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.14331"), %arg1239: tensor<f32> loc("reduce.14331")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc5977)
        stablehlo.return %7360 : tensor<f32> loc(#loc5977)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc5977)
      %6342 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6343 = stablehlo.multiply %6341, %6342 : tensor<1x128xf32> loc(#loc5978)
      %6344 = stablehlo.reshape %6343 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc5979)
      %6345 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6346 = stablehlo.add %6344, %6345 : tensor<1x128x1xf32> loc(#loc5980)
      %6347 = stablehlo.rsqrt %6346 : tensor<1x128x1xf32> loc(#loc5981)
      %6348 = stablehlo.reshape %6347 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc5982)
      %6349 = stablehlo.broadcast_in_dim %6348, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc5983)
      %6350 = stablehlo.multiply %6337, %6349 : tensor<1x128x360xf32> loc(#loc5984)
      %6351 = stablehlo.multiply %6212, %6350 : tensor<1x128x360xf32> loc(#loc5985)
      %6352 = stablehlo.convert %6351 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc5986)
      %6353 = stablehlo.reshape %6352 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc5987)
      %6354 = stablehlo.reshape %arg1187 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc5988)
      %6355 = stablehlo.reshape %6354 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc5989)
      %6356 = stablehlo.transpose %6355, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc5990)
      %6357 = stablehlo.dot_general %6353, %6356, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5991)
      %6358 = "stablehlo.all_reduce"(%6357) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14445"), %arg1239: tensor<bf16> loc("dot.14445")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc5991)
        stablehlo.return %7360 : tensor<bf16> loc(#loc5991)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc5991)
      %6359 = stablehlo.reshape %6358 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5992)
      %6360 = stablehlo.reshape %arg1186 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc5993)
      %6361 = stablehlo.reshape %6360 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc5994)
      %6362 = stablehlo.broadcast_in_dim %6361, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc5995)
      %6363 = stablehlo.add %6359, %6362 : tensor<1x128x1024xbf16> loc(#loc5996)
      %6364 = stablehlo.reshape %6363 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc5997)
      %6365 = stablehlo.transpose %6364, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc5998)
      %6366 = stablehlo.slice %6365 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc5999)
      %6367 = stablehlo.multiply %6366, %64 : tensor<1x16x128x32xbf16> loc(#loc6000)
      %6368 = stablehlo.slice %6365 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6001)
      %6369 = stablehlo.multiply %6368, %70 : tensor<1x16x128x32xbf16> loc(#loc6002)
      %6370 = stablehlo.subtract %6367, %6369 : tensor<1x16x128x32xbf16> loc(#loc6003)
      %6371 = stablehlo.multiply %6368, %64 : tensor<1x16x128x32xbf16> loc(#loc6004)
      %6372 = stablehlo.multiply %6366, %70 : tensor<1x16x128x32xbf16> loc(#loc6005)
      %6373 = stablehlo.add %6371, %6372 : tensor<1x16x128x32xbf16> loc(#loc6006)
      %6374 = stablehlo.concatenate %6370, %6373, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6007)
      %6375 = stablehlo.reshape %arg1185 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6008)
      %6376 = stablehlo.reshape %6375 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6009)
      %6377 = stablehlo.transpose %6376, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6010)
      %6378 = stablehlo.dot_general %6353, %6377, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6011)
      %6379 = "stablehlo.all_reduce"(%6378) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14396"), %arg1239: tensor<bf16> loc("dot.14396")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6011)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6011)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6011)
      %6380 = stablehlo.reshape %6379 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6012)
      %6381 = stablehlo.reshape %arg1184 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6013)
      %6382 = stablehlo.reshape %6381 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6014)
      %6383 = stablehlo.broadcast_in_dim %6382, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6015)
      %6384 = stablehlo.add %6380, %6383 : tensor<1x128x128xbf16> loc(#loc6016)
      %6385 = stablehlo.reshape %6384 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6017)
      %6386 = stablehlo.transpose %6385, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6018)
      %6387 = stablehlo.slice %6386 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6019)
      %6388 = stablehlo.multiply %6387, %90 : tensor<1x2x128x32xbf16> loc(#loc6020)
      %6389 = stablehlo.slice %6386 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6021)
      %6390 = stablehlo.multiply %6389, %93 : tensor<1x2x128x32xbf16> loc(#loc6022)
      %6391 = stablehlo.subtract %6388, %6390 : tensor<1x2x128x32xbf16> loc(#loc6023)
      %6392 = stablehlo.multiply %6389, %90 : tensor<1x2x128x32xbf16> loc(#loc6024)
      %6393 = stablehlo.multiply %6387, %93 : tensor<1x2x128x32xbf16> loc(#loc6025)
      %6394 = stablehlo.add %6392, %6393 : tensor<1x2x128x32xbf16> loc(#loc6026)
      %6395 = stablehlo.concatenate %6391, %6394, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6027)
      %6396 = stablehlo.broadcast_in_dim %6395, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6028)
      %6397 = stablehlo.reshape %6396 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6029)
      %6398 = stablehlo.transpose %6397, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc6030)
      %6399 = stablehlo.dot_general %6374, %6398, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6031)
      %6400 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %6401 = stablehlo.multiply %6399, %6400 : tensor<1x16x128x128xbf16> loc(#loc6032)
      %6402 = stablehlo.add %6401, %341 : tensor<1x16x128x128xbf16> loc(#loc6033)
      %6403 = stablehlo.reshape %arg1183 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc565)
      %6404 = "stablehlo.all_to_all"(%6403) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc565)
      %6405 = stablehlo.slice %6404 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc565)
      %6406 = stablehlo.reshape %6405 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc565)
      %6407 = stablehlo.reshape %6406 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc6034)
      %6408 = stablehlo.reshape %6407 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc6035)
      %6409 = stablehlo.broadcast_in_dim %6408, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc6036)
      %6410 = stablehlo.concatenate %6402, %6409, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6037)
      %6411 = stablehlo.reshape %arg1193 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6038)
      %6412 = stablehlo.reshape %6411 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6039)
      %6413 = stablehlo.convert %6412 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6040)
      %6414 = stablehlo.broadcast_in_dim %6413, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6041)
      %6415 = stablehlo.reduce(%6410 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6042)
      %6416 = stablehlo.broadcast_in_dim %6415, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6043)
      %6417 = stablehlo.subtract %6410, %6416 : tensor<1x16x128x129xbf16> loc(#loc6044)
      %6418 = stablehlo.reduce(%6417 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6045)
      %6419 = stablehlo.broadcast_in_dim %6418, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6046)
      %6420 = stablehlo.subtract %6417, %6419 : tensor<1x16x128x129xbf16> loc(#loc6047)
      %6421 = stablehlo.exponential %6420 : tensor<1x16x128x129xbf16> loc(#loc6048)
      %6422 = stablehlo.reduce(%6421 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6049)
      %6423 = stablehlo.broadcast_in_dim %6422, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6050)
      %6424 = stablehlo.divide %6421, %6423 : tensor<1x16x128x129xbf16> loc(#loc6051)
      %6425 = stablehlo.slice %6424 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6052)
      %6426 = stablehlo.reshape %arg649 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6053)
      %6427 = stablehlo.reshape %6426 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6054)
      %6428 = stablehlo.transpose %6427, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6055)
      %6429 = stablehlo.dot_general %6353, %6428, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6056)
      %6430 = "stablehlo.all_reduce"(%6429) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14359"), %arg1239: tensor<bf16> loc("dot.14359")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6056)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6056)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6056)
      %6431 = stablehlo.reshape %6430 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6057)
      %6432 = stablehlo.reshape %arg648 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6058)
      %6433 = stablehlo.reshape %6432 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6059)
      %6434 = stablehlo.broadcast_in_dim %6433, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6060)
      %6435 = stablehlo.add %6431, %6434 : tensor<1x128x128xbf16> loc(#loc6061)
      %6436 = stablehlo.reshape %6435 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6062)
      %6437 = stablehlo.transpose %6436, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6063)
      %6438 = stablehlo.broadcast_in_dim %6437, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6064)
      %6439 = stablehlo.reshape %6438 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6065)
      %6440 = stablehlo.dot_general %6425, %6439, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6066)
      %6441 = stablehlo.transpose %6440, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6067)
      %6442 = stablehlo.reshape %6441 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc6068)
      %6443 = stablehlo.reshape %arg647 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc6069)
      %6444 = stablehlo.reshape %6443 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc6070)
      %6445 = stablehlo.transpose %6444, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc6071)
      %6446 = stablehlo.dot_general %6442, %6445, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc6072)
      %6447 = "stablehlo.all_reduce"(%6446) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14552"), %arg1239: tensor<bf16> loc("dot.14552")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6072)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6072)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6072)
      %6448 = stablehlo.reshape %6447 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6073)
      %6449 = stablehlo.reshape %arg646 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6074)
      %6450 = stablehlo.reshape %6449 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6075)
      %6451 = stablehlo.broadcast_in_dim %6450, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6076)
      %6452 = stablehlo.add %6448, %6451 : tensor<1x128x360xbf16> loc(#loc6077)
      %6453 = stablehlo.add %6336, %6452 : tensor<1x128x360xbf16> loc(#loc6078)
      %6454 = stablehlo.reshape %arg1188 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6079)
      %6455 = stablehlo.reshape %6454 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6080)
      %6456 = stablehlo.convert %6455 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6081)
      %6457 = stablehlo.broadcast_in_dim %6456, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6082)
      %6458 = stablehlo.convert %6453 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6083)
      %6459 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6460 = stablehlo.power %6458, %6459 : tensor<1x128x360xf32> loc(#loc6084)
      %6461 = stablehlo.reduce(%6460 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6085)
      %6462 = "stablehlo.all_reduce"(%6461) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.14570"), %arg1239: tensor<f32> loc("reduce.14570")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6085)
        stablehlo.return %7360 : tensor<f32> loc(#loc6085)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6085)
      %6463 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6464 = stablehlo.multiply %6462, %6463 : tensor<1x128xf32> loc(#loc6086)
      %6465 = stablehlo.reshape %6464 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6087)
      %6466 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6467 = stablehlo.add %6465, %6466 : tensor<1x128x1xf32> loc(#loc6088)
      %6468 = stablehlo.rsqrt %6467 : tensor<1x128x1xf32> loc(#loc6089)
      %6469 = stablehlo.reshape %6468 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6090)
      %6470 = stablehlo.broadcast_in_dim %6469, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6091)
      %6471 = stablehlo.multiply %6458, %6470 : tensor<1x128x360xf32> loc(#loc6092)
      %6472 = stablehlo.multiply %6457, %6471 : tensor<1x128x360xf32> loc(#loc6093)
      %6473 = stablehlo.convert %6472 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6094)
      %6474 = stablehlo.reshape %6473 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6095)
      %6475 = stablehlo.broadcast_in_dim %6474, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6096)
      %6476 = stablehlo.dot_general %6475, %arg1192, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6097)
      %6477 = "stablehlo.all_reduce"(%6476) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14682"), %arg1239: tensor<bf16> loc("dot.14682")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6097)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6097)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6097)
      %6478 = stablehlo.reshape %arg1191 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc6098)
      %6479 = stablehlo.reshape %6478 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc6099)
      %6480 = stablehlo.broadcast_in_dim %6479, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6100)
      %6481 = stablehlo.add %6477, %6480 : tensor<32x128x5760xbf16> loc(#loc6101)
      %6482 = stablehlo.slice %6481 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6102)
      %6483 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6484 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6485 = stablehlo.clamp %6484, %6482, %6483 : tensor<32x128x2880xbf16> loc(#loc6103)
      %6486 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6487 = stablehlo.add %6485, %6486 : tensor<32x128x2880xbf16> loc(#loc6104)
      %6488 = stablehlo.slice %6481 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6105)
      %6489 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6490 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6491 = stablehlo.clamp %6489, %6488, %6490 : tensor<32x128x2880xbf16> loc(#loc6106)
      %6492 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6493 = stablehlo.multiply %6491, %6492 : tensor<32x128x2880xbf16> loc(#loc6107)
      %6494 = stablehlo.logistic %6493 : tensor<32x128x2880xbf16> loc(#loc6108)
      %6495 = stablehlo.multiply %6491, %6494 : tensor<32x128x2880xbf16> loc(#loc6109)
      %6496 = stablehlo.multiply %6487, %6495 : tensor<32x128x2880xbf16> loc(#loc6110)
      %6497 = stablehlo.dot_general %6496, %arg1190, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6111)
      %6498 = stablehlo.reshape %arg1189 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc6112)
      %6499 = stablehlo.reshape %6498 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc6113)
      %6500 = stablehlo.broadcast_in_dim %6499, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6114)
      %6501 = stablehlo.add %6497, %6500 : tensor<32x128x360xbf16> loc(#loc6115)
      %6502 = stablehlo.reshape %6501 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6116)
      %6503 = stablehlo.reshape %arg645 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6117)
      %6504 = stablehlo.reshape %6503 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6118)
      %6505 = stablehlo.transpose %6504, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6119)
      %6506 = stablehlo.dot_general %6474, %6505, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6120)
      %6507 = "stablehlo.all_reduce"(%6506) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14598"), %arg1239: tensor<bf16> loc("dot.14598")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6120)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6120)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6120)
      %6508 = stablehlo.reshape %arg644 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6121)
      %6509 = stablehlo.reshape %6508 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6122)
      %6510 = stablehlo.broadcast_in_dim %6509, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc6123)
      %6511 = stablehlo.add %6507, %6510 : tensor<128x128xbf16> loc(#loc6124)
      %6512:2 = "stablehlo.sort"(%6511, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.14618"), %arg1239: tensor<bf16> loc("sort.14618"), %arg1240: tensor<i32> loc("sort.14618"), %arg1241: tensor<i32> loc("sort.14618")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc6126)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc6125)
      %6513 = stablehlo.slice %6512#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc6127)
      %6514 = stablehlo.convert %6513 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc6128)
      %6515 = stablehlo.reshape %6514 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc6129)
      %6516 = stablehlo.concatenate %231, %6515, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc6130)
      %6517 = stablehlo.slice %6512#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc6131)
      %6518 = stablehlo.reduce(%6517 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6132)
      %6519 = stablehlo.broadcast_in_dim %6518, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6133)
      %6520 = stablehlo.subtract %6517, %6519 : tensor<128x4xbf16> loc(#loc6134)
      %6521 = stablehlo.exponential %6520 : tensor<128x4xbf16> loc(#loc6135)
      %6522 = stablehlo.reduce(%6521 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6136)
      %6523 = stablehlo.broadcast_in_dim %6522, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6137)
      %6524 = stablehlo.divide %6521, %6523 : tensor<128x4xbf16> loc(#loc6138)
      %6525 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %6526 = "stablehlo.all_gather"(%6525) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %6527 = "stablehlo.scatter"(%6526, %6516, %6524) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.14652"), %arg1239: tensor<bf16> loc("scatter.14652")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc6139)
      %6528 = stablehlo.reshape %6527 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc6139)
      %6529 = "stablehlo.all_to_all"(%6528) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc6139)
      %6530 = stablehlo.slice %6529 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc6139)
      %6531 = stablehlo.reshape %6530 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc6139)
      %6532 = stablehlo.transpose %6531, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc6140)
      %6533 = stablehlo.reshape %6532 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc6141)
      %6534 = stablehlo.broadcast_in_dim %6533, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6142)
      %6535 = stablehlo.multiply %6502, %6534 : tensor<32x1x128x360xbf16> loc(#loc6143)
      %6536 = stablehlo.reduce(%6535 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc6144)
      %6537 = "stablehlo.all_reduce"(%6536) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.14724"), %arg1239: tensor<bf16> loc("reduce.14724")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6144)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6144)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6144)
      %6538 = stablehlo.add %6453, %6537 : tensor<1x128x360xbf16> loc(#loc6145)
      %6539 = stablehlo.convert %6538 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6146)
      %6540 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6541 = stablehlo.power %6539, %6540 : tensor<1x128x360xf32> loc(#loc6147)
      %6542 = stablehlo.reduce(%6541 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6148)
      %6543 = "stablehlo.all_reduce"(%6542) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.14737"), %arg1239: tensor<f32> loc("reduce.14737")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6148)
        stablehlo.return %7360 : tensor<f32> loc(#loc6148)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6148)
      %6544 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6545 = stablehlo.multiply %6543, %6544 : tensor<1x128xf32> loc(#loc6149)
      %6546 = stablehlo.reshape %6545 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6150)
      %6547 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6548 = stablehlo.add %6546, %6547 : tensor<1x128x1xf32> loc(#loc6151)
      %6549 = stablehlo.rsqrt %6548 : tensor<1x128x1xf32> loc(#loc6152)
      %6550 = stablehlo.reshape %6549 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6153)
      %6551 = stablehlo.broadcast_in_dim %6550, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6154)
      %6552 = stablehlo.multiply %6539, %6551 : tensor<1x128x360xf32> loc(#loc6155)
      %6553 = stablehlo.multiply %6414, %6552 : tensor<1x128x360xf32> loc(#loc6156)
      %6554 = stablehlo.convert %6553 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6157)
      %6555 = stablehlo.reshape %6554 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6158)
      %6556 = stablehlo.reshape %arg1198 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc6159)
      %6557 = stablehlo.reshape %6556 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc6160)
      %6558 = stablehlo.transpose %6557, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc6161)
      %6559 = stablehlo.dot_general %6555, %6558, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6162)
      %6560 = "stablehlo.all_reduce"(%6559) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14851"), %arg1239: tensor<bf16> loc("dot.14851")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6162)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6162)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6162)
      %6561 = stablehlo.reshape %6560 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6163)
      %6562 = stablehlo.reshape %arg1197 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc6164)
      %6563 = stablehlo.reshape %6562 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc6165)
      %6564 = stablehlo.broadcast_in_dim %6563, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6166)
      %6565 = stablehlo.add %6561, %6564 : tensor<1x128x1024xbf16> loc(#loc6167)
      %6566 = stablehlo.reshape %6565 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6168)
      %6567 = stablehlo.transpose %6566, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6169)
      %6568 = stablehlo.slice %6567 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6170)
      %6569 = stablehlo.multiply %6568, %64 : tensor<1x16x128x32xbf16> loc(#loc6171)
      %6570 = stablehlo.slice %6567 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6172)
      %6571 = stablehlo.multiply %6570, %70 : tensor<1x16x128x32xbf16> loc(#loc6173)
      %6572 = stablehlo.subtract %6569, %6571 : tensor<1x16x128x32xbf16> loc(#loc6174)
      %6573 = stablehlo.multiply %6570, %64 : tensor<1x16x128x32xbf16> loc(#loc6175)
      %6574 = stablehlo.multiply %6568, %70 : tensor<1x16x128x32xbf16> loc(#loc6176)
      %6575 = stablehlo.add %6573, %6574 : tensor<1x16x128x32xbf16> loc(#loc6177)
      %6576 = stablehlo.concatenate %6572, %6575, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6178)
      %6577 = stablehlo.reshape %arg1196 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6179)
      %6578 = stablehlo.reshape %6577 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6180)
      %6579 = stablehlo.transpose %6578, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6181)
      %6580 = stablehlo.dot_general %6555, %6579, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6182)
      %6581 = "stablehlo.all_reduce"(%6580) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14802"), %arg1239: tensor<bf16> loc("dot.14802")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6182)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6182)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6182)
      %6582 = stablehlo.reshape %6581 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6183)
      %6583 = stablehlo.reshape %arg1195 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6184)
      %6584 = stablehlo.reshape %6583 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6185)
      %6585 = stablehlo.broadcast_in_dim %6584, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6186)
      %6586 = stablehlo.add %6582, %6585 : tensor<1x128x128xbf16> loc(#loc6187)
      %6587 = stablehlo.reshape %6586 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6188)
      %6588 = stablehlo.transpose %6587, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6189)
      %6589 = stablehlo.slice %6588 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6190)
      %6590 = stablehlo.multiply %6589, %90 : tensor<1x2x128x32xbf16> loc(#loc6191)
      %6591 = stablehlo.slice %6588 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6192)
      %6592 = stablehlo.multiply %6591, %93 : tensor<1x2x128x32xbf16> loc(#loc6193)
      %6593 = stablehlo.subtract %6590, %6592 : tensor<1x2x128x32xbf16> loc(#loc6194)
      %6594 = stablehlo.multiply %6591, %90 : tensor<1x2x128x32xbf16> loc(#loc6195)
      %6595 = stablehlo.multiply %6589, %93 : tensor<1x2x128x32xbf16> loc(#loc6196)
      %6596 = stablehlo.add %6594, %6595 : tensor<1x2x128x32xbf16> loc(#loc6197)
      %6597 = stablehlo.concatenate %6593, %6596, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6198)
      %6598 = stablehlo.broadcast_in_dim %6597, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6199)
      %6599 = stablehlo.reshape %6598 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6200)
      %6600 = stablehlo.transpose %6599, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc6201)
      %6601 = stablehlo.dot_general %6576, %6600, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6202)
      %6602 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %6603 = stablehlo.multiply %6601, %6602 : tensor<1x16x128x128xbf16> loc(#loc6203)
      %6604 = stablehlo.add %6603, %128 : tensor<1x16x128x128xbf16> loc(#loc6204)
      %6605 = stablehlo.reshape %arg1194 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc576)
      %6606 = "stablehlo.all_to_all"(%6605) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc576)
      %6607 = stablehlo.slice %6606 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc576)
      %6608 = stablehlo.reshape %6607 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc576)
      %6609 = stablehlo.reshape %6608 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc6205)
      %6610 = stablehlo.reshape %6609 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc6206)
      %6611 = stablehlo.broadcast_in_dim %6610, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc6207)
      %6612 = stablehlo.concatenate %6604, %6611, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6208)
      %6613 = stablehlo.reshape %arg1204 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6209)
      %6614 = stablehlo.reshape %6613 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6210)
      %6615 = stablehlo.convert %6614 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6211)
      %6616 = stablehlo.broadcast_in_dim %6615, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6212)
      %6617 = stablehlo.reduce(%6612 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6213)
      %6618 = stablehlo.broadcast_in_dim %6617, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6214)
      %6619 = stablehlo.subtract %6612, %6618 : tensor<1x16x128x129xbf16> loc(#loc6215)
      %6620 = stablehlo.reduce(%6619 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6216)
      %6621 = stablehlo.broadcast_in_dim %6620, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6217)
      %6622 = stablehlo.subtract %6619, %6621 : tensor<1x16x128x129xbf16> loc(#loc6218)
      %6623 = stablehlo.exponential %6622 : tensor<1x16x128x129xbf16> loc(#loc6219)
      %6624 = stablehlo.reduce(%6623 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6220)
      %6625 = stablehlo.broadcast_in_dim %6624, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6221)
      %6626 = stablehlo.divide %6623, %6625 : tensor<1x16x128x129xbf16> loc(#loc6222)
      %6627 = stablehlo.slice %6626 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6223)
      %6628 = stablehlo.reshape %arg643 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6224)
      %6629 = stablehlo.reshape %6628 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6225)
      %6630 = stablehlo.transpose %6629, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6226)
      %6631 = stablehlo.dot_general %6555, %6630, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6227)
      %6632 = "stablehlo.all_reduce"(%6631) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14765"), %arg1239: tensor<bf16> loc("dot.14765")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6227)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6227)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6227)
      %6633 = stablehlo.reshape %6632 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6228)
      %6634 = stablehlo.reshape %arg642 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6229)
      %6635 = stablehlo.reshape %6634 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6230)
      %6636 = stablehlo.broadcast_in_dim %6635, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6231)
      %6637 = stablehlo.add %6633, %6636 : tensor<1x128x128xbf16> loc(#loc6232)
      %6638 = stablehlo.reshape %6637 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6233)
      %6639 = stablehlo.transpose %6638, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6234)
      %6640 = stablehlo.broadcast_in_dim %6639, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6235)
      %6641 = stablehlo.reshape %6640 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6236)
      %6642 = stablehlo.dot_general %6627, %6641, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6237)
      %6643 = stablehlo.transpose %6642, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6238)
      %6644 = stablehlo.reshape %6643 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc6239)
      %6645 = stablehlo.reshape %arg641 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc6240)
      %6646 = stablehlo.reshape %6645 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc6241)
      %6647 = stablehlo.transpose %6646, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc6242)
      %6648 = stablehlo.dot_general %6644, %6647, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc6243)
      %6649 = "stablehlo.all_reduce"(%6648) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.14958"), %arg1239: tensor<bf16> loc("dot.14958")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6243)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6243)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6243)
      %6650 = stablehlo.reshape %6649 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6244)
      %6651 = stablehlo.reshape %arg640 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6245)
      %6652 = stablehlo.reshape %6651 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6246)
      %6653 = stablehlo.broadcast_in_dim %6652, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6247)
      %6654 = stablehlo.add %6650, %6653 : tensor<1x128x360xbf16> loc(#loc6248)
      %6655 = stablehlo.add %6538, %6654 : tensor<1x128x360xbf16> loc(#loc6249)
      %6656 = stablehlo.reshape %arg1199 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6250)
      %6657 = stablehlo.reshape %6656 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6251)
      %6658 = stablehlo.convert %6657 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6252)
      %6659 = stablehlo.broadcast_in_dim %6658, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6253)
      %6660 = stablehlo.convert %6655 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6254)
      %6661 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6662 = stablehlo.power %6660, %6661 : tensor<1x128x360xf32> loc(#loc6255)
      %6663 = stablehlo.reduce(%6662 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6256)
      %6664 = "stablehlo.all_reduce"(%6663) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.14976"), %arg1239: tensor<f32> loc("reduce.14976")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6256)
        stablehlo.return %7360 : tensor<f32> loc(#loc6256)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6256)
      %6665 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6666 = stablehlo.multiply %6664, %6665 : tensor<1x128xf32> loc(#loc6257)
      %6667 = stablehlo.reshape %6666 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6258)
      %6668 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6669 = stablehlo.add %6667, %6668 : tensor<1x128x1xf32> loc(#loc6259)
      %6670 = stablehlo.rsqrt %6669 : tensor<1x128x1xf32> loc(#loc6260)
      %6671 = stablehlo.reshape %6670 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6261)
      %6672 = stablehlo.broadcast_in_dim %6671, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6262)
      %6673 = stablehlo.multiply %6660, %6672 : tensor<1x128x360xf32> loc(#loc6263)
      %6674 = stablehlo.multiply %6659, %6673 : tensor<1x128x360xf32> loc(#loc6264)
      %6675 = stablehlo.convert %6674 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6265)
      %6676 = stablehlo.reshape %6675 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6266)
      %6677 = stablehlo.broadcast_in_dim %6676, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6267)
      %6678 = stablehlo.dot_general %6677, %arg1203, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6268)
      %6679 = "stablehlo.all_reduce"(%6678) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15088"), %arg1239: tensor<bf16> loc("dot.15088")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6268)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6268)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6268)
      %6680 = stablehlo.reshape %arg1202 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc6269)
      %6681 = stablehlo.reshape %6680 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc6270)
      %6682 = stablehlo.broadcast_in_dim %6681, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6271)
      %6683 = stablehlo.add %6679, %6682 : tensor<32x128x5760xbf16> loc(#loc6272)
      %6684 = stablehlo.slice %6683 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6273)
      %6685 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6686 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6687 = stablehlo.clamp %6686, %6684, %6685 : tensor<32x128x2880xbf16> loc(#loc6274)
      %6688 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6689 = stablehlo.add %6687, %6688 : tensor<32x128x2880xbf16> loc(#loc6275)
      %6690 = stablehlo.slice %6683 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6276)
      %6691 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6692 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6693 = stablehlo.clamp %6691, %6690, %6692 : tensor<32x128x2880xbf16> loc(#loc6277)
      %6694 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6695 = stablehlo.multiply %6693, %6694 : tensor<32x128x2880xbf16> loc(#loc6278)
      %6696 = stablehlo.logistic %6695 : tensor<32x128x2880xbf16> loc(#loc6279)
      %6697 = stablehlo.multiply %6693, %6696 : tensor<32x128x2880xbf16> loc(#loc6280)
      %6698 = stablehlo.multiply %6689, %6697 : tensor<32x128x2880xbf16> loc(#loc6281)
      %6699 = stablehlo.dot_general %6698, %arg1201, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6282)
      %6700 = stablehlo.reshape %arg1200 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc6283)
      %6701 = stablehlo.reshape %6700 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc6284)
      %6702 = stablehlo.broadcast_in_dim %6701, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6285)
      %6703 = stablehlo.add %6699, %6702 : tensor<32x128x360xbf16> loc(#loc6286)
      %6704 = stablehlo.reshape %6703 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6287)
      %6705 = stablehlo.reshape %arg639 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6288)
      %6706 = stablehlo.reshape %6705 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6289)
      %6707 = stablehlo.transpose %6706, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6290)
      %6708 = stablehlo.dot_general %6676, %6707, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6291)
      %6709 = "stablehlo.all_reduce"(%6708) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15004"), %arg1239: tensor<bf16> loc("dot.15004")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6291)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6291)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6291)
      %6710 = stablehlo.reshape %arg638 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6292)
      %6711 = stablehlo.reshape %6710 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6293)
      %6712 = stablehlo.broadcast_in_dim %6711, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc6294)
      %6713 = stablehlo.add %6709, %6712 : tensor<128x128xbf16> loc(#loc6295)
      %6714:2 = "stablehlo.sort"(%6713, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.15024"), %arg1239: tensor<bf16> loc("sort.15024"), %arg1240: tensor<i32> loc("sort.15024"), %arg1241: tensor<i32> loc("sort.15024")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc6297)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc6296)
      %6715 = stablehlo.slice %6714#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc6298)
      %6716 = stablehlo.convert %6715 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc6299)
      %6717 = stablehlo.reshape %6716 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc6300)
      %6718 = stablehlo.concatenate %231, %6717, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc6301)
      %6719 = stablehlo.slice %6714#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc6302)
      %6720 = stablehlo.reduce(%6719 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6303)
      %6721 = stablehlo.broadcast_in_dim %6720, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6304)
      %6722 = stablehlo.subtract %6719, %6721 : tensor<128x4xbf16> loc(#loc6305)
      %6723 = stablehlo.exponential %6722 : tensor<128x4xbf16> loc(#loc6306)
      %6724 = stablehlo.reduce(%6723 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6307)
      %6725 = stablehlo.broadcast_in_dim %6724, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6308)
      %6726 = stablehlo.divide %6723, %6725 : tensor<128x4xbf16> loc(#loc6309)
      %6727 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %6728 = "stablehlo.all_gather"(%6727) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %6729 = "stablehlo.scatter"(%6728, %6718, %6726) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.15058"), %arg1239: tensor<bf16> loc("scatter.15058")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc6310)
      %6730 = stablehlo.reshape %6729 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc6310)
      %6731 = "stablehlo.all_to_all"(%6730) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc6310)
      %6732 = stablehlo.slice %6731 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc6310)
      %6733 = stablehlo.reshape %6732 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc6310)
      %6734 = stablehlo.transpose %6733, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc6311)
      %6735 = stablehlo.reshape %6734 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc6312)
      %6736 = stablehlo.broadcast_in_dim %6735, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6313)
      %6737 = stablehlo.multiply %6704, %6736 : tensor<32x1x128x360xbf16> loc(#loc6314)
      %6738 = stablehlo.reduce(%6737 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc6315)
      %6739 = "stablehlo.all_reduce"(%6738) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.15130"), %arg1239: tensor<bf16> loc("reduce.15130")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6315)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6315)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6315)
      %6740 = stablehlo.add %6655, %6739 : tensor<1x128x360xbf16> loc(#loc6316)
      %6741 = stablehlo.convert %6740 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6317)
      %6742 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6743 = stablehlo.power %6741, %6742 : tensor<1x128x360xf32> loc(#loc6318)
      %6744 = stablehlo.reduce(%6743 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6319)
      %6745 = "stablehlo.all_reduce"(%6744) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.15143"), %arg1239: tensor<f32> loc("reduce.15143")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6319)
        stablehlo.return %7360 : tensor<f32> loc(#loc6319)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6319)
      %6746 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6747 = stablehlo.multiply %6745, %6746 : tensor<1x128xf32> loc(#loc6320)
      %6748 = stablehlo.reshape %6747 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6321)
      %6749 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6750 = stablehlo.add %6748, %6749 : tensor<1x128x1xf32> loc(#loc6322)
      %6751 = stablehlo.rsqrt %6750 : tensor<1x128x1xf32> loc(#loc6323)
      %6752 = stablehlo.reshape %6751 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6324)
      %6753 = stablehlo.broadcast_in_dim %6752, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6325)
      %6754 = stablehlo.multiply %6741, %6753 : tensor<1x128x360xf32> loc(#loc6326)
      %6755 = stablehlo.multiply %6616, %6754 : tensor<1x128x360xf32> loc(#loc6327)
      %6756 = stablehlo.convert %6755 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6328)
      %6757 = stablehlo.reshape %6756 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6329)
      %6758 = stablehlo.reshape %arg1209 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc6330)
      %6759 = stablehlo.reshape %6758 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc6331)
      %6760 = stablehlo.transpose %6759, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc6332)
      %6761 = stablehlo.dot_general %6757, %6760, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6333)
      %6762 = "stablehlo.all_reduce"(%6761) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15257"), %arg1239: tensor<bf16> loc("dot.15257")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6333)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6333)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6333)
      %6763 = stablehlo.reshape %6762 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6334)
      %6764 = stablehlo.reshape %arg1208 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc6335)
      %6765 = stablehlo.reshape %6764 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc6336)
      %6766 = stablehlo.broadcast_in_dim %6765, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6337)
      %6767 = stablehlo.add %6763, %6766 : tensor<1x128x1024xbf16> loc(#loc6338)
      %6768 = stablehlo.reshape %6767 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6339)
      %6769 = stablehlo.transpose %6768, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6340)
      %6770 = stablehlo.slice %6769 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6341)
      %6771 = stablehlo.multiply %6770, %64 : tensor<1x16x128x32xbf16> loc(#loc6342)
      %6772 = stablehlo.slice %6769 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6343)
      %6773 = stablehlo.multiply %6772, %70 : tensor<1x16x128x32xbf16> loc(#loc6344)
      %6774 = stablehlo.subtract %6771, %6773 : tensor<1x16x128x32xbf16> loc(#loc6345)
      %6775 = stablehlo.multiply %6772, %64 : tensor<1x16x128x32xbf16> loc(#loc6346)
      %6776 = stablehlo.multiply %6770, %70 : tensor<1x16x128x32xbf16> loc(#loc6347)
      %6777 = stablehlo.add %6775, %6776 : tensor<1x16x128x32xbf16> loc(#loc6348)
      %6778 = stablehlo.concatenate %6774, %6777, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6349)
      %6779 = stablehlo.reshape %arg1207 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6350)
      %6780 = stablehlo.reshape %6779 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6351)
      %6781 = stablehlo.transpose %6780, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6352)
      %6782 = stablehlo.dot_general %6757, %6781, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6353)
      %6783 = "stablehlo.all_reduce"(%6782) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15208"), %arg1239: tensor<bf16> loc("dot.15208")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6353)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6353)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6353)
      %6784 = stablehlo.reshape %6783 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6354)
      %6785 = stablehlo.reshape %arg1206 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6355)
      %6786 = stablehlo.reshape %6785 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6356)
      %6787 = stablehlo.broadcast_in_dim %6786, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6357)
      %6788 = stablehlo.add %6784, %6787 : tensor<1x128x128xbf16> loc(#loc6358)
      %6789 = stablehlo.reshape %6788 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6359)
      %6790 = stablehlo.transpose %6789, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6360)
      %6791 = stablehlo.slice %6790 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6361)
      %6792 = stablehlo.multiply %6791, %90 : tensor<1x2x128x32xbf16> loc(#loc6362)
      %6793 = stablehlo.slice %6790 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6363)
      %6794 = stablehlo.multiply %6793, %93 : tensor<1x2x128x32xbf16> loc(#loc6364)
      %6795 = stablehlo.subtract %6792, %6794 : tensor<1x2x128x32xbf16> loc(#loc6365)
      %6796 = stablehlo.multiply %6793, %90 : tensor<1x2x128x32xbf16> loc(#loc6366)
      %6797 = stablehlo.multiply %6791, %93 : tensor<1x2x128x32xbf16> loc(#loc6367)
      %6798 = stablehlo.add %6796, %6797 : tensor<1x2x128x32xbf16> loc(#loc6368)
      %6799 = stablehlo.concatenate %6795, %6798, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6369)
      %6800 = stablehlo.broadcast_in_dim %6799, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6370)
      %6801 = stablehlo.reshape %6800 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6371)
      %6802 = stablehlo.transpose %6801, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc6372)
      %6803 = stablehlo.dot_general %6778, %6802, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6373)
      %6804 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %6805 = stablehlo.multiply %6803, %6804 : tensor<1x16x128x128xbf16> loc(#loc6374)
      %6806 = stablehlo.add %6805, %341 : tensor<1x16x128x128xbf16> loc(#loc6375)
      %6807 = stablehlo.reshape %arg1205 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc587)
      %6808 = "stablehlo.all_to_all"(%6807) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc587)
      %6809 = stablehlo.slice %6808 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc587)
      %6810 = stablehlo.reshape %6809 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc587)
      %6811 = stablehlo.reshape %6810 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc6376)
      %6812 = stablehlo.reshape %6811 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc6377)
      %6813 = stablehlo.broadcast_in_dim %6812, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc6378)
      %6814 = stablehlo.concatenate %6806, %6813, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6379)
      %6815 = stablehlo.reshape %arg1215 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6380)
      %6816 = stablehlo.reshape %6815 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6381)
      %6817 = stablehlo.convert %6816 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6382)
      %6818 = stablehlo.broadcast_in_dim %6817, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6383)
      %6819 = stablehlo.reduce(%6814 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6384)
      %6820 = stablehlo.broadcast_in_dim %6819, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6385)
      %6821 = stablehlo.subtract %6814, %6820 : tensor<1x16x128x129xbf16> loc(#loc6386)
      %6822 = stablehlo.reduce(%6821 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6387)
      %6823 = stablehlo.broadcast_in_dim %6822, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6388)
      %6824 = stablehlo.subtract %6821, %6823 : tensor<1x16x128x129xbf16> loc(#loc6389)
      %6825 = stablehlo.exponential %6824 : tensor<1x16x128x129xbf16> loc(#loc6390)
      %6826 = stablehlo.reduce(%6825 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6391)
      %6827 = stablehlo.broadcast_in_dim %6826, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6392)
      %6828 = stablehlo.divide %6825, %6827 : tensor<1x16x128x129xbf16> loc(#loc6393)
      %6829 = stablehlo.slice %6828 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6394)
      %6830 = stablehlo.reshape %arg637 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6395)
      %6831 = stablehlo.reshape %6830 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6396)
      %6832 = stablehlo.transpose %6831, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6397)
      %6833 = stablehlo.dot_general %6757, %6832, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6398)
      %6834 = "stablehlo.all_reduce"(%6833) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15171"), %arg1239: tensor<bf16> loc("dot.15171")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6398)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6398)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6398)
      %6835 = stablehlo.reshape %6834 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6399)
      %6836 = stablehlo.reshape %arg636 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6400)
      %6837 = stablehlo.reshape %6836 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6401)
      %6838 = stablehlo.broadcast_in_dim %6837, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6402)
      %6839 = stablehlo.add %6835, %6838 : tensor<1x128x128xbf16> loc(#loc6403)
      %6840 = stablehlo.reshape %6839 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6404)
      %6841 = stablehlo.transpose %6840, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6405)
      %6842 = stablehlo.broadcast_in_dim %6841, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6406)
      %6843 = stablehlo.reshape %6842 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6407)
      %6844 = stablehlo.dot_general %6829, %6843, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6408)
      %6845 = stablehlo.transpose %6844, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6409)
      %6846 = stablehlo.reshape %6845 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc6410)
      %6847 = stablehlo.reshape %arg635 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc6411)
      %6848 = stablehlo.reshape %6847 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc6412)
      %6849 = stablehlo.transpose %6848, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc6413)
      %6850 = stablehlo.dot_general %6846, %6849, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc6414)
      %6851 = "stablehlo.all_reduce"(%6850) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15364"), %arg1239: tensor<bf16> loc("dot.15364")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6414)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6414)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6414)
      %6852 = stablehlo.reshape %6851 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6415)
      %6853 = stablehlo.reshape %arg634 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6416)
      %6854 = stablehlo.reshape %6853 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6417)
      %6855 = stablehlo.broadcast_in_dim %6854, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6418)
      %6856 = stablehlo.add %6852, %6855 : tensor<1x128x360xbf16> loc(#loc6419)
      %6857 = stablehlo.add %6740, %6856 : tensor<1x128x360xbf16> loc(#loc6420)
      %6858 = stablehlo.reshape %arg1210 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6421)
      %6859 = stablehlo.reshape %6858 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6422)
      %6860 = stablehlo.convert %6859 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6423)
      %6861 = stablehlo.broadcast_in_dim %6860, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6424)
      %6862 = stablehlo.convert %6857 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6425)
      %6863 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6864 = stablehlo.power %6862, %6863 : tensor<1x128x360xf32> loc(#loc6426)
      %6865 = stablehlo.reduce(%6864 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6427)
      %6866 = "stablehlo.all_reduce"(%6865) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.15382"), %arg1239: tensor<f32> loc("reduce.15382")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6427)
        stablehlo.return %7360 : tensor<f32> loc(#loc6427)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6427)
      %6867 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6868 = stablehlo.multiply %6866, %6867 : tensor<1x128xf32> loc(#loc6428)
      %6869 = stablehlo.reshape %6868 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6429)
      %6870 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6871 = stablehlo.add %6869, %6870 : tensor<1x128x1xf32> loc(#loc6430)
      %6872 = stablehlo.rsqrt %6871 : tensor<1x128x1xf32> loc(#loc6431)
      %6873 = stablehlo.reshape %6872 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6432)
      %6874 = stablehlo.broadcast_in_dim %6873, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6433)
      %6875 = stablehlo.multiply %6862, %6874 : tensor<1x128x360xf32> loc(#loc6434)
      %6876 = stablehlo.multiply %6861, %6875 : tensor<1x128x360xf32> loc(#loc6435)
      %6877 = stablehlo.convert %6876 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6436)
      %6878 = stablehlo.reshape %6877 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6437)
      %6879 = stablehlo.broadcast_in_dim %6878, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6438)
      %6880 = stablehlo.dot_general %6879, %arg1214, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6439)
      %6881 = "stablehlo.all_reduce"(%6880) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15494"), %arg1239: tensor<bf16> loc("dot.15494")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6439)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6439)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6439)
      %6882 = stablehlo.reshape %arg1213 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc6440)
      %6883 = stablehlo.reshape %6882 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc6441)
      %6884 = stablehlo.broadcast_in_dim %6883, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6442)
      %6885 = stablehlo.add %6881, %6884 : tensor<32x128x5760xbf16> loc(#loc6443)
      %6886 = stablehlo.slice %6885 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6444)
      %6887 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6888 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6889 = stablehlo.clamp %6888, %6886, %6887 : tensor<32x128x2880xbf16> loc(#loc6445)
      %6890 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6891 = stablehlo.add %6889, %6890 : tensor<32x128x2880xbf16> loc(#loc6446)
      %6892 = stablehlo.slice %6885 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6447)
      %6893 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6894 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6895 = stablehlo.clamp %6893, %6892, %6894 : tensor<32x128x2880xbf16> loc(#loc6448)
      %6896 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %6897 = stablehlo.multiply %6895, %6896 : tensor<32x128x2880xbf16> loc(#loc6449)
      %6898 = stablehlo.logistic %6897 : tensor<32x128x2880xbf16> loc(#loc6450)
      %6899 = stablehlo.multiply %6895, %6898 : tensor<32x128x2880xbf16> loc(#loc6451)
      %6900 = stablehlo.multiply %6891, %6899 : tensor<32x128x2880xbf16> loc(#loc6452)
      %6901 = stablehlo.dot_general %6900, %arg1212, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6453)
      %6902 = stablehlo.reshape %arg1211 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc6454)
      %6903 = stablehlo.reshape %6902 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc6455)
      %6904 = stablehlo.broadcast_in_dim %6903, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6456)
      %6905 = stablehlo.add %6901, %6904 : tensor<32x128x360xbf16> loc(#loc6457)
      %6906 = stablehlo.reshape %6905 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6458)
      %6907 = stablehlo.reshape %arg633 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6459)
      %6908 = stablehlo.reshape %6907 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6460)
      %6909 = stablehlo.transpose %6908, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6461)
      %6910 = stablehlo.dot_general %6878, %6909, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6462)
      %6911 = "stablehlo.all_reduce"(%6910) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15410"), %arg1239: tensor<bf16> loc("dot.15410")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6462)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6462)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6462)
      %6912 = stablehlo.reshape %arg632 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6463)
      %6913 = stablehlo.reshape %6912 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6464)
      %6914 = stablehlo.broadcast_in_dim %6913, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc6465)
      %6915 = stablehlo.add %6911, %6914 : tensor<128x128xbf16> loc(#loc6466)
      %6916:2 = "stablehlo.sort"(%6915, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.15430"), %arg1239: tensor<bf16> loc("sort.15430"), %arg1240: tensor<i32> loc("sort.15430"), %arg1241: tensor<i32> loc("sort.15430")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc6468)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc6467)
      %6917 = stablehlo.slice %6916#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc6469)
      %6918 = stablehlo.convert %6917 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc6470)
      %6919 = stablehlo.reshape %6918 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc6471)
      %6920 = stablehlo.concatenate %231, %6919, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc6472)
      %6921 = stablehlo.slice %6916#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc6473)
      %6922 = stablehlo.reduce(%6921 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6474)
      %6923 = stablehlo.broadcast_in_dim %6922, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6475)
      %6924 = stablehlo.subtract %6921, %6923 : tensor<128x4xbf16> loc(#loc6476)
      %6925 = stablehlo.exponential %6924 : tensor<128x4xbf16> loc(#loc6477)
      %6926 = stablehlo.reduce(%6925 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6478)
      %6927 = stablehlo.broadcast_in_dim %6926, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6479)
      %6928 = stablehlo.divide %6925, %6927 : tensor<128x4xbf16> loc(#loc6480)
      %6929 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %6930 = "stablehlo.all_gather"(%6929) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %6931 = "stablehlo.scatter"(%6930, %6920, %6928) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.15464"), %arg1239: tensor<bf16> loc("scatter.15464")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc6481)
      %6932 = stablehlo.reshape %6931 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc6481)
      %6933 = "stablehlo.all_to_all"(%6932) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc6481)
      %6934 = stablehlo.slice %6933 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc6481)
      %6935 = stablehlo.reshape %6934 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc6481)
      %6936 = stablehlo.transpose %6935, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc6482)
      %6937 = stablehlo.reshape %6936 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc6483)
      %6938 = stablehlo.broadcast_in_dim %6937, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6484)
      %6939 = stablehlo.multiply %6906, %6938 : tensor<32x1x128x360xbf16> loc(#loc6485)
      %6940 = stablehlo.reduce(%6939 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc6486)
      %6941 = "stablehlo.all_reduce"(%6940) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.15536"), %arg1239: tensor<bf16> loc("reduce.15536")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6486)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6486)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6486)
      %6942 = stablehlo.add %6857, %6941 : tensor<1x128x360xbf16> loc(#loc6487)
      %6943 = stablehlo.convert %6942 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6488)
      %6944 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %6945 = stablehlo.power %6943, %6944 : tensor<1x128x360xf32> loc(#loc6489)
      %6946 = stablehlo.reduce(%6945 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6490)
      %6947 = "stablehlo.all_reduce"(%6946) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.15549"), %arg1239: tensor<f32> loc("reduce.15549")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6490)
        stablehlo.return %7360 : tensor<f32> loc(#loc6490)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6490)
      %6948 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %6949 = stablehlo.multiply %6947, %6948 : tensor<1x128xf32> loc(#loc6491)
      %6950 = stablehlo.reshape %6949 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6492)
      %6951 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %6952 = stablehlo.add %6950, %6951 : tensor<1x128x1xf32> loc(#loc6493)
      %6953 = stablehlo.rsqrt %6952 : tensor<1x128x1xf32> loc(#loc6494)
      %6954 = stablehlo.reshape %6953 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6495)
      %6955 = stablehlo.broadcast_in_dim %6954, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6496)
      %6956 = stablehlo.multiply %6943, %6955 : tensor<1x128x360xf32> loc(#loc6497)
      %6957 = stablehlo.multiply %6818, %6956 : tensor<1x128x360xf32> loc(#loc6498)
      %6958 = stablehlo.convert %6957 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6499)
      %6959 = stablehlo.reshape %6958 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6500)
      %6960 = stablehlo.reshape %arg1220 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc6501)
      %6961 = stablehlo.reshape %6960 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc6502)
      %6962 = stablehlo.transpose %6961, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc6503)
      %6963 = stablehlo.dot_general %6959, %6962, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6504)
      %6964 = "stablehlo.all_reduce"(%6963) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15663"), %arg1239: tensor<bf16> loc("dot.15663")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6504)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6504)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6504)
      %6965 = stablehlo.reshape %6964 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6505)
      %6966 = stablehlo.reshape %arg1219 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc6506)
      %6967 = stablehlo.reshape %6966 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc6507)
      %6968 = stablehlo.broadcast_in_dim %6967, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6508)
      %6969 = stablehlo.add %6965, %6968 : tensor<1x128x1024xbf16> loc(#loc6509)
      %6970 = stablehlo.reshape %6969 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6510)
      %6971 = stablehlo.transpose %6970, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6511)
      %6972 = stablehlo.slice %6971 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6512)
      %6973 = stablehlo.multiply %6972, %64 : tensor<1x16x128x32xbf16> loc(#loc6513)
      %6974 = stablehlo.slice %6971 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6514)
      %6975 = stablehlo.multiply %6974, %70 : tensor<1x16x128x32xbf16> loc(#loc6515)
      %6976 = stablehlo.subtract %6973, %6975 : tensor<1x16x128x32xbf16> loc(#loc6516)
      %6977 = stablehlo.multiply %6974, %64 : tensor<1x16x128x32xbf16> loc(#loc6517)
      %6978 = stablehlo.multiply %6972, %70 : tensor<1x16x128x32xbf16> loc(#loc6518)
      %6979 = stablehlo.add %6977, %6978 : tensor<1x16x128x32xbf16> loc(#loc6519)
      %6980 = stablehlo.concatenate %6976, %6979, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6520)
      %6981 = stablehlo.reshape %arg1218 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6521)
      %6982 = stablehlo.reshape %6981 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6522)
      %6983 = stablehlo.transpose %6982, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6523)
      %6984 = stablehlo.dot_general %6959, %6983, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6524)
      %6985 = "stablehlo.all_reduce"(%6984) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15614"), %arg1239: tensor<bf16> loc("dot.15614")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6524)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6524)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6524)
      %6986 = stablehlo.reshape %6985 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6525)
      %6987 = stablehlo.reshape %arg1217 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6526)
      %6988 = stablehlo.reshape %6987 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6527)
      %6989 = stablehlo.broadcast_in_dim %6988, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6528)
      %6990 = stablehlo.add %6986, %6989 : tensor<1x128x128xbf16> loc(#loc6529)
      %6991 = stablehlo.reshape %6990 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6530)
      %6992 = stablehlo.transpose %6991, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6531)
      %6993 = stablehlo.slice %6992 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6532)
      %6994 = stablehlo.multiply %6993, %90 : tensor<1x2x128x32xbf16> loc(#loc6533)
      %6995 = stablehlo.slice %6992 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6534)
      %6996 = stablehlo.multiply %6995, %93 : tensor<1x2x128x32xbf16> loc(#loc6535)
      %6997 = stablehlo.subtract %6994, %6996 : tensor<1x2x128x32xbf16> loc(#loc6536)
      %6998 = stablehlo.multiply %6995, %90 : tensor<1x2x128x32xbf16> loc(#loc6537)
      %6999 = stablehlo.multiply %6993, %93 : tensor<1x2x128x32xbf16> loc(#loc6538)
      %7000 = stablehlo.add %6998, %6999 : tensor<1x2x128x32xbf16> loc(#loc6539)
      %7001 = stablehlo.concatenate %6997, %7000, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6540)
      %7002 = stablehlo.broadcast_in_dim %7001, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6541)
      %7003 = stablehlo.reshape %7002 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6542)
      %7004 = stablehlo.transpose %7003, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc6543)
      %7005 = stablehlo.dot_general %6980, %7004, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6544)
      %7006 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x16x128x128xbf16> loc(#loc)
      %7007 = stablehlo.multiply %7005, %7006 : tensor<1x16x128x128xbf16> loc(#loc6545)
      %7008 = stablehlo.add %7007, %128 : tensor<1x16x128x128xbf16> loc(#loc6546)
      %7009 = stablehlo.reshape %arg1216 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc598)
      %7010 = "stablehlo.all_to_all"(%7009) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc598)
      %7011 = stablehlo.slice %7010 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc598)
      %7012 = stablehlo.reshape %7011 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc598)
      %7013 = stablehlo.reshape %7012 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc6547)
      %7014 = stablehlo.reshape %7013 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc6548)
      %7015 = stablehlo.broadcast_in_dim %7014, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc6549)
      %7016 = stablehlo.concatenate %7008, %7015, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6550)
      %7017 = stablehlo.reshape %arg1226 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6551)
      %7018 = stablehlo.reshape %7017 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6552)
      %7019 = stablehlo.convert %7018 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6553)
      %7020 = stablehlo.broadcast_in_dim %7019, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6554)
      %7021 = stablehlo.reduce(%7016 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6555)
      %7022 = stablehlo.broadcast_in_dim %7021, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6556)
      %7023 = stablehlo.subtract %7016, %7022 : tensor<1x16x128x129xbf16> loc(#loc6557)
      %7024 = stablehlo.reduce(%7023 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6558)
      %7025 = stablehlo.broadcast_in_dim %7024, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6559)
      %7026 = stablehlo.subtract %7023, %7025 : tensor<1x16x128x129xbf16> loc(#loc6560)
      %7027 = stablehlo.exponential %7026 : tensor<1x16x128x129xbf16> loc(#loc6561)
      %7028 = stablehlo.reduce(%7027 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6562)
      %7029 = stablehlo.broadcast_in_dim %7028, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6563)
      %7030 = stablehlo.divide %7027, %7029 : tensor<1x16x128x129xbf16> loc(#loc6564)
      %7031 = stablehlo.slice %7030 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6565)
      %7032 = stablehlo.reshape %arg631 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6566)
      %7033 = stablehlo.reshape %7032 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6567)
      %7034 = stablehlo.transpose %7033, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6568)
      %7035 = stablehlo.dot_general %6959, %7034, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6569)
      %7036 = "stablehlo.all_reduce"(%7035) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15577"), %arg1239: tensor<bf16> loc("dot.15577")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6569)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6569)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6569)
      %7037 = stablehlo.reshape %7036 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6570)
      %7038 = stablehlo.reshape %arg630 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6571)
      %7039 = stablehlo.reshape %7038 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6572)
      %7040 = stablehlo.broadcast_in_dim %7039, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6573)
      %7041 = stablehlo.add %7037, %7040 : tensor<1x128x128xbf16> loc(#loc6574)
      %7042 = stablehlo.reshape %7041 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6575)
      %7043 = stablehlo.transpose %7042, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6576)
      %7044 = stablehlo.broadcast_in_dim %7043, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6577)
      %7045 = stablehlo.reshape %7044 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6578)
      %7046 = stablehlo.dot_general %7031, %7045, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6579)
      %7047 = stablehlo.transpose %7046, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6580)
      %7048 = stablehlo.reshape %7047 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc6581)
      %7049 = stablehlo.reshape %arg629 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc6582)
      %7050 = stablehlo.reshape %7049 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc6583)
      %7051 = stablehlo.transpose %7050, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc6584)
      %7052 = stablehlo.dot_general %7048, %7051, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc6585)
      %7053 = "stablehlo.all_reduce"(%7052) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15770"), %arg1239: tensor<bf16> loc("dot.15770")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6585)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6585)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6585)
      %7054 = stablehlo.reshape %7053 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6586)
      %7055 = stablehlo.reshape %arg628 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6587)
      %7056 = stablehlo.reshape %7055 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6588)
      %7057 = stablehlo.broadcast_in_dim %7056, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6589)
      %7058 = stablehlo.add %7054, %7057 : tensor<1x128x360xbf16> loc(#loc6590)
      %7059 = stablehlo.add %6942, %7058 : tensor<1x128x360xbf16> loc(#loc6591)
      %7060 = stablehlo.reshape %arg1221 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6592)
      %7061 = stablehlo.reshape %7060 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6593)
      %7062 = stablehlo.convert %7061 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6594)
      %7063 = stablehlo.broadcast_in_dim %7062, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6595)
      %7064 = stablehlo.convert %7059 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6596)
      %7065 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %7066 = stablehlo.power %7064, %7065 : tensor<1x128x360xf32> loc(#loc6597)
      %7067 = stablehlo.reduce(%7066 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6598)
      %7068 = "stablehlo.all_reduce"(%7067) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.15788"), %arg1239: tensor<f32> loc("reduce.15788")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6598)
        stablehlo.return %7360 : tensor<f32> loc(#loc6598)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6598)
      %7069 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %7070 = stablehlo.multiply %7068, %7069 : tensor<1x128xf32> loc(#loc6599)
      %7071 = stablehlo.reshape %7070 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6600)
      %7072 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %7073 = stablehlo.add %7071, %7072 : tensor<1x128x1xf32> loc(#loc6601)
      %7074 = stablehlo.rsqrt %7073 : tensor<1x128x1xf32> loc(#loc6602)
      %7075 = stablehlo.reshape %7074 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6603)
      %7076 = stablehlo.broadcast_in_dim %7075, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6604)
      %7077 = stablehlo.multiply %7064, %7076 : tensor<1x128x360xf32> loc(#loc6605)
      %7078 = stablehlo.multiply %7063, %7077 : tensor<1x128x360xf32> loc(#loc6606)
      %7079 = stablehlo.convert %7078 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6607)
      %7080 = stablehlo.reshape %7079 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6608)
      %7081 = stablehlo.broadcast_in_dim %7080, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6609)
      %7082 = stablehlo.dot_general %7081, %arg1225, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6610)
      %7083 = "stablehlo.all_reduce"(%7082) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15900"), %arg1239: tensor<bf16> loc("dot.15900")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6610)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6610)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6610)
      %7084 = stablehlo.reshape %arg1224 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc6611)
      %7085 = stablehlo.reshape %7084 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc6612)
      %7086 = stablehlo.broadcast_in_dim %7085, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6613)
      %7087 = stablehlo.add %7083, %7086 : tensor<32x128x5760xbf16> loc(#loc6614)
      %7088 = stablehlo.slice %7087 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6615)
      %7089 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7090 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7091 = stablehlo.clamp %7090, %7088, %7089 : tensor<32x128x2880xbf16> loc(#loc6616)
      %7092 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7093 = stablehlo.add %7091, %7092 : tensor<32x128x2880xbf16> loc(#loc6617)
      %7094 = stablehlo.slice %7087 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6618)
      %7095 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7096 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7097 = stablehlo.clamp %7095, %7094, %7096 : tensor<32x128x2880xbf16> loc(#loc6619)
      %7098 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7099 = stablehlo.multiply %7097, %7098 : tensor<32x128x2880xbf16> loc(#loc6620)
      %7100 = stablehlo.logistic %7099 : tensor<32x128x2880xbf16> loc(#loc6621)
      %7101 = stablehlo.multiply %7097, %7100 : tensor<32x128x2880xbf16> loc(#loc6622)
      %7102 = stablehlo.multiply %7093, %7101 : tensor<32x128x2880xbf16> loc(#loc6623)
      %7103 = stablehlo.dot_general %7102, %arg1223, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6624)
      %7104 = stablehlo.reshape %arg1222 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc6625)
      %7105 = stablehlo.reshape %7104 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc6626)
      %7106 = stablehlo.broadcast_in_dim %7105, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6627)
      %7107 = stablehlo.add %7103, %7106 : tensor<32x128x360xbf16> loc(#loc6628)
      %7108 = stablehlo.reshape %7107 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6629)
      %7109 = stablehlo.reshape %arg627 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6630)
      %7110 = stablehlo.reshape %7109 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6631)
      %7111 = stablehlo.transpose %7110, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6632)
      %7112 = stablehlo.dot_general %7080, %7111, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6633)
      %7113 = "stablehlo.all_reduce"(%7112) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15816"), %arg1239: tensor<bf16> loc("dot.15816")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6633)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6633)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6633)
      %7114 = stablehlo.reshape %arg626 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6634)
      %7115 = stablehlo.reshape %7114 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6635)
      %7116 = stablehlo.broadcast_in_dim %7115, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc6636)
      %7117 = stablehlo.add %7113, %7116 : tensor<128x128xbf16> loc(#loc6637)
      %7118:2 = "stablehlo.sort"(%7117, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.15836"), %arg1239: tensor<bf16> loc("sort.15836"), %arg1240: tensor<i32> loc("sort.15836"), %arg1241: tensor<i32> loc("sort.15836")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc6639)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc6638)
      %7119 = stablehlo.slice %7118#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc6640)
      %7120 = stablehlo.convert %7119 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc6641)
      %7121 = stablehlo.reshape %7120 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc6642)
      %7122 = stablehlo.concatenate %231, %7121, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc6643)
      %7123 = stablehlo.slice %7118#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc6644)
      %7124 = stablehlo.reduce(%7123 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6645)
      %7125 = stablehlo.broadcast_in_dim %7124, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6646)
      %7126 = stablehlo.subtract %7123, %7125 : tensor<128x4xbf16> loc(#loc6647)
      %7127 = stablehlo.exponential %7126 : tensor<128x4xbf16> loc(#loc6648)
      %7128 = stablehlo.reduce(%7127 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6649)
      %7129 = stablehlo.broadcast_in_dim %7128, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6650)
      %7130 = stablehlo.divide %7127, %7129 : tensor<128x4xbf16> loc(#loc6651)
      %7131 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<bf16>) -> tensor<128x32xbf16> loc(#loc)
      %7132 = "stablehlo.all_gather"(%7131) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %7133 = "stablehlo.scatter"(%7132, %7122, %7130) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.15870"), %arg1239: tensor<bf16> loc("scatter.15870")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc6652)
      %7134 = stablehlo.reshape %7133 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc6652)
      %7135 = "stablehlo.all_to_all"(%7134) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc6652)
      %7136 = stablehlo.slice %7135 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc6652)
      %7137 = stablehlo.reshape %7136 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc6652)
      %7138 = stablehlo.transpose %7137, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc6653)
      %7139 = stablehlo.reshape %7138 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc6654)
      %7140 = stablehlo.broadcast_in_dim %7139, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6655)
      %7141 = stablehlo.multiply %7108, %7140 : tensor<32x1x128x360xbf16> loc(#loc6656)
      %7142 = stablehlo.reduce(%7141 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc6657)
      %7143 = "stablehlo.all_reduce"(%7142) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.15942"), %arg1239: tensor<bf16> loc("reduce.15942")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6657)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6657)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6657)
      %7144 = stablehlo.add %7059, %7143 : tensor<1x128x360xbf16> loc(#loc6658)
      %7145 = stablehlo.convert %7144 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6659)
      %7146 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %7147 = stablehlo.power %7145, %7146 : tensor<1x128x360xf32> loc(#loc6660)
      %7148 = stablehlo.reduce(%7147 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6661)
      %7149 = "stablehlo.all_reduce"(%7148) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.15955"), %arg1239: tensor<f32> loc("reduce.15955")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6661)
        stablehlo.return %7360 : tensor<f32> loc(#loc6661)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6661)
      %7150 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %7151 = stablehlo.multiply %7149, %7150 : tensor<1x128xf32> loc(#loc6662)
      %7152 = stablehlo.reshape %7151 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6663)
      %7153 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %7154 = stablehlo.add %7152, %7153 : tensor<1x128x1xf32> loc(#loc6664)
      %7155 = stablehlo.rsqrt %7154 : tensor<1x128x1xf32> loc(#loc6665)
      %7156 = stablehlo.reshape %7155 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6666)
      %7157 = stablehlo.broadcast_in_dim %7156, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6667)
      %7158 = stablehlo.multiply %7145, %7157 : tensor<1x128x360xf32> loc(#loc6668)
      %7159 = stablehlo.multiply %7020, %7158 : tensor<1x128x360xf32> loc(#loc6669)
      %7160 = stablehlo.convert %7159 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6670)
      %7161 = stablehlo.reshape %7160 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6671)
      %7162 = stablehlo.reshape %arg1231 : (tensor<1024x360xbf16>) -> tensor<1x1024x360xbf16> loc(#loc6672)
      %7163 = stablehlo.reshape %7162 : (tensor<1x1024x360xbf16>) -> tensor<1024x360xbf16> loc(#loc6673)
      %7164 = stablehlo.transpose %7163, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,4096]{0,1}"} : (tensor<1024x360xbf16>) -> tensor<360x1024xbf16> loc(#loc6674)
      %7165 = stablehlo.dot_general %7161, %7164, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6675)
      %7166 = "stablehlo.all_reduce"(%7165) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.16069"), %arg1239: tensor<bf16> loc("dot.16069")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6675)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6675)
      }) : (tensor<128x1024xbf16>) -> tensor<128x1024xbf16> loc(#loc6675)
      %7167 = stablehlo.reshape %7166 : (tensor<128x1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6676)
      %7168 = stablehlo.reshape %arg1230 : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc6677)
      %7169 = stablehlo.reshape %7168 : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16> loc(#loc6678)
      %7170 = stablehlo.broadcast_in_dim %7169, dims = [2] : (tensor<1024xbf16>) -> tensor<1x128x1024xbf16> loc(#loc6679)
      %7171 = stablehlo.add %7167, %7170 : tensor<1x128x1024xbf16> loc(#loc6680)
      %7172 = stablehlo.reshape %7171 : (tensor<1x128x1024xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6681)
      %7173 = stablehlo.transpose %7172, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,64]{3,1,2,0}"} : (tensor<1x128x16x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6682)
      %7174 = stablehlo.slice %7173 [0:1, 0:16, 0:128, 0:32] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6683)
      %7175 = stablehlo.multiply %7174, %64 : tensor<1x16x128x32xbf16> loc(#loc6684)
      %7176 = stablehlo.slice %7173 [0:1, 0:16, 0:128, 32:64] : (tensor<1x16x128x64xbf16>) -> tensor<1x16x128x32xbf16> loc(#loc6685)
      %7177 = stablehlo.multiply %7176, %70 : tensor<1x16x128x32xbf16> loc(#loc6686)
      %7178 = stablehlo.subtract %7175, %7177 : tensor<1x16x128x32xbf16> loc(#loc6687)
      %7179 = stablehlo.multiply %7176, %64 : tensor<1x16x128x32xbf16> loc(#loc6688)
      %7180 = stablehlo.multiply %7174, %70 : tensor<1x16x128x32xbf16> loc(#loc6689)
      %7181 = stablehlo.add %7179, %7180 : tensor<1x16x128x32xbf16> loc(#loc6690)
      %7182 = stablehlo.concatenate %7178, %7181, dim = 3 : (tensor<1x16x128x32xbf16>, tensor<1x16x128x32xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6691)
      %7183 = stablehlo.reshape %arg1229 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6692)
      %7184 = stablehlo.reshape %7183 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6693)
      %7185 = stablehlo.transpose %7184, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6694)
      %7186 = stablehlo.dot_general %7161, %7185, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6695)
      %7187 = "stablehlo.all_reduce"(%7186) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.16020"), %arg1239: tensor<bf16> loc("dot.16020")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6695)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6695)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6695)
      %7188 = stablehlo.reshape %7187 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6696)
      %7189 = stablehlo.reshape %arg1228 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6697)
      %7190 = stablehlo.reshape %7189 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6698)
      %7191 = stablehlo.broadcast_in_dim %7190, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6699)
      %7192 = stablehlo.add %7188, %7191 : tensor<1x128x128xbf16> loc(#loc6700)
      %7193 = stablehlo.reshape %7192 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6701)
      %7194 = stablehlo.transpose %7193, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6702)
      %7195 = stablehlo.slice %7194 [0:1, 0:2, 0:128, 0:32] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6703)
      %7196 = stablehlo.multiply %7195, %90 : tensor<1x2x128x32xbf16> loc(#loc6704)
      %7197 = stablehlo.slice %7194 [0:1, 0:2, 0:128, 32:64] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x128x32xbf16> loc(#loc6705)
      %7198 = stablehlo.multiply %7197, %93 : tensor<1x2x128x32xbf16> loc(#loc6706)
      %7199 = stablehlo.subtract %7196, %7198 : tensor<1x2x128x32xbf16> loc(#loc6707)
      %7200 = stablehlo.multiply %7197, %90 : tensor<1x2x128x32xbf16> loc(#loc6708)
      %7201 = stablehlo.multiply %7195, %93 : tensor<1x2x128x32xbf16> loc(#loc6709)
      %7202 = stablehlo.add %7200, %7201 : tensor<1x2x128x32xbf16> loc(#loc6710)
      %7203 = stablehlo.concatenate %7199, %7202, dim = 3 : (tensor<1x2x128x32xbf16>, tensor<1x2x128x32xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6711)
      %7204 = stablehlo.broadcast_in_dim %7203, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6712)
      %7205 = stablehlo.reshape %7204 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6713)
      %7206 = stablehlo.transpose %7205, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,64,128]{2,3,1,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x16x64x128xbf16> loc(#loc6714)
      %7207 = stablehlo.dot_general %7182, %7206, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x64xbf16>, tensor<1x16x64x128xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6715)
      %7208 = stablehlo.multiply %7207, %9 : tensor<1x16x128x128xbf16> loc(#loc6716)
      %7209 = stablehlo.add %7208, %341 : tensor<1x16x128x128xbf16> loc(#loc6717)
      %7210 = stablehlo.reshape %arg1227 : (tensor<64xbf16>) -> tensor<4x16xbf16> loc(#loc609)
      %7211 = "stablehlo.all_to_all"(%7210) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x16xbf16>) -> tensor<4x16xbf16> loc(#loc609)
      %7212 = stablehlo.slice %7211 [0:1, 0:16] : (tensor<4x16xbf16>) -> tensor<1x16xbf16> loc(#loc609)
      %7213 = stablehlo.reshape %7212 : (tensor<1x16xbf16>) -> tensor<16xbf16> loc(#loc609)
      %7214 = stablehlo.reshape %7213 : (tensor<16xbf16>) -> tensor<1x1x16xbf16> loc(#loc6718)
      %7215 = stablehlo.reshape %7214 : (tensor<1x1x16xbf16>) -> tensor<1x16x1xbf16> loc(#loc6719)
      %7216 = stablehlo.broadcast_in_dim %7215, dims = [0, 1, 3] : (tensor<1x16x1xbf16>) -> tensor<1x16x128x1xbf16> loc(#loc6720)
      %7217 = stablehlo.concatenate %7209, %7216, dim = 3 : (tensor<1x16x128x128xbf16>, tensor<1x16x128x1xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6721)
      %7218 = stablehlo.reshape %arg1237 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6722)
      %7219 = stablehlo.reshape %7218 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6723)
      %7220 = stablehlo.convert %7219 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6724)
      %7221 = stablehlo.broadcast_in_dim %7220, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6725)
      %7222 = stablehlo.reduce(%7217 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6726)
      %7223 = stablehlo.broadcast_in_dim %7222, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6727)
      %7224 = stablehlo.subtract %7217, %7223 : tensor<1x16x128x129xbf16> loc(#loc6728)
      %7225 = stablehlo.reduce(%7224 init: %cst_13) applies stablehlo.maximum across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6729)
      %7226 = stablehlo.broadcast_in_dim %7225, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6730)
      %7227 = stablehlo.subtract %7224, %7226 : tensor<1x16x128x129xbf16> loc(#loc6731)
      %7228 = stablehlo.exponential %7227 : tensor<1x16x128x129xbf16> loc(#loc6732)
      %7229 = stablehlo.reduce(%7228 init: %cst_15) applies stablehlo.add across dimensions = [3] : (tensor<1x16x128x129xbf16>, tensor<bf16>) -> tensor<1x16x128xbf16> loc(#loc6733)
      %7230 = stablehlo.broadcast_in_dim %7229, dims = [0, 1, 2] : (tensor<1x16x128xbf16>) -> tensor<1x16x128x129xbf16> loc(#loc6734)
      %7231 = stablehlo.divide %7228, %7230 : tensor<1x16x128x129xbf16> loc(#loc6735)
      %7232 = stablehlo.slice %7231 [0:1, 0:16, 0:128, 0:128] : (tensor<1x16x128x129xbf16>) -> tensor<1x16x128x128xbf16> loc(#loc6736)
      %7233 = stablehlo.reshape %arg625 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6737)
      %7234 = stablehlo.reshape %7233 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6738)
      %7235 = stablehlo.transpose %7234, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,512]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6739)
      %7236 = stablehlo.dot_general %7161, %7235, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6740)
      %7237 = "stablehlo.all_reduce"(%7236) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.15983"), %arg1239: tensor<bf16> loc("dot.15983")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6740)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6740)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6740)
      %7238 = stablehlo.reshape %7237 : (tensor<128x128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6741)
      %7239 = stablehlo.reshape %arg624 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6742)
      %7240 = stablehlo.reshape %7239 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6743)
      %7241 = stablehlo.broadcast_in_dim %7240, dims = [2] : (tensor<128xbf16>) -> tensor<1x128x128xbf16> loc(#loc6744)
      %7242 = stablehlo.add %7238, %7241 : tensor<1x128x128xbf16> loc(#loc6745)
      %7243 = stablehlo.reshape %7242 : (tensor<1x128x128xbf16>) -> tensor<1x128x2x64xbf16> loc(#loc6746)
      %7244 = stablehlo.transpose %7243, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,128,64]{3,1,2,0}"} : (tensor<1x128x2x64xbf16>) -> tensor<1x2x128x64xbf16> loc(#loc6747)
      %7245 = stablehlo.broadcast_in_dim %7244, dims = [0, 1, 3, 4] : (tensor<1x2x128x64xbf16>) -> tensor<1x2x8x128x64xbf16> loc(#loc6748)
      %7246 = stablehlo.reshape %7245 : (tensor<1x2x8x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6749)
      %7247 = stablehlo.dot_general %7232, %7246, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x16x128x128xbf16>, tensor<1x16x128x64xbf16>) -> tensor<1x16x128x64xbf16> loc(#loc6750)
      %7248 = stablehlo.transpose %7247, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,64,64]{3,1,2,0}"} : (tensor<1x16x128x64xbf16>) -> tensor<1x128x16x64xbf16> loc(#loc6751)
      %7249 = stablehlo.reshape %7248 : (tensor<1x128x16x64xbf16>) -> tensor<128x1024xbf16> loc(#loc6752)
      %7250 = stablehlo.reshape %arg623 : (tensor<360x1024xbf16>) -> tensor<1x360x1024xbf16> loc(#loc6753)
      %7251 = stablehlo.reshape %7250 : (tensor<1x360x1024xbf16>) -> tensor<360x1024xbf16> loc(#loc6754)
      %7252 = stablehlo.transpose %7251, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[4096,2880]{0,1}"} : (tensor<360x1024xbf16>) -> tensor<1024x360xbf16> loc(#loc6755)
      %7253 = stablehlo.dot_general %7249, %7252, contracting_dims = [1] x [0] : (tensor<128x1024xbf16>, tensor<1024x360xbf16>) -> tensor<128x360xbf16> loc(#loc6756)
      %7254 = "stablehlo.all_reduce"(%7253) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.16176"), %arg1239: tensor<bf16> loc("dot.16176")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6756)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6756)
      }) : (tensor<128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6756)
      %7255 = stablehlo.reshape %7254 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6757)
      %7256 = stablehlo.reshape %arg622 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6758)
      %7257 = stablehlo.reshape %7256 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6759)
      %7258 = stablehlo.broadcast_in_dim %7257, dims = [2] : (tensor<360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6760)
      %7259 = stablehlo.add %7255, %7258 : tensor<1x128x360xbf16> loc(#loc6761)
      %7260 = stablehlo.add %7144, %7259 : tensor<1x128x360xbf16> loc(#loc6762)
      %7261 = stablehlo.reshape %arg1232 : (tensor<360xbf16>) -> tensor<1x1x360xbf16> loc(#loc6763)
      %7262 = stablehlo.reshape %7261 : (tensor<1x1x360xbf16>) -> tensor<360xbf16> loc(#loc6764)
      %7263 = stablehlo.convert %7262 : (tensor<360xbf16>) -> tensor<360xf32> loc(#loc6765)
      %7264 = stablehlo.broadcast_in_dim %7263, dims = [2] : (tensor<360xf32>) -> tensor<1x128x360xf32> loc(#loc6766)
      %7265 = stablehlo.convert %7260 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6767)
      %7266 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x128x360xf32> loc(#loc)
      %7267 = stablehlo.power %7265, %7266 : tensor<1x128x360xf32> loc(#loc6768)
      %7268 = stablehlo.reduce(%7267 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6769)
      %7269 = "stablehlo.all_reduce"(%7268) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.16194"), %arg1239: tensor<f32> loc("reduce.16194")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6769)
        stablehlo.return %7360 : tensor<f32> loc(#loc6769)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6769)
      %7270 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x128xf32> loc(#loc)
      %7271 = stablehlo.multiply %7269, %7270 : tensor<1x128xf32> loc(#loc6770)
      %7272 = stablehlo.reshape %7271 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6771)
      %7273 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x128x1xf32> loc(#loc)
      %7274 = stablehlo.add %7272, %7273 : tensor<1x128x1xf32> loc(#loc6772)
      %7275 = stablehlo.rsqrt %7274 : tensor<1x128x1xf32> loc(#loc6773)
      %7276 = stablehlo.reshape %7275 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6774)
      %7277 = stablehlo.broadcast_in_dim %7276, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6775)
      %7278 = stablehlo.multiply %7265, %7277 : tensor<1x128x360xf32> loc(#loc6776)
      %7279 = stablehlo.multiply %7264, %7278 : tensor<1x128x360xf32> loc(#loc6777)
      %7280 = stablehlo.convert %7279 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6778)
      %7281 = stablehlo.reshape %7280 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6779)
      %7282 = stablehlo.broadcast_in_dim %7281, dims = [1, 2] : (tensor<128x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6780)
      %7283 = stablehlo.dot_general %7282, %arg1236, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x360xbf16>, tensor<32x360x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6781)
      %7284 = "stablehlo.all_reduce"(%7283) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.16306"), %arg1239: tensor<bf16> loc("dot.16306")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6781)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6781)
      }) : (tensor<32x128x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6781)
      %7285 = stablehlo.reshape %arg1235 : (tensor<32x5760xbf16>) -> tensor<1x32x5760xbf16> loc(#loc6782)
      %7286 = stablehlo.reshape %7285 : (tensor<1x32x5760xbf16>) -> tensor<32x5760xbf16> loc(#loc6783)
      %7287 = stablehlo.broadcast_in_dim %7286, dims = [0, 2] : (tensor<32x5760xbf16>) -> tensor<32x128x5760xbf16> loc(#loc6784)
      %7288 = stablehlo.add %7284, %7287 : tensor<32x128x5760xbf16> loc(#loc6785)
      %7289 = stablehlo.slice %7288 [0:32, 0:128, 1:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6786)
      %7290 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<bf16>) -> tensor<32x128x2880xbf16> loc(#loc)
      %7291 = stablehlo.clamp %6, %7289, %7290 : tensor<32x128x2880xbf16> loc(#loc6787)
      %7292 = stablehlo.add %7291, %4 : tensor<32x128x2880xbf16> loc(#loc6788)
      %7293 = stablehlo.slice %7288 [0:32, 0:128, 0:5760:2] : (tensor<32x128x5760xbf16>) -> tensor<32x128x2880xbf16> loc(#loc6789)
      %7294 = stablehlo.clamp %3, %7293, %5 : tensor<32x128x2880xbf16> loc(#loc6790)
      %7295 = stablehlo.multiply %7294, %2 : tensor<32x128x2880xbf16> loc(#loc6791)
      %7296 = stablehlo.logistic %7295 : tensor<32x128x2880xbf16> loc(#loc6792)
      %7297 = stablehlo.multiply %7294, %7296 : tensor<32x128x2880xbf16> loc(#loc6793)
      %7298 = stablehlo.multiply %7292, %7297 : tensor<32x128x2880xbf16> loc(#loc6794)
      %7299 = stablehlo.dot_general %7298, %arg1234, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<32x128x2880xbf16>, tensor<32x2880x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6795)
      %7300 = stablehlo.reshape %arg1233 : (tensor<32x360xbf16>) -> tensor<1x32x360xbf16> loc(#loc6796)
      %7301 = stablehlo.reshape %7300 : (tensor<1x32x360xbf16>) -> tensor<32x360xbf16> loc(#loc6797)
      %7302 = stablehlo.broadcast_in_dim %7301, dims = [0, 2] : (tensor<32x360xbf16>) -> tensor<32x128x360xbf16> loc(#loc6798)
      %7303 = stablehlo.add %7299, %7302 : tensor<32x128x360xbf16> loc(#loc6799)
      %7304 = stablehlo.reshape %7303 : (tensor<32x128x360xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6800)
      %7305 = stablehlo.reshape %arg621 : (tensor<128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6801)
      %7306 = stablehlo.reshape %7305 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6802)
      %7307 = stablehlo.transpose %7306, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,128]{0,1}"} : (tensor<128x360xbf16>) -> tensor<360x128xbf16> loc(#loc6803)
      %7308 = stablehlo.dot_general %7281, %7307, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x128xbf16>) -> tensor<128x128xbf16> loc(#loc6804)
      %7309 = "stablehlo.all_reduce"(%7308) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.16222"), %arg1239: tensor<bf16> loc("dot.16222")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6804)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6804)
      }) : (tensor<128x128xbf16>) -> tensor<128x128xbf16> loc(#loc6804)
      %7310 = stablehlo.reshape %arg620 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6805)
      %7311 = stablehlo.reshape %7310 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6806)
      %7312 = stablehlo.broadcast_in_dim %7311, dims = [1] : (tensor<128xbf16>) -> tensor<128x128xbf16> loc(#loc6807)
      %7313 = stablehlo.add %7309, %7312 : tensor<128x128xbf16> loc(#loc6808)
      %7314:2 = "stablehlo.sort"(%7313, %242) <{dimension = 1 : i64}> ({
      ^bb0(%arg1238: tensor<bf16> loc("sort.16242"), %arg1239: tensor<bf16> loc("sort.16242"), %arg1240: tensor<i32> loc("sort.16242"), %arg1241: tensor<i32> loc("sort.16242")):
        %7360 = stablehlo.compare  GT, %arg1238, %arg1239,  TOTALORDER : (tensor<bf16>, tensor<bf16>) -> tensor<i1> loc(#loc6810)
        stablehlo.return %7360 : tensor<i1> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x128xi32>) -> (tensor<128x128xbf16>, tensor<128x128xi32>) loc(#loc6809)
      %7315 = stablehlo.slice %7314#1 [0:128, 0:4] : (tensor<128x128xi32>) -> tensor<128x4xi32> loc(#loc6811)
      %7316 = stablehlo.convert %7315 : (tensor<128x4xi32>) -> tensor<128x4xi64> loc(#loc6812)
      %7317 = stablehlo.reshape %7316 : (tensor<128x4xi64>) -> tensor<128x4x1xi64> loc(#loc6813)
      %7318 = stablehlo.concatenate %231, %7317, dim = 2 : (tensor<128x4x1xi64>, tensor<128x4x1xi64>) -> tensor<128x4x2xi64> loc(#loc6814)
      %7319 = stablehlo.slice %7314#0 [0:128, 0:4] : (tensor<128x128xbf16>) -> tensor<128x4xbf16> loc(#loc6815)
      %7320 = stablehlo.reduce(%7319 init: %cst_13) applies stablehlo.maximum across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6816)
      %7321 = stablehlo.broadcast_in_dim %7320, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6817)
      %7322 = stablehlo.subtract %7319, %7321 : tensor<128x4xbf16> loc(#loc6818)
      %7323 = stablehlo.exponential %7322 : tensor<128x4xbf16> loc(#loc6819)
      %7324 = stablehlo.reduce(%7323 init: %cst_15) applies stablehlo.add across dimensions = [1] : (tensor<128x4xbf16>, tensor<bf16>) -> tensor<128xbf16> loc(#loc6820)
      %7325 = stablehlo.broadcast_in_dim %7324, dims = [0] : (tensor<128xbf16>) -> tensor<128x4xbf16> loc(#loc6821)
      %7326 = stablehlo.divide %7323, %7325 : tensor<128x4xbf16> loc(#loc6822)
      %7327 = "stablehlo.all_gather"(%1) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> : (tensor<128x32xbf16>) -> tensor<128x128xbf16> loc(#loc)
      %7328 = "stablehlo.scatter"(%7327, %7318, %7326) <{scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 2>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("scatter.16276"), %arg1239: tensor<bf16> loc("scatter.16276")):
        stablehlo.return %arg1239 : tensor<bf16> loc(#loc)
      }) : (tensor<128x128xbf16>, tensor<128x4x2xi64>, tensor<128x4xbf16>) -> tensor<128x128xbf16> loc(#loc6823)
      %7329 = stablehlo.reshape %7328 : (tensor<128x128xbf16>) -> tensor<128x4x32xbf16> loc(#loc6823)
      %7330 = "stablehlo.all_to_all"(%7329) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 1 : i64}> : (tensor<128x4x32xbf16>) -> tensor<128x4x32xbf16> loc(#loc6823)
      %7331 = stablehlo.slice %7330 [0:128, 0:1, 0:32] : (tensor<128x4x32xbf16>) -> tensor<128x1x32xbf16> loc(#loc6823)
      %7332 = stablehlo.reshape %7331 : (tensor<128x1x32xbf16>) -> tensor<128x32xbf16> loc(#loc6823)
      %7333 = stablehlo.transpose %7332, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[128,128]{0,1}"} : (tensor<128x32xbf16>) -> tensor<32x128xbf16> loc(#loc6824)
      %7334 = stablehlo.reshape %7333 : (tensor<32x128xbf16>) -> tensor<32x1x128xbf16> loc(#loc6825)
      %7335 = stablehlo.broadcast_in_dim %7334, dims = [0, 1, 2] : (tensor<32x1x128xbf16>) -> tensor<32x1x128x360xbf16> loc(#loc6826)
      %7336 = stablehlo.multiply %7304, %7335 : tensor<32x1x128x360xbf16> loc(#loc6827)
      %7337 = stablehlo.reduce(%7336 init: %cst_15) applies stablehlo.add across dimensions = [0] : (tensor<32x1x128x360xbf16>, tensor<bf16>) -> tensor<1x128x360xbf16> loc(#loc6828)
      %7338 = "stablehlo.all_reduce"(%7337) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("reduce.16348"), %arg1239: tensor<bf16> loc("reduce.16348")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6828)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6828)
      }) : (tensor<1x128x360xbf16>) -> tensor<1x128x360xbf16> loc(#loc6828)
      %7339 = stablehlo.add %7260, %7338 : tensor<1x128x360xbf16> loc(#loc6829)
      %7340 = stablehlo.convert %7339 : (tensor<1x128x360xbf16>) -> tensor<1x128x360xf32> loc(#loc6830)
      %7341 = stablehlo.power %7340, %13 : tensor<1x128x360xf32> loc(#loc6831)
      %7342 = stablehlo.reduce(%7341 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x128x360xf32>, tensor<f32>) -> tensor<1x128xf32> loc(#loc6832)
      %7343 = "stablehlo.all_reduce"(%7342) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<f32> loc("reduce.16361"), %arg1239: tensor<f32> loc("reduce.16361")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<f32> loc(#loc6832)
        stablehlo.return %7360 : tensor<f32> loc(#loc6832)
      }) : (tensor<1x128xf32>) -> tensor<1x128xf32> loc(#loc6832)
      %7344 = stablehlo.multiply %7343, %12 : tensor<1x128xf32> loc(#loc6833)
      %7345 = stablehlo.reshape %7344 : (tensor<1x128xf32>) -> tensor<1x128x1xf32> loc(#loc6834)
      %7346 = stablehlo.add %7345, %11 : tensor<1x128x1xf32> loc(#loc6835)
      %7347 = stablehlo.rsqrt %7346 : tensor<1x128x1xf32> loc(#loc6836)
      %7348 = stablehlo.reshape %7347 : (tensor<1x128x1xf32>) -> tensor<1x128xf32> loc(#loc6837)
      %7349 = stablehlo.broadcast_in_dim %7348, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<1x128x360xf32> loc(#loc6838)
      %7350 = stablehlo.multiply %7340, %7349 : tensor<1x128x360xf32> loc(#loc6839)
      %7351 = stablehlo.multiply %7221, %7350 : tensor<1x128x360xf32> loc(#loc6840)
      %7352 = stablehlo.convert %7351 : (tensor<1x128x360xf32>) -> tensor<1x128x360xbf16> loc(#loc6841)
      %7353 = stablehlo.reshape %7352 : (tensor<1x128x360xbf16>) -> tensor<128x360xbf16> loc(#loc6842)
      %7354 = stablehlo.reshape %arg619 : (tensor<50272x360xbf16>) -> tensor<1x50272x360xbf16> loc(#loc6843)
      %7355 = stablehlo.reshape %7354 : (tensor<1x50272x360xbf16>) -> tensor<50272x360xbf16> loc(#loc6844)
      %7356 = stablehlo.transpose %7355, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[2880,201088]{0,1}"} : (tensor<50272x360xbf16>) -> tensor<360x50272xbf16> loc(#loc6845)
      %7357 = stablehlo.dot_general %7353, %7356, contracting_dims = [1] x [0] : (tensor<128x360xbf16>, tensor<360x50272xbf16>) -> tensor<128x50272xbf16> loc(#loc6846)
      %7358 = "stablehlo.all_reduce"(%7357) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg1238: tensor<bf16> loc("dot.16389"), %arg1239: tensor<bf16> loc("dot.16389")):
        %7360 = stablehlo.add %arg1238, %arg1239 : tensor<bf16> loc(#loc6846)
        stablehlo.return %7360 : tensor<bf16> loc(#loc6846)
      }) : (tensor<128x50272xbf16>) -> tensor<128x50272xbf16> loc(#loc6846)
      %7359 = stablehlo.reshape %7358 : (tensor<128x50272xbf16>) -> tensor<1x128x50272xbf16> loc(#loc6847)
      sdy.return %7359 : tensor<1x128x50272xbf16> loc(#loc)
    } : (tensor<201088x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<128xbf16>, tensor<128x2880xbf16>, tensor<2880xbf16>, tensor<2880x4096xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<1x128xi64>, tensor<201088x2880xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<1x128xi64>, tensor<i1>, tensor<32xf32>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>, tensor<64xbf16>, tensor<512xbf16>, tensor<512x2880xbf16>, tensor<4096xbf16>, tensor<4096x2880xbf16>, tensor<2880xbf16>, tensor<128x2880xbf16>, tensor<128x2880x2880xbf16>, tensor<128x5760xbf16>, tensor<128x2880x5760xbf16>, tensor<2880xbf16>) -> tensor<1x128x201088xbf16> loc(#loc)
    return %0 : tensor<1x128x201088xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc620 = loc("reshape.1562")
#loc621 = loc("reshape.1564")
#loc622 = loc("convert.1565")
#loc623 = loc("broadcast.1566")
#loc624 = loc("reshape.1527")
#loc625 = loc("reshape.1529")
#loc626 = loc("reshape.1522")
#loc627 = loc("reshape.1525")
#loc628 = loc("convert.1530")
#loc629 = loc("gather.1531")
#loc630 = loc("reshape.1532")
#loc631 = loc("convert.1533")
#loc632 = loc("power.1535")
#loc634 = loc("multiply.1551")
#loc635 = loc("reshape.1552")
#loc636 = loc("add.1556")
#loc637 = loc("rsqrt.1557")
#loc638 = loc("reshape.1558")
#loc639 = loc("broadcast.1559")
#loc640 = loc("multiply.1560")
#loc641 = loc("multiply.1567")
#loc642 = loc("convert.1568")
#loc643 = loc("reshape.1783")
#loc644 = loc("reshape.1779")
#loc645 = loc("reshape.1781")
#loc646 = loc("transpose.1782")
#loc648 = loc("reshape.1785")
#loc649 = loc("reshape.1775")
#loc650 = loc("reshape.1777")
#loc651 = loc("broadcast.1788")
#loc652 = loc("add.1789")
#loc653 = loc("reshape.1790")
#loc654 = loc("transpose.1791")
#loc655 = loc("slice.1792")
#loc656 = loc("reshape.1705")
#loc657 = loc("reshape.1709")
#loc658 = loc("dot.1712")
#loc659 = loc("transpose.1713")
#loc660 = loc("cosine.1743")
#loc661 = loc("multiply.1745")
#loc662 = loc("convert.1746")
#loc663 = loc("broadcast.1808")
#loc664 = loc("multiply.1809")
#loc665 = loc("slice.1796")
#loc666 = loc("sine.1714")
#loc667 = loc("multiply.1716")
#loc668 = loc("convert.1717")
#loc669 = loc("broadcast.1805")
#loc670 = loc("multiply.1806")
#loc671 = loc("subtract.1812")
#loc672 = loc("multiply.1799")
#loc673 = loc("multiply.1795")
#loc674 = loc("add.1802")
#loc675 = loc("concatenate.1813")
#loc676 = loc("reshape.1725")
#loc677 = loc("reshape.1727")
#loc678 = loc("transpose.1728")
#loc680 = loc("reshape.1731")
#loc681 = loc("reshape.1721")
#loc682 = loc("reshape.1723")
#loc683 = loc("broadcast.1734")
#loc684 = loc("add.1735")
#loc685 = loc("reshape.1736")
#loc686 = loc("transpose.1737")
#loc687 = loc("slice.1738")
#loc688 = loc("broadcast.1760")
#loc689 = loc("multiply.1761")
#loc690 = loc("slice.1748")
#loc691 = loc("broadcast.1757")
#loc692 = loc("multiply.1758")
#loc693 = loc("subtract.1764")
#loc694 = loc("multiply.1751")
#loc695 = loc("multiply.1741")
#loc696 = loc("add.1754")
#loc697 = loc("concatenate.1765")
#loc698 = loc("broadcast.1769")
#loc699 = loc("reshape.1770")
#loc700 = loc("transpose.1771")
#loc701 = loc("dot.1814")
#loc702 = loc("multiply.1817")
#loc703 = loc("broadcast.1685")
#loc704 = loc("broadcast.1659")
#loc705 = loc("broadcast.1661")
#loc706 = loc("compare.1662")
#loc707 = loc("and.1673")
#loc708 = loc("broadcast.1651")
#loc709 = loc("compare.1652")
#loc710 = loc("and.1676")
#loc711 = loc("and.1686")
#loc712 = loc("broadcast.1689")
#loc713 = loc("reshape.1641")
#loc714 = loc("reshape.1643")
#loc715 = loc("convert.1644")
#loc716 = loc("gather.1645")
#loc717 = loc("broadcast.1691")
#loc718 = loc("and.1692")
#loc719 = loc("reshape.1693")
#loc720 = loc("select.1696")
#loc721 = loc("reshape.1820")
#loc722 = loc("broadcast.1821")
#loc723 = loc("add.1822")
#loc724 = loc("reshape.1585")
#loc725 = loc("reshape.1590")
#loc726 = loc("broadcast.1591")
#loc727 = loc("concatenate.1823")
#loc728 = loc("reshape.2096")
#loc729 = loc("reshape.2098")
#loc730 = loc("convert.2099")
#loc731 = loc("broadcast.2100")
#loc732 = loc("reduce.1830")
#loc733 = loc("broadcast.1866")
#loc734 = loc("subtract.1867")
#loc735 = loc("reduce.1873")
#loc736 = loc("broadcast.1874")
#loc737 = loc("subtract.1875")
#loc738 = loc("exponential.1876")
#loc739 = loc("reduce.1882")
#loc740 = loc("broadcast.1883")
#loc741 = loc("divide.1884")
#loc742 = loc("slice.1885")
#loc743 = loc("reshape.1514")
#loc744 = loc("reshape.1516")
#loc745 = loc("transpose.1517")
#loc747 = loc("reshape.1571")
#loc748 = loc("reshape.1510")
#loc749 = loc("reshape.1512")
#loc750 = loc("broadcast.1574")
#loc751 = loc("add.1575")
#loc752 = loc("reshape.1576")
#loc753 = loc("transpose.1577")
#loc754 = loc("broadcast.1581")
#loc755 = loc("reshape.1582")
#loc756 = loc("dot.1886")
#loc757 = loc("transpose.1888")
#loc758 = loc("reshape.1890")
#loc759 = loc("reshape.1504")
#loc760 = loc("reshape.1506")
#loc761 = loc("transpose.1507")
#loc763 = loc("reshape.1892")
#loc764 = loc("reshape.1500")
#loc765 = loc("reshape.1502")
#loc766 = loc("broadcast.1895")
#loc767 = loc("add.1896")
#loc768 = loc("add.1899")
#loc769 = loc("reshape.1929")
#loc770 = loc("reshape.1931")
#loc771 = loc("convert.1932")
#loc772 = loc("broadcast.1933")
#loc773 = loc("convert.1900")
#loc774 = loc("power.1902")
#loc776 = loc("multiply.1918")
#loc777 = loc("reshape.1919")
#loc778 = loc("add.1923")
#loc779 = loc("rsqrt.1924")
#loc780 = loc("reshape.1925")
#loc781 = loc("broadcast.1926")
#loc782 = loc("multiply.1927")
#loc783 = loc("multiply.1934")
#loc784 = loc("convert.1935")
#loc785 = loc("reshape.2014")
#loc786 = loc("broadcast.2018")
#loc788 = loc("reshape.2008")
#loc789 = loc("reshape.2010")
#loc790 = loc("broadcast.2025")
#loc791 = loc("add.2026")
#loc792 = loc("slice.2039")
#loc793 = loc("clamp.2042")
#loc794 = loc("add.2045")
#loc795 = loc("slice.2027")
#loc796 = loc("clamp.2030")
#loc797 = loc("multiply.2032")
#loc798 = loc("logistic.2033")
#loc799 = loc("multiply.2034")
#loc800 = loc("multiply.2046")
#loc801 = loc("dot.2047")
#loc802 = loc("reshape.1997")
#loc803 = loc("reshape.1999")
#loc804 = loc("broadcast.2051")
#loc805 = loc("add.2052")
#loc806 = loc("reshape.2053")
#loc807 = loc("iota.1985")
#loc808 = loc("reshape.1490")
#loc809 = loc("reshape.1492")
#loc810 = loc("transpose.1493")
#loc812 = loc("reshape.1483")
#loc813 = loc("reshape.1485")
#loc814 = loc("broadcast.1942")
#loc815 = loc("add.1943")
#loc816 = loc("iota.1944")
#loc818 = loc("compare.1956")
#loc819 = loc("slice.1961")
#loc820 = loc("convert.1962")
#loc821 = loc("reshape.1986")
#loc822 = loc("concatenate.1987")
#loc823 = loc("slice.1959")
#loc824 = loc("reduce.1968")
#loc825 = loc("broadcast.1969")
#loc826 = loc("subtract.1970")
#loc827 = loc("exponential.1971")
#loc828 = loc("reduce.1977")
#loc829 = loc("broadcast.1978")
#loc830 = loc("divide.1979")
#loc832 = loc("transpose.1992")
#loc833 = loc("reshape.1993")
#loc834 = loc("broadcast.2055")
#loc835 = loc("multiply.2056")
#loc837 = loc("add.2066")
#loc838 = loc("convert.2067")
#loc839 = loc("power.2069")
#loc841 = loc("multiply.2085")
#loc842 = loc("reshape.2086")
#loc843 = loc("add.2090")
#loc844 = loc("rsqrt.2091")
#loc845 = loc("reshape.2092")
#loc846 = loc("broadcast.2093")
#loc847 = loc("multiply.2094")
#loc848 = loc("multiply.2101")
#loc849 = loc("convert.2102")
#loc850 = loc("reshape.2264")
#loc851 = loc("reshape.2260")
#loc852 = loc("reshape.2262")
#loc853 = loc("transpose.2263")
#loc855 = loc("reshape.2266")
#loc856 = loc("reshape.2256")
#loc857 = loc("reshape.2258")
#loc858 = loc("broadcast.2269")
#loc859 = loc("add.2270")
#loc860 = loc("reshape.2271")
#loc861 = loc("transpose.2272")
#loc862 = loc("slice.2273")
#loc863 = loc("multiply.2290")
#loc864 = loc("slice.2277")
#loc865 = loc("multiply.2287")
#loc866 = loc("subtract.2293")
#loc867 = loc("multiply.2280")
#loc868 = loc("multiply.2276")
#loc869 = loc("add.2283")
#loc870 = loc("concatenate.2294")
#loc871 = loc("reshape.2211")
#loc872 = loc("reshape.2213")
#loc873 = loc("transpose.2214")
#loc875 = loc("reshape.2217")
#loc876 = loc("reshape.2207")
#loc877 = loc("reshape.2209")
#loc878 = loc("broadcast.2220")
#loc879 = loc("add.2221")
#loc880 = loc("reshape.2222")
#loc881 = loc("transpose.2223")
#loc882 = loc("slice.2224")
#loc883 = loc("multiply.2242")
#loc884 = loc("slice.2229")
#loc885 = loc("multiply.2239")
#loc886 = loc("subtract.2245")
#loc887 = loc("multiply.2232")
#loc888 = loc("multiply.2227")
#loc889 = loc("add.2235")
#loc890 = loc("concatenate.2246")
#loc891 = loc("broadcast.2250")
#loc892 = loc("reshape.2251")
#loc893 = loc("transpose.2252")
#loc894 = loc("dot.2295")
#loc895 = loc("multiply.2298")
#loc896 = loc("and.2191")
#loc897 = loc("broadcast.2194")
#loc898 = loc("and.2197")
#loc899 = loc("reshape.2198")
#loc900 = loc("select.2201")
#loc901 = loc("reshape.2301")
#loc902 = loc("broadcast.2302")
#loc903 = loc("add.2303")
#loc904 = loc("reshape.2119")
#loc905 = loc("reshape.2124")
#loc906 = loc("broadcast.2125")
#loc907 = loc("concatenate.2304")
#loc908 = loc("reshape.2577")
#loc909 = loc("reshape.2579")
#loc910 = loc("convert.2580")
#loc911 = loc("broadcast.2581")
#loc912 = loc("reduce.2311")
#loc913 = loc("broadcast.2347")
#loc914 = loc("subtract.2348")
#loc915 = loc("reduce.2354")
#loc916 = loc("broadcast.2355")
#loc917 = loc("subtract.2356")
#loc918 = loc("exponential.2357")
#loc919 = loc("reduce.2363")
#loc920 = loc("broadcast.2364")
#loc921 = loc("divide.2365")
#loc922 = loc("slice.2366")
#loc923 = loc("reshape.1472")
#loc924 = loc("reshape.1474")
#loc925 = loc("transpose.1475")
#loc927 = loc("reshape.2105")
#loc928 = loc("reshape.1468")
#loc929 = loc("reshape.1470")
#loc930 = loc("broadcast.2108")
#loc931 = loc("add.2109")
#loc932 = loc("reshape.2110")
#loc933 = loc("transpose.2111")
#loc934 = loc("broadcast.2115")
#loc935 = loc("reshape.2116")
#loc936 = loc("dot.2367")
#loc937 = loc("transpose.2369")
#loc938 = loc("reshape.2371")
#loc939 = loc("reshape.1462")
#loc940 = loc("reshape.1464")
#loc941 = loc("transpose.1465")
#loc943 = loc("reshape.2373")
#loc944 = loc("reshape.1458")
#loc945 = loc("reshape.1460")
#loc946 = loc("broadcast.2376")
#loc947 = loc("add.2377")
#loc948 = loc("add.2380")
#loc949 = loc("reshape.2410")
#loc950 = loc("reshape.2412")
#loc951 = loc("convert.2413")
#loc952 = loc("broadcast.2414")
#loc953 = loc("convert.2381")
#loc954 = loc("power.2383")
#loc956 = loc("multiply.2399")
#loc957 = loc("reshape.2400")
#loc958 = loc("add.2404")
#loc959 = loc("rsqrt.2405")
#loc960 = loc("reshape.2406")
#loc961 = loc("broadcast.2407")
#loc962 = loc("multiply.2408")
#loc963 = loc("multiply.2415")
#loc964 = loc("convert.2416")
#loc965 = loc("reshape.2495")
#loc966 = loc("broadcast.2499")
#loc968 = loc("reshape.2489")
#loc969 = loc("reshape.2491")
#loc970 = loc("broadcast.2506")
#loc971 = loc("add.2507")
#loc972 = loc("slice.2520")
#loc973 = loc("clamp.2523")
#loc974 = loc("add.2526")
#loc975 = loc("slice.2508")
#loc976 = loc("clamp.2511")
#loc977 = loc("multiply.2513")
#loc978 = loc("logistic.2514")
#loc979 = loc("multiply.2515")
#loc980 = loc("multiply.2527")
#loc981 = loc("dot.2528")
#loc982 = loc("reshape.2478")
#loc983 = loc("reshape.2480")
#loc984 = loc("broadcast.2532")
#loc985 = loc("add.2533")
#loc986 = loc("reshape.2534")
#loc987 = loc("reshape.1448")
#loc988 = loc("reshape.1450")
#loc989 = loc("transpose.1451")
#loc991 = loc("reshape.1441")
#loc992 = loc("reshape.1443")
#loc993 = loc("broadcast.2423")
#loc994 = loc("add.2424")
#loc996 = loc("compare.2437")
#loc997 = loc("slice.2442")
#loc998 = loc("convert.2443")
#loc999 = loc("reshape.2467")
#loc1000 = loc("concatenate.2468")
#loc1001 = loc("slice.2440")
#loc1002 = loc("reduce.2449")
#loc1003 = loc("broadcast.2450")
#loc1004 = loc("subtract.2451")
#loc1005 = loc("exponential.2452")
#loc1006 = loc("reduce.2458")
#loc1007 = loc("broadcast.2459")
#loc1008 = loc("divide.2460")
#loc1010 = loc("transpose.2473")
#loc1011 = loc("reshape.2474")
#loc1012 = loc("broadcast.2536")
#loc1013 = loc("multiply.2537")
#loc1015 = loc("add.2547")
#loc1016 = loc("convert.2548")
#loc1017 = loc("power.2550")
#loc1019 = loc("multiply.2566")
#loc1020 = loc("reshape.2567")
#loc1021 = loc("add.2571")
#loc1022 = loc("rsqrt.2572")
#loc1023 = loc("reshape.2573")
#loc1024 = loc("broadcast.2574")
#loc1025 = loc("multiply.2575")
#loc1026 = loc("multiply.2582")
#loc1027 = loc("convert.2583")
#loc1028 = loc("reshape.2670")
#loc1029 = loc("reshape.2666")
#loc1030 = loc("reshape.2668")
#loc1031 = loc("transpose.2669")
#loc1033 = loc("reshape.2672")
#loc1034 = loc("reshape.2662")
#loc1035 = loc("reshape.2664")
#loc1036 = loc("broadcast.2675")
#loc1037 = loc("add.2676")
#loc1038 = loc("reshape.2677")
#loc1039 = loc("transpose.2678")
#loc1040 = loc("slice.2679")
#loc1041 = loc("multiply.2696")
#loc1042 = loc("slice.2683")
#loc1043 = loc("multiply.2693")
#loc1044 = loc("subtract.2699")
#loc1045 = loc("multiply.2686")
#loc1046 = loc("multiply.2682")
#loc1047 = loc("add.2689")
#loc1048 = loc("concatenate.2700")
#loc1049 = loc("reshape.2617")
#loc1050 = loc("reshape.2619")
#loc1051 = loc("transpose.2620")
#loc1053 = loc("reshape.2623")
#loc1054 = loc("reshape.2613")
#loc1055 = loc("reshape.2615")
#loc1056 = loc("broadcast.2626")
#loc1057 = loc("add.2627")
#loc1058 = loc("reshape.2628")
#loc1059 = loc("transpose.2629")
#loc1060 = loc("slice.2630")
#loc1061 = loc("multiply.2648")
#loc1062 = loc("slice.2635")
#loc1063 = loc("multiply.2645")
#loc1064 = loc("subtract.2651")
#loc1065 = loc("multiply.2638")
#loc1066 = loc("multiply.2633")
#loc1067 = loc("add.2641")
#loc1068 = loc("concatenate.2652")
#loc1069 = loc("broadcast.2656")
#loc1070 = loc("reshape.2657")
#loc1071 = loc("transpose.2658")
#loc1072 = loc("dot.2701")
#loc1073 = loc("multiply.2704")
#loc1074 = loc("add.2709")
#loc1075 = loc("reshape.2600")
#loc1076 = loc("reshape.2605")
#loc1077 = loc("broadcast.2606")
#loc1078 = loc("concatenate.2710")
#loc1079 = loc("reshape.2983")
#loc1080 = loc("reshape.2985")
#loc1081 = loc("convert.2986")
#loc1082 = loc("broadcast.2987")
#loc1083 = loc("reduce.2717")
#loc1084 = loc("broadcast.2753")
#loc1085 = loc("subtract.2754")
#loc1086 = loc("reduce.2760")
#loc1087 = loc("broadcast.2761")
#loc1088 = loc("subtract.2762")
#loc1089 = loc("exponential.2763")
#loc1090 = loc("reduce.2769")
#loc1091 = loc("broadcast.2770")
#loc1092 = loc("divide.2771")
#loc1093 = loc("slice.2772")
#loc1094 = loc("reshape.1430")
#loc1095 = loc("reshape.1432")
#loc1096 = loc("transpose.1433")
#loc1098 = loc("reshape.2586")
#loc1099 = loc("reshape.1426")
#loc1100 = loc("reshape.1428")
#loc1101 = loc("broadcast.2589")
#loc1102 = loc("add.2590")
#loc1103 = loc("reshape.2591")
#loc1104 = loc("transpose.2592")
#loc1105 = loc("broadcast.2596")
#loc1106 = loc("reshape.2597")
#loc1107 = loc("dot.2773")
#loc1108 = loc("transpose.2775")
#loc1109 = loc("reshape.2777")
#loc1110 = loc("reshape.1420")
#loc1111 = loc("reshape.1422")
#loc1112 = loc("transpose.1423")
#loc1114 = loc("reshape.2779")
#loc1115 = loc("reshape.1416")
#loc1116 = loc("reshape.1418")
#loc1117 = loc("broadcast.2782")
#loc1118 = loc("add.2783")
#loc1119 = loc("add.2786")
#loc1120 = loc("reshape.2816")
#loc1121 = loc("reshape.2818")
#loc1122 = loc("convert.2819")
#loc1123 = loc("broadcast.2820")
#loc1124 = loc("convert.2787")
#loc1125 = loc("power.2789")
#loc1127 = loc("multiply.2805")
#loc1128 = loc("reshape.2806")
#loc1129 = loc("add.2810")
#loc1130 = loc("rsqrt.2811")
#loc1131 = loc("reshape.2812")
#loc1132 = loc("broadcast.2813")
#loc1133 = loc("multiply.2814")
#loc1134 = loc("multiply.2821")
#loc1135 = loc("convert.2822")
#loc1136 = loc("reshape.2901")
#loc1137 = loc("broadcast.2905")
#loc1139 = loc("reshape.2895")
#loc1140 = loc("reshape.2897")
#loc1141 = loc("broadcast.2912")
#loc1142 = loc("add.2913")
#loc1143 = loc("slice.2926")
#loc1144 = loc("clamp.2929")
#loc1145 = loc("add.2932")
#loc1146 = loc("slice.2914")
#loc1147 = loc("clamp.2917")
#loc1148 = loc("multiply.2919")
#loc1149 = loc("logistic.2920")
#loc1150 = loc("multiply.2921")
#loc1151 = loc("multiply.2933")
#loc1152 = loc("dot.2934")
#loc1153 = loc("reshape.2884")
#loc1154 = loc("reshape.2886")
#loc1155 = loc("broadcast.2938")
#loc1156 = loc("add.2939")
#loc1157 = loc("reshape.2940")
#loc1158 = loc("reshape.1406")
#loc1159 = loc("reshape.1408")
#loc1160 = loc("transpose.1409")
#loc1162 = loc("reshape.1399")
#loc1163 = loc("reshape.1401")
#loc1164 = loc("broadcast.2829")
#loc1165 = loc("add.2830")
#loc1167 = loc("compare.2843")
#loc1168 = loc("slice.2848")
#loc1169 = loc("convert.2849")
#loc1170 = loc("reshape.2873")
#loc1171 = loc("concatenate.2874")
#loc1172 = loc("slice.2846")
#loc1173 = loc("reduce.2855")
#loc1174 = loc("broadcast.2856")
#loc1175 = loc("subtract.2857")
#loc1176 = loc("exponential.2858")
#loc1177 = loc("reduce.2864")
#loc1178 = loc("broadcast.2865")
#loc1179 = loc("divide.2866")
#loc1181 = loc("transpose.2879")
#loc1182 = loc("reshape.2880")
#loc1183 = loc("broadcast.2942")
#loc1184 = loc("multiply.2943")
#loc1186 = loc("add.2953")
#loc1187 = loc("convert.2954")
#loc1188 = loc("power.2956")
#loc1190 = loc("multiply.2972")
#loc1191 = loc("reshape.2973")
#loc1192 = loc("add.2977")
#loc1193 = loc("rsqrt.2978")
#loc1194 = loc("reshape.2979")
#loc1195 = loc("broadcast.2980")
#loc1196 = loc("multiply.2981")
#loc1197 = loc("multiply.2988")
#loc1198 = loc("convert.2989")
#loc1199 = loc("reshape.3076")
#loc1200 = loc("reshape.3072")
#loc1201 = loc("reshape.3074")
#loc1202 = loc("transpose.3075")
#loc1204 = loc("reshape.3078")
#loc1205 = loc("reshape.3068")
#loc1206 = loc("reshape.3070")
#loc1207 = loc("broadcast.3081")
#loc1208 = loc("add.3082")
#loc1209 = loc("reshape.3083")
#loc1210 = loc("transpose.3084")
#loc1211 = loc("slice.3085")
#loc1212 = loc("multiply.3102")
#loc1213 = loc("slice.3089")
#loc1214 = loc("multiply.3099")
#loc1215 = loc("subtract.3105")
#loc1216 = loc("multiply.3092")
#loc1217 = loc("multiply.3088")
#loc1218 = loc("add.3095")
#loc1219 = loc("concatenate.3106")
#loc1220 = loc("reshape.3023")
#loc1221 = loc("reshape.3025")
#loc1222 = loc("transpose.3026")
#loc1224 = loc("reshape.3029")
#loc1225 = loc("reshape.3019")
#loc1226 = loc("reshape.3021")
#loc1227 = loc("broadcast.3032")
#loc1228 = loc("add.3033")
#loc1229 = loc("reshape.3034")
#loc1230 = loc("transpose.3035")
#loc1231 = loc("slice.3036")
#loc1232 = loc("multiply.3054")
#loc1233 = loc("slice.3041")
#loc1234 = loc("multiply.3051")
#loc1235 = loc("subtract.3057")
#loc1236 = loc("multiply.3044")
#loc1237 = loc("multiply.3039")
#loc1238 = loc("add.3047")
#loc1239 = loc("concatenate.3058")
#loc1240 = loc("broadcast.3062")
#loc1241 = loc("reshape.3063")
#loc1242 = loc("transpose.3064")
#loc1243 = loc("dot.3107")
#loc1244 = loc("multiply.3110")
#loc1245 = loc("add.3115")
#loc1246 = loc("reshape.3006")
#loc1247 = loc("reshape.3011")
#loc1248 = loc("broadcast.3012")
#loc1249 = loc("concatenate.3116")
#loc1250 = loc("reshape.3389")
#loc1251 = loc("reshape.3391")
#loc1252 = loc("convert.3392")
#loc1253 = loc("broadcast.3393")
#loc1254 = loc("reduce.3123")
#loc1255 = loc("broadcast.3159")
#loc1256 = loc("subtract.3160")
#loc1257 = loc("reduce.3166")
#loc1258 = loc("broadcast.3167")
#loc1259 = loc("subtract.3168")
#loc1260 = loc("exponential.3169")
#loc1261 = loc("reduce.3175")
#loc1262 = loc("broadcast.3176")
#loc1263 = loc("divide.3177")
#loc1264 = loc("slice.3178")
#loc1265 = loc("reshape.1388")
#loc1266 = loc("reshape.1390")
#loc1267 = loc("transpose.1391")
#loc1269 = loc("reshape.2992")
#loc1270 = loc("reshape.1384")
#loc1271 = loc("reshape.1386")
#loc1272 = loc("broadcast.2995")
#loc1273 = loc("add.2996")
#loc1274 = loc("reshape.2997")
#loc1275 = loc("transpose.2998")
#loc1276 = loc("broadcast.3002")
#loc1277 = loc("reshape.3003")
#loc1278 = loc("dot.3179")
#loc1279 = loc("transpose.3181")
#loc1280 = loc("reshape.3183")
#loc1281 = loc("reshape.1378")
#loc1282 = loc("reshape.1380")
#loc1283 = loc("transpose.1381")
#loc1285 = loc("reshape.3185")
#loc1286 = loc("reshape.1374")
#loc1287 = loc("reshape.1376")
#loc1288 = loc("broadcast.3188")
#loc1289 = loc("add.3189")
#loc1290 = loc("add.3192")
#loc1291 = loc("reshape.3222")
#loc1292 = loc("reshape.3224")
#loc1293 = loc("convert.3225")
#loc1294 = loc("broadcast.3226")
#loc1295 = loc("convert.3193")
#loc1296 = loc("power.3195")
#loc1298 = loc("multiply.3211")
#loc1299 = loc("reshape.3212")
#loc1300 = loc("add.3216")
#loc1301 = loc("rsqrt.3217")
#loc1302 = loc("reshape.3218")
#loc1303 = loc("broadcast.3219")
#loc1304 = loc("multiply.3220")
#loc1305 = loc("multiply.3227")
#loc1306 = loc("convert.3228")
#loc1307 = loc("reshape.3307")
#loc1308 = loc("broadcast.3311")
#loc1310 = loc("reshape.3301")
#loc1311 = loc("reshape.3303")
#loc1312 = loc("broadcast.3318")
#loc1313 = loc("add.3319")
#loc1314 = loc("slice.3332")
#loc1315 = loc("clamp.3335")
#loc1316 = loc("add.3338")
#loc1317 = loc("slice.3320")
#loc1318 = loc("clamp.3323")
#loc1319 = loc("multiply.3325")
#loc1320 = loc("logistic.3326")
#loc1321 = loc("multiply.3327")
#loc1322 = loc("multiply.3339")
#loc1323 = loc("dot.3340")
#loc1324 = loc("reshape.3290")
#loc1325 = loc("reshape.3292")
#loc1326 = loc("broadcast.3344")
#loc1327 = loc("add.3345")
#loc1328 = loc("reshape.3346")
#loc1329 = loc("reshape.1364")
#loc1330 = loc("reshape.1366")
#loc1331 = loc("transpose.1367")
#loc1333 = loc("reshape.1357")
#loc1334 = loc("reshape.1359")
#loc1335 = loc("broadcast.3235")
#loc1336 = loc("add.3236")
#loc1338 = loc("compare.3249")
#loc1339 = loc("slice.3254")
#loc1340 = loc("convert.3255")
#loc1341 = loc("reshape.3279")
#loc1342 = loc("concatenate.3280")
#loc1343 = loc("slice.3252")
#loc1344 = loc("reduce.3261")
#loc1345 = loc("broadcast.3262")
#loc1346 = loc("subtract.3263")
#loc1347 = loc("exponential.3264")
#loc1348 = loc("reduce.3270")
#loc1349 = loc("broadcast.3271")
#loc1350 = loc("divide.3272")
#loc1352 = loc("transpose.3285")
#loc1353 = loc("reshape.3286")
#loc1354 = loc("broadcast.3348")
#loc1355 = loc("multiply.3349")
#loc1357 = loc("add.3359")
#loc1358 = loc("convert.3360")
#loc1359 = loc("power.3362")
#loc1361 = loc("multiply.3378")
#loc1362 = loc("reshape.3379")
#loc1363 = loc("add.3383")
#loc1364 = loc("rsqrt.3384")
#loc1365 = loc("reshape.3385")
#loc1366 = loc("broadcast.3386")
#loc1367 = loc("multiply.3387")
#loc1368 = loc("multiply.3394")
#loc1369 = loc("convert.3395")
#loc1370 = loc("reshape.3482")
#loc1371 = loc("reshape.3478")
#loc1372 = loc("reshape.3480")
#loc1373 = loc("transpose.3481")
#loc1375 = loc("reshape.3484")
#loc1376 = loc("reshape.3474")
#loc1377 = loc("reshape.3476")
#loc1378 = loc("broadcast.3487")
#loc1379 = loc("add.3488")
#loc1380 = loc("reshape.3489")
#loc1381 = loc("transpose.3490")
#loc1382 = loc("slice.3491")
#loc1383 = loc("multiply.3508")
#loc1384 = loc("slice.3495")
#loc1385 = loc("multiply.3505")
#loc1386 = loc("subtract.3511")
#loc1387 = loc("multiply.3498")
#loc1388 = loc("multiply.3494")
#loc1389 = loc("add.3501")
#loc1390 = loc("concatenate.3512")
#loc1391 = loc("reshape.3429")
#loc1392 = loc("reshape.3431")
#loc1393 = loc("transpose.3432")
#loc1395 = loc("reshape.3435")
#loc1396 = loc("reshape.3425")
#loc1397 = loc("reshape.3427")
#loc1398 = loc("broadcast.3438")
#loc1399 = loc("add.3439")
#loc1400 = loc("reshape.3440")
#loc1401 = loc("transpose.3441")
#loc1402 = loc("slice.3442")
#loc1403 = loc("multiply.3460")
#loc1404 = loc("slice.3447")
#loc1405 = loc("multiply.3457")
#loc1406 = loc("subtract.3463")
#loc1407 = loc("multiply.3450")
#loc1408 = loc("multiply.3445")
#loc1409 = loc("add.3453")
#loc1410 = loc("concatenate.3464")
#loc1411 = loc("broadcast.3468")
#loc1412 = loc("reshape.3469")
#loc1413 = loc("transpose.3470")
#loc1414 = loc("dot.3513")
#loc1415 = loc("multiply.3516")
#loc1416 = loc("add.3521")
#loc1417 = loc("reshape.3412")
#loc1418 = loc("reshape.3417")
#loc1419 = loc("broadcast.3418")
#loc1420 = loc("concatenate.3522")
#loc1421 = loc("reshape.3795")
#loc1422 = loc("reshape.3797")
#loc1423 = loc("convert.3798")
#loc1424 = loc("broadcast.3799")
#loc1425 = loc("reduce.3529")
#loc1426 = loc("broadcast.3565")
#loc1427 = loc("subtract.3566")
#loc1428 = loc("reduce.3572")
#loc1429 = loc("broadcast.3573")
#loc1430 = loc("subtract.3574")
#loc1431 = loc("exponential.3575")
#loc1432 = loc("reduce.3581")
#loc1433 = loc("broadcast.3582")
#loc1434 = loc("divide.3583")
#loc1435 = loc("slice.3584")
#loc1436 = loc("reshape.1346")
#loc1437 = loc("reshape.1348")
#loc1438 = loc("transpose.1349")
#loc1440 = loc("reshape.3398")
#loc1441 = loc("reshape.1342")
#loc1442 = loc("reshape.1344")
#loc1443 = loc("broadcast.3401")
#loc1444 = loc("add.3402")
#loc1445 = loc("reshape.3403")
#loc1446 = loc("transpose.3404")
#loc1447 = loc("broadcast.3408")
#loc1448 = loc("reshape.3409")
#loc1449 = loc("dot.3585")
#loc1450 = loc("transpose.3587")
#loc1451 = loc("reshape.3589")
#loc1452 = loc("reshape.1336")
#loc1453 = loc("reshape.1338")
#loc1454 = loc("transpose.1339")
#loc1456 = loc("reshape.3591")
#loc1457 = loc("reshape.1332")
#loc1458 = loc("reshape.1334")
#loc1459 = loc("broadcast.3594")
#loc1460 = loc("add.3595")
#loc1461 = loc("add.3598")
#loc1462 = loc("reshape.3628")
#loc1463 = loc("reshape.3630")
#loc1464 = loc("convert.3631")
#loc1465 = loc("broadcast.3632")
#loc1466 = loc("convert.3599")
#loc1467 = loc("power.3601")
#loc1469 = loc("multiply.3617")
#loc1470 = loc("reshape.3618")
#loc1471 = loc("add.3622")
#loc1472 = loc("rsqrt.3623")
#loc1473 = loc("reshape.3624")
#loc1474 = loc("broadcast.3625")
#loc1475 = loc("multiply.3626")
#loc1476 = loc("multiply.3633")
#loc1477 = loc("convert.3634")
#loc1478 = loc("reshape.3713")
#loc1479 = loc("broadcast.3717")
#loc1481 = loc("reshape.3707")
#loc1482 = loc("reshape.3709")
#loc1483 = loc("broadcast.3724")
#loc1484 = loc("add.3725")
#loc1485 = loc("slice.3738")
#loc1486 = loc("clamp.3741")
#loc1487 = loc("add.3744")
#loc1488 = loc("slice.3726")
#loc1489 = loc("clamp.3729")
#loc1490 = loc("multiply.3731")
#loc1491 = loc("logistic.3732")
#loc1492 = loc("multiply.3733")
#loc1493 = loc("multiply.3745")
#loc1494 = loc("dot.3746")
#loc1495 = loc("reshape.3696")
#loc1496 = loc("reshape.3698")
#loc1497 = loc("broadcast.3750")
#loc1498 = loc("add.3751")
#loc1499 = loc("reshape.3752")
#loc1500 = loc("reshape.1322")
#loc1501 = loc("reshape.1324")
#loc1502 = loc("transpose.1325")
#loc1504 = loc("reshape.1315")
#loc1505 = loc("reshape.1317")
#loc1506 = loc("broadcast.3641")
#loc1507 = loc("add.3642")
#loc1509 = loc("compare.3655")
#loc1510 = loc("slice.3660")
#loc1511 = loc("convert.3661")
#loc1512 = loc("reshape.3685")
#loc1513 = loc("concatenate.3686")
#loc1514 = loc("slice.3658")
#loc1515 = loc("reduce.3667")
#loc1516 = loc("broadcast.3668")
#loc1517 = loc("subtract.3669")
#loc1518 = loc("exponential.3670")
#loc1519 = loc("reduce.3676")
#loc1520 = loc("broadcast.3677")
#loc1521 = loc("divide.3678")
#loc1523 = loc("transpose.3691")
#loc1524 = loc("reshape.3692")
#loc1525 = loc("broadcast.3754")
#loc1526 = loc("multiply.3755")
#loc1528 = loc("add.3765")
#loc1529 = loc("convert.3766")
#loc1530 = loc("power.3768")
#loc1532 = loc("multiply.3784")
#loc1533 = loc("reshape.3785")
#loc1534 = loc("add.3789")
#loc1535 = loc("rsqrt.3790")
#loc1536 = loc("reshape.3791")
#loc1537 = loc("broadcast.3792")
#loc1538 = loc("multiply.3793")
#loc1539 = loc("multiply.3800")
#loc1540 = loc("convert.3801")
#loc1541 = loc("reshape.3888")
#loc1542 = loc("reshape.3884")
#loc1543 = loc("reshape.3886")
#loc1544 = loc("transpose.3887")
#loc1546 = loc("reshape.3890")
#loc1547 = loc("reshape.3880")
#loc1548 = loc("reshape.3882")
#loc1549 = loc("broadcast.3893")
#loc1550 = loc("add.3894")
#loc1551 = loc("reshape.3895")
#loc1552 = loc("transpose.3896")
#loc1553 = loc("slice.3897")
#loc1554 = loc("multiply.3914")
#loc1555 = loc("slice.3901")
#loc1556 = loc("multiply.3911")
#loc1557 = loc("subtract.3917")
#loc1558 = loc("multiply.3904")
#loc1559 = loc("multiply.3900")
#loc1560 = loc("add.3907")
#loc1561 = loc("concatenate.3918")
#loc1562 = loc("reshape.3835")
#loc1563 = loc("reshape.3837")
#loc1564 = loc("transpose.3838")
#loc1566 = loc("reshape.3841")
#loc1567 = loc("reshape.3831")
#loc1568 = loc("reshape.3833")
#loc1569 = loc("broadcast.3844")
#loc1570 = loc("add.3845")
#loc1571 = loc("reshape.3846")
#loc1572 = loc("transpose.3847")
#loc1573 = loc("slice.3848")
#loc1574 = loc("multiply.3866")
#loc1575 = loc("slice.3853")
#loc1576 = loc("multiply.3863")
#loc1577 = loc("subtract.3869")
#loc1578 = loc("multiply.3856")
#loc1579 = loc("multiply.3851")
#loc1580 = loc("add.3859")
#loc1581 = loc("concatenate.3870")
#loc1582 = loc("broadcast.3874")
#loc1583 = loc("reshape.3875")
#loc1584 = loc("transpose.3876")
#loc1585 = loc("dot.3919")
#loc1586 = loc("multiply.3922")
#loc1587 = loc("add.3927")
#loc1588 = loc("reshape.3818")
#loc1589 = loc("reshape.3823")
#loc1590 = loc("broadcast.3824")
#loc1591 = loc("concatenate.3928")
#loc1592 = loc("reshape.4201")
#loc1593 = loc("reshape.4203")
#loc1594 = loc("convert.4204")
#loc1595 = loc("broadcast.4205")
#loc1596 = loc("reduce.3935")
#loc1597 = loc("broadcast.3971")
#loc1598 = loc("subtract.3972")
#loc1599 = loc("reduce.3978")
#loc1600 = loc("broadcast.3979")
#loc1601 = loc("subtract.3980")
#loc1602 = loc("exponential.3981")
#loc1603 = loc("reduce.3987")
#loc1604 = loc("broadcast.3988")
#loc1605 = loc("divide.3989")
#loc1606 = loc("slice.3990")
#loc1607 = loc("reshape.1304")
#loc1608 = loc("reshape.1306")
#loc1609 = loc("transpose.1307")
#loc1611 = loc("reshape.3804")
#loc1612 = loc("reshape.1300")
#loc1613 = loc("reshape.1302")
#loc1614 = loc("broadcast.3807")
#loc1615 = loc("add.3808")
#loc1616 = loc("reshape.3809")
#loc1617 = loc("transpose.3810")
#loc1618 = loc("broadcast.3814")
#loc1619 = loc("reshape.3815")
#loc1620 = loc("dot.3991")
#loc1621 = loc("transpose.3993")
#loc1622 = loc("reshape.3995")
#loc1623 = loc("reshape.1294")
#loc1624 = loc("reshape.1296")
#loc1625 = loc("transpose.1297")
#loc1627 = loc("reshape.3997")
#loc1628 = loc("reshape.1290")
#loc1629 = loc("reshape.1292")
#loc1630 = loc("broadcast.4000")
#loc1631 = loc("add.4001")
#loc1632 = loc("add.4004")
#loc1633 = loc("reshape.4034")
#loc1634 = loc("reshape.4036")
#loc1635 = loc("convert.4037")
#loc1636 = loc("broadcast.4038")
#loc1637 = loc("convert.4005")
#loc1638 = loc("power.4007")
#loc1640 = loc("multiply.4023")
#loc1641 = loc("reshape.4024")
#loc1642 = loc("add.4028")
#loc1643 = loc("rsqrt.4029")
#loc1644 = loc("reshape.4030")
#loc1645 = loc("broadcast.4031")
#loc1646 = loc("multiply.4032")
#loc1647 = loc("multiply.4039")
#loc1648 = loc("convert.4040")
#loc1649 = loc("reshape.4119")
#loc1650 = loc("broadcast.4123")
#loc1652 = loc("reshape.4113")
#loc1653 = loc("reshape.4115")
#loc1654 = loc("broadcast.4130")
#loc1655 = loc("add.4131")
#loc1656 = loc("slice.4144")
#loc1657 = loc("clamp.4147")
#loc1658 = loc("add.4150")
#loc1659 = loc("slice.4132")
#loc1660 = loc("clamp.4135")
#loc1661 = loc("multiply.4137")
#loc1662 = loc("logistic.4138")
#loc1663 = loc("multiply.4139")
#loc1664 = loc("multiply.4151")
#loc1665 = loc("dot.4152")
#loc1666 = loc("reshape.4102")
#loc1667 = loc("reshape.4104")
#loc1668 = loc("broadcast.4156")
#loc1669 = loc("add.4157")
#loc1670 = loc("reshape.4158")
#loc1671 = loc("reshape.1280")
#loc1672 = loc("reshape.1282")
#loc1673 = loc("transpose.1283")
#loc1675 = loc("reshape.1273")
#loc1676 = loc("reshape.1275")
#loc1677 = loc("broadcast.4047")
#loc1678 = loc("add.4048")
#loc1680 = loc("compare.4061")
#loc1681 = loc("slice.4066")
#loc1682 = loc("convert.4067")
#loc1683 = loc("reshape.4091")
#loc1684 = loc("concatenate.4092")
#loc1685 = loc("slice.4064")
#loc1686 = loc("reduce.4073")
#loc1687 = loc("broadcast.4074")
#loc1688 = loc("subtract.4075")
#loc1689 = loc("exponential.4076")
#loc1690 = loc("reduce.4082")
#loc1691 = loc("broadcast.4083")
#loc1692 = loc("divide.4084")
#loc1694 = loc("transpose.4097")
#loc1695 = loc("reshape.4098")
#loc1696 = loc("broadcast.4160")
#loc1697 = loc("multiply.4161")
#loc1699 = loc("add.4171")
#loc1700 = loc("convert.4172")
#loc1701 = loc("power.4174")
#loc1703 = loc("multiply.4190")
#loc1704 = loc("reshape.4191")
#loc1705 = loc("add.4195")
#loc1706 = loc("rsqrt.4196")
#loc1707 = loc("reshape.4197")
#loc1708 = loc("broadcast.4198")
#loc1709 = loc("multiply.4199")
#loc1710 = loc("multiply.4206")
#loc1711 = loc("convert.4207")
#loc1712 = loc("reshape.4294")
#loc1713 = loc("reshape.4290")
#loc1714 = loc("reshape.4292")
#loc1715 = loc("transpose.4293")
#loc1717 = loc("reshape.4296")
#loc1718 = loc("reshape.4286")
#loc1719 = loc("reshape.4288")
#loc1720 = loc("broadcast.4299")
#loc1721 = loc("add.4300")
#loc1722 = loc("reshape.4301")
#loc1723 = loc("transpose.4302")
#loc1724 = loc("slice.4303")
#loc1725 = loc("multiply.4320")
#loc1726 = loc("slice.4307")
#loc1727 = loc("multiply.4317")
#loc1728 = loc("subtract.4323")
#loc1729 = loc("multiply.4310")
#loc1730 = loc("multiply.4306")
#loc1731 = loc("add.4313")
#loc1732 = loc("concatenate.4324")
#loc1733 = loc("reshape.4241")
#loc1734 = loc("reshape.4243")
#loc1735 = loc("transpose.4244")
#loc1737 = loc("reshape.4247")
#loc1738 = loc("reshape.4237")
#loc1739 = loc("reshape.4239")
#loc1740 = loc("broadcast.4250")
#loc1741 = loc("add.4251")
#loc1742 = loc("reshape.4252")
#loc1743 = loc("transpose.4253")
#loc1744 = loc("slice.4254")
#loc1745 = loc("multiply.4272")
#loc1746 = loc("slice.4259")
#loc1747 = loc("multiply.4269")
#loc1748 = loc("subtract.4275")
#loc1749 = loc("multiply.4262")
#loc1750 = loc("multiply.4257")
#loc1751 = loc("add.4265")
#loc1752 = loc("concatenate.4276")
#loc1753 = loc("broadcast.4280")
#loc1754 = loc("reshape.4281")
#loc1755 = loc("transpose.4282")
#loc1756 = loc("dot.4325")
#loc1757 = loc("multiply.4328")
#loc1758 = loc("add.4333")
#loc1759 = loc("reshape.4224")
#loc1760 = loc("reshape.4229")
#loc1761 = loc("broadcast.4230")
#loc1762 = loc("concatenate.4334")
#loc1763 = loc("reshape.4607")
#loc1764 = loc("reshape.4609")
#loc1765 = loc("convert.4610")
#loc1766 = loc("broadcast.4611")
#loc1767 = loc("reduce.4341")
#loc1768 = loc("broadcast.4377")
#loc1769 = loc("subtract.4378")
#loc1770 = loc("reduce.4384")
#loc1771 = loc("broadcast.4385")
#loc1772 = loc("subtract.4386")
#loc1773 = loc("exponential.4387")
#loc1774 = loc("reduce.4393")
#loc1775 = loc("broadcast.4394")
#loc1776 = loc("divide.4395")
#loc1777 = loc("slice.4396")
#loc1778 = loc("reshape.1262")
#loc1779 = loc("reshape.1264")
#loc1780 = loc("transpose.1265")
#loc1782 = loc("reshape.4210")
#loc1783 = loc("reshape.1258")
#loc1784 = loc("reshape.1260")
#loc1785 = loc("broadcast.4213")
#loc1786 = loc("add.4214")
#loc1787 = loc("reshape.4215")
#loc1788 = loc("transpose.4216")
#loc1789 = loc("broadcast.4220")
#loc1790 = loc("reshape.4221")
#loc1791 = loc("dot.4397")
#loc1792 = loc("transpose.4399")
#loc1793 = loc("reshape.4401")
#loc1794 = loc("reshape.1252")
#loc1795 = loc("reshape.1254")
#loc1796 = loc("transpose.1255")
#loc1798 = loc("reshape.4403")
#loc1799 = loc("reshape.1248")
#loc1800 = loc("reshape.1250")
#loc1801 = loc("broadcast.4406")
#loc1802 = loc("add.4407")
#loc1803 = loc("add.4410")
#loc1804 = loc("reshape.4440")
#loc1805 = loc("reshape.4442")
#loc1806 = loc("convert.4443")
#loc1807 = loc("broadcast.4444")
#loc1808 = loc("convert.4411")
#loc1809 = loc("power.4413")
#loc1811 = loc("multiply.4429")
#loc1812 = loc("reshape.4430")
#loc1813 = loc("add.4434")
#loc1814 = loc("rsqrt.4435")
#loc1815 = loc("reshape.4436")
#loc1816 = loc("broadcast.4437")
#loc1817 = loc("multiply.4438")
#loc1818 = loc("multiply.4445")
#loc1819 = loc("convert.4446")
#loc1820 = loc("reshape.4525")
#loc1821 = loc("broadcast.4529")
#loc1823 = loc("reshape.4519")
#loc1824 = loc("reshape.4521")
#loc1825 = loc("broadcast.4536")
#loc1826 = loc("add.4537")
#loc1827 = loc("slice.4550")
#loc1828 = loc("clamp.4553")
#loc1829 = loc("add.4556")
#loc1830 = loc("slice.4538")
#loc1831 = loc("clamp.4541")
#loc1832 = loc("multiply.4543")
#loc1833 = loc("logistic.4544")
#loc1834 = loc("multiply.4545")
#loc1835 = loc("multiply.4557")
#loc1836 = loc("dot.4558")
#loc1837 = loc("reshape.4508")
#loc1838 = loc("reshape.4510")
#loc1839 = loc("broadcast.4562")
#loc1840 = loc("add.4563")
#loc1841 = loc("reshape.4564")
#loc1842 = loc("reshape.1238")
#loc1843 = loc("reshape.1240")
#loc1844 = loc("transpose.1241")
#loc1846 = loc("reshape.1231")
#loc1847 = loc("reshape.1233")
#loc1848 = loc("broadcast.4453")
#loc1849 = loc("add.4454")
#loc1851 = loc("compare.4467")
#loc1852 = loc("slice.4472")
#loc1853 = loc("convert.4473")
#loc1854 = loc("reshape.4497")
#loc1855 = loc("concatenate.4498")
#loc1856 = loc("slice.4470")
#loc1857 = loc("reduce.4479")
#loc1858 = loc("broadcast.4480")
#loc1859 = loc("subtract.4481")
#loc1860 = loc("exponential.4482")
#loc1861 = loc("reduce.4488")
#loc1862 = loc("broadcast.4489")
#loc1863 = loc("divide.4490")
#loc1865 = loc("transpose.4503")
#loc1866 = loc("reshape.4504")
#loc1867 = loc("broadcast.4566")
#loc1868 = loc("multiply.4567")
#loc1870 = loc("add.4577")
#loc1871 = loc("convert.4578")
#loc1872 = loc("power.4580")
#loc1874 = loc("multiply.4596")
#loc1875 = loc("reshape.4597")
#loc1876 = loc("add.4601")
#loc1877 = loc("rsqrt.4602")
#loc1878 = loc("reshape.4603")
#loc1879 = loc("broadcast.4604")
#loc1880 = loc("multiply.4605")
#loc1881 = loc("multiply.4612")
#loc1882 = loc("convert.4613")
#loc1883 = loc("reshape.4700")
#loc1884 = loc("reshape.4696")
#loc1885 = loc("reshape.4698")
#loc1886 = loc("transpose.4699")
#loc1888 = loc("reshape.4702")
#loc1889 = loc("reshape.4692")
#loc1890 = loc("reshape.4694")
#loc1891 = loc("broadcast.4705")
#loc1892 = loc("add.4706")
#loc1893 = loc("reshape.4707")
#loc1894 = loc("transpose.4708")
#loc1895 = loc("slice.4709")
#loc1896 = loc("multiply.4726")
#loc1897 = loc("slice.4713")
#loc1898 = loc("multiply.4723")
#loc1899 = loc("subtract.4729")
#loc1900 = loc("multiply.4716")
#loc1901 = loc("multiply.4712")
#loc1902 = loc("add.4719")
#loc1903 = loc("concatenate.4730")
#loc1904 = loc("reshape.4647")
#loc1905 = loc("reshape.4649")
#loc1906 = loc("transpose.4650")
#loc1908 = loc("reshape.4653")
#loc1909 = loc("reshape.4643")
#loc1910 = loc("reshape.4645")
#loc1911 = loc("broadcast.4656")
#loc1912 = loc("add.4657")
#loc1913 = loc("reshape.4658")
#loc1914 = loc("transpose.4659")
#loc1915 = loc("slice.4660")
#loc1916 = loc("multiply.4678")
#loc1917 = loc("slice.4665")
#loc1918 = loc("multiply.4675")
#loc1919 = loc("subtract.4681")
#loc1920 = loc("multiply.4668")
#loc1921 = loc("multiply.4663")
#loc1922 = loc("add.4671")
#loc1923 = loc("concatenate.4682")
#loc1924 = loc("broadcast.4686")
#loc1925 = loc("reshape.4687")
#loc1926 = loc("transpose.4688")
#loc1927 = loc("dot.4731")
#loc1928 = loc("multiply.4734")
#loc1929 = loc("add.4739")
#loc1930 = loc("reshape.4630")
#loc1931 = loc("reshape.4635")
#loc1932 = loc("broadcast.4636")
#loc1933 = loc("concatenate.4740")
#loc1934 = loc("reshape.5013")
#loc1935 = loc("reshape.5015")
#loc1936 = loc("convert.5016")
#loc1937 = loc("broadcast.5017")
#loc1938 = loc("reduce.4747")
#loc1939 = loc("broadcast.4783")
#loc1940 = loc("subtract.4784")
#loc1941 = loc("reduce.4790")
#loc1942 = loc("broadcast.4791")
#loc1943 = loc("subtract.4792")
#loc1944 = loc("exponential.4793")
#loc1945 = loc("reduce.4799")
#loc1946 = loc("broadcast.4800")
#loc1947 = loc("divide.4801")
#loc1948 = loc("slice.4802")
#loc1949 = loc("reshape.1220")
#loc1950 = loc("reshape.1222")
#loc1951 = loc("transpose.1223")
#loc1953 = loc("reshape.4616")
#loc1954 = loc("reshape.1216")
#loc1955 = loc("reshape.1218")
#loc1956 = loc("broadcast.4619")
#loc1957 = loc("add.4620")
#loc1958 = loc("reshape.4621")
#loc1959 = loc("transpose.4622")
#loc1960 = loc("broadcast.4626")
#loc1961 = loc("reshape.4627")
#loc1962 = loc("dot.4803")
#loc1963 = loc("transpose.4805")
#loc1964 = loc("reshape.4807")
#loc1965 = loc("reshape.1210")
#loc1966 = loc("reshape.1212")
#loc1967 = loc("transpose.1213")
#loc1969 = loc("reshape.4809")
#loc1970 = loc("reshape.1206")
#loc1971 = loc("reshape.1208")
#loc1972 = loc("broadcast.4812")
#loc1973 = loc("add.4813")
#loc1974 = loc("add.4816")
#loc1975 = loc("reshape.4846")
#loc1976 = loc("reshape.4848")
#loc1977 = loc("convert.4849")
#loc1978 = loc("broadcast.4850")
#loc1979 = loc("convert.4817")
#loc1980 = loc("power.4819")
#loc1982 = loc("multiply.4835")
#loc1983 = loc("reshape.4836")
#loc1984 = loc("add.4840")
#loc1985 = loc("rsqrt.4841")
#loc1986 = loc("reshape.4842")
#loc1987 = loc("broadcast.4843")
#loc1988 = loc("multiply.4844")
#loc1989 = loc("multiply.4851")
#loc1990 = loc("convert.4852")
#loc1991 = loc("reshape.4931")
#loc1992 = loc("broadcast.4935")
#loc1994 = loc("reshape.4925")
#loc1995 = loc("reshape.4927")
#loc1996 = loc("broadcast.4942")
#loc1997 = loc("add.4943")
#loc1998 = loc("slice.4956")
#loc1999 = loc("clamp.4959")
#loc2000 = loc("add.4962")
#loc2001 = loc("slice.4944")
#loc2002 = loc("clamp.4947")
#loc2003 = loc("multiply.4949")
#loc2004 = loc("logistic.4950")
#loc2005 = loc("multiply.4951")
#loc2006 = loc("multiply.4963")
#loc2007 = loc("dot.4964")
#loc2008 = loc("reshape.4914")
#loc2009 = loc("reshape.4916")
#loc2010 = loc("broadcast.4968")
#loc2011 = loc("add.4969")
#loc2012 = loc("reshape.4970")
#loc2013 = loc("reshape.1196")
#loc2014 = loc("reshape.1198")
#loc2015 = loc("transpose.1199")
#loc2017 = loc("reshape.1189")
#loc2018 = loc("reshape.1191")
#loc2019 = loc("broadcast.4859")
#loc2020 = loc("add.4860")
#loc2022 = loc("compare.4873")
#loc2023 = loc("slice.4878")
#loc2024 = loc("convert.4879")
#loc2025 = loc("reshape.4903")
#loc2026 = loc("concatenate.4904")
#loc2027 = loc("slice.4876")
#loc2028 = loc("reduce.4885")
#loc2029 = loc("broadcast.4886")
#loc2030 = loc("subtract.4887")
#loc2031 = loc("exponential.4888")
#loc2032 = loc("reduce.4894")
#loc2033 = loc("broadcast.4895")
#loc2034 = loc("divide.4896")
#loc2036 = loc("transpose.4909")
#loc2037 = loc("reshape.4910")
#loc2038 = loc("broadcast.4972")
#loc2039 = loc("multiply.4973")
#loc2041 = loc("add.4983")
#loc2042 = loc("convert.4984")
#loc2043 = loc("power.4986")
#loc2045 = loc("multiply.5002")
#loc2046 = loc("reshape.5003")
#loc2047 = loc("add.5007")
#loc2048 = loc("rsqrt.5008")
#loc2049 = loc("reshape.5009")
#loc2050 = loc("broadcast.5010")
#loc2051 = loc("multiply.5011")
#loc2052 = loc("multiply.5018")
#loc2053 = loc("convert.5019")
#loc2054 = loc("reshape.5106")
#loc2055 = loc("reshape.5102")
#loc2056 = loc("reshape.5104")
#loc2057 = loc("transpose.5105")
#loc2059 = loc("reshape.5108")
#loc2060 = loc("reshape.5098")
#loc2061 = loc("reshape.5100")
#loc2062 = loc("broadcast.5111")
#loc2063 = loc("add.5112")
#loc2064 = loc("reshape.5113")
#loc2065 = loc("transpose.5114")
#loc2066 = loc("slice.5115")
#loc2067 = loc("multiply.5132")
#loc2068 = loc("slice.5119")
#loc2069 = loc("multiply.5129")
#loc2070 = loc("subtract.5135")
#loc2071 = loc("multiply.5122")
#loc2072 = loc("multiply.5118")
#loc2073 = loc("add.5125")
#loc2074 = loc("concatenate.5136")
#loc2075 = loc("reshape.5053")
#loc2076 = loc("reshape.5055")
#loc2077 = loc("transpose.5056")
#loc2079 = loc("reshape.5059")
#loc2080 = loc("reshape.5049")
#loc2081 = loc("reshape.5051")
#loc2082 = loc("broadcast.5062")
#loc2083 = loc("add.5063")
#loc2084 = loc("reshape.5064")
#loc2085 = loc("transpose.5065")
#loc2086 = loc("slice.5066")
#loc2087 = loc("multiply.5084")
#loc2088 = loc("slice.5071")
#loc2089 = loc("multiply.5081")
#loc2090 = loc("subtract.5087")
#loc2091 = loc("multiply.5074")
#loc2092 = loc("multiply.5069")
#loc2093 = loc("add.5077")
#loc2094 = loc("concatenate.5088")
#loc2095 = loc("broadcast.5092")
#loc2096 = loc("reshape.5093")
#loc2097 = loc("transpose.5094")
#loc2098 = loc("dot.5137")
#loc2099 = loc("multiply.5140")
#loc2100 = loc("add.5145")
#loc2101 = loc("reshape.5036")
#loc2102 = loc("reshape.5041")
#loc2103 = loc("broadcast.5042")
#loc2104 = loc("concatenate.5146")
#loc2105 = loc("reshape.5419")
#loc2106 = loc("reshape.5421")
#loc2107 = loc("convert.5422")
#loc2108 = loc("broadcast.5423")
#loc2109 = loc("reduce.5153")
#loc2110 = loc("broadcast.5189")
#loc2111 = loc("subtract.5190")
#loc2112 = loc("reduce.5196")
#loc2113 = loc("broadcast.5197")
#loc2114 = loc("subtract.5198")
#loc2115 = loc("exponential.5199")
#loc2116 = loc("reduce.5205")
#loc2117 = loc("broadcast.5206")
#loc2118 = loc("divide.5207")
#loc2119 = loc("slice.5208")
#loc2120 = loc("reshape.1178")
#loc2121 = loc("reshape.1180")
#loc2122 = loc("transpose.1181")
#loc2124 = loc("reshape.5022")
#loc2125 = loc("reshape.1174")
#loc2126 = loc("reshape.1176")
#loc2127 = loc("broadcast.5025")
#loc2128 = loc("add.5026")
#loc2129 = loc("reshape.5027")
#loc2130 = loc("transpose.5028")
#loc2131 = loc("broadcast.5032")
#loc2132 = loc("reshape.5033")
#loc2133 = loc("dot.5209")
#loc2134 = loc("transpose.5211")
#loc2135 = loc("reshape.5213")
#loc2136 = loc("reshape.1168")
#loc2137 = loc("reshape.1170")
#loc2138 = loc("transpose.1171")
#loc2140 = loc("reshape.5215")
#loc2141 = loc("reshape.1164")
#loc2142 = loc("reshape.1166")
#loc2143 = loc("broadcast.5218")
#loc2144 = loc("add.5219")
#loc2145 = loc("add.5222")
#loc2146 = loc("reshape.5252")
#loc2147 = loc("reshape.5254")
#loc2148 = loc("convert.5255")
#loc2149 = loc("broadcast.5256")
#loc2150 = loc("convert.5223")
#loc2151 = loc("power.5225")
#loc2153 = loc("multiply.5241")
#loc2154 = loc("reshape.5242")
#loc2155 = loc("add.5246")
#loc2156 = loc("rsqrt.5247")
#loc2157 = loc("reshape.5248")
#loc2158 = loc("broadcast.5249")
#loc2159 = loc("multiply.5250")
#loc2160 = loc("multiply.5257")
#loc2161 = loc("convert.5258")
#loc2162 = loc("reshape.5337")
#loc2163 = loc("broadcast.5341")
#loc2165 = loc("reshape.5331")
#loc2166 = loc("reshape.5333")
#loc2167 = loc("broadcast.5348")
#loc2168 = loc("add.5349")
#loc2169 = loc("slice.5362")
#loc2170 = loc("clamp.5365")
#loc2171 = loc("add.5368")
#loc2172 = loc("slice.5350")
#loc2173 = loc("clamp.5353")
#loc2174 = loc("multiply.5355")
#loc2175 = loc("logistic.5356")
#loc2176 = loc("multiply.5357")
#loc2177 = loc("multiply.5369")
#loc2178 = loc("dot.5370")
#loc2179 = loc("reshape.5320")
#loc2180 = loc("reshape.5322")
#loc2181 = loc("broadcast.5374")
#loc2182 = loc("add.5375")
#loc2183 = loc("reshape.5376")
#loc2184 = loc("reshape.1154")
#loc2185 = loc("reshape.1156")
#loc2186 = loc("transpose.1157")
#loc2188 = loc("reshape.1147")
#loc2189 = loc("reshape.1149")
#loc2190 = loc("broadcast.5265")
#loc2191 = loc("add.5266")
#loc2193 = loc("compare.5279")
#loc2194 = loc("slice.5284")
#loc2195 = loc("convert.5285")
#loc2196 = loc("reshape.5309")
#loc2197 = loc("concatenate.5310")
#loc2198 = loc("slice.5282")
#loc2199 = loc("reduce.5291")
#loc2200 = loc("broadcast.5292")
#loc2201 = loc("subtract.5293")
#loc2202 = loc("exponential.5294")
#loc2203 = loc("reduce.5300")
#loc2204 = loc("broadcast.5301")
#loc2205 = loc("divide.5302")
#loc2207 = loc("transpose.5315")
#loc2208 = loc("reshape.5316")
#loc2209 = loc("broadcast.5378")
#loc2210 = loc("multiply.5379")
#loc2212 = loc("add.5389")
#loc2213 = loc("convert.5390")
#loc2214 = loc("power.5392")
#loc2216 = loc("multiply.5408")
#loc2217 = loc("reshape.5409")
#loc2218 = loc("add.5413")
#loc2219 = loc("rsqrt.5414")
#loc2220 = loc("reshape.5415")
#loc2221 = loc("broadcast.5416")
#loc2222 = loc("multiply.5417")
#loc2223 = loc("multiply.5424")
#loc2224 = loc("convert.5425")
#loc2225 = loc("reshape.5512")
#loc2226 = loc("reshape.5508")
#loc2227 = loc("reshape.5510")
#loc2228 = loc("transpose.5511")
#loc2230 = loc("reshape.5514")
#loc2231 = loc("reshape.5504")
#loc2232 = loc("reshape.5506")
#loc2233 = loc("broadcast.5517")
#loc2234 = loc("add.5518")
#loc2235 = loc("reshape.5519")
#loc2236 = loc("transpose.5520")
#loc2237 = loc("slice.5521")
#loc2238 = loc("multiply.5538")
#loc2239 = loc("slice.5525")
#loc2240 = loc("multiply.5535")
#loc2241 = loc("subtract.5541")
#loc2242 = loc("multiply.5528")
#loc2243 = loc("multiply.5524")
#loc2244 = loc("add.5531")
#loc2245 = loc("concatenate.5542")
#loc2246 = loc("reshape.5459")
#loc2247 = loc("reshape.5461")
#loc2248 = loc("transpose.5462")
#loc2250 = loc("reshape.5465")
#loc2251 = loc("reshape.5455")
#loc2252 = loc("reshape.5457")
#loc2253 = loc("broadcast.5468")
#loc2254 = loc("add.5469")
#loc2255 = loc("reshape.5470")
#loc2256 = loc("transpose.5471")
#loc2257 = loc("slice.5472")
#loc2258 = loc("multiply.5490")
#loc2259 = loc("slice.5477")
#loc2260 = loc("multiply.5487")
#loc2261 = loc("subtract.5493")
#loc2262 = loc("multiply.5480")
#loc2263 = loc("multiply.5475")
#loc2264 = loc("add.5483")
#loc2265 = loc("concatenate.5494")
#loc2266 = loc("broadcast.5498")
#loc2267 = loc("reshape.5499")
#loc2268 = loc("transpose.5500")
#loc2269 = loc("dot.5543")
#loc2270 = loc("multiply.5546")
#loc2271 = loc("add.5551")
#loc2272 = loc("reshape.5442")
#loc2273 = loc("reshape.5447")
#loc2274 = loc("broadcast.5448")
#loc2275 = loc("concatenate.5552")
#loc2276 = loc("reshape.5825")
#loc2277 = loc("reshape.5827")
#loc2278 = loc("convert.5828")
#loc2279 = loc("broadcast.5829")
#loc2280 = loc("reduce.5559")
#loc2281 = loc("broadcast.5595")
#loc2282 = loc("subtract.5596")
#loc2283 = loc("reduce.5602")
#loc2284 = loc("broadcast.5603")
#loc2285 = loc("subtract.5604")
#loc2286 = loc("exponential.5605")
#loc2287 = loc("reduce.5611")
#loc2288 = loc("broadcast.5612")
#loc2289 = loc("divide.5613")
#loc2290 = loc("slice.5614")
#loc2291 = loc("reshape.1136")
#loc2292 = loc("reshape.1138")
#loc2293 = loc("transpose.1139")
#loc2295 = loc("reshape.5428")
#loc2296 = loc("reshape.1132")
#loc2297 = loc("reshape.1134")
#loc2298 = loc("broadcast.5431")
#loc2299 = loc("add.5432")
#loc2300 = loc("reshape.5433")
#loc2301 = loc("transpose.5434")
#loc2302 = loc("broadcast.5438")
#loc2303 = loc("reshape.5439")
#loc2304 = loc("dot.5615")
#loc2305 = loc("transpose.5617")
#loc2306 = loc("reshape.5619")
#loc2307 = loc("reshape.1126")
#loc2308 = loc("reshape.1128")
#loc2309 = loc("transpose.1129")
#loc2311 = loc("reshape.5621")
#loc2312 = loc("reshape.1122")
#loc2313 = loc("reshape.1124")
#loc2314 = loc("broadcast.5624")
#loc2315 = loc("add.5625")
#loc2316 = loc("add.5628")
#loc2317 = loc("reshape.5658")
#loc2318 = loc("reshape.5660")
#loc2319 = loc("convert.5661")
#loc2320 = loc("broadcast.5662")
#loc2321 = loc("convert.5629")
#loc2322 = loc("power.5631")
#loc2324 = loc("multiply.5647")
#loc2325 = loc("reshape.5648")
#loc2326 = loc("add.5652")
#loc2327 = loc("rsqrt.5653")
#loc2328 = loc("reshape.5654")
#loc2329 = loc("broadcast.5655")
#loc2330 = loc("multiply.5656")
#loc2331 = loc("multiply.5663")
#loc2332 = loc("convert.5664")
#loc2333 = loc("reshape.5743")
#loc2334 = loc("broadcast.5747")
#loc2336 = loc("reshape.5737")
#loc2337 = loc("reshape.5739")
#loc2338 = loc("broadcast.5754")
#loc2339 = loc("add.5755")
#loc2340 = loc("slice.5768")
#loc2341 = loc("clamp.5771")
#loc2342 = loc("add.5774")
#loc2343 = loc("slice.5756")
#loc2344 = loc("clamp.5759")
#loc2345 = loc("multiply.5761")
#loc2346 = loc("logistic.5762")
#loc2347 = loc("multiply.5763")
#loc2348 = loc("multiply.5775")
#loc2349 = loc("dot.5776")
#loc2350 = loc("reshape.5726")
#loc2351 = loc("reshape.5728")
#loc2352 = loc("broadcast.5780")
#loc2353 = loc("add.5781")
#loc2354 = loc("reshape.5782")
#loc2355 = loc("reshape.1112")
#loc2356 = loc("reshape.1114")
#loc2357 = loc("transpose.1115")
#loc2359 = loc("reshape.1105")
#loc2360 = loc("reshape.1107")
#loc2361 = loc("broadcast.5671")
#loc2362 = loc("add.5672")
#loc2364 = loc("compare.5685")
#loc2365 = loc("slice.5690")
#loc2366 = loc("convert.5691")
#loc2367 = loc("reshape.5715")
#loc2368 = loc("concatenate.5716")
#loc2369 = loc("slice.5688")
#loc2370 = loc("reduce.5697")
#loc2371 = loc("broadcast.5698")
#loc2372 = loc("subtract.5699")
#loc2373 = loc("exponential.5700")
#loc2374 = loc("reduce.5706")
#loc2375 = loc("broadcast.5707")
#loc2376 = loc("divide.5708")
#loc2378 = loc("transpose.5721")
#loc2379 = loc("reshape.5722")
#loc2380 = loc("broadcast.5784")
#loc2381 = loc("multiply.5785")
#loc2383 = loc("add.5795")
#loc2384 = loc("convert.5796")
#loc2385 = loc("power.5798")
#loc2387 = loc("multiply.5814")
#loc2388 = loc("reshape.5815")
#loc2389 = loc("add.5819")
#loc2390 = loc("rsqrt.5820")
#loc2391 = loc("reshape.5821")
#loc2392 = loc("broadcast.5822")
#loc2393 = loc("multiply.5823")
#loc2394 = loc("multiply.5830")
#loc2395 = loc("convert.5831")
#loc2396 = loc("reshape.5918")
#loc2397 = loc("reshape.5914")
#loc2398 = loc("reshape.5916")
#loc2399 = loc("transpose.5917")
#loc2401 = loc("reshape.5920")
#loc2402 = loc("reshape.5910")
#loc2403 = loc("reshape.5912")
#loc2404 = loc("broadcast.5923")
#loc2405 = loc("add.5924")
#loc2406 = loc("reshape.5925")
#loc2407 = loc("transpose.5926")
#loc2408 = loc("slice.5927")
#loc2409 = loc("multiply.5944")
#loc2410 = loc("slice.5931")
#loc2411 = loc("multiply.5941")
#loc2412 = loc("subtract.5947")
#loc2413 = loc("multiply.5934")
#loc2414 = loc("multiply.5930")
#loc2415 = loc("add.5937")
#loc2416 = loc("concatenate.5948")
#loc2417 = loc("reshape.5865")
#loc2418 = loc("reshape.5867")
#loc2419 = loc("transpose.5868")
#loc2421 = loc("reshape.5871")
#loc2422 = loc("reshape.5861")
#loc2423 = loc("reshape.5863")
#loc2424 = loc("broadcast.5874")
#loc2425 = loc("add.5875")
#loc2426 = loc("reshape.5876")
#loc2427 = loc("transpose.5877")
#loc2428 = loc("slice.5878")
#loc2429 = loc("multiply.5896")
#loc2430 = loc("slice.5883")
#loc2431 = loc("multiply.5893")
#loc2432 = loc("subtract.5899")
#loc2433 = loc("multiply.5886")
#loc2434 = loc("multiply.5881")
#loc2435 = loc("add.5889")
#loc2436 = loc("concatenate.5900")
#loc2437 = loc("broadcast.5904")
#loc2438 = loc("reshape.5905")
#loc2439 = loc("transpose.5906")
#loc2440 = loc("dot.5949")
#loc2441 = loc("multiply.5952")
#loc2442 = loc("add.5957")
#loc2443 = loc("reshape.5848")
#loc2444 = loc("reshape.5853")
#loc2445 = loc("broadcast.5854")
#loc2446 = loc("concatenate.5958")
#loc2447 = loc("reshape.6231")
#loc2448 = loc("reshape.6233")
#loc2449 = loc("convert.6234")
#loc2450 = loc("broadcast.6235")
#loc2451 = loc("reduce.5965")
#loc2452 = loc("broadcast.6001")
#loc2453 = loc("subtract.6002")
#loc2454 = loc("reduce.6008")
#loc2455 = loc("broadcast.6009")
#loc2456 = loc("subtract.6010")
#loc2457 = loc("exponential.6011")
#loc2458 = loc("reduce.6017")
#loc2459 = loc("broadcast.6018")
#loc2460 = loc("divide.6019")
#loc2461 = loc("slice.6020")
#loc2462 = loc("reshape.1094")
#loc2463 = loc("reshape.1096")
#loc2464 = loc("transpose.1097")
#loc2466 = loc("reshape.5834")
#loc2467 = loc("reshape.1090")
#loc2468 = loc("reshape.1092")
#loc2469 = loc("broadcast.5837")
#loc2470 = loc("add.5838")
#loc2471 = loc("reshape.5839")
#loc2472 = loc("transpose.5840")
#loc2473 = loc("broadcast.5844")
#loc2474 = loc("reshape.5845")
#loc2475 = loc("dot.6021")
#loc2476 = loc("transpose.6023")
#loc2477 = loc("reshape.6025")
#loc2478 = loc("reshape.1084")
#loc2479 = loc("reshape.1086")
#loc2480 = loc("transpose.1087")
#loc2482 = loc("reshape.6027")
#loc2483 = loc("reshape.1080")
#loc2484 = loc("reshape.1082")
#loc2485 = loc("broadcast.6030")
#loc2486 = loc("add.6031")
#loc2487 = loc("add.6034")
#loc2488 = loc("reshape.6064")
#loc2489 = loc("reshape.6066")
#loc2490 = loc("convert.6067")
#loc2491 = loc("broadcast.6068")
#loc2492 = loc("convert.6035")
#loc2493 = loc("power.6037")
#loc2495 = loc("multiply.6053")
#loc2496 = loc("reshape.6054")
#loc2497 = loc("add.6058")
#loc2498 = loc("rsqrt.6059")
#loc2499 = loc("reshape.6060")
#loc2500 = loc("broadcast.6061")
#loc2501 = loc("multiply.6062")
#loc2502 = loc("multiply.6069")
#loc2503 = loc("convert.6070")
#loc2504 = loc("reshape.6149")
#loc2505 = loc("broadcast.6153")
#loc2507 = loc("reshape.6143")
#loc2508 = loc("reshape.6145")
#loc2509 = loc("broadcast.6160")
#loc2510 = loc("add.6161")
#loc2511 = loc("slice.6174")
#loc2512 = loc("clamp.6177")
#loc2513 = loc("add.6180")
#loc2514 = loc("slice.6162")
#loc2515 = loc("clamp.6165")
#loc2516 = loc("multiply.6167")
#loc2517 = loc("logistic.6168")
#loc2518 = loc("multiply.6169")
#loc2519 = loc("multiply.6181")
#loc2520 = loc("dot.6182")
#loc2521 = loc("reshape.6132")
#loc2522 = loc("reshape.6134")
#loc2523 = loc("broadcast.6186")
#loc2524 = loc("add.6187")
#loc2525 = loc("reshape.6188")
#loc2526 = loc("reshape.1070")
#loc2527 = loc("reshape.1072")
#loc2528 = loc("transpose.1073")
#loc2530 = loc("reshape.1063")
#loc2531 = loc("reshape.1065")
#loc2532 = loc("broadcast.6077")
#loc2533 = loc("add.6078")
#loc2535 = loc("compare.6091")
#loc2536 = loc("slice.6096")
#loc2537 = loc("convert.6097")
#loc2538 = loc("reshape.6121")
#loc2539 = loc("concatenate.6122")
#loc2540 = loc("slice.6094")
#loc2541 = loc("reduce.6103")
#loc2542 = loc("broadcast.6104")
#loc2543 = loc("subtract.6105")
#loc2544 = loc("exponential.6106")
#loc2545 = loc("reduce.6112")
#loc2546 = loc("broadcast.6113")
#loc2547 = loc("divide.6114")
#loc2549 = loc("transpose.6127")
#loc2550 = loc("reshape.6128")
#loc2551 = loc("broadcast.6190")
#loc2552 = loc("multiply.6191")
#loc2554 = loc("add.6201")
#loc2555 = loc("convert.6202")
#loc2556 = loc("power.6204")
#loc2558 = loc("multiply.6220")
#loc2559 = loc("reshape.6221")
#loc2560 = loc("add.6225")
#loc2561 = loc("rsqrt.6226")
#loc2562 = loc("reshape.6227")
#loc2563 = loc("broadcast.6228")
#loc2564 = loc("multiply.6229")
#loc2565 = loc("multiply.6236")
#loc2566 = loc("convert.6237")
#loc2567 = loc("reshape.6324")
#loc2568 = loc("reshape.6320")
#loc2569 = loc("reshape.6322")
#loc2570 = loc("transpose.6323")
#loc2572 = loc("reshape.6326")
#loc2573 = loc("reshape.6316")
#loc2574 = loc("reshape.6318")
#loc2575 = loc("broadcast.6329")
#loc2576 = loc("add.6330")
#loc2577 = loc("reshape.6331")
#loc2578 = loc("transpose.6332")
#loc2579 = loc("slice.6333")
#loc2580 = loc("multiply.6350")
#loc2581 = loc("slice.6337")
#loc2582 = loc("multiply.6347")
#loc2583 = loc("subtract.6353")
#loc2584 = loc("multiply.6340")
#loc2585 = loc("multiply.6336")
#loc2586 = loc("add.6343")
#loc2587 = loc("concatenate.6354")
#loc2588 = loc("reshape.6271")
#loc2589 = loc("reshape.6273")
#loc2590 = loc("transpose.6274")
#loc2592 = loc("reshape.6277")
#loc2593 = loc("reshape.6267")
#loc2594 = loc("reshape.6269")
#loc2595 = loc("broadcast.6280")
#loc2596 = loc("add.6281")
#loc2597 = loc("reshape.6282")
#loc2598 = loc("transpose.6283")
#loc2599 = loc("slice.6284")
#loc2600 = loc("multiply.6302")
#loc2601 = loc("slice.6289")
#loc2602 = loc("multiply.6299")
#loc2603 = loc("subtract.6305")
#loc2604 = loc("multiply.6292")
#loc2605 = loc("multiply.6287")
#loc2606 = loc("add.6295")
#loc2607 = loc("concatenate.6306")
#loc2608 = loc("broadcast.6310")
#loc2609 = loc("reshape.6311")
#loc2610 = loc("transpose.6312")
#loc2611 = loc("dot.6355")
#loc2612 = loc("multiply.6358")
#loc2613 = loc("add.6363")
#loc2614 = loc("reshape.6254")
#loc2615 = loc("reshape.6259")
#loc2616 = loc("broadcast.6260")
#loc2617 = loc("concatenate.6364")
#loc2618 = loc("reshape.6637")
#loc2619 = loc("reshape.6639")
#loc2620 = loc("convert.6640")
#loc2621 = loc("broadcast.6641")
#loc2622 = loc("reduce.6371")
#loc2623 = loc("broadcast.6407")
#loc2624 = loc("subtract.6408")
#loc2625 = loc("reduce.6414")
#loc2626 = loc("broadcast.6415")
#loc2627 = loc("subtract.6416")
#loc2628 = loc("exponential.6417")
#loc2629 = loc("reduce.6423")
#loc2630 = loc("broadcast.6424")
#loc2631 = loc("divide.6425")
#loc2632 = loc("slice.6426")
#loc2633 = loc("reshape.1052")
#loc2634 = loc("reshape.1054")
#loc2635 = loc("transpose.1055")
#loc2637 = loc("reshape.6240")
#loc2638 = loc("reshape.1048")
#loc2639 = loc("reshape.1050")
#loc2640 = loc("broadcast.6243")
#loc2641 = loc("add.6244")
#loc2642 = loc("reshape.6245")
#loc2643 = loc("transpose.6246")
#loc2644 = loc("broadcast.6250")
#loc2645 = loc("reshape.6251")
#loc2646 = loc("dot.6427")
#loc2647 = loc("transpose.6429")
#loc2648 = loc("reshape.6431")
#loc2649 = loc("reshape.1042")
#loc2650 = loc("reshape.1044")
#loc2651 = loc("transpose.1045")
#loc2653 = loc("reshape.6433")
#loc2654 = loc("reshape.1038")
#loc2655 = loc("reshape.1040")
#loc2656 = loc("broadcast.6436")
#loc2657 = loc("add.6437")
#loc2658 = loc("add.6440")
#loc2659 = loc("reshape.6470")
#loc2660 = loc("reshape.6472")
#loc2661 = loc("convert.6473")
#loc2662 = loc("broadcast.6474")
#loc2663 = loc("convert.6441")
#loc2664 = loc("power.6443")
#loc2666 = loc("multiply.6459")
#loc2667 = loc("reshape.6460")
#loc2668 = loc("add.6464")
#loc2669 = loc("rsqrt.6465")
#loc2670 = loc("reshape.6466")
#loc2671 = loc("broadcast.6467")
#loc2672 = loc("multiply.6468")
#loc2673 = loc("multiply.6475")
#loc2674 = loc("convert.6476")
#loc2675 = loc("reshape.6555")
#loc2676 = loc("broadcast.6559")
#loc2678 = loc("reshape.6549")
#loc2679 = loc("reshape.6551")
#loc2680 = loc("broadcast.6566")
#loc2681 = loc("add.6567")
#loc2682 = loc("slice.6580")
#loc2683 = loc("clamp.6583")
#loc2684 = loc("add.6586")
#loc2685 = loc("slice.6568")
#loc2686 = loc("clamp.6571")
#loc2687 = loc("multiply.6573")
#loc2688 = loc("logistic.6574")
#loc2689 = loc("multiply.6575")
#loc2690 = loc("multiply.6587")
#loc2691 = loc("dot.6588")
#loc2692 = loc("reshape.6538")
#loc2693 = loc("reshape.6540")
#loc2694 = loc("broadcast.6592")
#loc2695 = loc("add.6593")
#loc2696 = loc("reshape.6594")
#loc2697 = loc("reshape.1028")
#loc2698 = loc("reshape.1030")
#loc2699 = loc("transpose.1031")
#loc2701 = loc("reshape.1021")
#loc2702 = loc("reshape.1023")
#loc2703 = loc("broadcast.6483")
#loc2704 = loc("add.6484")
#loc2706 = loc("compare.6497")
#loc2707 = loc("slice.6502")
#loc2708 = loc("convert.6503")
#loc2709 = loc("reshape.6527")
#loc2710 = loc("concatenate.6528")
#loc2711 = loc("slice.6500")
#loc2712 = loc("reduce.6509")
#loc2713 = loc("broadcast.6510")
#loc2714 = loc("subtract.6511")
#loc2715 = loc("exponential.6512")
#loc2716 = loc("reduce.6518")
#loc2717 = loc("broadcast.6519")
#loc2718 = loc("divide.6520")
#loc2720 = loc("transpose.6533")
#loc2721 = loc("reshape.6534")
#loc2722 = loc("broadcast.6596")
#loc2723 = loc("multiply.6597")
#loc2725 = loc("add.6607")
#loc2726 = loc("convert.6608")
#loc2727 = loc("power.6610")
#loc2729 = loc("multiply.6626")
#loc2730 = loc("reshape.6627")
#loc2731 = loc("add.6631")
#loc2732 = loc("rsqrt.6632")
#loc2733 = loc("reshape.6633")
#loc2734 = loc("broadcast.6634")
#loc2735 = loc("multiply.6635")
#loc2736 = loc("multiply.6642")
#loc2737 = loc("convert.6643")
#loc2738 = loc("reshape.6730")
#loc2739 = loc("reshape.6726")
#loc2740 = loc("reshape.6728")
#loc2741 = loc("transpose.6729")
#loc2743 = loc("reshape.6732")
#loc2744 = loc("reshape.6722")
#loc2745 = loc("reshape.6724")
#loc2746 = loc("broadcast.6735")
#loc2747 = loc("add.6736")
#loc2748 = loc("reshape.6737")
#loc2749 = loc("transpose.6738")
#loc2750 = loc("slice.6739")
#loc2751 = loc("multiply.6756")
#loc2752 = loc("slice.6743")
#loc2753 = loc("multiply.6753")
#loc2754 = loc("subtract.6759")
#loc2755 = loc("multiply.6746")
#loc2756 = loc("multiply.6742")
#loc2757 = loc("add.6749")
#loc2758 = loc("concatenate.6760")
#loc2759 = loc("reshape.6677")
#loc2760 = loc("reshape.6679")
#loc2761 = loc("transpose.6680")
#loc2763 = loc("reshape.6683")
#loc2764 = loc("reshape.6673")
#loc2765 = loc("reshape.6675")
#loc2766 = loc("broadcast.6686")
#loc2767 = loc("add.6687")
#loc2768 = loc("reshape.6688")
#loc2769 = loc("transpose.6689")
#loc2770 = loc("slice.6690")
#loc2771 = loc("multiply.6708")
#loc2772 = loc("slice.6695")
#loc2773 = loc("multiply.6705")
#loc2774 = loc("subtract.6711")
#loc2775 = loc("multiply.6698")
#loc2776 = loc("multiply.6693")
#loc2777 = loc("add.6701")
#loc2778 = loc("concatenate.6712")
#loc2779 = loc("broadcast.6716")
#loc2780 = loc("reshape.6717")
#loc2781 = loc("transpose.6718")
#loc2782 = loc("dot.6761")
#loc2783 = loc("multiply.6764")
#loc2784 = loc("add.6769")
#loc2785 = loc("reshape.6660")
#loc2786 = loc("reshape.6665")
#loc2787 = loc("broadcast.6666")
#loc2788 = loc("concatenate.6770")
#loc2789 = loc("reshape.7043")
#loc2790 = loc("reshape.7045")
#loc2791 = loc("convert.7046")
#loc2792 = loc("broadcast.7047")
#loc2793 = loc("reduce.6777")
#loc2794 = loc("broadcast.6813")
#loc2795 = loc("subtract.6814")
#loc2796 = loc("reduce.6820")
#loc2797 = loc("broadcast.6821")
#loc2798 = loc("subtract.6822")
#loc2799 = loc("exponential.6823")
#loc2800 = loc("reduce.6829")
#loc2801 = loc("broadcast.6830")
#loc2802 = loc("divide.6831")
#loc2803 = loc("slice.6832")
#loc2804 = loc("reshape.1010")
#loc2805 = loc("reshape.1012")
#loc2806 = loc("transpose.1013")
#loc2808 = loc("reshape.6646")
#loc2809 = loc("reshape.1006")
#loc2810 = loc("reshape.1008")
#loc2811 = loc("broadcast.6649")
#loc2812 = loc("add.6650")
#loc2813 = loc("reshape.6651")
#loc2814 = loc("transpose.6652")
#loc2815 = loc("broadcast.6656")
#loc2816 = loc("reshape.6657")
#loc2817 = loc("dot.6833")
#loc2818 = loc("transpose.6835")
#loc2819 = loc("reshape.6837")
#loc2820 = loc("reshape.1000")
#loc2821 = loc("reshape.1002")
#loc2822 = loc("transpose.1003")
#loc2824 = loc("reshape.6839")
#loc2825 = loc("reshape.996")
#loc2826 = loc("reshape.998")
#loc2827 = loc("broadcast.6842")
#loc2828 = loc("add.6843")
#loc2829 = loc("add.6846")
#loc2830 = loc("reshape.6876")
#loc2831 = loc("reshape.6878")
#loc2832 = loc("convert.6879")
#loc2833 = loc("broadcast.6880")
#loc2834 = loc("convert.6847")
#loc2835 = loc("power.6849")
#loc2837 = loc("multiply.6865")
#loc2838 = loc("reshape.6866")
#loc2839 = loc("add.6870")
#loc2840 = loc("rsqrt.6871")
#loc2841 = loc("reshape.6872")
#loc2842 = loc("broadcast.6873")
#loc2843 = loc("multiply.6874")
#loc2844 = loc("multiply.6881")
#loc2845 = loc("convert.6882")
#loc2846 = loc("reshape.6961")
#loc2847 = loc("broadcast.6965")
#loc2849 = loc("reshape.6955")
#loc2850 = loc("reshape.6957")
#loc2851 = loc("broadcast.6972")
#loc2852 = loc("add.6973")
#loc2853 = loc("slice.6986")
#loc2854 = loc("clamp.6989")
#loc2855 = loc("add.6992")
#loc2856 = loc("slice.6974")
#loc2857 = loc("clamp.6977")
#loc2858 = loc("multiply.6979")
#loc2859 = loc("logistic.6980")
#loc2860 = loc("multiply.6981")
#loc2861 = loc("multiply.6993")
#loc2862 = loc("dot.6994")
#loc2863 = loc("reshape.6944")
#loc2864 = loc("reshape.6946")
#loc2865 = loc("broadcast.6998")
#loc2866 = loc("add.6999")
#loc2867 = loc("reshape.7000")
#loc2868 = loc("reshape.986")
#loc2869 = loc("reshape.988")
#loc2870 = loc("transpose.989")
#loc2872 = loc("reshape.979")
#loc2873 = loc("reshape.981")
#loc2874 = loc("broadcast.6889")
#loc2875 = loc("add.6890")
#loc2877 = loc("compare.6903")
#loc2878 = loc("slice.6908")
#loc2879 = loc("convert.6909")
#loc2880 = loc("reshape.6933")
#loc2881 = loc("concatenate.6934")
#loc2882 = loc("slice.6906")
#loc2883 = loc("reduce.6915")
#loc2884 = loc("broadcast.6916")
#loc2885 = loc("subtract.6917")
#loc2886 = loc("exponential.6918")
#loc2887 = loc("reduce.6924")
#loc2888 = loc("broadcast.6925")
#loc2889 = loc("divide.6926")
#loc2891 = loc("transpose.6939")
#loc2892 = loc("reshape.6940")
#loc2893 = loc("broadcast.7002")
#loc2894 = loc("multiply.7003")
#loc2896 = loc("add.7013")
#loc2897 = loc("convert.7014")
#loc2898 = loc("power.7016")
#loc2900 = loc("multiply.7032")
#loc2901 = loc("reshape.7033")
#loc2902 = loc("add.7037")
#loc2903 = loc("rsqrt.7038")
#loc2904 = loc("reshape.7039")
#loc2905 = loc("broadcast.7040")
#loc2906 = loc("multiply.7041")
#loc2907 = loc("multiply.7048")
#loc2908 = loc("convert.7049")
#loc2909 = loc("reshape.7136")
#loc2910 = loc("reshape.7132")
#loc2911 = loc("reshape.7134")
#loc2912 = loc("transpose.7135")
#loc2914 = loc("reshape.7138")
#loc2915 = loc("reshape.7128")
#loc2916 = loc("reshape.7130")
#loc2917 = loc("broadcast.7141")
#loc2918 = loc("add.7142")
#loc2919 = loc("reshape.7143")
#loc2920 = loc("transpose.7144")
#loc2921 = loc("slice.7145")
#loc2922 = loc("multiply.7162")
#loc2923 = loc("slice.7149")
#loc2924 = loc("multiply.7159")
#loc2925 = loc("subtract.7165")
#loc2926 = loc("multiply.7152")
#loc2927 = loc("multiply.7148")
#loc2928 = loc("add.7155")
#loc2929 = loc("concatenate.7166")
#loc2930 = loc("reshape.7083")
#loc2931 = loc("reshape.7085")
#loc2932 = loc("transpose.7086")
#loc2934 = loc("reshape.7089")
#loc2935 = loc("reshape.7079")
#loc2936 = loc("reshape.7081")
#loc2937 = loc("broadcast.7092")
#loc2938 = loc("add.7093")
#loc2939 = loc("reshape.7094")
#loc2940 = loc("transpose.7095")
#loc2941 = loc("slice.7096")
#loc2942 = loc("multiply.7114")
#loc2943 = loc("slice.7101")
#loc2944 = loc("multiply.7111")
#loc2945 = loc("subtract.7117")
#loc2946 = loc("multiply.7104")
#loc2947 = loc("multiply.7099")
#loc2948 = loc("add.7107")
#loc2949 = loc("concatenate.7118")
#loc2950 = loc("broadcast.7122")
#loc2951 = loc("reshape.7123")
#loc2952 = loc("transpose.7124")
#loc2953 = loc("dot.7167")
#loc2954 = loc("multiply.7170")
#loc2955 = loc("add.7175")
#loc2956 = loc("reshape.7066")
#loc2957 = loc("reshape.7071")
#loc2958 = loc("broadcast.7072")
#loc2959 = loc("concatenate.7176")
#loc2960 = loc("reshape.7449")
#loc2961 = loc("reshape.7451")
#loc2962 = loc("convert.7452")
#loc2963 = loc("broadcast.7453")
#loc2964 = loc("reduce.7183")
#loc2965 = loc("broadcast.7219")
#loc2966 = loc("subtract.7220")
#loc2967 = loc("reduce.7226")
#loc2968 = loc("broadcast.7227")
#loc2969 = loc("subtract.7228")
#loc2970 = loc("exponential.7229")
#loc2971 = loc("reduce.7235")
#loc2972 = loc("broadcast.7236")
#loc2973 = loc("divide.7237")
#loc2974 = loc("slice.7238")
#loc2975 = loc("reshape.968")
#loc2976 = loc("reshape.970")
#loc2977 = loc("transpose.971")
#loc2979 = loc("reshape.7052")
#loc2980 = loc("reshape.964")
#loc2981 = loc("reshape.966")
#loc2982 = loc("broadcast.7055")
#loc2983 = loc("add.7056")
#loc2984 = loc("reshape.7057")
#loc2985 = loc("transpose.7058")
#loc2986 = loc("broadcast.7062")
#loc2987 = loc("reshape.7063")
#loc2988 = loc("dot.7239")
#loc2989 = loc("transpose.7241")
#loc2990 = loc("reshape.7243")
#loc2991 = loc("reshape.958")
#loc2992 = loc("reshape.960")
#loc2993 = loc("transpose.961")
#loc2995 = loc("reshape.7245")
#loc2996 = loc("reshape.954")
#loc2997 = loc("reshape.956")
#loc2998 = loc("broadcast.7248")
#loc2999 = loc("add.7249")
#loc3000 = loc("add.7252")
#loc3001 = loc("reshape.7282")
#loc3002 = loc("reshape.7284")
#loc3003 = loc("convert.7285")
#loc3004 = loc("broadcast.7286")
#loc3005 = loc("convert.7253")
#loc3006 = loc("power.7255")
#loc3008 = loc("multiply.7271")
#loc3009 = loc("reshape.7272")
#loc3010 = loc("add.7276")
#loc3011 = loc("rsqrt.7277")
#loc3012 = loc("reshape.7278")
#loc3013 = loc("broadcast.7279")
#loc3014 = loc("multiply.7280")
#loc3015 = loc("multiply.7287")
#loc3016 = loc("convert.7288")
#loc3017 = loc("reshape.7367")
#loc3018 = loc("broadcast.7371")
#loc3020 = loc("reshape.7361")
#loc3021 = loc("reshape.7363")
#loc3022 = loc("broadcast.7378")
#loc3023 = loc("add.7379")
#loc3024 = loc("slice.7392")
#loc3025 = loc("clamp.7395")
#loc3026 = loc("add.7398")
#loc3027 = loc("slice.7380")
#loc3028 = loc("clamp.7383")
#loc3029 = loc("multiply.7385")
#loc3030 = loc("logistic.7386")
#loc3031 = loc("multiply.7387")
#loc3032 = loc("multiply.7399")
#loc3033 = loc("dot.7400")
#loc3034 = loc("reshape.7350")
#loc3035 = loc("reshape.7352")
#loc3036 = loc("broadcast.7404")
#loc3037 = loc("add.7405")
#loc3038 = loc("reshape.7406")
#loc3039 = loc("reshape.944")
#loc3040 = loc("reshape.946")
#loc3041 = loc("transpose.947")
#loc3043 = loc("reshape.937")
#loc3044 = loc("reshape.939")
#loc3045 = loc("broadcast.7295")
#loc3046 = loc("add.7296")
#loc3048 = loc("compare.7309")
#loc3049 = loc("slice.7314")
#loc3050 = loc("convert.7315")
#loc3051 = loc("reshape.7339")
#loc3052 = loc("concatenate.7340")
#loc3053 = loc("slice.7312")
#loc3054 = loc("reduce.7321")
#loc3055 = loc("broadcast.7322")
#loc3056 = loc("subtract.7323")
#loc3057 = loc("exponential.7324")
#loc3058 = loc("reduce.7330")
#loc3059 = loc("broadcast.7331")
#loc3060 = loc("divide.7332")
#loc3062 = loc("transpose.7345")
#loc3063 = loc("reshape.7346")
#loc3064 = loc("broadcast.7408")
#loc3065 = loc("multiply.7409")
#loc3067 = loc("add.7419")
#loc3068 = loc("convert.7420")
#loc3069 = loc("power.7422")
#loc3071 = loc("multiply.7438")
#loc3072 = loc("reshape.7439")
#loc3073 = loc("add.7443")
#loc3074 = loc("rsqrt.7444")
#loc3075 = loc("reshape.7445")
#loc3076 = loc("broadcast.7446")
#loc3077 = loc("multiply.7447")
#loc3078 = loc("multiply.7454")
#loc3079 = loc("convert.7455")
#loc3080 = loc("reshape.7542")
#loc3081 = loc("reshape.7538")
#loc3082 = loc("reshape.7540")
#loc3083 = loc("transpose.7541")
#loc3085 = loc("reshape.7544")
#loc3086 = loc("reshape.7534")
#loc3087 = loc("reshape.7536")
#loc3088 = loc("broadcast.7547")
#loc3089 = loc("add.7548")
#loc3090 = loc("reshape.7549")
#loc3091 = loc("transpose.7550")
#loc3092 = loc("slice.7551")
#loc3093 = loc("multiply.7568")
#loc3094 = loc("slice.7555")
#loc3095 = loc("multiply.7565")
#loc3096 = loc("subtract.7571")
#loc3097 = loc("multiply.7558")
#loc3098 = loc("multiply.7554")
#loc3099 = loc("add.7561")
#loc3100 = loc("concatenate.7572")
#loc3101 = loc("reshape.7489")
#loc3102 = loc("reshape.7491")
#loc3103 = loc("transpose.7492")
#loc3105 = loc("reshape.7495")
#loc3106 = loc("reshape.7485")
#loc3107 = loc("reshape.7487")
#loc3108 = loc("broadcast.7498")
#loc3109 = loc("add.7499")
#loc3110 = loc("reshape.7500")
#loc3111 = loc("transpose.7501")
#loc3112 = loc("slice.7502")
#loc3113 = loc("multiply.7520")
#loc3114 = loc("slice.7507")
#loc3115 = loc("multiply.7517")
#loc3116 = loc("subtract.7523")
#loc3117 = loc("multiply.7510")
#loc3118 = loc("multiply.7505")
#loc3119 = loc("add.7513")
#loc3120 = loc("concatenate.7524")
#loc3121 = loc("broadcast.7528")
#loc3122 = loc("reshape.7529")
#loc3123 = loc("transpose.7530")
#loc3124 = loc("dot.7573")
#loc3125 = loc("multiply.7576")
#loc3126 = loc("add.7581")
#loc3127 = loc("reshape.7472")
#loc3128 = loc("reshape.7477")
#loc3129 = loc("broadcast.7478")
#loc3130 = loc("concatenate.7582")
#loc3131 = loc("reshape.7855")
#loc3132 = loc("reshape.7857")
#loc3133 = loc("convert.7858")
#loc3134 = loc("broadcast.7859")
#loc3135 = loc("reduce.7589")
#loc3136 = loc("broadcast.7625")
#loc3137 = loc("subtract.7626")
#loc3138 = loc("reduce.7632")
#loc3139 = loc("broadcast.7633")
#loc3140 = loc("subtract.7634")
#loc3141 = loc("exponential.7635")
#loc3142 = loc("reduce.7641")
#loc3143 = loc("broadcast.7642")
#loc3144 = loc("divide.7643")
#loc3145 = loc("slice.7644")
#loc3146 = loc("reshape.926")
#loc3147 = loc("reshape.928")
#loc3148 = loc("transpose.929")
#loc3150 = loc("reshape.7458")
#loc3151 = loc("reshape.922")
#loc3152 = loc("reshape.924")
#loc3153 = loc("broadcast.7461")
#loc3154 = loc("add.7462")
#loc3155 = loc("reshape.7463")
#loc3156 = loc("transpose.7464")
#loc3157 = loc("broadcast.7468")
#loc3158 = loc("reshape.7469")
#loc3159 = loc("dot.7645")
#loc3160 = loc("transpose.7647")
#loc3161 = loc("reshape.7649")
#loc3162 = loc("reshape.916")
#loc3163 = loc("reshape.918")
#loc3164 = loc("transpose.919")
#loc3166 = loc("reshape.7651")
#loc3167 = loc("reshape.912")
#loc3168 = loc("reshape.914")
#loc3169 = loc("broadcast.7654")
#loc3170 = loc("add.7655")
#loc3171 = loc("add.7658")
#loc3172 = loc("reshape.7688")
#loc3173 = loc("reshape.7690")
#loc3174 = loc("convert.7691")
#loc3175 = loc("broadcast.7692")
#loc3176 = loc("convert.7659")
#loc3177 = loc("power.7661")
#loc3179 = loc("multiply.7677")
#loc3180 = loc("reshape.7678")
#loc3181 = loc("add.7682")
#loc3182 = loc("rsqrt.7683")
#loc3183 = loc("reshape.7684")
#loc3184 = loc("broadcast.7685")
#loc3185 = loc("multiply.7686")
#loc3186 = loc("multiply.7693")
#loc3187 = loc("convert.7694")
#loc3188 = loc("reshape.7773")
#loc3189 = loc("broadcast.7777")
#loc3191 = loc("reshape.7767")
#loc3192 = loc("reshape.7769")
#loc3193 = loc("broadcast.7784")
#loc3194 = loc("add.7785")
#loc3195 = loc("slice.7798")
#loc3196 = loc("clamp.7801")
#loc3197 = loc("add.7804")
#loc3198 = loc("slice.7786")
#loc3199 = loc("clamp.7789")
#loc3200 = loc("multiply.7791")
#loc3201 = loc("logistic.7792")
#loc3202 = loc("multiply.7793")
#loc3203 = loc("multiply.7805")
#loc3204 = loc("dot.7806")
#loc3205 = loc("reshape.7756")
#loc3206 = loc("reshape.7758")
#loc3207 = loc("broadcast.7810")
#loc3208 = loc("add.7811")
#loc3209 = loc("reshape.7812")
#loc3210 = loc("reshape.902")
#loc3211 = loc("reshape.904")
#loc3212 = loc("transpose.905")
#loc3214 = loc("reshape.895")
#loc3215 = loc("reshape.897")
#loc3216 = loc("broadcast.7701")
#loc3217 = loc("add.7702")
#loc3219 = loc("compare.7715")
#loc3220 = loc("slice.7720")
#loc3221 = loc("convert.7721")
#loc3222 = loc("reshape.7745")
#loc3223 = loc("concatenate.7746")
#loc3224 = loc("slice.7718")
#loc3225 = loc("reduce.7727")
#loc3226 = loc("broadcast.7728")
#loc3227 = loc("subtract.7729")
#loc3228 = loc("exponential.7730")
#loc3229 = loc("reduce.7736")
#loc3230 = loc("broadcast.7737")
#loc3231 = loc("divide.7738")
#loc3233 = loc("transpose.7751")
#loc3234 = loc("reshape.7752")
#loc3235 = loc("broadcast.7814")
#loc3236 = loc("multiply.7815")
#loc3238 = loc("add.7825")
#loc3239 = loc("convert.7826")
#loc3240 = loc("power.7828")
#loc3242 = loc("multiply.7844")
#loc3243 = loc("reshape.7845")
#loc3244 = loc("add.7849")
#loc3245 = loc("rsqrt.7850")
#loc3246 = loc("reshape.7851")
#loc3247 = loc("broadcast.7852")
#loc3248 = loc("multiply.7853")
#loc3249 = loc("multiply.7860")
#loc3250 = loc("convert.7861")
#loc3251 = loc("reshape.7948")
#loc3252 = loc("reshape.7944")
#loc3253 = loc("reshape.7946")
#loc3254 = loc("transpose.7947")
#loc3256 = loc("reshape.7950")
#loc3257 = loc("reshape.7940")
#loc3258 = loc("reshape.7942")
#loc3259 = loc("broadcast.7953")
#loc3260 = loc("add.7954")
#loc3261 = loc("reshape.7955")
#loc3262 = loc("transpose.7956")
#loc3263 = loc("slice.7957")
#loc3264 = loc("multiply.7974")
#loc3265 = loc("slice.7961")
#loc3266 = loc("multiply.7971")
#loc3267 = loc("subtract.7977")
#loc3268 = loc("multiply.7964")
#loc3269 = loc("multiply.7960")
#loc3270 = loc("add.7967")
#loc3271 = loc("concatenate.7978")
#loc3272 = loc("reshape.7895")
#loc3273 = loc("reshape.7897")
#loc3274 = loc("transpose.7898")
#loc3276 = loc("reshape.7901")
#loc3277 = loc("reshape.7891")
#loc3278 = loc("reshape.7893")
#loc3279 = loc("broadcast.7904")
#loc3280 = loc("add.7905")
#loc3281 = loc("reshape.7906")
#loc3282 = loc("transpose.7907")
#loc3283 = loc("slice.7908")
#loc3284 = loc("multiply.7926")
#loc3285 = loc("slice.7913")
#loc3286 = loc("multiply.7923")
#loc3287 = loc("subtract.7929")
#loc3288 = loc("multiply.7916")
#loc3289 = loc("multiply.7911")
#loc3290 = loc("add.7919")
#loc3291 = loc("concatenate.7930")
#loc3292 = loc("broadcast.7934")
#loc3293 = loc("reshape.7935")
#loc3294 = loc("transpose.7936")
#loc3295 = loc("dot.7979")
#loc3296 = loc("multiply.7982")
#loc3297 = loc("add.7987")
#loc3298 = loc("reshape.7878")
#loc3299 = loc("reshape.7883")
#loc3300 = loc("broadcast.7884")
#loc3301 = loc("concatenate.7988")
#loc3302 = loc("reshape.8261")
#loc3303 = loc("reshape.8263")
#loc3304 = loc("convert.8264")
#loc3305 = loc("broadcast.8265")
#loc3306 = loc("reduce.7995")
#loc3307 = loc("broadcast.8031")
#loc3308 = loc("subtract.8032")
#loc3309 = loc("reduce.8038")
#loc3310 = loc("broadcast.8039")
#loc3311 = loc("subtract.8040")
#loc3312 = loc("exponential.8041")
#loc3313 = loc("reduce.8047")
#loc3314 = loc("broadcast.8048")
#loc3315 = loc("divide.8049")
#loc3316 = loc("slice.8050")
#loc3317 = loc("reshape.884")
#loc3318 = loc("reshape.886")
#loc3319 = loc("transpose.887")
#loc3321 = loc("reshape.7864")
#loc3322 = loc("reshape.880")
#loc3323 = loc("reshape.882")
#loc3324 = loc("broadcast.7867")
#loc3325 = loc("add.7868")
#loc3326 = loc("reshape.7869")
#loc3327 = loc("transpose.7870")
#loc3328 = loc("broadcast.7874")
#loc3329 = loc("reshape.7875")
#loc3330 = loc("dot.8051")
#loc3331 = loc("transpose.8053")
#loc3332 = loc("reshape.8055")
#loc3333 = loc("reshape.874")
#loc3334 = loc("reshape.876")
#loc3335 = loc("transpose.877")
#loc3337 = loc("reshape.8057")
#loc3338 = loc("reshape.870")
#loc3339 = loc("reshape.872")
#loc3340 = loc("broadcast.8060")
#loc3341 = loc("add.8061")
#loc3342 = loc("add.8064")
#loc3343 = loc("reshape.8094")
#loc3344 = loc("reshape.8096")
#loc3345 = loc("convert.8097")
#loc3346 = loc("broadcast.8098")
#loc3347 = loc("convert.8065")
#loc3348 = loc("power.8067")
#loc3350 = loc("multiply.8083")
#loc3351 = loc("reshape.8084")
#loc3352 = loc("add.8088")
#loc3353 = loc("rsqrt.8089")
#loc3354 = loc("reshape.8090")
#loc3355 = loc("broadcast.8091")
#loc3356 = loc("multiply.8092")
#loc3357 = loc("multiply.8099")
#loc3358 = loc("convert.8100")
#loc3359 = loc("reshape.8179")
#loc3360 = loc("broadcast.8183")
#loc3362 = loc("reshape.8173")
#loc3363 = loc("reshape.8175")
#loc3364 = loc("broadcast.8190")
#loc3365 = loc("add.8191")
#loc3366 = loc("slice.8204")
#loc3367 = loc("clamp.8207")
#loc3368 = loc("add.8210")
#loc3369 = loc("slice.8192")
#loc3370 = loc("clamp.8195")
#loc3371 = loc("multiply.8197")
#loc3372 = loc("logistic.8198")
#loc3373 = loc("multiply.8199")
#loc3374 = loc("multiply.8211")
#loc3375 = loc("dot.8212")
#loc3376 = loc("reshape.8162")
#loc3377 = loc("reshape.8164")
#loc3378 = loc("broadcast.8216")
#loc3379 = loc("add.8217")
#loc3380 = loc("reshape.8218")
#loc3381 = loc("reshape.860")
#loc3382 = loc("reshape.862")
#loc3383 = loc("transpose.863")
#loc3385 = loc("reshape.853")
#loc3386 = loc("reshape.855")
#loc3387 = loc("broadcast.8107")
#loc3388 = loc("add.8108")
#loc3390 = loc("compare.8121")
#loc3391 = loc("slice.8126")
#loc3392 = loc("convert.8127")
#loc3393 = loc("reshape.8151")
#loc3394 = loc("concatenate.8152")
#loc3395 = loc("slice.8124")
#loc3396 = loc("reduce.8133")
#loc3397 = loc("broadcast.8134")
#loc3398 = loc("subtract.8135")
#loc3399 = loc("exponential.8136")
#loc3400 = loc("reduce.8142")
#loc3401 = loc("broadcast.8143")
#loc3402 = loc("divide.8144")
#loc3404 = loc("transpose.8157")
#loc3405 = loc("reshape.8158")
#loc3406 = loc("broadcast.8220")
#loc3407 = loc("multiply.8221")
#loc3409 = loc("add.8231")
#loc3410 = loc("convert.8232")
#loc3411 = loc("power.8234")
#loc3413 = loc("multiply.8250")
#loc3414 = loc("reshape.8251")
#loc3415 = loc("add.8255")
#loc3416 = loc("rsqrt.8256")
#loc3417 = loc("reshape.8257")
#loc3418 = loc("broadcast.8258")
#loc3419 = loc("multiply.8259")
#loc3420 = loc("multiply.8266")
#loc3421 = loc("convert.8267")
#loc3422 = loc("reshape.8354")
#loc3423 = loc("reshape.8350")
#loc3424 = loc("reshape.8352")
#loc3425 = loc("transpose.8353")
#loc3427 = loc("reshape.8356")
#loc3428 = loc("reshape.8346")
#loc3429 = loc("reshape.8348")
#loc3430 = loc("broadcast.8359")
#loc3431 = loc("add.8360")
#loc3432 = loc("reshape.8361")
#loc3433 = loc("transpose.8362")
#loc3434 = loc("slice.8363")
#loc3435 = loc("multiply.8380")
#loc3436 = loc("slice.8367")
#loc3437 = loc("multiply.8377")
#loc3438 = loc("subtract.8383")
#loc3439 = loc("multiply.8370")
#loc3440 = loc("multiply.8366")
#loc3441 = loc("add.8373")
#loc3442 = loc("concatenate.8384")
#loc3443 = loc("reshape.8301")
#loc3444 = loc("reshape.8303")
#loc3445 = loc("transpose.8304")
#loc3447 = loc("reshape.8307")
#loc3448 = loc("reshape.8297")
#loc3449 = loc("reshape.8299")
#loc3450 = loc("broadcast.8310")
#loc3451 = loc("add.8311")
#loc3452 = loc("reshape.8312")
#loc3453 = loc("transpose.8313")
#loc3454 = loc("slice.8314")
#loc3455 = loc("multiply.8332")
#loc3456 = loc("slice.8319")
#loc3457 = loc("multiply.8329")
#loc3458 = loc("subtract.8335")
#loc3459 = loc("multiply.8322")
#loc3460 = loc("multiply.8317")
#loc3461 = loc("add.8325")
#loc3462 = loc("concatenate.8336")
#loc3463 = loc("broadcast.8340")
#loc3464 = loc("reshape.8341")
#loc3465 = loc("transpose.8342")
#loc3466 = loc("dot.8385")
#loc3467 = loc("multiply.8388")
#loc3468 = loc("add.8393")
#loc3469 = loc("reshape.8284")
#loc3470 = loc("reshape.8289")
#loc3471 = loc("broadcast.8290")
#loc3472 = loc("concatenate.8394")
#loc3473 = loc("reshape.8667")
#loc3474 = loc("reshape.8669")
#loc3475 = loc("convert.8670")
#loc3476 = loc("broadcast.8671")
#loc3477 = loc("reduce.8401")
#loc3478 = loc("broadcast.8437")
#loc3479 = loc("subtract.8438")
#loc3480 = loc("reduce.8444")
#loc3481 = loc("broadcast.8445")
#loc3482 = loc("subtract.8446")
#loc3483 = loc("exponential.8447")
#loc3484 = loc("reduce.8453")
#loc3485 = loc("broadcast.8454")
#loc3486 = loc("divide.8455")
#loc3487 = loc("slice.8456")
#loc3488 = loc("reshape.842")
#loc3489 = loc("reshape.844")
#loc3490 = loc("transpose.845")
#loc3492 = loc("reshape.8270")
#loc3493 = loc("reshape.838")
#loc3494 = loc("reshape.840")
#loc3495 = loc("broadcast.8273")
#loc3496 = loc("add.8274")
#loc3497 = loc("reshape.8275")
#loc3498 = loc("transpose.8276")
#loc3499 = loc("broadcast.8280")
#loc3500 = loc("reshape.8281")
#loc3501 = loc("dot.8457")
#loc3502 = loc("transpose.8459")
#loc3503 = loc("reshape.8461")
#loc3504 = loc("reshape.832")
#loc3505 = loc("reshape.834")
#loc3506 = loc("transpose.835")
#loc3508 = loc("reshape.8463")
#loc3509 = loc("reshape.828")
#loc3510 = loc("reshape.830")
#loc3511 = loc("broadcast.8466")
#loc3512 = loc("add.8467")
#loc3513 = loc("add.8470")
#loc3514 = loc("reshape.8500")
#loc3515 = loc("reshape.8502")
#loc3516 = loc("convert.8503")
#loc3517 = loc("broadcast.8504")
#loc3518 = loc("convert.8471")
#loc3519 = loc("power.8473")
#loc3521 = loc("multiply.8489")
#loc3522 = loc("reshape.8490")
#loc3523 = loc("add.8494")
#loc3524 = loc("rsqrt.8495")
#loc3525 = loc("reshape.8496")
#loc3526 = loc("broadcast.8497")
#loc3527 = loc("multiply.8498")
#loc3528 = loc("multiply.8505")
#loc3529 = loc("convert.8506")
#loc3530 = loc("reshape.8585")
#loc3531 = loc("broadcast.8589")
#loc3533 = loc("reshape.8579")
#loc3534 = loc("reshape.8581")
#loc3535 = loc("broadcast.8596")
#loc3536 = loc("add.8597")
#loc3537 = loc("slice.8610")
#loc3538 = loc("clamp.8613")
#loc3539 = loc("add.8616")
#loc3540 = loc("slice.8598")
#loc3541 = loc("clamp.8601")
#loc3542 = loc("multiply.8603")
#loc3543 = loc("logistic.8604")
#loc3544 = loc("multiply.8605")
#loc3545 = loc("multiply.8617")
#loc3546 = loc("dot.8618")
#loc3547 = loc("reshape.8568")
#loc3548 = loc("reshape.8570")
#loc3549 = loc("broadcast.8622")
#loc3550 = loc("add.8623")
#loc3551 = loc("reshape.8624")
#loc3552 = loc("reshape.818")
#loc3553 = loc("reshape.820")
#loc3554 = loc("transpose.821")
#loc3556 = loc("reshape.811")
#loc3557 = loc("reshape.813")
#loc3558 = loc("broadcast.8513")
#loc3559 = loc("add.8514")
#loc3561 = loc("compare.8527")
#loc3562 = loc("slice.8532")
#loc3563 = loc("convert.8533")
#loc3564 = loc("reshape.8557")
#loc3565 = loc("concatenate.8558")
#loc3566 = loc("slice.8530")
#loc3567 = loc("reduce.8539")
#loc3568 = loc("broadcast.8540")
#loc3569 = loc("subtract.8541")
#loc3570 = loc("exponential.8542")
#loc3571 = loc("reduce.8548")
#loc3572 = loc("broadcast.8549")
#loc3573 = loc("divide.8550")
#loc3575 = loc("transpose.8563")
#loc3576 = loc("reshape.8564")
#loc3577 = loc("broadcast.8626")
#loc3578 = loc("multiply.8627")
#loc3580 = loc("add.8637")
#loc3581 = loc("convert.8638")
#loc3582 = loc("power.8640")
#loc3584 = loc("multiply.8656")
#loc3585 = loc("reshape.8657")
#loc3586 = loc("add.8661")
#loc3587 = loc("rsqrt.8662")
#loc3588 = loc("reshape.8663")
#loc3589 = loc("broadcast.8664")
#loc3590 = loc("multiply.8665")
#loc3591 = loc("multiply.8672")
#loc3592 = loc("convert.8673")
#loc3593 = loc("reshape.8760")
#loc3594 = loc("reshape.8756")
#loc3595 = loc("reshape.8758")
#loc3596 = loc("transpose.8759")
#loc3598 = loc("reshape.8762")
#loc3599 = loc("reshape.8752")
#loc3600 = loc("reshape.8754")
#loc3601 = loc("broadcast.8765")
#loc3602 = loc("add.8766")
#loc3603 = loc("reshape.8767")
#loc3604 = loc("transpose.8768")
#loc3605 = loc("slice.8769")
#loc3606 = loc("multiply.8786")
#loc3607 = loc("slice.8773")
#loc3608 = loc("multiply.8783")
#loc3609 = loc("subtract.8789")
#loc3610 = loc("multiply.8776")
#loc3611 = loc("multiply.8772")
#loc3612 = loc("add.8779")
#loc3613 = loc("concatenate.8790")
#loc3614 = loc("reshape.8707")
#loc3615 = loc("reshape.8709")
#loc3616 = loc("transpose.8710")
#loc3618 = loc("reshape.8713")
#loc3619 = loc("reshape.8703")
#loc3620 = loc("reshape.8705")
#loc3621 = loc("broadcast.8716")
#loc3622 = loc("add.8717")
#loc3623 = loc("reshape.8718")
#loc3624 = loc("transpose.8719")
#loc3625 = loc("slice.8720")
#loc3626 = loc("multiply.8738")
#loc3627 = loc("slice.8725")
#loc3628 = loc("multiply.8735")
#loc3629 = loc("subtract.8741")
#loc3630 = loc("multiply.8728")
#loc3631 = loc("multiply.8723")
#loc3632 = loc("add.8731")
#loc3633 = loc("concatenate.8742")
#loc3634 = loc("broadcast.8746")
#loc3635 = loc("reshape.8747")
#loc3636 = loc("transpose.8748")
#loc3637 = loc("dot.8791")
#loc3638 = loc("multiply.8794")
#loc3639 = loc("add.8799")
#loc3640 = loc("reshape.8690")
#loc3641 = loc("reshape.8695")
#loc3642 = loc("broadcast.8696")
#loc3643 = loc("concatenate.8800")
#loc3644 = loc("reshape.9073")
#loc3645 = loc("reshape.9075")
#loc3646 = loc("convert.9076")
#loc3647 = loc("broadcast.9077")
#loc3648 = loc("reduce.8807")
#loc3649 = loc("broadcast.8843")
#loc3650 = loc("subtract.8844")
#loc3651 = loc("reduce.8850")
#loc3652 = loc("broadcast.8851")
#loc3653 = loc("subtract.8852")
#loc3654 = loc("exponential.8853")
#loc3655 = loc("reduce.8859")
#loc3656 = loc("broadcast.8860")
#loc3657 = loc("divide.8861")
#loc3658 = loc("slice.8862")
#loc3659 = loc("reshape.800")
#loc3660 = loc("reshape.802")
#loc3661 = loc("transpose.803")
#loc3663 = loc("reshape.8676")
#loc3664 = loc("reshape.796")
#loc3665 = loc("reshape.798")
#loc3666 = loc("broadcast.8679")
#loc3667 = loc("add.8680")
#loc3668 = loc("reshape.8681")
#loc3669 = loc("transpose.8682")
#loc3670 = loc("broadcast.8686")
#loc3671 = loc("reshape.8687")
#loc3672 = loc("dot.8863")
#loc3673 = loc("transpose.8865")
#loc3674 = loc("reshape.8867")
#loc3675 = loc("reshape.790")
#loc3676 = loc("reshape.792")
#loc3677 = loc("transpose.793")
#loc3679 = loc("reshape.8869")
#loc3680 = loc("reshape.786")
#loc3681 = loc("reshape.788")
#loc3682 = loc("broadcast.8872")
#loc3683 = loc("add.8873")
#loc3684 = loc("add.8876")
#loc3685 = loc("reshape.8906")
#loc3686 = loc("reshape.8908")
#loc3687 = loc("convert.8909")
#loc3688 = loc("broadcast.8910")
#loc3689 = loc("convert.8877")
#loc3690 = loc("power.8879")
#loc3692 = loc("multiply.8895")
#loc3693 = loc("reshape.8896")
#loc3694 = loc("add.8900")
#loc3695 = loc("rsqrt.8901")
#loc3696 = loc("reshape.8902")
#loc3697 = loc("broadcast.8903")
#loc3698 = loc("multiply.8904")
#loc3699 = loc("multiply.8911")
#loc3700 = loc("convert.8912")
#loc3701 = loc("reshape.8991")
#loc3702 = loc("broadcast.8995")
#loc3704 = loc("reshape.8985")
#loc3705 = loc("reshape.8987")
#loc3706 = loc("broadcast.9002")
#loc3707 = loc("add.9003")
#loc3708 = loc("slice.9016")
#loc3709 = loc("clamp.9019")
#loc3710 = loc("add.9022")
#loc3711 = loc("slice.9004")
#loc3712 = loc("clamp.9007")
#loc3713 = loc("multiply.9009")
#loc3714 = loc("logistic.9010")
#loc3715 = loc("multiply.9011")
#loc3716 = loc("multiply.9023")
#loc3717 = loc("dot.9024")
#loc3718 = loc("reshape.8974")
#loc3719 = loc("reshape.8976")
#loc3720 = loc("broadcast.9028")
#loc3721 = loc("add.9029")
#loc3722 = loc("reshape.9030")
#loc3723 = loc("reshape.776")
#loc3724 = loc("reshape.778")
#loc3725 = loc("transpose.779")
#loc3727 = loc("reshape.769")
#loc3728 = loc("reshape.771")
#loc3729 = loc("broadcast.8919")
#loc3730 = loc("add.8920")
#loc3732 = loc("compare.8933")
#loc3733 = loc("slice.8938")
#loc3734 = loc("convert.8939")
#loc3735 = loc("reshape.8963")
#loc3736 = loc("concatenate.8964")
#loc3737 = loc("slice.8936")
#loc3738 = loc("reduce.8945")
#loc3739 = loc("broadcast.8946")
#loc3740 = loc("subtract.8947")
#loc3741 = loc("exponential.8948")
#loc3742 = loc("reduce.8954")
#loc3743 = loc("broadcast.8955")
#loc3744 = loc("divide.8956")
#loc3746 = loc("transpose.8969")
#loc3747 = loc("reshape.8970")
#loc3748 = loc("broadcast.9032")
#loc3749 = loc("multiply.9033")
#loc3751 = loc("add.9043")
#loc3752 = loc("convert.9044")
#loc3753 = loc("power.9046")
#loc3755 = loc("multiply.9062")
#loc3756 = loc("reshape.9063")
#loc3757 = loc("add.9067")
#loc3758 = loc("rsqrt.9068")
#loc3759 = loc("reshape.9069")
#loc3760 = loc("broadcast.9070")
#loc3761 = loc("multiply.9071")
#loc3762 = loc("multiply.9078")
#loc3763 = loc("convert.9079")
#loc3764 = loc("reshape.9166")
#loc3765 = loc("reshape.9162")
#loc3766 = loc("reshape.9164")
#loc3767 = loc("transpose.9165")
#loc3769 = loc("reshape.9168")
#loc3770 = loc("reshape.9158")
#loc3771 = loc("reshape.9160")
#loc3772 = loc("broadcast.9171")
#loc3773 = loc("add.9172")
#loc3774 = loc("reshape.9173")
#loc3775 = loc("transpose.9174")
#loc3776 = loc("slice.9175")
#loc3777 = loc("multiply.9192")
#loc3778 = loc("slice.9179")
#loc3779 = loc("multiply.9189")
#loc3780 = loc("subtract.9195")
#loc3781 = loc("multiply.9182")
#loc3782 = loc("multiply.9178")
#loc3783 = loc("add.9185")
#loc3784 = loc("concatenate.9196")
#loc3785 = loc("reshape.9113")
#loc3786 = loc("reshape.9115")
#loc3787 = loc("transpose.9116")
#loc3789 = loc("reshape.9119")
#loc3790 = loc("reshape.9109")
#loc3791 = loc("reshape.9111")
#loc3792 = loc("broadcast.9122")
#loc3793 = loc("add.9123")
#loc3794 = loc("reshape.9124")
#loc3795 = loc("transpose.9125")
#loc3796 = loc("slice.9126")
#loc3797 = loc("multiply.9144")
#loc3798 = loc("slice.9131")
#loc3799 = loc("multiply.9141")
#loc3800 = loc("subtract.9147")
#loc3801 = loc("multiply.9134")
#loc3802 = loc("multiply.9129")
#loc3803 = loc("add.9137")
#loc3804 = loc("concatenate.9148")
#loc3805 = loc("broadcast.9152")
#loc3806 = loc("reshape.9153")
#loc3807 = loc("transpose.9154")
#loc3808 = loc("dot.9197")
#loc3809 = loc("multiply.9200")
#loc3810 = loc("add.9205")
#loc3811 = loc("reshape.9096")
#loc3812 = loc("reshape.9101")
#loc3813 = loc("broadcast.9102")
#loc3814 = loc("concatenate.9206")
#loc3815 = loc("reshape.9479")
#loc3816 = loc("reshape.9481")
#loc3817 = loc("convert.9482")
#loc3818 = loc("broadcast.9483")
#loc3819 = loc("reduce.9213")
#loc3820 = loc("broadcast.9249")
#loc3821 = loc("subtract.9250")
#loc3822 = loc("reduce.9256")
#loc3823 = loc("broadcast.9257")
#loc3824 = loc("subtract.9258")
#loc3825 = loc("exponential.9259")
#loc3826 = loc("reduce.9265")
#loc3827 = loc("broadcast.9266")
#loc3828 = loc("divide.9267")
#loc3829 = loc("slice.9268")
#loc3830 = loc("reshape.758")
#loc3831 = loc("reshape.760")
#loc3832 = loc("transpose.761")
#loc3834 = loc("reshape.9082")
#loc3835 = loc("reshape.754")
#loc3836 = loc("reshape.756")
#loc3837 = loc("broadcast.9085")
#loc3838 = loc("add.9086")
#loc3839 = loc("reshape.9087")
#loc3840 = loc("transpose.9088")
#loc3841 = loc("broadcast.9092")
#loc3842 = loc("reshape.9093")
#loc3843 = loc("dot.9269")
#loc3844 = loc("transpose.9271")
#loc3845 = loc("reshape.9273")
#loc3846 = loc("reshape.748")
#loc3847 = loc("reshape.750")
#loc3848 = loc("transpose.751")
#loc3850 = loc("reshape.9275")
#loc3851 = loc("reshape.744")
#loc3852 = loc("reshape.746")
#loc3853 = loc("broadcast.9278")
#loc3854 = loc("add.9279")
#loc3855 = loc("add.9282")
#loc3856 = loc("reshape.9312")
#loc3857 = loc("reshape.9314")
#loc3858 = loc("convert.9315")
#loc3859 = loc("broadcast.9316")
#loc3860 = loc("convert.9283")
#loc3861 = loc("power.9285")
#loc3863 = loc("multiply.9301")
#loc3864 = loc("reshape.9302")
#loc3865 = loc("add.9306")
#loc3866 = loc("rsqrt.9307")
#loc3867 = loc("reshape.9308")
#loc3868 = loc("broadcast.9309")
#loc3869 = loc("multiply.9310")
#loc3870 = loc("multiply.9317")
#loc3871 = loc("convert.9318")
#loc3872 = loc("reshape.9397")
#loc3873 = loc("broadcast.9401")
#loc3875 = loc("reshape.9391")
#loc3876 = loc("reshape.9393")
#loc3877 = loc("broadcast.9408")
#loc3878 = loc("add.9409")
#loc3879 = loc("slice.9422")
#loc3880 = loc("clamp.9425")
#loc3881 = loc("add.9428")
#loc3882 = loc("slice.9410")
#loc3883 = loc("clamp.9413")
#loc3884 = loc("multiply.9415")
#loc3885 = loc("logistic.9416")
#loc3886 = loc("multiply.9417")
#loc3887 = loc("multiply.9429")
#loc3888 = loc("dot.9430")
#loc3889 = loc("reshape.9380")
#loc3890 = loc("reshape.9382")
#loc3891 = loc("broadcast.9434")
#loc3892 = loc("add.9435")
#loc3893 = loc("reshape.9436")
#loc3894 = loc("reshape.734")
#loc3895 = loc("reshape.736")
#loc3896 = loc("transpose.737")
#loc3898 = loc("reshape.727")
#loc3899 = loc("reshape.729")
#loc3900 = loc("broadcast.9325")
#loc3901 = loc("add.9326")
#loc3903 = loc("compare.9339")
#loc3904 = loc("slice.9344")
#loc3905 = loc("convert.9345")
#loc3906 = loc("reshape.9369")
#loc3907 = loc("concatenate.9370")
#loc3908 = loc("slice.9342")
#loc3909 = loc("reduce.9351")
#loc3910 = loc("broadcast.9352")
#loc3911 = loc("subtract.9353")
#loc3912 = loc("exponential.9354")
#loc3913 = loc("reduce.9360")
#loc3914 = loc("broadcast.9361")
#loc3915 = loc("divide.9362")
#loc3917 = loc("transpose.9375")
#loc3918 = loc("reshape.9376")
#loc3919 = loc("broadcast.9438")
#loc3920 = loc("multiply.9439")
#loc3922 = loc("add.9449")
#loc3923 = loc("convert.9450")
#loc3924 = loc("power.9452")
#loc3926 = loc("multiply.9468")
#loc3927 = loc("reshape.9469")
#loc3928 = loc("add.9473")
#loc3929 = loc("rsqrt.9474")
#loc3930 = loc("reshape.9475")
#loc3931 = loc("broadcast.9476")
#loc3932 = loc("multiply.9477")
#loc3933 = loc("multiply.9484")
#loc3934 = loc("convert.9485")
#loc3935 = loc("reshape.9572")
#loc3936 = loc("reshape.9568")
#loc3937 = loc("reshape.9570")
#loc3938 = loc("transpose.9571")
#loc3940 = loc("reshape.9574")
#loc3941 = loc("reshape.9564")
#loc3942 = loc("reshape.9566")
#loc3943 = loc("broadcast.9577")
#loc3944 = loc("add.9578")
#loc3945 = loc("reshape.9579")
#loc3946 = loc("transpose.9580")
#loc3947 = loc("slice.9581")
#loc3948 = loc("multiply.9598")
#loc3949 = loc("slice.9585")
#loc3950 = loc("multiply.9595")
#loc3951 = loc("subtract.9601")
#loc3952 = loc("multiply.9588")
#loc3953 = loc("multiply.9584")
#loc3954 = loc("add.9591")
#loc3955 = loc("concatenate.9602")
#loc3956 = loc("reshape.9519")
#loc3957 = loc("reshape.9521")
#loc3958 = loc("transpose.9522")
#loc3960 = loc("reshape.9525")
#loc3961 = loc("reshape.9515")
#loc3962 = loc("reshape.9517")
#loc3963 = loc("broadcast.9528")
#loc3964 = loc("add.9529")
#loc3965 = loc("reshape.9530")
#loc3966 = loc("transpose.9531")
#loc3967 = loc("slice.9532")
#loc3968 = loc("multiply.9550")
#loc3969 = loc("slice.9537")
#loc3970 = loc("multiply.9547")
#loc3971 = loc("subtract.9553")
#loc3972 = loc("multiply.9540")
#loc3973 = loc("multiply.9535")
#loc3974 = loc("add.9543")
#loc3975 = loc("concatenate.9554")
#loc3976 = loc("broadcast.9558")
#loc3977 = loc("reshape.9559")
#loc3978 = loc("transpose.9560")
#loc3979 = loc("dot.9603")
#loc3980 = loc("multiply.9606")
#loc3981 = loc("add.9611")
#loc3982 = loc("reshape.9502")
#loc3983 = loc("reshape.9507")
#loc3984 = loc("broadcast.9508")
#loc3985 = loc("concatenate.9612")
#loc3986 = loc("reshape.9885")
#loc3987 = loc("reshape.9887")
#loc3988 = loc("convert.9888")
#loc3989 = loc("broadcast.9889")
#loc3990 = loc("reduce.9619")
#loc3991 = loc("broadcast.9655")
#loc3992 = loc("subtract.9656")
#loc3993 = loc("reduce.9662")
#loc3994 = loc("broadcast.9663")
#loc3995 = loc("subtract.9664")
#loc3996 = loc("exponential.9665")
#loc3997 = loc("reduce.9671")
#loc3998 = loc("broadcast.9672")
#loc3999 = loc("divide.9673")
#loc4000 = loc("slice.9674")
#loc4001 = loc("reshape.716")
#loc4002 = loc("reshape.718")
#loc4003 = loc("transpose.719")
#loc4005 = loc("reshape.9488")
#loc4006 = loc("reshape.712")
#loc4007 = loc("reshape.714")
#loc4008 = loc("broadcast.9491")
#loc4009 = loc("add.9492")
#loc4010 = loc("reshape.9493")
#loc4011 = loc("transpose.9494")
#loc4012 = loc("broadcast.9498")
#loc4013 = loc("reshape.9499")
#loc4014 = loc("dot.9675")
#loc4015 = loc("transpose.9677")
#loc4016 = loc("reshape.9679")
#loc4017 = loc("reshape.706")
#loc4018 = loc("reshape.708")
#loc4019 = loc("transpose.709")
#loc4021 = loc("reshape.9681")
#loc4022 = loc("reshape.702")
#loc4023 = loc("reshape.704")
#loc4024 = loc("broadcast.9684")
#loc4025 = loc("add.9685")
#loc4026 = loc("add.9688")
#loc4027 = loc("reshape.9718")
#loc4028 = loc("reshape.9720")
#loc4029 = loc("convert.9721")
#loc4030 = loc("broadcast.9722")
#loc4031 = loc("convert.9689")
#loc4032 = loc("power.9691")
#loc4034 = loc("multiply.9707")
#loc4035 = loc("reshape.9708")
#loc4036 = loc("add.9712")
#loc4037 = loc("rsqrt.9713")
#loc4038 = loc("reshape.9714")
#loc4039 = loc("broadcast.9715")
#loc4040 = loc("multiply.9716")
#loc4041 = loc("multiply.9723")
#loc4042 = loc("convert.9724")
#loc4043 = loc("reshape.9803")
#loc4044 = loc("broadcast.9807")
#loc4046 = loc("reshape.9797")
#loc4047 = loc("reshape.9799")
#loc4048 = loc("broadcast.9814")
#loc4049 = loc("add.9815")
#loc4050 = loc("slice.9828")
#loc4051 = loc("clamp.9831")
#loc4052 = loc("add.9834")
#loc4053 = loc("slice.9816")
#loc4054 = loc("clamp.9819")
#loc4055 = loc("multiply.9821")
#loc4056 = loc("logistic.9822")
#loc4057 = loc("multiply.9823")
#loc4058 = loc("multiply.9835")
#loc4059 = loc("dot.9836")
#loc4060 = loc("reshape.9786")
#loc4061 = loc("reshape.9788")
#loc4062 = loc("broadcast.9840")
#loc4063 = loc("add.9841")
#loc4064 = loc("reshape.9842")
#loc4065 = loc("reshape.692")
#loc4066 = loc("reshape.694")
#loc4067 = loc("transpose.695")
#loc4069 = loc("reshape.685")
#loc4070 = loc("reshape.687")
#loc4071 = loc("broadcast.9731")
#loc4072 = loc("add.9732")
#loc4074 = loc("compare.9745")
#loc4075 = loc("slice.9750")
#loc4076 = loc("convert.9751")
#loc4077 = loc("reshape.9775")
#loc4078 = loc("concatenate.9776")
#loc4079 = loc("slice.9748")
#loc4080 = loc("reduce.9757")
#loc4081 = loc("broadcast.9758")
#loc4082 = loc("subtract.9759")
#loc4083 = loc("exponential.9760")
#loc4084 = loc("reduce.9766")
#loc4085 = loc("broadcast.9767")
#loc4086 = loc("divide.9768")
#loc4088 = loc("transpose.9781")
#loc4089 = loc("reshape.9782")
#loc4090 = loc("broadcast.9844")
#loc4091 = loc("multiply.9845")
#loc4093 = loc("add.9855")
#loc4094 = loc("convert.9856")
#loc4095 = loc("power.9858")
#loc4097 = loc("multiply.9874")
#loc4098 = loc("reshape.9875")
#loc4099 = loc("add.9879")
#loc4100 = loc("rsqrt.9880")
#loc4101 = loc("reshape.9881")
#loc4102 = loc("broadcast.9882")
#loc4103 = loc("multiply.9883")
#loc4104 = loc("multiply.9890")
#loc4105 = loc("convert.9891")
#loc4106 = loc("reshape.9978")
#loc4107 = loc("reshape.9974")
#loc4108 = loc("reshape.9976")
#loc4109 = loc("transpose.9977")
#loc4111 = loc("reshape.9980")
#loc4112 = loc("reshape.9970")
#loc4113 = loc("reshape.9972")
#loc4114 = loc("broadcast.9983")
#loc4115 = loc("add.9984")
#loc4116 = loc("reshape.9985")
#loc4117 = loc("transpose.9986")
#loc4118 = loc("slice.9987")
#loc4119 = loc("multiply.10004")
#loc4120 = loc("slice.9991")
#loc4121 = loc("multiply.10001")
#loc4122 = loc("subtract.10007")
#loc4123 = loc("multiply.9994")
#loc4124 = loc("multiply.9990")
#loc4125 = loc("add.9997")
#loc4126 = loc("concatenate.10008")
#loc4127 = loc("reshape.9925")
#loc4128 = loc("reshape.9927")
#loc4129 = loc("transpose.9928")
#loc4131 = loc("reshape.9931")
#loc4132 = loc("reshape.9921")
#loc4133 = loc("reshape.9923")
#loc4134 = loc("broadcast.9934")
#loc4135 = loc("add.9935")
#loc4136 = loc("reshape.9936")
#loc4137 = loc("transpose.9937")
#loc4138 = loc("slice.9938")
#loc4139 = loc("multiply.9956")
#loc4140 = loc("slice.9943")
#loc4141 = loc("multiply.9953")
#loc4142 = loc("subtract.9959")
#loc4143 = loc("multiply.9946")
#loc4144 = loc("multiply.9941")
#loc4145 = loc("add.9949")
#loc4146 = loc("concatenate.9960")
#loc4147 = loc("broadcast.9964")
#loc4148 = loc("reshape.9965")
#loc4149 = loc("transpose.9966")
#loc4150 = loc("dot.10009")
#loc4151 = loc("multiply.10012")
#loc4152 = loc("add.10017")
#loc4153 = loc("reshape.9908")
#loc4154 = loc("reshape.9913")
#loc4155 = loc("broadcast.9914")
#loc4156 = loc("concatenate.10018")
#loc4157 = loc("reshape.10291")
#loc4158 = loc("reshape.10293")
#loc4159 = loc("convert.10294")
#loc4160 = loc("broadcast.10295")
#loc4161 = loc("reduce.10025")
#loc4162 = loc("broadcast.10061")
#loc4163 = loc("subtract.10062")
#loc4164 = loc("reduce.10068")
#loc4165 = loc("broadcast.10069")
#loc4166 = loc("subtract.10070")
#loc4167 = loc("exponential.10071")
#loc4168 = loc("reduce.10077")
#loc4169 = loc("broadcast.10078")
#loc4170 = loc("divide.10079")
#loc4171 = loc("slice.10080")
#loc4172 = loc("reshape.674")
#loc4173 = loc("reshape.676")
#loc4174 = loc("transpose.677")
#loc4176 = loc("reshape.9894")
#loc4177 = loc("reshape.670")
#loc4178 = loc("reshape.672")
#loc4179 = loc("broadcast.9897")
#loc4180 = loc("add.9898")
#loc4181 = loc("reshape.9899")
#loc4182 = loc("transpose.9900")
#loc4183 = loc("broadcast.9904")
#loc4184 = loc("reshape.9905")
#loc4185 = loc("dot.10081")
#loc4186 = loc("transpose.10083")
#loc4187 = loc("reshape.10085")
#loc4188 = loc("reshape.664")
#loc4189 = loc("reshape.666")
#loc4190 = loc("transpose.667")
#loc4192 = loc("reshape.10087")
#loc4193 = loc("reshape.660")
#loc4194 = loc("reshape.662")
#loc4195 = loc("broadcast.10090")
#loc4196 = loc("add.10091")
#loc4197 = loc("add.10094")
#loc4198 = loc("reshape.10124")
#loc4199 = loc("reshape.10126")
#loc4200 = loc("convert.10127")
#loc4201 = loc("broadcast.10128")
#loc4202 = loc("convert.10095")
#loc4203 = loc("power.10097")
#loc4205 = loc("multiply.10113")
#loc4206 = loc("reshape.10114")
#loc4207 = loc("add.10118")
#loc4208 = loc("rsqrt.10119")
#loc4209 = loc("reshape.10120")
#loc4210 = loc("broadcast.10121")
#loc4211 = loc("multiply.10122")
#loc4212 = loc("multiply.10129")
#loc4213 = loc("convert.10130")
#loc4214 = loc("reshape.10209")
#loc4215 = loc("broadcast.10213")
#loc4217 = loc("reshape.10203")
#loc4218 = loc("reshape.10205")
#loc4219 = loc("broadcast.10220")
#loc4220 = loc("add.10221")
#loc4221 = loc("slice.10234")
#loc4222 = loc("clamp.10237")
#loc4223 = loc("add.10240")
#loc4224 = loc("slice.10222")
#loc4225 = loc("clamp.10225")
#loc4226 = loc("multiply.10227")
#loc4227 = loc("logistic.10228")
#loc4228 = loc("multiply.10229")
#loc4229 = loc("multiply.10241")
#loc4230 = loc("dot.10242")
#loc4231 = loc("reshape.10192")
#loc4232 = loc("reshape.10194")
#loc4233 = loc("broadcast.10246")
#loc4234 = loc("add.10247")
#loc4235 = loc("reshape.10248")
#loc4236 = loc("reshape.650")
#loc4237 = loc("reshape.652")
#loc4238 = loc("transpose.653")
#loc4240 = loc("reshape.643")
#loc4241 = loc("reshape.645")
#loc4242 = loc("broadcast.10137")
#loc4243 = loc("add.10138")
#loc4245 = loc("compare.10151")
#loc4246 = loc("slice.10156")
#loc4247 = loc("convert.10157")
#loc4248 = loc("reshape.10181")
#loc4249 = loc("concatenate.10182")
#loc4250 = loc("slice.10154")
#loc4251 = loc("reduce.10163")
#loc4252 = loc("broadcast.10164")
#loc4253 = loc("subtract.10165")
#loc4254 = loc("exponential.10166")
#loc4255 = loc("reduce.10172")
#loc4256 = loc("broadcast.10173")
#loc4257 = loc("divide.10174")
#loc4259 = loc("transpose.10187")
#loc4260 = loc("reshape.10188")
#loc4261 = loc("broadcast.10250")
#loc4262 = loc("multiply.10251")
#loc4264 = loc("add.10261")
#loc4265 = loc("convert.10262")
#loc4266 = loc("power.10264")
#loc4268 = loc("multiply.10280")
#loc4269 = loc("reshape.10281")
#loc4270 = loc("add.10285")
#loc4271 = loc("rsqrt.10286")
#loc4272 = loc("reshape.10287")
#loc4273 = loc("broadcast.10288")
#loc4274 = loc("multiply.10289")
#loc4275 = loc("multiply.10296")
#loc4276 = loc("convert.10297")
#loc4277 = loc("reshape.10384")
#loc4278 = loc("reshape.10380")
#loc4279 = loc("reshape.10382")
#loc4280 = loc("transpose.10383")
#loc4282 = loc("reshape.10386")
#loc4283 = loc("reshape.10376")
#loc4284 = loc("reshape.10378")
#loc4285 = loc("broadcast.10389")
#loc4286 = loc("add.10390")
#loc4287 = loc("reshape.10391")
#loc4288 = loc("transpose.10392")
#loc4289 = loc("slice.10393")
#loc4290 = loc("multiply.10410")
#loc4291 = loc("slice.10397")
#loc4292 = loc("multiply.10407")
#loc4293 = loc("subtract.10413")
#loc4294 = loc("multiply.10400")
#loc4295 = loc("multiply.10396")
#loc4296 = loc("add.10403")
#loc4297 = loc("concatenate.10414")
#loc4298 = loc("reshape.10331")
#loc4299 = loc("reshape.10333")
#loc4300 = loc("transpose.10334")
#loc4302 = loc("reshape.10337")
#loc4303 = loc("reshape.10327")
#loc4304 = loc("reshape.10329")
#loc4305 = loc("broadcast.10340")
#loc4306 = loc("add.10341")
#loc4307 = loc("reshape.10342")
#loc4308 = loc("transpose.10343")
#loc4309 = loc("slice.10344")
#loc4310 = loc("multiply.10362")
#loc4311 = loc("slice.10349")
#loc4312 = loc("multiply.10359")
#loc4313 = loc("subtract.10365")
#loc4314 = loc("multiply.10352")
#loc4315 = loc("multiply.10347")
#loc4316 = loc("add.10355")
#loc4317 = loc("concatenate.10366")
#loc4318 = loc("broadcast.10370")
#loc4319 = loc("reshape.10371")
#loc4320 = loc("transpose.10372")
#loc4321 = loc("dot.10415")
#loc4322 = loc("multiply.10418")
#loc4323 = loc("add.10423")
#loc4324 = loc("reshape.10314")
#loc4325 = loc("reshape.10319")
#loc4326 = loc("broadcast.10320")
#loc4327 = loc("concatenate.10424")
#loc4328 = loc("reshape.10697")
#loc4329 = loc("reshape.10699")
#loc4330 = loc("convert.10700")
#loc4331 = loc("broadcast.10701")
#loc4332 = loc("reduce.10431")
#loc4333 = loc("broadcast.10467")
#loc4334 = loc("subtract.10468")
#loc4335 = loc("reduce.10474")
#loc4336 = loc("broadcast.10475")
#loc4337 = loc("subtract.10476")
#loc4338 = loc("exponential.10477")
#loc4339 = loc("reduce.10483")
#loc4340 = loc("broadcast.10484")
#loc4341 = loc("divide.10485")
#loc4342 = loc("slice.10486")
#loc4343 = loc("reshape.632")
#loc4344 = loc("reshape.634")
#loc4345 = loc("transpose.635")
#loc4347 = loc("reshape.10300")
#loc4348 = loc("reshape.628")
#loc4349 = loc("reshape.630")
#loc4350 = loc("broadcast.10303")
#loc4351 = loc("add.10304")
#loc4352 = loc("reshape.10305")
#loc4353 = loc("transpose.10306")
#loc4354 = loc("broadcast.10310")
#loc4355 = loc("reshape.10311")
#loc4356 = loc("dot.10487")
#loc4357 = loc("transpose.10489")
#loc4358 = loc("reshape.10491")
#loc4359 = loc("reshape.622")
#loc4360 = loc("reshape.624")
#loc4361 = loc("transpose.625")
#loc4363 = loc("reshape.10493")
#loc4364 = loc("reshape.618")
#loc4365 = loc("reshape.620")
#loc4366 = loc("broadcast.10496")
#loc4367 = loc("add.10497")
#loc4368 = loc("add.10500")
#loc4369 = loc("reshape.10530")
#loc4370 = loc("reshape.10532")
#loc4371 = loc("convert.10533")
#loc4372 = loc("broadcast.10534")
#loc4373 = loc("convert.10501")
#loc4374 = loc("power.10503")
#loc4376 = loc("multiply.10519")
#loc4377 = loc("reshape.10520")
#loc4378 = loc("add.10524")
#loc4379 = loc("rsqrt.10525")
#loc4380 = loc("reshape.10526")
#loc4381 = loc("broadcast.10527")
#loc4382 = loc("multiply.10528")
#loc4383 = loc("multiply.10535")
#loc4384 = loc("convert.10536")
#loc4385 = loc("reshape.10615")
#loc4386 = loc("broadcast.10619")
#loc4388 = loc("reshape.10609")
#loc4389 = loc("reshape.10611")
#loc4390 = loc("broadcast.10626")
#loc4391 = loc("add.10627")
#loc4392 = loc("slice.10640")
#loc4393 = loc("clamp.10643")
#loc4394 = loc("add.10646")
#loc4395 = loc("slice.10628")
#loc4396 = loc("clamp.10631")
#loc4397 = loc("multiply.10633")
#loc4398 = loc("logistic.10634")
#loc4399 = loc("multiply.10635")
#loc4400 = loc("multiply.10647")
#loc4401 = loc("dot.10648")
#loc4402 = loc("reshape.10598")
#loc4403 = loc("reshape.10600")
#loc4404 = loc("broadcast.10652")
#loc4405 = loc("add.10653")
#loc4406 = loc("reshape.10654")
#loc4407 = loc("reshape.608")
#loc4408 = loc("reshape.610")
#loc4409 = loc("transpose.611")
#loc4411 = loc("reshape.601")
#loc4412 = loc("reshape.603")
#loc4413 = loc("broadcast.10543")
#loc4414 = loc("add.10544")
#loc4416 = loc("compare.10557")
#loc4417 = loc("slice.10562")
#loc4418 = loc("convert.10563")
#loc4419 = loc("reshape.10587")
#loc4420 = loc("concatenate.10588")
#loc4421 = loc("slice.10560")
#loc4422 = loc("reduce.10569")
#loc4423 = loc("broadcast.10570")
#loc4424 = loc("subtract.10571")
#loc4425 = loc("exponential.10572")
#loc4426 = loc("reduce.10578")
#loc4427 = loc("broadcast.10579")
#loc4428 = loc("divide.10580")
#loc4430 = loc("transpose.10593")
#loc4431 = loc("reshape.10594")
#loc4432 = loc("broadcast.10656")
#loc4433 = loc("multiply.10657")
#loc4435 = loc("add.10667")
#loc4436 = loc("convert.10668")
#loc4437 = loc("power.10670")
#loc4439 = loc("multiply.10686")
#loc4440 = loc("reshape.10687")
#loc4441 = loc("add.10691")
#loc4442 = loc("rsqrt.10692")
#loc4443 = loc("reshape.10693")
#loc4444 = loc("broadcast.10694")
#loc4445 = loc("multiply.10695")
#loc4446 = loc("multiply.10702")
#loc4447 = loc("convert.10703")
#loc4448 = loc("reshape.10790")
#loc4449 = loc("reshape.10786")
#loc4450 = loc("reshape.10788")
#loc4451 = loc("transpose.10789")
#loc4453 = loc("reshape.10792")
#loc4454 = loc("reshape.10782")
#loc4455 = loc("reshape.10784")
#loc4456 = loc("broadcast.10795")
#loc4457 = loc("add.10796")
#loc4458 = loc("reshape.10797")
#loc4459 = loc("transpose.10798")
#loc4460 = loc("slice.10799")
#loc4461 = loc("multiply.10816")
#loc4462 = loc("slice.10803")
#loc4463 = loc("multiply.10813")
#loc4464 = loc("subtract.10819")
#loc4465 = loc("multiply.10806")
#loc4466 = loc("multiply.10802")
#loc4467 = loc("add.10809")
#loc4468 = loc("concatenate.10820")
#loc4469 = loc("reshape.10737")
#loc4470 = loc("reshape.10739")
#loc4471 = loc("transpose.10740")
#loc4473 = loc("reshape.10743")
#loc4474 = loc("reshape.10733")
#loc4475 = loc("reshape.10735")
#loc4476 = loc("broadcast.10746")
#loc4477 = loc("add.10747")
#loc4478 = loc("reshape.10748")
#loc4479 = loc("transpose.10749")
#loc4480 = loc("slice.10750")
#loc4481 = loc("multiply.10768")
#loc4482 = loc("slice.10755")
#loc4483 = loc("multiply.10765")
#loc4484 = loc("subtract.10771")
#loc4485 = loc("multiply.10758")
#loc4486 = loc("multiply.10753")
#loc4487 = loc("add.10761")
#loc4488 = loc("concatenate.10772")
#loc4489 = loc("broadcast.10776")
#loc4490 = loc("reshape.10777")
#loc4491 = loc("transpose.10778")
#loc4492 = loc("dot.10821")
#loc4493 = loc("multiply.10824")
#loc4494 = loc("add.10829")
#loc4495 = loc("reshape.10720")
#loc4496 = loc("reshape.10725")
#loc4497 = loc("broadcast.10726")
#loc4498 = loc("concatenate.10830")
#loc4499 = loc("reshape.11103")
#loc4500 = loc("reshape.11105")
#loc4501 = loc("convert.11106")
#loc4502 = loc("broadcast.11107")
#loc4503 = loc("reduce.10837")
#loc4504 = loc("broadcast.10873")
#loc4505 = loc("subtract.10874")
#loc4506 = loc("reduce.10880")
#loc4507 = loc("broadcast.10881")
#loc4508 = loc("subtract.10882")
#loc4509 = loc("exponential.10883")
#loc4510 = loc("reduce.10889")
#loc4511 = loc("broadcast.10890")
#loc4512 = loc("divide.10891")
#loc4513 = loc("slice.10892")
#loc4514 = loc("reshape.590")
#loc4515 = loc("reshape.592")
#loc4516 = loc("transpose.593")
#loc4518 = loc("reshape.10706")
#loc4519 = loc("reshape.586")
#loc4520 = loc("reshape.588")
#loc4521 = loc("broadcast.10709")
#loc4522 = loc("add.10710")
#loc4523 = loc("reshape.10711")
#loc4524 = loc("transpose.10712")
#loc4525 = loc("broadcast.10716")
#loc4526 = loc("reshape.10717")
#loc4527 = loc("dot.10893")
#loc4528 = loc("transpose.10895")
#loc4529 = loc("reshape.10897")
#loc4530 = loc("reshape.580")
#loc4531 = loc("reshape.582")
#loc4532 = loc("transpose.583")
#loc4534 = loc("reshape.10899")
#loc4535 = loc("reshape.576")
#loc4536 = loc("reshape.578")
#loc4537 = loc("broadcast.10902")
#loc4538 = loc("add.10903")
#loc4539 = loc("add.10906")
#loc4540 = loc("reshape.10936")
#loc4541 = loc("reshape.10938")
#loc4542 = loc("convert.10939")
#loc4543 = loc("broadcast.10940")
#loc4544 = loc("convert.10907")
#loc4545 = loc("power.10909")
#loc4547 = loc("multiply.10925")
#loc4548 = loc("reshape.10926")
#loc4549 = loc("add.10930")
#loc4550 = loc("rsqrt.10931")
#loc4551 = loc("reshape.10932")
#loc4552 = loc("broadcast.10933")
#loc4553 = loc("multiply.10934")
#loc4554 = loc("multiply.10941")
#loc4555 = loc("convert.10942")
#loc4556 = loc("reshape.11021")
#loc4557 = loc("broadcast.11025")
#loc4559 = loc("reshape.11015")
#loc4560 = loc("reshape.11017")
#loc4561 = loc("broadcast.11032")
#loc4562 = loc("add.11033")
#loc4563 = loc("slice.11046")
#loc4564 = loc("clamp.11049")
#loc4565 = loc("add.11052")
#loc4566 = loc("slice.11034")
#loc4567 = loc("clamp.11037")
#loc4568 = loc("multiply.11039")
#loc4569 = loc("logistic.11040")
#loc4570 = loc("multiply.11041")
#loc4571 = loc("multiply.11053")
#loc4572 = loc("dot.11054")
#loc4573 = loc("reshape.11004")
#loc4574 = loc("reshape.11006")
#loc4575 = loc("broadcast.11058")
#loc4576 = loc("add.11059")
#loc4577 = loc("reshape.11060")
#loc4578 = loc("reshape.566")
#loc4579 = loc("reshape.568")
#loc4580 = loc("transpose.569")
#loc4582 = loc("reshape.559")
#loc4583 = loc("reshape.561")
#loc4584 = loc("broadcast.10949")
#loc4585 = loc("add.10950")
#loc4587 = loc("compare.10963")
#loc4588 = loc("slice.10968")
#loc4589 = loc("convert.10969")
#loc4590 = loc("reshape.10993")
#loc4591 = loc("concatenate.10994")
#loc4592 = loc("slice.10966")
#loc4593 = loc("reduce.10975")
#loc4594 = loc("broadcast.10976")
#loc4595 = loc("subtract.10977")
#loc4596 = loc("exponential.10978")
#loc4597 = loc("reduce.10984")
#loc4598 = loc("broadcast.10985")
#loc4599 = loc("divide.10986")
#loc4601 = loc("transpose.10999")
#loc4602 = loc("reshape.11000")
#loc4603 = loc("broadcast.11062")
#loc4604 = loc("multiply.11063")
#loc4606 = loc("add.11073")
#loc4607 = loc("convert.11074")
#loc4608 = loc("power.11076")
#loc4610 = loc("multiply.11092")
#loc4611 = loc("reshape.11093")
#loc4612 = loc("add.11097")
#loc4613 = loc("rsqrt.11098")
#loc4614 = loc("reshape.11099")
#loc4615 = loc("broadcast.11100")
#loc4616 = loc("multiply.11101")
#loc4617 = loc("multiply.11108")
#loc4618 = loc("convert.11109")
#loc4619 = loc("reshape.11196")
#loc4620 = loc("reshape.11192")
#loc4621 = loc("reshape.11194")
#loc4622 = loc("transpose.11195")
#loc4624 = loc("reshape.11198")
#loc4625 = loc("reshape.11188")
#loc4626 = loc("reshape.11190")
#loc4627 = loc("broadcast.11201")
#loc4628 = loc("add.11202")
#loc4629 = loc("reshape.11203")
#loc4630 = loc("transpose.11204")
#loc4631 = loc("slice.11205")
#loc4632 = loc("multiply.11222")
#loc4633 = loc("slice.11209")
#loc4634 = loc("multiply.11219")
#loc4635 = loc("subtract.11225")
#loc4636 = loc("multiply.11212")
#loc4637 = loc("multiply.11208")
#loc4638 = loc("add.11215")
#loc4639 = loc("concatenate.11226")
#loc4640 = loc("reshape.11143")
#loc4641 = loc("reshape.11145")
#loc4642 = loc("transpose.11146")
#loc4644 = loc("reshape.11149")
#loc4645 = loc("reshape.11139")
#loc4646 = loc("reshape.11141")
#loc4647 = loc("broadcast.11152")
#loc4648 = loc("add.11153")
#loc4649 = loc("reshape.11154")
#loc4650 = loc("transpose.11155")
#loc4651 = loc("slice.11156")
#loc4652 = loc("multiply.11174")
#loc4653 = loc("slice.11161")
#loc4654 = loc("multiply.11171")
#loc4655 = loc("subtract.11177")
#loc4656 = loc("multiply.11164")
#loc4657 = loc("multiply.11159")
#loc4658 = loc("add.11167")
#loc4659 = loc("concatenate.11178")
#loc4660 = loc("broadcast.11182")
#loc4661 = loc("reshape.11183")
#loc4662 = loc("transpose.11184")
#loc4663 = loc("dot.11227")
#loc4664 = loc("multiply.11230")
#loc4665 = loc("add.11235")
#loc4666 = loc("reshape.11126")
#loc4667 = loc("reshape.11131")
#loc4668 = loc("broadcast.11132")
#loc4669 = loc("concatenate.11236")
#loc4670 = loc("reshape.11509")
#loc4671 = loc("reshape.11511")
#loc4672 = loc("convert.11512")
#loc4673 = loc("broadcast.11513")
#loc4674 = loc("reduce.11243")
#loc4675 = loc("broadcast.11279")
#loc4676 = loc("subtract.11280")
#loc4677 = loc("reduce.11286")
#loc4678 = loc("broadcast.11287")
#loc4679 = loc("subtract.11288")
#loc4680 = loc("exponential.11289")
#loc4681 = loc("reduce.11295")
#loc4682 = loc("broadcast.11296")
#loc4683 = loc("divide.11297")
#loc4684 = loc("slice.11298")
#loc4685 = loc("reshape.548")
#loc4686 = loc("reshape.550")
#loc4687 = loc("transpose.551")
#loc4689 = loc("reshape.11112")
#loc4690 = loc("reshape.544")
#loc4691 = loc("reshape.546")
#loc4692 = loc("broadcast.11115")
#loc4693 = loc("add.11116")
#loc4694 = loc("reshape.11117")
#loc4695 = loc("transpose.11118")
#loc4696 = loc("broadcast.11122")
#loc4697 = loc("reshape.11123")
#loc4698 = loc("dot.11299")
#loc4699 = loc("transpose.11301")
#loc4700 = loc("reshape.11303")
#loc4701 = loc("reshape.538")
#loc4702 = loc("reshape.540")
#loc4703 = loc("transpose.541")
#loc4705 = loc("reshape.11305")
#loc4706 = loc("reshape.534")
#loc4707 = loc("reshape.536")
#loc4708 = loc("broadcast.11308")
#loc4709 = loc("add.11309")
#loc4710 = loc("add.11312")
#loc4711 = loc("reshape.11342")
#loc4712 = loc("reshape.11344")
#loc4713 = loc("convert.11345")
#loc4714 = loc("broadcast.11346")
#loc4715 = loc("convert.11313")
#loc4716 = loc("power.11315")
#loc4718 = loc("multiply.11331")
#loc4719 = loc("reshape.11332")
#loc4720 = loc("add.11336")
#loc4721 = loc("rsqrt.11337")
#loc4722 = loc("reshape.11338")
#loc4723 = loc("broadcast.11339")
#loc4724 = loc("multiply.11340")
#loc4725 = loc("multiply.11347")
#loc4726 = loc("convert.11348")
#loc4727 = loc("reshape.11427")
#loc4728 = loc("broadcast.11431")
#loc4730 = loc("reshape.11421")
#loc4731 = loc("reshape.11423")
#loc4732 = loc("broadcast.11438")
#loc4733 = loc("add.11439")
#loc4734 = loc("slice.11452")
#loc4735 = loc("clamp.11455")
#loc4736 = loc("add.11458")
#loc4737 = loc("slice.11440")
#loc4738 = loc("clamp.11443")
#loc4739 = loc("multiply.11445")
#loc4740 = loc("logistic.11446")
#loc4741 = loc("multiply.11447")
#loc4742 = loc("multiply.11459")
#loc4743 = loc("dot.11460")
#loc4744 = loc("reshape.11410")
#loc4745 = loc("reshape.11412")
#loc4746 = loc("broadcast.11464")
#loc4747 = loc("add.11465")
#loc4748 = loc("reshape.11466")
#loc4749 = loc("reshape.524")
#loc4750 = loc("reshape.526")
#loc4751 = loc("transpose.527")
#loc4753 = loc("reshape.517")
#loc4754 = loc("reshape.519")
#loc4755 = loc("broadcast.11355")
#loc4756 = loc("add.11356")
#loc4758 = loc("compare.11369")
#loc4759 = loc("slice.11374")
#loc4760 = loc("convert.11375")
#loc4761 = loc("reshape.11399")
#loc4762 = loc("concatenate.11400")
#loc4763 = loc("slice.11372")
#loc4764 = loc("reduce.11381")
#loc4765 = loc("broadcast.11382")
#loc4766 = loc("subtract.11383")
#loc4767 = loc("exponential.11384")
#loc4768 = loc("reduce.11390")
#loc4769 = loc("broadcast.11391")
#loc4770 = loc("divide.11392")
#loc4772 = loc("transpose.11405")
#loc4773 = loc("reshape.11406")
#loc4774 = loc("broadcast.11468")
#loc4775 = loc("multiply.11469")
#loc4777 = loc("add.11479")
#loc4778 = loc("convert.11480")
#loc4779 = loc("power.11482")
#loc4781 = loc("multiply.11498")
#loc4782 = loc("reshape.11499")
#loc4783 = loc("add.11503")
#loc4784 = loc("rsqrt.11504")
#loc4785 = loc("reshape.11505")
#loc4786 = loc("broadcast.11506")
#loc4787 = loc("multiply.11507")
#loc4788 = loc("multiply.11514")
#loc4789 = loc("convert.11515")
#loc4790 = loc("reshape.11602")
#loc4791 = loc("reshape.11598")
#loc4792 = loc("reshape.11600")
#loc4793 = loc("transpose.11601")
#loc4795 = loc("reshape.11604")
#loc4796 = loc("reshape.11594")
#loc4797 = loc("reshape.11596")
#loc4798 = loc("broadcast.11607")
#loc4799 = loc("add.11608")
#loc4800 = loc("reshape.11609")
#loc4801 = loc("transpose.11610")
#loc4802 = loc("slice.11611")
#loc4803 = loc("multiply.11628")
#loc4804 = loc("slice.11615")
#loc4805 = loc("multiply.11625")
#loc4806 = loc("subtract.11631")
#loc4807 = loc("multiply.11618")
#loc4808 = loc("multiply.11614")
#loc4809 = loc("add.11621")
#loc4810 = loc("concatenate.11632")
#loc4811 = loc("reshape.11549")
#loc4812 = loc("reshape.11551")
#loc4813 = loc("transpose.11552")
#loc4815 = loc("reshape.11555")
#loc4816 = loc("reshape.11545")
#loc4817 = loc("reshape.11547")
#loc4818 = loc("broadcast.11558")
#loc4819 = loc("add.11559")
#loc4820 = loc("reshape.11560")
#loc4821 = loc("transpose.11561")
#loc4822 = loc("slice.11562")
#loc4823 = loc("multiply.11580")
#loc4824 = loc("slice.11567")
#loc4825 = loc("multiply.11577")
#loc4826 = loc("subtract.11583")
#loc4827 = loc("multiply.11570")
#loc4828 = loc("multiply.11565")
#loc4829 = loc("add.11573")
#loc4830 = loc("concatenate.11584")
#loc4831 = loc("broadcast.11588")
#loc4832 = loc("reshape.11589")
#loc4833 = loc("transpose.11590")
#loc4834 = loc("dot.11633")
#loc4835 = loc("multiply.11636")
#loc4836 = loc("add.11641")
#loc4837 = loc("reshape.11532")
#loc4838 = loc("reshape.11537")
#loc4839 = loc("broadcast.11538")
#loc4840 = loc("concatenate.11642")
#loc4841 = loc("reshape.11915")
#loc4842 = loc("reshape.11917")
#loc4843 = loc("convert.11918")
#loc4844 = loc("broadcast.11919")
#loc4845 = loc("reduce.11649")
#loc4846 = loc("broadcast.11685")
#loc4847 = loc("subtract.11686")
#loc4848 = loc("reduce.11692")
#loc4849 = loc("broadcast.11693")
#loc4850 = loc("subtract.11694")
#loc4851 = loc("exponential.11695")
#loc4852 = loc("reduce.11701")
#loc4853 = loc("broadcast.11702")
#loc4854 = loc("divide.11703")
#loc4855 = loc("slice.11704")
#loc4856 = loc("reshape.506")
#loc4857 = loc("reshape.508")
#loc4858 = loc("transpose.509")
#loc4860 = loc("reshape.11518")
#loc4861 = loc("reshape.502")
#loc4862 = loc("reshape.504")
#loc4863 = loc("broadcast.11521")
#loc4864 = loc("add.11522")
#loc4865 = loc("reshape.11523")
#loc4866 = loc("transpose.11524")
#loc4867 = loc("broadcast.11528")
#loc4868 = loc("reshape.11529")
#loc4869 = loc("dot.11705")
#loc4870 = loc("transpose.11707")
#loc4871 = loc("reshape.11709")
#loc4872 = loc("reshape.496")
#loc4873 = loc("reshape.498")
#loc4874 = loc("transpose.499")
#loc4876 = loc("reshape.11711")
#loc4877 = loc("reshape.492")
#loc4878 = loc("reshape.494")
#loc4879 = loc("broadcast.11714")
#loc4880 = loc("add.11715")
#loc4881 = loc("add.11718")
#loc4882 = loc("reshape.11748")
#loc4883 = loc("reshape.11750")
#loc4884 = loc("convert.11751")
#loc4885 = loc("broadcast.11752")
#loc4886 = loc("convert.11719")
#loc4887 = loc("power.11721")
#loc4889 = loc("multiply.11737")
#loc4890 = loc("reshape.11738")
#loc4891 = loc("add.11742")
#loc4892 = loc("rsqrt.11743")
#loc4893 = loc("reshape.11744")
#loc4894 = loc("broadcast.11745")
#loc4895 = loc("multiply.11746")
#loc4896 = loc("multiply.11753")
#loc4897 = loc("convert.11754")
#loc4898 = loc("reshape.11833")
#loc4899 = loc("broadcast.11837")
#loc4901 = loc("reshape.11827")
#loc4902 = loc("reshape.11829")
#loc4903 = loc("broadcast.11844")
#loc4904 = loc("add.11845")
#loc4905 = loc("slice.11858")
#loc4906 = loc("clamp.11861")
#loc4907 = loc("add.11864")
#loc4908 = loc("slice.11846")
#loc4909 = loc("clamp.11849")
#loc4910 = loc("multiply.11851")
#loc4911 = loc("logistic.11852")
#loc4912 = loc("multiply.11853")
#loc4913 = loc("multiply.11865")
#loc4914 = loc("dot.11866")
#loc4915 = loc("reshape.11816")
#loc4916 = loc("reshape.11818")
#loc4917 = loc("broadcast.11870")
#loc4918 = loc("add.11871")
#loc4919 = loc("reshape.11872")
#loc4920 = loc("reshape.482")
#loc4921 = loc("reshape.484")
#loc4922 = loc("transpose.485")
#loc4924 = loc("reshape.475")
#loc4925 = loc("reshape.477")
#loc4926 = loc("broadcast.11761")
#loc4927 = loc("add.11762")
#loc4929 = loc("compare.11775")
#loc4930 = loc("slice.11780")
#loc4931 = loc("convert.11781")
#loc4932 = loc("reshape.11805")
#loc4933 = loc("concatenate.11806")
#loc4934 = loc("slice.11778")
#loc4935 = loc("reduce.11787")
#loc4936 = loc("broadcast.11788")
#loc4937 = loc("subtract.11789")
#loc4938 = loc("exponential.11790")
#loc4939 = loc("reduce.11796")
#loc4940 = loc("broadcast.11797")
#loc4941 = loc("divide.11798")
#loc4943 = loc("transpose.11811")
#loc4944 = loc("reshape.11812")
#loc4945 = loc("broadcast.11874")
#loc4946 = loc("multiply.11875")
#loc4948 = loc("add.11885")
#loc4949 = loc("convert.11886")
#loc4950 = loc("power.11888")
#loc4952 = loc("multiply.11904")
#loc4953 = loc("reshape.11905")
#loc4954 = loc("add.11909")
#loc4955 = loc("rsqrt.11910")
#loc4956 = loc("reshape.11911")
#loc4957 = loc("broadcast.11912")
#loc4958 = loc("multiply.11913")
#loc4959 = loc("multiply.11920")
#loc4960 = loc("convert.11921")
#loc4961 = loc("reshape.12008")
#loc4962 = loc("reshape.12004")
#loc4963 = loc("reshape.12006")
#loc4964 = loc("transpose.12007")
#loc4966 = loc("reshape.12010")
#loc4967 = loc("reshape.12000")
#loc4968 = loc("reshape.12002")
#loc4969 = loc("broadcast.12013")
#loc4970 = loc("add.12014")
#loc4971 = loc("reshape.12015")
#loc4972 = loc("transpose.12016")
#loc4973 = loc("slice.12017")
#loc4974 = loc("multiply.12034")
#loc4975 = loc("slice.12021")
#loc4976 = loc("multiply.12031")
#loc4977 = loc("subtract.12037")
#loc4978 = loc("multiply.12024")
#loc4979 = loc("multiply.12020")
#loc4980 = loc("add.12027")
#loc4981 = loc("concatenate.12038")
#loc4982 = loc("reshape.11955")
#loc4983 = loc("reshape.11957")
#loc4984 = loc("transpose.11958")
#loc4986 = loc("reshape.11961")
#loc4987 = loc("reshape.11951")
#loc4988 = loc("reshape.11953")
#loc4989 = loc("broadcast.11964")
#loc4990 = loc("add.11965")
#loc4991 = loc("reshape.11966")
#loc4992 = loc("transpose.11967")
#loc4993 = loc("slice.11968")
#loc4994 = loc("multiply.11986")
#loc4995 = loc("slice.11973")
#loc4996 = loc("multiply.11983")
#loc4997 = loc("subtract.11989")
#loc4998 = loc("multiply.11976")
#loc4999 = loc("multiply.11971")
#loc5000 = loc("add.11979")
#loc5001 = loc("concatenate.11990")
#loc5002 = loc("broadcast.11994")
#loc5003 = loc("reshape.11995")
#loc5004 = loc("transpose.11996")
#loc5005 = loc("dot.12039")
#loc5006 = loc("multiply.12042")
#loc5007 = loc("add.12047")
#loc5008 = loc("reshape.11938")
#loc5009 = loc("reshape.11943")
#loc5010 = loc("broadcast.11944")
#loc5011 = loc("concatenate.12048")
#loc5012 = loc("reshape.12321")
#loc5013 = loc("reshape.12323")
#loc5014 = loc("convert.12324")
#loc5015 = loc("broadcast.12325")
#loc5016 = loc("reduce.12055")
#loc5017 = loc("broadcast.12091")
#loc5018 = loc("subtract.12092")
#loc5019 = loc("reduce.12098")
#loc5020 = loc("broadcast.12099")
#loc5021 = loc("subtract.12100")
#loc5022 = loc("exponential.12101")
#loc5023 = loc("reduce.12107")
#loc5024 = loc("broadcast.12108")
#loc5025 = loc("divide.12109")
#loc5026 = loc("slice.12110")
#loc5027 = loc("reshape.464")
#loc5028 = loc("reshape.466")
#loc5029 = loc("transpose.467")
#loc5031 = loc("reshape.11924")
#loc5032 = loc("reshape.460")
#loc5033 = loc("reshape.462")
#loc5034 = loc("broadcast.11927")
#loc5035 = loc("add.11928")
#loc5036 = loc("reshape.11929")
#loc5037 = loc("transpose.11930")
#loc5038 = loc("broadcast.11934")
#loc5039 = loc("reshape.11935")
#loc5040 = loc("dot.12111")
#loc5041 = loc("transpose.12113")
#loc5042 = loc("reshape.12115")
#loc5043 = loc("reshape.454")
#loc5044 = loc("reshape.456")
#loc5045 = loc("transpose.457")
#loc5047 = loc("reshape.12117")
#loc5048 = loc("reshape.450")
#loc5049 = loc("reshape.452")
#loc5050 = loc("broadcast.12120")
#loc5051 = loc("add.12121")
#loc5052 = loc("add.12124")
#loc5053 = loc("reshape.12154")
#loc5054 = loc("reshape.12156")
#loc5055 = loc("convert.12157")
#loc5056 = loc("broadcast.12158")
#loc5057 = loc("convert.12125")
#loc5058 = loc("power.12127")
#loc5060 = loc("multiply.12143")
#loc5061 = loc("reshape.12144")
#loc5062 = loc("add.12148")
#loc5063 = loc("rsqrt.12149")
#loc5064 = loc("reshape.12150")
#loc5065 = loc("broadcast.12151")
#loc5066 = loc("multiply.12152")
#loc5067 = loc("multiply.12159")
#loc5068 = loc("convert.12160")
#loc5069 = loc("reshape.12239")
#loc5070 = loc("broadcast.12243")
#loc5072 = loc("reshape.12233")
#loc5073 = loc("reshape.12235")
#loc5074 = loc("broadcast.12250")
#loc5075 = loc("add.12251")
#loc5076 = loc("slice.12264")
#loc5077 = loc("clamp.12267")
#loc5078 = loc("add.12270")
#loc5079 = loc("slice.12252")
#loc5080 = loc("clamp.12255")
#loc5081 = loc("multiply.12257")
#loc5082 = loc("logistic.12258")
#loc5083 = loc("multiply.12259")
#loc5084 = loc("multiply.12271")
#loc5085 = loc("dot.12272")
#loc5086 = loc("reshape.12222")
#loc5087 = loc("reshape.12224")
#loc5088 = loc("broadcast.12276")
#loc5089 = loc("add.12277")
#loc5090 = loc("reshape.12278")
#loc5091 = loc("reshape.440")
#loc5092 = loc("reshape.442")
#loc5093 = loc("transpose.443")
#loc5095 = loc("reshape.433")
#loc5096 = loc("reshape.435")
#loc5097 = loc("broadcast.12167")
#loc5098 = loc("add.12168")
#loc5100 = loc("compare.12181")
#loc5101 = loc("slice.12186")
#loc5102 = loc("convert.12187")
#loc5103 = loc("reshape.12211")
#loc5104 = loc("concatenate.12212")
#loc5105 = loc("slice.12184")
#loc5106 = loc("reduce.12193")
#loc5107 = loc("broadcast.12194")
#loc5108 = loc("subtract.12195")
#loc5109 = loc("exponential.12196")
#loc5110 = loc("reduce.12202")
#loc5111 = loc("broadcast.12203")
#loc5112 = loc("divide.12204")
#loc5114 = loc("transpose.12217")
#loc5115 = loc("reshape.12218")
#loc5116 = loc("broadcast.12280")
#loc5117 = loc("multiply.12281")
#loc5119 = loc("add.12291")
#loc5120 = loc("convert.12292")
#loc5121 = loc("power.12294")
#loc5123 = loc("multiply.12310")
#loc5124 = loc("reshape.12311")
#loc5125 = loc("add.12315")
#loc5126 = loc("rsqrt.12316")
#loc5127 = loc("reshape.12317")
#loc5128 = loc("broadcast.12318")
#loc5129 = loc("multiply.12319")
#loc5130 = loc("multiply.12326")
#loc5131 = loc("convert.12327")
#loc5132 = loc("reshape.12414")
#loc5133 = loc("reshape.12410")
#loc5134 = loc("reshape.12412")
#loc5135 = loc("transpose.12413")
#loc5137 = loc("reshape.12416")
#loc5138 = loc("reshape.12406")
#loc5139 = loc("reshape.12408")
#loc5140 = loc("broadcast.12419")
#loc5141 = loc("add.12420")
#loc5142 = loc("reshape.12421")
#loc5143 = loc("transpose.12422")
#loc5144 = loc("slice.12423")
#loc5145 = loc("multiply.12440")
#loc5146 = loc("slice.12427")
#loc5147 = loc("multiply.12437")
#loc5148 = loc("subtract.12443")
#loc5149 = loc("multiply.12430")
#loc5150 = loc("multiply.12426")
#loc5151 = loc("add.12433")
#loc5152 = loc("concatenate.12444")
#loc5153 = loc("reshape.12361")
#loc5154 = loc("reshape.12363")
#loc5155 = loc("transpose.12364")
#loc5157 = loc("reshape.12367")
#loc5158 = loc("reshape.12357")
#loc5159 = loc("reshape.12359")
#loc5160 = loc("broadcast.12370")
#loc5161 = loc("add.12371")
#loc5162 = loc("reshape.12372")
#loc5163 = loc("transpose.12373")
#loc5164 = loc("slice.12374")
#loc5165 = loc("multiply.12392")
#loc5166 = loc("slice.12379")
#loc5167 = loc("multiply.12389")
#loc5168 = loc("subtract.12395")
#loc5169 = loc("multiply.12382")
#loc5170 = loc("multiply.12377")
#loc5171 = loc("add.12385")
#loc5172 = loc("concatenate.12396")
#loc5173 = loc("broadcast.12400")
#loc5174 = loc("reshape.12401")
#loc5175 = loc("transpose.12402")
#loc5176 = loc("dot.12445")
#loc5177 = loc("multiply.12448")
#loc5178 = loc("add.12453")
#loc5179 = loc("reshape.12344")
#loc5180 = loc("reshape.12349")
#loc5181 = loc("broadcast.12350")
#loc5182 = loc("concatenate.12454")
#loc5183 = loc("reshape.12727")
#loc5184 = loc("reshape.12729")
#loc5185 = loc("convert.12730")
#loc5186 = loc("broadcast.12731")
#loc5187 = loc("reduce.12461")
#loc5188 = loc("broadcast.12497")
#loc5189 = loc("subtract.12498")
#loc5190 = loc("reduce.12504")
#loc5191 = loc("broadcast.12505")
#loc5192 = loc("subtract.12506")
#loc5193 = loc("exponential.12507")
#loc5194 = loc("reduce.12513")
#loc5195 = loc("broadcast.12514")
#loc5196 = loc("divide.12515")
#loc5197 = loc("slice.12516")
#loc5198 = loc("reshape.422")
#loc5199 = loc("reshape.424")
#loc5200 = loc("transpose.425")
#loc5202 = loc("reshape.12330")
#loc5203 = loc("reshape.418")
#loc5204 = loc("reshape.420")
#loc5205 = loc("broadcast.12333")
#loc5206 = loc("add.12334")
#loc5207 = loc("reshape.12335")
#loc5208 = loc("transpose.12336")
#loc5209 = loc("broadcast.12340")
#loc5210 = loc("reshape.12341")
#loc5211 = loc("dot.12517")
#loc5212 = loc("transpose.12519")
#loc5213 = loc("reshape.12521")
#loc5214 = loc("reshape.412")
#loc5215 = loc("reshape.414")
#loc5216 = loc("transpose.415")
#loc5218 = loc("reshape.12523")
#loc5219 = loc("reshape.408")
#loc5220 = loc("reshape.410")
#loc5221 = loc("broadcast.12526")
#loc5222 = loc("add.12527")
#loc5223 = loc("add.12530")
#loc5224 = loc("reshape.12560")
#loc5225 = loc("reshape.12562")
#loc5226 = loc("convert.12563")
#loc5227 = loc("broadcast.12564")
#loc5228 = loc("convert.12531")
#loc5229 = loc("power.12533")
#loc5231 = loc("multiply.12549")
#loc5232 = loc("reshape.12550")
#loc5233 = loc("add.12554")
#loc5234 = loc("rsqrt.12555")
#loc5235 = loc("reshape.12556")
#loc5236 = loc("broadcast.12557")
#loc5237 = loc("multiply.12558")
#loc5238 = loc("multiply.12565")
#loc5239 = loc("convert.12566")
#loc5240 = loc("reshape.12645")
#loc5241 = loc("broadcast.12649")
#loc5243 = loc("reshape.12639")
#loc5244 = loc("reshape.12641")
#loc5245 = loc("broadcast.12656")
#loc5246 = loc("add.12657")
#loc5247 = loc("slice.12670")
#loc5248 = loc("clamp.12673")
#loc5249 = loc("add.12676")
#loc5250 = loc("slice.12658")
#loc5251 = loc("clamp.12661")
#loc5252 = loc("multiply.12663")
#loc5253 = loc("logistic.12664")
#loc5254 = loc("multiply.12665")
#loc5255 = loc("multiply.12677")
#loc5256 = loc("dot.12678")
#loc5257 = loc("reshape.12628")
#loc5258 = loc("reshape.12630")
#loc5259 = loc("broadcast.12682")
#loc5260 = loc("add.12683")
#loc5261 = loc("reshape.12684")
#loc5262 = loc("reshape.398")
#loc5263 = loc("reshape.400")
#loc5264 = loc("transpose.401")
#loc5266 = loc("reshape.391")
#loc5267 = loc("reshape.393")
#loc5268 = loc("broadcast.12573")
#loc5269 = loc("add.12574")
#loc5271 = loc("compare.12587")
#loc5272 = loc("slice.12592")
#loc5273 = loc("convert.12593")
#loc5274 = loc("reshape.12617")
#loc5275 = loc("concatenate.12618")
#loc5276 = loc("slice.12590")
#loc5277 = loc("reduce.12599")
#loc5278 = loc("broadcast.12600")
#loc5279 = loc("subtract.12601")
#loc5280 = loc("exponential.12602")
#loc5281 = loc("reduce.12608")
#loc5282 = loc("broadcast.12609")
#loc5283 = loc("divide.12610")
#loc5285 = loc("transpose.12623")
#loc5286 = loc("reshape.12624")
#loc5287 = loc("broadcast.12686")
#loc5288 = loc("multiply.12687")
#loc5290 = loc("add.12697")
#loc5291 = loc("convert.12698")
#loc5292 = loc("power.12700")
#loc5294 = loc("multiply.12716")
#loc5295 = loc("reshape.12717")
#loc5296 = loc("add.12721")
#loc5297 = loc("rsqrt.12722")
#loc5298 = loc("reshape.12723")
#loc5299 = loc("broadcast.12724")
#loc5300 = loc("multiply.12725")
#loc5301 = loc("multiply.12732")
#loc5302 = loc("convert.12733")
#loc5303 = loc("reshape.12820")
#loc5304 = loc("reshape.12816")
#loc5305 = loc("reshape.12818")
#loc5306 = loc("transpose.12819")
#loc5308 = loc("reshape.12822")
#loc5309 = loc("reshape.12812")
#loc5310 = loc("reshape.12814")
#loc5311 = loc("broadcast.12825")
#loc5312 = loc("add.12826")
#loc5313 = loc("reshape.12827")
#loc5314 = loc("transpose.12828")
#loc5315 = loc("slice.12829")
#loc5316 = loc("multiply.12846")
#loc5317 = loc("slice.12833")
#loc5318 = loc("multiply.12843")
#loc5319 = loc("subtract.12849")
#loc5320 = loc("multiply.12836")
#loc5321 = loc("multiply.12832")
#loc5322 = loc("add.12839")
#loc5323 = loc("concatenate.12850")
#loc5324 = loc("reshape.12767")
#loc5325 = loc("reshape.12769")
#loc5326 = loc("transpose.12770")
#loc5328 = loc("reshape.12773")
#loc5329 = loc("reshape.12763")
#loc5330 = loc("reshape.12765")
#loc5331 = loc("broadcast.12776")
#loc5332 = loc("add.12777")
#loc5333 = loc("reshape.12778")
#loc5334 = loc("transpose.12779")
#loc5335 = loc("slice.12780")
#loc5336 = loc("multiply.12798")
#loc5337 = loc("slice.12785")
#loc5338 = loc("multiply.12795")
#loc5339 = loc("subtract.12801")
#loc5340 = loc("multiply.12788")
#loc5341 = loc("multiply.12783")
#loc5342 = loc("add.12791")
#loc5343 = loc("concatenate.12802")
#loc5344 = loc("broadcast.12806")
#loc5345 = loc("reshape.12807")
#loc5346 = loc("transpose.12808")
#loc5347 = loc("dot.12851")
#loc5348 = loc("multiply.12854")
#loc5349 = loc("add.12859")
#loc5350 = loc("reshape.12750")
#loc5351 = loc("reshape.12755")
#loc5352 = loc("broadcast.12756")
#loc5353 = loc("concatenate.12860")
#loc5354 = loc("reshape.13133")
#loc5355 = loc("reshape.13135")
#loc5356 = loc("convert.13136")
#loc5357 = loc("broadcast.13137")
#loc5358 = loc("reduce.12867")
#loc5359 = loc("broadcast.12903")
#loc5360 = loc("subtract.12904")
#loc5361 = loc("reduce.12910")
#loc5362 = loc("broadcast.12911")
#loc5363 = loc("subtract.12912")
#loc5364 = loc("exponential.12913")
#loc5365 = loc("reduce.12919")
#loc5366 = loc("broadcast.12920")
#loc5367 = loc("divide.12921")
#loc5368 = loc("slice.12922")
#loc5369 = loc("reshape.380")
#loc5370 = loc("reshape.382")
#loc5371 = loc("transpose.383")
#loc5373 = loc("reshape.12736")
#loc5374 = loc("reshape.376")
#loc5375 = loc("reshape.378")
#loc5376 = loc("broadcast.12739")
#loc5377 = loc("add.12740")
#loc5378 = loc("reshape.12741")
#loc5379 = loc("transpose.12742")
#loc5380 = loc("broadcast.12746")
#loc5381 = loc("reshape.12747")
#loc5382 = loc("dot.12923")
#loc5383 = loc("transpose.12925")
#loc5384 = loc("reshape.12927")
#loc5385 = loc("reshape.370")
#loc5386 = loc("reshape.372")
#loc5387 = loc("transpose.373")
#loc5389 = loc("reshape.12929")
#loc5390 = loc("reshape.366")
#loc5391 = loc("reshape.368")
#loc5392 = loc("broadcast.12932")
#loc5393 = loc("add.12933")
#loc5394 = loc("add.12936")
#loc5395 = loc("reshape.12966")
#loc5396 = loc("reshape.12968")
#loc5397 = loc("convert.12969")
#loc5398 = loc("broadcast.12970")
#loc5399 = loc("convert.12937")
#loc5400 = loc("power.12939")
#loc5402 = loc("multiply.12955")
#loc5403 = loc("reshape.12956")
#loc5404 = loc("add.12960")
#loc5405 = loc("rsqrt.12961")
#loc5406 = loc("reshape.12962")
#loc5407 = loc("broadcast.12963")
#loc5408 = loc("multiply.12964")
#loc5409 = loc("multiply.12971")
#loc5410 = loc("convert.12972")
#loc5411 = loc("reshape.13051")
#loc5412 = loc("broadcast.13055")
#loc5414 = loc("reshape.13045")
#loc5415 = loc("reshape.13047")
#loc5416 = loc("broadcast.13062")
#loc5417 = loc("add.13063")
#loc5418 = loc("slice.13076")
#loc5419 = loc("clamp.13079")
#loc5420 = loc("add.13082")
#loc5421 = loc("slice.13064")
#loc5422 = loc("clamp.13067")
#loc5423 = loc("multiply.13069")
#loc5424 = loc("logistic.13070")
#loc5425 = loc("multiply.13071")
#loc5426 = loc("multiply.13083")
#loc5427 = loc("dot.13084")
#loc5428 = loc("reshape.13034")
#loc5429 = loc("reshape.13036")
#loc5430 = loc("broadcast.13088")
#loc5431 = loc("add.13089")
#loc5432 = loc("reshape.13090")
#loc5433 = loc("reshape.356")
#loc5434 = loc("reshape.358")
#loc5435 = loc("transpose.359")
#loc5437 = loc("reshape.349")
#loc5438 = loc("reshape.351")
#loc5439 = loc("broadcast.12979")
#loc5440 = loc("add.12980")
#loc5442 = loc("compare.12993")
#loc5443 = loc("slice.12998")
#loc5444 = loc("convert.12999")
#loc5445 = loc("reshape.13023")
#loc5446 = loc("concatenate.13024")
#loc5447 = loc("slice.12996")
#loc5448 = loc("reduce.13005")
#loc5449 = loc("broadcast.13006")
#loc5450 = loc("subtract.13007")
#loc5451 = loc("exponential.13008")
#loc5452 = loc("reduce.13014")
#loc5453 = loc("broadcast.13015")
#loc5454 = loc("divide.13016")
#loc5456 = loc("transpose.13029")
#loc5457 = loc("reshape.13030")
#loc5458 = loc("broadcast.13092")
#loc5459 = loc("multiply.13093")
#loc5461 = loc("add.13103")
#loc5462 = loc("convert.13104")
#loc5463 = loc("power.13106")
#loc5465 = loc("multiply.13122")
#loc5466 = loc("reshape.13123")
#loc5467 = loc("add.13127")
#loc5468 = loc("rsqrt.13128")
#loc5469 = loc("reshape.13129")
#loc5470 = loc("broadcast.13130")
#loc5471 = loc("multiply.13131")
#loc5472 = loc("multiply.13138")
#loc5473 = loc("convert.13139")
#loc5474 = loc("reshape.13226")
#loc5475 = loc("reshape.13222")
#loc5476 = loc("reshape.13224")
#loc5477 = loc("transpose.13225")
#loc5479 = loc("reshape.13228")
#loc5480 = loc("reshape.13218")
#loc5481 = loc("reshape.13220")
#loc5482 = loc("broadcast.13231")
#loc5483 = loc("add.13232")
#loc5484 = loc("reshape.13233")
#loc5485 = loc("transpose.13234")
#loc5486 = loc("slice.13235")
#loc5487 = loc("multiply.13252")
#loc5488 = loc("slice.13239")
#loc5489 = loc("multiply.13249")
#loc5490 = loc("subtract.13255")
#loc5491 = loc("multiply.13242")
#loc5492 = loc("multiply.13238")
#loc5493 = loc("add.13245")
#loc5494 = loc("concatenate.13256")
#loc5495 = loc("reshape.13173")
#loc5496 = loc("reshape.13175")
#loc5497 = loc("transpose.13176")
#loc5499 = loc("reshape.13179")
#loc5500 = loc("reshape.13169")
#loc5501 = loc("reshape.13171")
#loc5502 = loc("broadcast.13182")
#loc5503 = loc("add.13183")
#loc5504 = loc("reshape.13184")
#loc5505 = loc("transpose.13185")
#loc5506 = loc("slice.13186")
#loc5507 = loc("multiply.13204")
#loc5508 = loc("slice.13191")
#loc5509 = loc("multiply.13201")
#loc5510 = loc("subtract.13207")
#loc5511 = loc("multiply.13194")
#loc5512 = loc("multiply.13189")
#loc5513 = loc("add.13197")
#loc5514 = loc("concatenate.13208")
#loc5515 = loc("broadcast.13212")
#loc5516 = loc("reshape.13213")
#loc5517 = loc("transpose.13214")
#loc5518 = loc("dot.13257")
#loc5519 = loc("multiply.13260")
#loc5520 = loc("add.13265")
#loc5521 = loc("reshape.13156")
#loc5522 = loc("reshape.13161")
#loc5523 = loc("broadcast.13162")
#loc5524 = loc("concatenate.13266")
#loc5525 = loc("reshape.13539")
#loc5526 = loc("reshape.13541")
#loc5527 = loc("convert.13542")
#loc5528 = loc("broadcast.13543")
#loc5529 = loc("reduce.13273")
#loc5530 = loc("broadcast.13309")
#loc5531 = loc("subtract.13310")
#loc5532 = loc("reduce.13316")
#loc5533 = loc("broadcast.13317")
#loc5534 = loc("subtract.13318")
#loc5535 = loc("exponential.13319")
#loc5536 = loc("reduce.13325")
#loc5537 = loc("broadcast.13326")
#loc5538 = loc("divide.13327")
#loc5539 = loc("slice.13328")
#loc5540 = loc("reshape.338")
#loc5541 = loc("reshape.340")
#loc5542 = loc("transpose.341")
#loc5544 = loc("reshape.13142")
#loc5545 = loc("reshape.334")
#loc5546 = loc("reshape.336")
#loc5547 = loc("broadcast.13145")
#loc5548 = loc("add.13146")
#loc5549 = loc("reshape.13147")
#loc5550 = loc("transpose.13148")
#loc5551 = loc("broadcast.13152")
#loc5552 = loc("reshape.13153")
#loc5553 = loc("dot.13329")
#loc5554 = loc("transpose.13331")
#loc5555 = loc("reshape.13333")
#loc5556 = loc("reshape.328")
#loc5557 = loc("reshape.330")
#loc5558 = loc("transpose.331")
#loc5560 = loc("reshape.13335")
#loc5561 = loc("reshape.324")
#loc5562 = loc("reshape.326")
#loc5563 = loc("broadcast.13338")
#loc5564 = loc("add.13339")
#loc5565 = loc("add.13342")
#loc5566 = loc("reshape.13372")
#loc5567 = loc("reshape.13374")
#loc5568 = loc("convert.13375")
#loc5569 = loc("broadcast.13376")
#loc5570 = loc("convert.13343")
#loc5571 = loc("power.13345")
#loc5573 = loc("multiply.13361")
#loc5574 = loc("reshape.13362")
#loc5575 = loc("add.13366")
#loc5576 = loc("rsqrt.13367")
#loc5577 = loc("reshape.13368")
#loc5578 = loc("broadcast.13369")
#loc5579 = loc("multiply.13370")
#loc5580 = loc("multiply.13377")
#loc5581 = loc("convert.13378")
#loc5582 = loc("reshape.13457")
#loc5583 = loc("broadcast.13461")
#loc5585 = loc("reshape.13451")
#loc5586 = loc("reshape.13453")
#loc5587 = loc("broadcast.13468")
#loc5588 = loc("add.13469")
#loc5589 = loc("slice.13482")
#loc5590 = loc("clamp.13485")
#loc5591 = loc("add.13488")
#loc5592 = loc("slice.13470")
#loc5593 = loc("clamp.13473")
#loc5594 = loc("multiply.13475")
#loc5595 = loc("logistic.13476")
#loc5596 = loc("multiply.13477")
#loc5597 = loc("multiply.13489")
#loc5598 = loc("dot.13490")
#loc5599 = loc("reshape.13440")
#loc5600 = loc("reshape.13442")
#loc5601 = loc("broadcast.13494")
#loc5602 = loc("add.13495")
#loc5603 = loc("reshape.13496")
#loc5604 = loc("reshape.314")
#loc5605 = loc("reshape.316")
#loc5606 = loc("transpose.317")
#loc5608 = loc("reshape.307")
#loc5609 = loc("reshape.309")
#loc5610 = loc("broadcast.13385")
#loc5611 = loc("add.13386")
#loc5613 = loc("compare.13399")
#loc5614 = loc("slice.13404")
#loc5615 = loc("convert.13405")
#loc5616 = loc("reshape.13429")
#loc5617 = loc("concatenate.13430")
#loc5618 = loc("slice.13402")
#loc5619 = loc("reduce.13411")
#loc5620 = loc("broadcast.13412")
#loc5621 = loc("subtract.13413")
#loc5622 = loc("exponential.13414")
#loc5623 = loc("reduce.13420")
#loc5624 = loc("broadcast.13421")
#loc5625 = loc("divide.13422")
#loc5627 = loc("transpose.13435")
#loc5628 = loc("reshape.13436")
#loc5629 = loc("broadcast.13498")
#loc5630 = loc("multiply.13499")
#loc5632 = loc("add.13509")
#loc5633 = loc("convert.13510")
#loc5634 = loc("power.13512")
#loc5636 = loc("multiply.13528")
#loc5637 = loc("reshape.13529")
#loc5638 = loc("add.13533")
#loc5639 = loc("rsqrt.13534")
#loc5640 = loc("reshape.13535")
#loc5641 = loc("broadcast.13536")
#loc5642 = loc("multiply.13537")
#loc5643 = loc("multiply.13544")
#loc5644 = loc("convert.13545")
#loc5645 = loc("reshape.13632")
#loc5646 = loc("reshape.13628")
#loc5647 = loc("reshape.13630")
#loc5648 = loc("transpose.13631")
#loc5650 = loc("reshape.13634")
#loc5651 = loc("reshape.13624")
#loc5652 = loc("reshape.13626")
#loc5653 = loc("broadcast.13637")
#loc5654 = loc("add.13638")
#loc5655 = loc("reshape.13639")
#loc5656 = loc("transpose.13640")
#loc5657 = loc("slice.13641")
#loc5658 = loc("multiply.13658")
#loc5659 = loc("slice.13645")
#loc5660 = loc("multiply.13655")
#loc5661 = loc("subtract.13661")
#loc5662 = loc("multiply.13648")
#loc5663 = loc("multiply.13644")
#loc5664 = loc("add.13651")
#loc5665 = loc("concatenate.13662")
#loc5666 = loc("reshape.13579")
#loc5667 = loc("reshape.13581")
#loc5668 = loc("transpose.13582")
#loc5670 = loc("reshape.13585")
#loc5671 = loc("reshape.13575")
#loc5672 = loc("reshape.13577")
#loc5673 = loc("broadcast.13588")
#loc5674 = loc("add.13589")
#loc5675 = loc("reshape.13590")
#loc5676 = loc("transpose.13591")
#loc5677 = loc("slice.13592")
#loc5678 = loc("multiply.13610")
#loc5679 = loc("slice.13597")
#loc5680 = loc("multiply.13607")
#loc5681 = loc("subtract.13613")
#loc5682 = loc("multiply.13600")
#loc5683 = loc("multiply.13595")
#loc5684 = loc("add.13603")
#loc5685 = loc("concatenate.13614")
#loc5686 = loc("broadcast.13618")
#loc5687 = loc("reshape.13619")
#loc5688 = loc("transpose.13620")
#loc5689 = loc("dot.13663")
#loc5690 = loc("multiply.13666")
#loc5691 = loc("add.13671")
#loc5692 = loc("reshape.13562")
#loc5693 = loc("reshape.13567")
#loc5694 = loc("broadcast.13568")
#loc5695 = loc("concatenate.13672")
#loc5696 = loc("reshape.13945")
#loc5697 = loc("reshape.13947")
#loc5698 = loc("convert.13948")
#loc5699 = loc("broadcast.13949")
#loc5700 = loc("reduce.13679")
#loc5701 = loc("broadcast.13715")
#loc5702 = loc("subtract.13716")
#loc5703 = loc("reduce.13722")
#loc5704 = loc("broadcast.13723")
#loc5705 = loc("subtract.13724")
#loc5706 = loc("exponential.13725")
#loc5707 = loc("reduce.13731")
#loc5708 = loc("broadcast.13732")
#loc5709 = loc("divide.13733")
#loc5710 = loc("slice.13734")
#loc5711 = loc("reshape.296")
#loc5712 = loc("reshape.298")
#loc5713 = loc("transpose.299")
#loc5715 = loc("reshape.13548")
#loc5716 = loc("reshape.292")
#loc5717 = loc("reshape.294")
#loc5718 = loc("broadcast.13551")
#loc5719 = loc("add.13552")
#loc5720 = loc("reshape.13553")
#loc5721 = loc("transpose.13554")
#loc5722 = loc("broadcast.13558")
#loc5723 = loc("reshape.13559")
#loc5724 = loc("dot.13735")
#loc5725 = loc("transpose.13737")
#loc5726 = loc("reshape.13739")
#loc5727 = loc("reshape.286")
#loc5728 = loc("reshape.288")
#loc5729 = loc("transpose.289")
#loc5731 = loc("reshape.13741")
#loc5732 = loc("reshape.282")
#loc5733 = loc("reshape.284")
#loc5734 = loc("broadcast.13744")
#loc5735 = loc("add.13745")
#loc5736 = loc("add.13748")
#loc5737 = loc("reshape.13778")
#loc5738 = loc("reshape.13780")
#loc5739 = loc("convert.13781")
#loc5740 = loc("broadcast.13782")
#loc5741 = loc("convert.13749")
#loc5742 = loc("power.13751")
#loc5744 = loc("multiply.13767")
#loc5745 = loc("reshape.13768")
#loc5746 = loc("add.13772")
#loc5747 = loc("rsqrt.13773")
#loc5748 = loc("reshape.13774")
#loc5749 = loc("broadcast.13775")
#loc5750 = loc("multiply.13776")
#loc5751 = loc("multiply.13783")
#loc5752 = loc("convert.13784")
#loc5753 = loc("reshape.13863")
#loc5754 = loc("broadcast.13867")
#loc5756 = loc("reshape.13857")
#loc5757 = loc("reshape.13859")
#loc5758 = loc("broadcast.13874")
#loc5759 = loc("add.13875")
#loc5760 = loc("slice.13888")
#loc5761 = loc("clamp.13891")
#loc5762 = loc("add.13894")
#loc5763 = loc("slice.13876")
#loc5764 = loc("clamp.13879")
#loc5765 = loc("multiply.13881")
#loc5766 = loc("logistic.13882")
#loc5767 = loc("multiply.13883")
#loc5768 = loc("multiply.13895")
#loc5769 = loc("dot.13896")
#loc5770 = loc("reshape.13846")
#loc5771 = loc("reshape.13848")
#loc5772 = loc("broadcast.13900")
#loc5773 = loc("add.13901")
#loc5774 = loc("reshape.13902")
#loc5775 = loc("reshape.272")
#loc5776 = loc("reshape.274")
#loc5777 = loc("transpose.275")
#loc5779 = loc("reshape.265")
#loc5780 = loc("reshape.267")
#loc5781 = loc("broadcast.13791")
#loc5782 = loc("add.13792")
#loc5784 = loc("compare.13805")
#loc5785 = loc("slice.13810")
#loc5786 = loc("convert.13811")
#loc5787 = loc("reshape.13835")
#loc5788 = loc("concatenate.13836")
#loc5789 = loc("slice.13808")
#loc5790 = loc("reduce.13817")
#loc5791 = loc("broadcast.13818")
#loc5792 = loc("subtract.13819")
#loc5793 = loc("exponential.13820")
#loc5794 = loc("reduce.13826")
#loc5795 = loc("broadcast.13827")
#loc5796 = loc("divide.13828")
#loc5798 = loc("transpose.13841")
#loc5799 = loc("reshape.13842")
#loc5800 = loc("broadcast.13904")
#loc5801 = loc("multiply.13905")
#loc5803 = loc("add.13915")
#loc5804 = loc("convert.13916")
#loc5805 = loc("power.13918")
#loc5807 = loc("multiply.13934")
#loc5808 = loc("reshape.13935")
#loc5809 = loc("add.13939")
#loc5810 = loc("rsqrt.13940")
#loc5811 = loc("reshape.13941")
#loc5812 = loc("broadcast.13942")
#loc5813 = loc("multiply.13943")
#loc5814 = loc("multiply.13950")
#loc5815 = loc("convert.13951")
#loc5816 = loc("reshape.14038")
#loc5817 = loc("reshape.14034")
#loc5818 = loc("reshape.14036")
#loc5819 = loc("transpose.14037")
#loc5821 = loc("reshape.14040")
#loc5822 = loc("reshape.14030")
#loc5823 = loc("reshape.14032")
#loc5824 = loc("broadcast.14043")
#loc5825 = loc("add.14044")
#loc5826 = loc("reshape.14045")
#loc5827 = loc("transpose.14046")
#loc5828 = loc("slice.14047")
#loc5829 = loc("multiply.14064")
#loc5830 = loc("slice.14051")
#loc5831 = loc("multiply.14061")
#loc5832 = loc("subtract.14067")
#loc5833 = loc("multiply.14054")
#loc5834 = loc("multiply.14050")
#loc5835 = loc("add.14057")
#loc5836 = loc("concatenate.14068")
#loc5837 = loc("reshape.13985")
#loc5838 = loc("reshape.13987")
#loc5839 = loc("transpose.13988")
#loc5841 = loc("reshape.13991")
#loc5842 = loc("reshape.13981")
#loc5843 = loc("reshape.13983")
#loc5844 = loc("broadcast.13994")
#loc5845 = loc("add.13995")
#loc5846 = loc("reshape.13996")
#loc5847 = loc("transpose.13997")
#loc5848 = loc("slice.13998")
#loc5849 = loc("multiply.14016")
#loc5850 = loc("slice.14003")
#loc5851 = loc("multiply.14013")
#loc5852 = loc("subtract.14019")
#loc5853 = loc("multiply.14006")
#loc5854 = loc("multiply.14001")
#loc5855 = loc("add.14009")
#loc5856 = loc("concatenate.14020")
#loc5857 = loc("broadcast.14024")
#loc5858 = loc("reshape.14025")
#loc5859 = loc("transpose.14026")
#loc5860 = loc("dot.14069")
#loc5861 = loc("multiply.14072")
#loc5862 = loc("add.14077")
#loc5863 = loc("reshape.13968")
#loc5864 = loc("reshape.13973")
#loc5865 = loc("broadcast.13974")
#loc5866 = loc("concatenate.14078")
#loc5867 = loc("reshape.14351")
#loc5868 = loc("reshape.14353")
#loc5869 = loc("convert.14354")
#loc5870 = loc("broadcast.14355")
#loc5871 = loc("reduce.14085")
#loc5872 = loc("broadcast.14121")
#loc5873 = loc("subtract.14122")
#loc5874 = loc("reduce.14128")
#loc5875 = loc("broadcast.14129")
#loc5876 = loc("subtract.14130")
#loc5877 = loc("exponential.14131")
#loc5878 = loc("reduce.14137")
#loc5879 = loc("broadcast.14138")
#loc5880 = loc("divide.14139")
#loc5881 = loc("slice.14140")
#loc5882 = loc("reshape.254")
#loc5883 = loc("reshape.256")
#loc5884 = loc("transpose.257")
#loc5886 = loc("reshape.13954")
#loc5887 = loc("reshape.250")
#loc5888 = loc("reshape.252")
#loc5889 = loc("broadcast.13957")
#loc5890 = loc("add.13958")
#loc5891 = loc("reshape.13959")
#loc5892 = loc("transpose.13960")
#loc5893 = loc("broadcast.13964")
#loc5894 = loc("reshape.13965")
#loc5895 = loc("dot.14141")
#loc5896 = loc("transpose.14143")
#loc5897 = loc("reshape.14145")
#loc5898 = loc("reshape.244")
#loc5899 = loc("reshape.246")
#loc5900 = loc("transpose.247")
#loc5902 = loc("reshape.14147")
#loc5903 = loc("reshape.240")
#loc5904 = loc("reshape.242")
#loc5905 = loc("broadcast.14150")
#loc5906 = loc("add.14151")
#loc5907 = loc("add.14154")
#loc5908 = loc("reshape.14184")
#loc5909 = loc("reshape.14186")
#loc5910 = loc("convert.14187")
#loc5911 = loc("broadcast.14188")
#loc5912 = loc("convert.14155")
#loc5913 = loc("power.14157")
#loc5915 = loc("multiply.14173")
#loc5916 = loc("reshape.14174")
#loc5917 = loc("add.14178")
#loc5918 = loc("rsqrt.14179")
#loc5919 = loc("reshape.14180")
#loc5920 = loc("broadcast.14181")
#loc5921 = loc("multiply.14182")
#loc5922 = loc("multiply.14189")
#loc5923 = loc("convert.14190")
#loc5924 = loc("reshape.14269")
#loc5925 = loc("broadcast.14273")
#loc5927 = loc("reshape.14263")
#loc5928 = loc("reshape.14265")
#loc5929 = loc("broadcast.14280")
#loc5930 = loc("add.14281")
#loc5931 = loc("slice.14294")
#loc5932 = loc("clamp.14297")
#loc5933 = loc("add.14300")
#loc5934 = loc("slice.14282")
#loc5935 = loc("clamp.14285")
#loc5936 = loc("multiply.14287")
#loc5937 = loc("logistic.14288")
#loc5938 = loc("multiply.14289")
#loc5939 = loc("multiply.14301")
#loc5940 = loc("dot.14302")
#loc5941 = loc("reshape.14252")
#loc5942 = loc("reshape.14254")
#loc5943 = loc("broadcast.14306")
#loc5944 = loc("add.14307")
#loc5945 = loc("reshape.14308")
#loc5946 = loc("reshape.230")
#loc5947 = loc("reshape.232")
#loc5948 = loc("transpose.233")
#loc5950 = loc("reshape.223")
#loc5951 = loc("reshape.225")
#loc5952 = loc("broadcast.14197")
#loc5953 = loc("add.14198")
#loc5955 = loc("compare.14211")
#loc5956 = loc("slice.14216")
#loc5957 = loc("convert.14217")
#loc5958 = loc("reshape.14241")
#loc5959 = loc("concatenate.14242")
#loc5960 = loc("slice.14214")
#loc5961 = loc("reduce.14223")
#loc5962 = loc("broadcast.14224")
#loc5963 = loc("subtract.14225")
#loc5964 = loc("exponential.14226")
#loc5965 = loc("reduce.14232")
#loc5966 = loc("broadcast.14233")
#loc5967 = loc("divide.14234")
#loc5969 = loc("transpose.14247")
#loc5970 = loc("reshape.14248")
#loc5971 = loc("broadcast.14310")
#loc5972 = loc("multiply.14311")
#loc5974 = loc("add.14321")
#loc5975 = loc("convert.14322")
#loc5976 = loc("power.14324")
#loc5978 = loc("multiply.14340")
#loc5979 = loc("reshape.14341")
#loc5980 = loc("add.14345")
#loc5981 = loc("rsqrt.14346")
#loc5982 = loc("reshape.14347")
#loc5983 = loc("broadcast.14348")
#loc5984 = loc("multiply.14349")
#loc5985 = loc("multiply.14356")
#loc5986 = loc("convert.14357")
#loc5987 = loc("reshape.14444")
#loc5988 = loc("reshape.14440")
#loc5989 = loc("reshape.14442")
#loc5990 = loc("transpose.14443")
#loc5992 = loc("reshape.14446")
#loc5993 = loc("reshape.14436")
#loc5994 = loc("reshape.14438")
#loc5995 = loc("broadcast.14449")
#loc5996 = loc("add.14450")
#loc5997 = loc("reshape.14451")
#loc5998 = loc("transpose.14452")
#loc5999 = loc("slice.14453")
#loc6000 = loc("multiply.14470")
#loc6001 = loc("slice.14457")
#loc6002 = loc("multiply.14467")
#loc6003 = loc("subtract.14473")
#loc6004 = loc("multiply.14460")
#loc6005 = loc("multiply.14456")
#loc6006 = loc("add.14463")
#loc6007 = loc("concatenate.14474")
#loc6008 = loc("reshape.14391")
#loc6009 = loc("reshape.14393")
#loc6010 = loc("transpose.14394")
#loc6012 = loc("reshape.14397")
#loc6013 = loc("reshape.14387")
#loc6014 = loc("reshape.14389")
#loc6015 = loc("broadcast.14400")
#loc6016 = loc("add.14401")
#loc6017 = loc("reshape.14402")
#loc6018 = loc("transpose.14403")
#loc6019 = loc("slice.14404")
#loc6020 = loc("multiply.14422")
#loc6021 = loc("slice.14409")
#loc6022 = loc("multiply.14419")
#loc6023 = loc("subtract.14425")
#loc6024 = loc("multiply.14412")
#loc6025 = loc("multiply.14407")
#loc6026 = loc("add.14415")
#loc6027 = loc("concatenate.14426")
#loc6028 = loc("broadcast.14430")
#loc6029 = loc("reshape.14431")
#loc6030 = loc("transpose.14432")
#loc6031 = loc("dot.14475")
#loc6032 = loc("multiply.14478")
#loc6033 = loc("add.14483")
#loc6034 = loc("reshape.14374")
#loc6035 = loc("reshape.14379")
#loc6036 = loc("broadcast.14380")
#loc6037 = loc("concatenate.14484")
#loc6038 = loc("reshape.14757")
#loc6039 = loc("reshape.14759")
#loc6040 = loc("convert.14760")
#loc6041 = loc("broadcast.14761")
#loc6042 = loc("reduce.14491")
#loc6043 = loc("broadcast.14527")
#loc6044 = loc("subtract.14528")
#loc6045 = loc("reduce.14534")
#loc6046 = loc("broadcast.14535")
#loc6047 = loc("subtract.14536")
#loc6048 = loc("exponential.14537")
#loc6049 = loc("reduce.14543")
#loc6050 = loc("broadcast.14544")
#loc6051 = loc("divide.14545")
#loc6052 = loc("slice.14546")
#loc6053 = loc("reshape.212")
#loc6054 = loc("reshape.214")
#loc6055 = loc("transpose.215")
#loc6057 = loc("reshape.14360")
#loc6058 = loc("reshape.208")
#loc6059 = loc("reshape.210")
#loc6060 = loc("broadcast.14363")
#loc6061 = loc("add.14364")
#loc6062 = loc("reshape.14365")
#loc6063 = loc("transpose.14366")
#loc6064 = loc("broadcast.14370")
#loc6065 = loc("reshape.14371")
#loc6066 = loc("dot.14547")
#loc6067 = loc("transpose.14549")
#loc6068 = loc("reshape.14551")
#loc6069 = loc("reshape.202")
#loc6070 = loc("reshape.204")
#loc6071 = loc("transpose.205")
#loc6073 = loc("reshape.14553")
#loc6074 = loc("reshape.198")
#loc6075 = loc("reshape.200")
#loc6076 = loc("broadcast.14556")
#loc6077 = loc("add.14557")
#loc6078 = loc("add.14560")
#loc6079 = loc("reshape.14590")
#loc6080 = loc("reshape.14592")
#loc6081 = loc("convert.14593")
#loc6082 = loc("broadcast.14594")
#loc6083 = loc("convert.14561")
#loc6084 = loc("power.14563")
#loc6086 = loc("multiply.14579")
#loc6087 = loc("reshape.14580")
#loc6088 = loc("add.14584")
#loc6089 = loc("rsqrt.14585")
#loc6090 = loc("reshape.14586")
#loc6091 = loc("broadcast.14587")
#loc6092 = loc("multiply.14588")
#loc6093 = loc("multiply.14595")
#loc6094 = loc("convert.14596")
#loc6095 = loc("reshape.14675")
#loc6096 = loc("broadcast.14679")
#loc6098 = loc("reshape.14669")
#loc6099 = loc("reshape.14671")
#loc6100 = loc("broadcast.14686")
#loc6101 = loc("add.14687")
#loc6102 = loc("slice.14700")
#loc6103 = loc("clamp.14703")
#loc6104 = loc("add.14706")
#loc6105 = loc("slice.14688")
#loc6106 = loc("clamp.14691")
#loc6107 = loc("multiply.14693")
#loc6108 = loc("logistic.14694")
#loc6109 = loc("multiply.14695")
#loc6110 = loc("multiply.14707")
#loc6111 = loc("dot.14708")
#loc6112 = loc("reshape.14658")
#loc6113 = loc("reshape.14660")
#loc6114 = loc("broadcast.14712")
#loc6115 = loc("add.14713")
#loc6116 = loc("reshape.14714")
#loc6117 = loc("reshape.188")
#loc6118 = loc("reshape.190")
#loc6119 = loc("transpose.191")
#loc6121 = loc("reshape.181")
#loc6122 = loc("reshape.183")
#loc6123 = loc("broadcast.14603")
#loc6124 = loc("add.14604")
#loc6126 = loc("compare.14617")
#loc6127 = loc("slice.14622")
#loc6128 = loc("convert.14623")
#loc6129 = loc("reshape.14647")
#loc6130 = loc("concatenate.14648")
#loc6131 = loc("slice.14620")
#loc6132 = loc("reduce.14629")
#loc6133 = loc("broadcast.14630")
#loc6134 = loc("subtract.14631")
#loc6135 = loc("exponential.14632")
#loc6136 = loc("reduce.14638")
#loc6137 = loc("broadcast.14639")
#loc6138 = loc("divide.14640")
#loc6140 = loc("transpose.14653")
#loc6141 = loc("reshape.14654")
#loc6142 = loc("broadcast.14716")
#loc6143 = loc("multiply.14717")
#loc6145 = loc("add.14727")
#loc6146 = loc("convert.14728")
#loc6147 = loc("power.14730")
#loc6149 = loc("multiply.14746")
#loc6150 = loc("reshape.14747")
#loc6151 = loc("add.14751")
#loc6152 = loc("rsqrt.14752")
#loc6153 = loc("reshape.14753")
#loc6154 = loc("broadcast.14754")
#loc6155 = loc("multiply.14755")
#loc6156 = loc("multiply.14762")
#loc6157 = loc("convert.14763")
#loc6158 = loc("reshape.14850")
#loc6159 = loc("reshape.14846")
#loc6160 = loc("reshape.14848")
#loc6161 = loc("transpose.14849")
#loc6163 = loc("reshape.14852")
#loc6164 = loc("reshape.14842")
#loc6165 = loc("reshape.14844")
#loc6166 = loc("broadcast.14855")
#loc6167 = loc("add.14856")
#loc6168 = loc("reshape.14857")
#loc6169 = loc("transpose.14858")
#loc6170 = loc("slice.14859")
#loc6171 = loc("multiply.14876")
#loc6172 = loc("slice.14863")
#loc6173 = loc("multiply.14873")
#loc6174 = loc("subtract.14879")
#loc6175 = loc("multiply.14866")
#loc6176 = loc("multiply.14862")
#loc6177 = loc("add.14869")
#loc6178 = loc("concatenate.14880")
#loc6179 = loc("reshape.14797")
#loc6180 = loc("reshape.14799")
#loc6181 = loc("transpose.14800")
#loc6183 = loc("reshape.14803")
#loc6184 = loc("reshape.14793")
#loc6185 = loc("reshape.14795")
#loc6186 = loc("broadcast.14806")
#loc6187 = loc("add.14807")
#loc6188 = loc("reshape.14808")
#loc6189 = loc("transpose.14809")
#loc6190 = loc("slice.14810")
#loc6191 = loc("multiply.14828")
#loc6192 = loc("slice.14815")
#loc6193 = loc("multiply.14825")
#loc6194 = loc("subtract.14831")
#loc6195 = loc("multiply.14818")
#loc6196 = loc("multiply.14813")
#loc6197 = loc("add.14821")
#loc6198 = loc("concatenate.14832")
#loc6199 = loc("broadcast.14836")
#loc6200 = loc("reshape.14837")
#loc6201 = loc("transpose.14838")
#loc6202 = loc("dot.14881")
#loc6203 = loc("multiply.14884")
#loc6204 = loc("add.14889")
#loc6205 = loc("reshape.14780")
#loc6206 = loc("reshape.14785")
#loc6207 = loc("broadcast.14786")
#loc6208 = loc("concatenate.14890")
#loc6209 = loc("reshape.15163")
#loc6210 = loc("reshape.15165")
#loc6211 = loc("convert.15166")
#loc6212 = loc("broadcast.15167")
#loc6213 = loc("reduce.14897")
#loc6214 = loc("broadcast.14933")
#loc6215 = loc("subtract.14934")
#loc6216 = loc("reduce.14940")
#loc6217 = loc("broadcast.14941")
#loc6218 = loc("subtract.14942")
#loc6219 = loc("exponential.14943")
#loc6220 = loc("reduce.14949")
#loc6221 = loc("broadcast.14950")
#loc6222 = loc("divide.14951")
#loc6223 = loc("slice.14952")
#loc6224 = loc("reshape.170")
#loc6225 = loc("reshape.172")
#loc6226 = loc("transpose.173")
#loc6228 = loc("reshape.14766")
#loc6229 = loc("reshape.166")
#loc6230 = loc("reshape.168")
#loc6231 = loc("broadcast.14769")
#loc6232 = loc("add.14770")
#loc6233 = loc("reshape.14771")
#loc6234 = loc("transpose.14772")
#loc6235 = loc("broadcast.14776")
#loc6236 = loc("reshape.14777")
#loc6237 = loc("dot.14953")
#loc6238 = loc("transpose.14955")
#loc6239 = loc("reshape.14957")
#loc6240 = loc("reshape.160")
#loc6241 = loc("reshape.162")
#loc6242 = loc("transpose.163")
#loc6244 = loc("reshape.14959")
#loc6245 = loc("reshape.156")
#loc6246 = loc("reshape.158")
#loc6247 = loc("broadcast.14962")
#loc6248 = loc("add.14963")
#loc6249 = loc("add.14966")
#loc6250 = loc("reshape.14996")
#loc6251 = loc("reshape.14998")
#loc6252 = loc("convert.14999")
#loc6253 = loc("broadcast.15000")
#loc6254 = loc("convert.14967")
#loc6255 = loc("power.14969")
#loc6257 = loc("multiply.14985")
#loc6258 = loc("reshape.14986")
#loc6259 = loc("add.14990")
#loc6260 = loc("rsqrt.14991")
#loc6261 = loc("reshape.14992")
#loc6262 = loc("broadcast.14993")
#loc6263 = loc("multiply.14994")
#loc6264 = loc("multiply.15001")
#loc6265 = loc("convert.15002")
#loc6266 = loc("reshape.15081")
#loc6267 = loc("broadcast.15085")
#loc6269 = loc("reshape.15075")
#loc6270 = loc("reshape.15077")
#loc6271 = loc("broadcast.15092")
#loc6272 = loc("add.15093")
#loc6273 = loc("slice.15106")
#loc6274 = loc("clamp.15109")
#loc6275 = loc("add.15112")
#loc6276 = loc("slice.15094")
#loc6277 = loc("clamp.15097")
#loc6278 = loc("multiply.15099")
#loc6279 = loc("logistic.15100")
#loc6280 = loc("multiply.15101")
#loc6281 = loc("multiply.15113")
#loc6282 = loc("dot.15114")
#loc6283 = loc("reshape.15064")
#loc6284 = loc("reshape.15066")
#loc6285 = loc("broadcast.15118")
#loc6286 = loc("add.15119")
#loc6287 = loc("reshape.15120")
#loc6288 = loc("reshape.146")
#loc6289 = loc("reshape.148")
#loc6290 = loc("transpose.149")
#loc6292 = loc("reshape.139")
#loc6293 = loc("reshape.141")
#loc6294 = loc("broadcast.15009")
#loc6295 = loc("add.15010")
#loc6297 = loc("compare.15023")
#loc6298 = loc("slice.15028")
#loc6299 = loc("convert.15029")
#loc6300 = loc("reshape.15053")
#loc6301 = loc("concatenate.15054")
#loc6302 = loc("slice.15026")
#loc6303 = loc("reduce.15035")
#loc6304 = loc("broadcast.15036")
#loc6305 = loc("subtract.15037")
#loc6306 = loc("exponential.15038")
#loc6307 = loc("reduce.15044")
#loc6308 = loc("broadcast.15045")
#loc6309 = loc("divide.15046")
#loc6311 = loc("transpose.15059")
#loc6312 = loc("reshape.15060")
#loc6313 = loc("broadcast.15122")
#loc6314 = loc("multiply.15123")
#loc6316 = loc("add.15133")
#loc6317 = loc("convert.15134")
#loc6318 = loc("power.15136")
#loc6320 = loc("multiply.15152")
#loc6321 = loc("reshape.15153")
#loc6322 = loc("add.15157")
#loc6323 = loc("rsqrt.15158")
#loc6324 = loc("reshape.15159")
#loc6325 = loc("broadcast.15160")
#loc6326 = loc("multiply.15161")
#loc6327 = loc("multiply.15168")
#loc6328 = loc("convert.15169")
#loc6329 = loc("reshape.15256")
#loc6330 = loc("reshape.15252")
#loc6331 = loc("reshape.15254")
#loc6332 = loc("transpose.15255")
#loc6334 = loc("reshape.15258")
#loc6335 = loc("reshape.15248")
#loc6336 = loc("reshape.15250")
#loc6337 = loc("broadcast.15261")
#loc6338 = loc("add.15262")
#loc6339 = loc("reshape.15263")
#loc6340 = loc("transpose.15264")
#loc6341 = loc("slice.15265")
#loc6342 = loc("multiply.15282")
#loc6343 = loc("slice.15269")
#loc6344 = loc("multiply.15279")
#loc6345 = loc("subtract.15285")
#loc6346 = loc("multiply.15272")
#loc6347 = loc("multiply.15268")
#loc6348 = loc("add.15275")
#loc6349 = loc("concatenate.15286")
#loc6350 = loc("reshape.15203")
#loc6351 = loc("reshape.15205")
#loc6352 = loc("transpose.15206")
#loc6354 = loc("reshape.15209")
#loc6355 = loc("reshape.15199")
#loc6356 = loc("reshape.15201")
#loc6357 = loc("broadcast.15212")
#loc6358 = loc("add.15213")
#loc6359 = loc("reshape.15214")
#loc6360 = loc("transpose.15215")
#loc6361 = loc("slice.15216")
#loc6362 = loc("multiply.15234")
#loc6363 = loc("slice.15221")
#loc6364 = loc("multiply.15231")
#loc6365 = loc("subtract.15237")
#loc6366 = loc("multiply.15224")
#loc6367 = loc("multiply.15219")
#loc6368 = loc("add.15227")
#loc6369 = loc("concatenate.15238")
#loc6370 = loc("broadcast.15242")
#loc6371 = loc("reshape.15243")
#loc6372 = loc("transpose.15244")
#loc6373 = loc("dot.15287")
#loc6374 = loc("multiply.15290")
#loc6375 = loc("add.15295")
#loc6376 = loc("reshape.15186")
#loc6377 = loc("reshape.15191")
#loc6378 = loc("broadcast.15192")
#loc6379 = loc("concatenate.15296")
#loc6380 = loc("reshape.15569")
#loc6381 = loc("reshape.15571")
#loc6382 = loc("convert.15572")
#loc6383 = loc("broadcast.15573")
#loc6384 = loc("reduce.15303")
#loc6385 = loc("broadcast.15339")
#loc6386 = loc("subtract.15340")
#loc6387 = loc("reduce.15346")
#loc6388 = loc("broadcast.15347")
#loc6389 = loc("subtract.15348")
#loc6390 = loc("exponential.15349")
#loc6391 = loc("reduce.15355")
#loc6392 = loc("broadcast.15356")
#loc6393 = loc("divide.15357")
#loc6394 = loc("slice.15358")
#loc6395 = loc("reshape.128")
#loc6396 = loc("reshape.130")
#loc6397 = loc("transpose.131")
#loc6399 = loc("reshape.15172")
#loc6400 = loc("reshape.124")
#loc6401 = loc("reshape.126")
#loc6402 = loc("broadcast.15175")
#loc6403 = loc("add.15176")
#loc6404 = loc("reshape.15177")
#loc6405 = loc("transpose.15178")
#loc6406 = loc("broadcast.15182")
#loc6407 = loc("reshape.15183")
#loc6408 = loc("dot.15359")
#loc6409 = loc("transpose.15361")
#loc6410 = loc("reshape.15363")
#loc6411 = loc("reshape.118")
#loc6412 = loc("reshape.120")
#loc6413 = loc("transpose.121")
#loc6415 = loc("reshape.15365")
#loc6416 = loc("reshape.114")
#loc6417 = loc("reshape.116")
#loc6418 = loc("broadcast.15368")
#loc6419 = loc("add.15369")
#loc6420 = loc("add.15372")
#loc6421 = loc("reshape.15402")
#loc6422 = loc("reshape.15404")
#loc6423 = loc("convert.15405")
#loc6424 = loc("broadcast.15406")
#loc6425 = loc("convert.15373")
#loc6426 = loc("power.15375")
#loc6428 = loc("multiply.15391")
#loc6429 = loc("reshape.15392")
#loc6430 = loc("add.15396")
#loc6431 = loc("rsqrt.15397")
#loc6432 = loc("reshape.15398")
#loc6433 = loc("broadcast.15399")
#loc6434 = loc("multiply.15400")
#loc6435 = loc("multiply.15407")
#loc6436 = loc("convert.15408")
#loc6437 = loc("reshape.15487")
#loc6438 = loc("broadcast.15491")
#loc6440 = loc("reshape.15481")
#loc6441 = loc("reshape.15483")
#loc6442 = loc("broadcast.15498")
#loc6443 = loc("add.15499")
#loc6444 = loc("slice.15512")
#loc6445 = loc("clamp.15515")
#loc6446 = loc("add.15518")
#loc6447 = loc("slice.15500")
#loc6448 = loc("clamp.15503")
#loc6449 = loc("multiply.15505")
#loc6450 = loc("logistic.15506")
#loc6451 = loc("multiply.15507")
#loc6452 = loc("multiply.15519")
#loc6453 = loc("dot.15520")
#loc6454 = loc("reshape.15470")
#loc6455 = loc("reshape.15472")
#loc6456 = loc("broadcast.15524")
#loc6457 = loc("add.15525")
#loc6458 = loc("reshape.15526")
#loc6459 = loc("reshape.104")
#loc6460 = loc("reshape.106")
#loc6461 = loc("transpose.107")
#loc6463 = loc("reshape.97")
#loc6464 = loc("reshape.99")
#loc6465 = loc("broadcast.15415")
#loc6466 = loc("add.15416")
#loc6468 = loc("compare.15429")
#loc6469 = loc("slice.15434")
#loc6470 = loc("convert.15435")
#loc6471 = loc("reshape.15459")
#loc6472 = loc("concatenate.15460")
#loc6473 = loc("slice.15432")
#loc6474 = loc("reduce.15441")
#loc6475 = loc("broadcast.15442")
#loc6476 = loc("subtract.15443")
#loc6477 = loc("exponential.15444")
#loc6478 = loc("reduce.15450")
#loc6479 = loc("broadcast.15451")
#loc6480 = loc("divide.15452")
#loc6482 = loc("transpose.15465")
#loc6483 = loc("reshape.15466")
#loc6484 = loc("broadcast.15528")
#loc6485 = loc("multiply.15529")
#loc6487 = loc("add.15539")
#loc6488 = loc("convert.15540")
#loc6489 = loc("power.15542")
#loc6491 = loc("multiply.15558")
#loc6492 = loc("reshape.15559")
#loc6493 = loc("add.15563")
#loc6494 = loc("rsqrt.15564")
#loc6495 = loc("reshape.15565")
#loc6496 = loc("broadcast.15566")
#loc6497 = loc("multiply.15567")
#loc6498 = loc("multiply.15574")
#loc6499 = loc("convert.15575")
#loc6500 = loc("reshape.15662")
#loc6501 = loc("reshape.15658")
#loc6502 = loc("reshape.15660")
#loc6503 = loc("transpose.15661")
#loc6505 = loc("reshape.15664")
#loc6506 = loc("reshape.15654")
#loc6507 = loc("reshape.15656")
#loc6508 = loc("broadcast.15667")
#loc6509 = loc("add.15668")
#loc6510 = loc("reshape.15669")
#loc6511 = loc("transpose.15670")
#loc6512 = loc("slice.15671")
#loc6513 = loc("multiply.15688")
#loc6514 = loc("slice.15675")
#loc6515 = loc("multiply.15685")
#loc6516 = loc("subtract.15691")
#loc6517 = loc("multiply.15678")
#loc6518 = loc("multiply.15674")
#loc6519 = loc("add.15681")
#loc6520 = loc("concatenate.15692")
#loc6521 = loc("reshape.15609")
#loc6522 = loc("reshape.15611")
#loc6523 = loc("transpose.15612")
#loc6525 = loc("reshape.15615")
#loc6526 = loc("reshape.15605")
#loc6527 = loc("reshape.15607")
#loc6528 = loc("broadcast.15618")
#loc6529 = loc("add.15619")
#loc6530 = loc("reshape.15620")
#loc6531 = loc("transpose.15621")
#loc6532 = loc("slice.15622")
#loc6533 = loc("multiply.15640")
#loc6534 = loc("slice.15627")
#loc6535 = loc("multiply.15637")
#loc6536 = loc("subtract.15643")
#loc6537 = loc("multiply.15630")
#loc6538 = loc("multiply.15625")
#loc6539 = loc("add.15633")
#loc6540 = loc("concatenate.15644")
#loc6541 = loc("broadcast.15648")
#loc6542 = loc("reshape.15649")
#loc6543 = loc("transpose.15650")
#loc6544 = loc("dot.15693")
#loc6545 = loc("multiply.15696")
#loc6546 = loc("add.15701")
#loc6547 = loc("reshape.15592")
#loc6548 = loc("reshape.15597")
#loc6549 = loc("broadcast.15598")
#loc6550 = loc("concatenate.15702")
#loc6551 = loc("reshape.15975")
#loc6552 = loc("reshape.15977")
#loc6553 = loc("convert.15978")
#loc6554 = loc("broadcast.15979")
#loc6555 = loc("reduce.15709")
#loc6556 = loc("broadcast.15745")
#loc6557 = loc("subtract.15746")
#loc6558 = loc("reduce.15752")
#loc6559 = loc("broadcast.15753")
#loc6560 = loc("subtract.15754")
#loc6561 = loc("exponential.15755")
#loc6562 = loc("reduce.15761")
#loc6563 = loc("broadcast.15762")
#loc6564 = loc("divide.15763")
#loc6565 = loc("slice.15764")
#loc6566 = loc("reshape.86")
#loc6567 = loc("reshape.88")
#loc6568 = loc("transpose.89")
#loc6570 = loc("reshape.15578")
#loc6571 = loc("reshape.82")
#loc6572 = loc("reshape.84")
#loc6573 = loc("broadcast.15581")
#loc6574 = loc("add.15582")
#loc6575 = loc("reshape.15583")
#loc6576 = loc("transpose.15584")
#loc6577 = loc("broadcast.15588")
#loc6578 = loc("reshape.15589")
#loc6579 = loc("dot.15765")
#loc6580 = loc("transpose.15767")
#loc6581 = loc("reshape.15769")
#loc6582 = loc("reshape.76")
#loc6583 = loc("reshape.78")
#loc6584 = loc("transpose.79")
#loc6586 = loc("reshape.15771")
#loc6587 = loc("reshape.72")
#loc6588 = loc("reshape.74")
#loc6589 = loc("broadcast.15774")
#loc6590 = loc("add.15775")
#loc6591 = loc("add.15778")
#loc6592 = loc("reshape.15808")
#loc6593 = loc("reshape.15810")
#loc6594 = loc("convert.15811")
#loc6595 = loc("broadcast.15812")
#loc6596 = loc("convert.15779")
#loc6597 = loc("power.15781")
#loc6599 = loc("multiply.15797")
#loc6600 = loc("reshape.15798")
#loc6601 = loc("add.15802")
#loc6602 = loc("rsqrt.15803")
#loc6603 = loc("reshape.15804")
#loc6604 = loc("broadcast.15805")
#loc6605 = loc("multiply.15806")
#loc6606 = loc("multiply.15813")
#loc6607 = loc("convert.15814")
#loc6608 = loc("reshape.15893")
#loc6609 = loc("broadcast.15897")
#loc6611 = loc("reshape.15887")
#loc6612 = loc("reshape.15889")
#loc6613 = loc("broadcast.15904")
#loc6614 = loc("add.15905")
#loc6615 = loc("slice.15918")
#loc6616 = loc("clamp.15921")
#loc6617 = loc("add.15924")
#loc6618 = loc("slice.15906")
#loc6619 = loc("clamp.15909")
#loc6620 = loc("multiply.15911")
#loc6621 = loc("logistic.15912")
#loc6622 = loc("multiply.15913")
#loc6623 = loc("multiply.15925")
#loc6624 = loc("dot.15926")
#loc6625 = loc("reshape.15876")
#loc6626 = loc("reshape.15878")
#loc6627 = loc("broadcast.15930")
#loc6628 = loc("add.15931")
#loc6629 = loc("reshape.15932")
#loc6630 = loc("reshape.62")
#loc6631 = loc("reshape.64")
#loc6632 = loc("transpose.65")
#loc6634 = loc("reshape.55")
#loc6635 = loc("reshape.57")
#loc6636 = loc("broadcast.15821")
#loc6637 = loc("add.15822")
#loc6639 = loc("compare.15835")
#loc6640 = loc("slice.15840")
#loc6641 = loc("convert.15841")
#loc6642 = loc("reshape.15865")
#loc6643 = loc("concatenate.15866")
#loc6644 = loc("slice.15838")
#loc6645 = loc("reduce.15847")
#loc6646 = loc("broadcast.15848")
#loc6647 = loc("subtract.15849")
#loc6648 = loc("exponential.15850")
#loc6649 = loc("reduce.15856")
#loc6650 = loc("broadcast.15857")
#loc6651 = loc("divide.15858")
#loc6653 = loc("transpose.15871")
#loc6654 = loc("reshape.15872")
#loc6655 = loc("broadcast.15934")
#loc6656 = loc("multiply.15935")
#loc6658 = loc("add.15945")
#loc6659 = loc("convert.15946")
#loc6660 = loc("power.15948")
#loc6662 = loc("multiply.15964")
#loc6663 = loc("reshape.15965")
#loc6664 = loc("add.15969")
#loc6665 = loc("rsqrt.15970")
#loc6666 = loc("reshape.15971")
#loc6667 = loc("broadcast.15972")
#loc6668 = loc("multiply.15973")
#loc6669 = loc("multiply.15980")
#loc6670 = loc("convert.15981")
#loc6671 = loc("reshape.16068")
#loc6672 = loc("reshape.16064")
#loc6673 = loc("reshape.16066")
#loc6674 = loc("transpose.16067")
#loc6676 = loc("reshape.16070")
#loc6677 = loc("reshape.16060")
#loc6678 = loc("reshape.16062")
#loc6679 = loc("broadcast.16073")
#loc6680 = loc("add.16074")
#loc6681 = loc("reshape.16075")
#loc6682 = loc("transpose.16076")
#loc6683 = loc("slice.16077")
#loc6684 = loc("multiply.16094")
#loc6685 = loc("slice.16081")
#loc6686 = loc("multiply.16091")
#loc6687 = loc("subtract.16097")
#loc6688 = loc("multiply.16084")
#loc6689 = loc("multiply.16080")
#loc6690 = loc("add.16087")
#loc6691 = loc("concatenate.16098")
#loc6692 = loc("reshape.16015")
#loc6693 = loc("reshape.16017")
#loc6694 = loc("transpose.16018")
#loc6696 = loc("reshape.16021")
#loc6697 = loc("reshape.16011")
#loc6698 = loc("reshape.16013")
#loc6699 = loc("broadcast.16024")
#loc6700 = loc("add.16025")
#loc6701 = loc("reshape.16026")
#loc6702 = loc("transpose.16027")
#loc6703 = loc("slice.16028")
#loc6704 = loc("multiply.16046")
#loc6705 = loc("slice.16033")
#loc6706 = loc("multiply.16043")
#loc6707 = loc("subtract.16049")
#loc6708 = loc("multiply.16036")
#loc6709 = loc("multiply.16031")
#loc6710 = loc("add.16039")
#loc6711 = loc("concatenate.16050")
#loc6712 = loc("broadcast.16054")
#loc6713 = loc("reshape.16055")
#loc6714 = loc("transpose.16056")
#loc6715 = loc("dot.16099")
#loc6716 = loc("multiply.16102")
#loc6717 = loc("add.16107")
#loc6718 = loc("reshape.15998")
#loc6719 = loc("reshape.16003")
#loc6720 = loc("broadcast.16004")
#loc6721 = loc("concatenate.16108")
#loc6722 = loc("reshape.16381")
#loc6723 = loc("reshape.16383")
#loc6724 = loc("convert.16384")
#loc6725 = loc("broadcast.16385")
#loc6726 = loc("reduce.16115")
#loc6727 = loc("broadcast.16151")
#loc6728 = loc("subtract.16152")
#loc6729 = loc("reduce.16158")
#loc6730 = loc("broadcast.16159")
#loc6731 = loc("subtract.16160")
#loc6732 = loc("exponential.16161")
#loc6733 = loc("reduce.16167")
#loc6734 = loc("broadcast.16168")
#loc6735 = loc("divide.16169")
#loc6736 = loc("slice.16170")
#loc6737 = loc("reshape.44")
#loc6738 = loc("reshape.46")
#loc6739 = loc("transpose.47")
#loc6741 = loc("reshape.15984")
#loc6742 = loc("reshape.40")
#loc6743 = loc("reshape.42")
#loc6744 = loc("broadcast.15987")
#loc6745 = loc("add.15988")
#loc6746 = loc("reshape.15989")
#loc6747 = loc("transpose.15990")
#loc6748 = loc("broadcast.15994")
#loc6749 = loc("reshape.15995")
#loc6750 = loc("dot.16171")
#loc6751 = loc("transpose.16173")
#loc6752 = loc("reshape.16175")
#loc6753 = loc("reshape.34")
#loc6754 = loc("reshape.36")
#loc6755 = loc("transpose.37")
#loc6757 = loc("reshape.16177")
#loc6758 = loc("reshape.30")
#loc6759 = loc("reshape.32")
#loc6760 = loc("broadcast.16180")
#loc6761 = loc("add.16181")
#loc6762 = loc("add.16184")
#loc6763 = loc("reshape.16214")
#loc6764 = loc("reshape.16216")
#loc6765 = loc("convert.16217")
#loc6766 = loc("broadcast.16218")
#loc6767 = loc("convert.16185")
#loc6768 = loc("power.16187")
#loc6770 = loc("multiply.16203")
#loc6771 = loc("reshape.16204")
#loc6772 = loc("add.16208")
#loc6773 = loc("rsqrt.16209")
#loc6774 = loc("reshape.16210")
#loc6775 = loc("broadcast.16211")
#loc6776 = loc("multiply.16212")
#loc6777 = loc("multiply.16219")
#loc6778 = loc("convert.16220")
#loc6779 = loc("reshape.16299")
#loc6780 = loc("broadcast.16303")
#loc6782 = loc("reshape.16293")
#loc6783 = loc("reshape.16295")
#loc6784 = loc("broadcast.16310")
#loc6785 = loc("add.16311")
#loc6786 = loc("slice.16324")
#loc6787 = loc("clamp.16327")
#loc6788 = loc("add.16330")
#loc6789 = loc("slice.16312")
#loc6790 = loc("clamp.16315")
#loc6791 = loc("multiply.16317")
#loc6792 = loc("logistic.16318")
#loc6793 = loc("multiply.16319")
#loc6794 = loc("multiply.16331")
#loc6795 = loc("dot.16332")
#loc6796 = loc("reshape.16282")
#loc6797 = loc("reshape.16284")
#loc6798 = loc("broadcast.16336")
#loc6799 = loc("add.16337")
#loc6800 = loc("reshape.16338")
#loc6801 = loc("reshape.20")
#loc6802 = loc("reshape.22")
#loc6803 = loc("transpose.23")
#loc6805 = loc("reshape.13")
#loc6806 = loc("reshape.15")
#loc6807 = loc("broadcast.16227")
#loc6808 = loc("add.16228")
#loc6810 = loc("compare.16241")
#loc6811 = loc("slice.16246")
#loc6812 = loc("convert.16247")
#loc6813 = loc("reshape.16271")
#loc6814 = loc("concatenate.16272")
#loc6815 = loc("slice.16244")
#loc6816 = loc("reduce.16253")
#loc6817 = loc("broadcast.16254")
#loc6818 = loc("subtract.16255")
#loc6819 = loc("exponential.16256")
#loc6820 = loc("reduce.16262")
#loc6821 = loc("broadcast.16263")
#loc6822 = loc("divide.16264")
#loc6824 = loc("transpose.16277")
#loc6825 = loc("reshape.16278")
#loc6826 = loc("broadcast.16340")
#loc6827 = loc("multiply.16341")
#loc6829 = loc("add.16351")
#loc6830 = loc("convert.16352")
#loc6831 = loc("power.16354")
#loc6833 = loc("multiply.16370")
#loc6834 = loc("reshape.16371")
#loc6835 = loc("add.16375")
#loc6836 = loc("rsqrt.16376")
#loc6837 = loc("reshape.16377")
#loc6838 = loc("broadcast.16378")
#loc6839 = loc("multiply.16379")
#loc6840 = loc("multiply.16386")
#loc6841 = loc("convert.16387")
#loc6842 = loc("reshape.16388")
#loc6843 = loc("reshape.2")
#loc6844 = loc("reshape.4")
#loc6845 = loc("transpose.5")
#loc6847 = loc("reshape.16390")
------------------ END OF MLIR MODULE ------------------
