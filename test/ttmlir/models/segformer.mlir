#loc = loc("Segformer":0:0)
module @Segformer {
  func.func @forward(%arg0: tensor<1x3x512x512xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "pixel_values"} loc("Segformer":0:0), %arg1: tensor<1x16384x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_7.1"} loc("Segformer":0:0), %arg2: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_7.6"} loc("Segformer":0:0), %arg3: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_7.8"} loc("Segformer":0:0), %arg4: tensor<1x16384x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_8.1"} loc("Segformer":0:0), %arg5: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_8.6"} loc("Segformer":0:0), %arg6: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_8.8"} loc("Segformer":0:0), %arg7: tensor<1x256x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_22.1"} loc("Segformer":0:0), %arg8: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_22.6"} loc("Segformer":0:0), %arg9: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_22.8"} loc("Segformer":0:0), %arg10: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_33"} loc("Segformer":0:0), %arg11: tensor<1x16384x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_52.1"} loc("Segformer":0:0), %arg12: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_52.6"} loc("Segformer":0:0), %arg13: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_52.8"} loc("Segformer":0:0), %arg14: tensor<1x16384x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_73.1"} loc("Segformer":0:0), %arg15: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_73.6"} loc("Segformer":0:0), %arg16: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_73.8"} loc("Segformer":0:0), %arg17: tensor<1x256x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_87.1"} loc("Segformer":0:0), %arg18: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_87.6"} loc("Segformer":0:0), %arg19: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_87.8"} loc("Segformer":0:0), %arg20: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_98"} loc("Segformer":0:0), %arg21: tensor<1x16384x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_117.1"} loc("Segformer":0:0), %arg22: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_117.6"} loc("Segformer":0:0), %arg23: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_117.8"} loc("Segformer":0:0), %arg24: tensor<1x16384x32xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_138.1"} loc("Segformer":0:0), %arg25: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_138.6"} loc("Segformer":0:0), %arg26: tensor<1x16384x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_138.8"} loc("Segformer":0:0), %arg27: tensor<1x4096x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_149.1"} loc("Segformer":0:0), %arg28: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_149.6"} loc("Segformer":0:0), %arg29: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_149.8"} loc("Segformer":0:0), %arg30: tensor<1x4096x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_150.1"} loc("Segformer":0:0), %arg31: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_150.6"} loc("Segformer":0:0), %arg32: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_150.8"} loc("Segformer":0:0), %arg33: tensor<1x256x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_165.1"} loc("Segformer":0:0), %arg34: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_165.6"} loc("Segformer":0:0), %arg35: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_165.8"} loc("Segformer":0:0), %arg36: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_177"} loc("Segformer":0:0), %arg37: tensor<1x4096x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_200.1"} loc("Segformer":0:0), %arg38: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_200.6"} loc("Segformer":0:0), %arg39: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_200.8"} loc("Segformer":0:0), %arg40: tensor<1x4096x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_221.1"} loc("Segformer":0:0), %arg41: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_221.6"} loc("Segformer":0:0), %arg42: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_221.8"} loc("Segformer":0:0), %arg43: tensor<1x256x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_236.1"} loc("Segformer":0:0), %arg44: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_236.6"} loc("Segformer":0:0), %arg45: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_236.8"} loc("Segformer":0:0), %arg46: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_248"} loc("Segformer":0:0), %arg47: tensor<1x4096x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_271.1"} loc("Segformer":0:0), %arg48: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_271.6"} loc("Segformer":0:0), %arg49: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_271.8"} loc("Segformer":0:0), %arg50: tensor<1x4096x64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_292.1"} loc("Segformer":0:0), %arg51: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_292.6"} loc("Segformer":0:0), %arg52: tensor<1x4096x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_292.8"} loc("Segformer":0:0), %arg53: tensor<1x1024x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_303.1"} loc("Segformer":0:0), %arg54: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_303.6"} loc("Segformer":0:0), %arg55: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_303.8"} loc("Segformer":0:0), %arg56: tensor<1x1024x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_304.1"} loc("Segformer":0:0), %arg57: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_304.6"} loc("Segformer":0:0), %arg58: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_304.8"} loc("Segformer":0:0), %arg59: tensor<1x256x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_319.1"} loc("Segformer":0:0), %arg60: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_319.6"} loc("Segformer":0:0), %arg61: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_319.8"} loc("Segformer":0:0), %arg62: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_331"} loc("Segformer":0:0), %arg63: tensor<1x1024x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_354.1"} loc("Segformer":0:0), %arg64: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_354.6"} loc("Segformer":0:0), %arg65: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_354.8"} loc("Segformer":0:0), %arg66: tensor<1x1024x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_375.1"} loc("Segformer":0:0), %arg67: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_375.6"} loc("Segformer":0:0), %arg68: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_375.8"} loc("Segformer":0:0), %arg69: tensor<1x256x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_390.1"} loc("Segformer":0:0), %arg70: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_390.6"} loc("Segformer":0:0), %arg71: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_390.8"} loc("Segformer":0:0), %arg72: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_402"} loc("Segformer":0:0), %arg73: tensor<1x1024x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_425.1"} loc("Segformer":0:0), %arg74: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_425.6"} loc("Segformer":0:0), %arg75: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_425.8"} loc("Segformer":0:0), %arg76: tensor<1x1024x160xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_446.1"} loc("Segformer":0:0), %arg77: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_446.6"} loc("Segformer":0:0), %arg78: tensor<1x1024x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_446.8"} loc("Segformer":0:0), %arg79: tensor<1x256x256xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_457.1"} loc("Segformer":0:0), %arg80: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_457.6"} loc("Segformer":0:0), %arg81: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_457.8"} loc("Segformer":0:0), %arg82: tensor<1x256x256xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_458.1"} loc("Segformer":0:0), %arg83: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_458.6"} loc("Segformer":0:0), %arg84: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_458.8"} loc("Segformer":0:0), %arg85: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_477"} loc("Segformer":0:0), %arg86: tensor<1x256x256xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_500.1"} loc("Segformer":0:0), %arg87: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_500.6"} loc("Segformer":0:0), %arg88: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_500.8"} loc("Segformer":0:0), %arg89: tensor<1x256x256xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_521.1"} loc("Segformer":0:0), %arg90: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_521.6"} loc("Segformer":0:0), %arg91: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_521.8"} loc("Segformer":0:0), %arg92: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_divide_540"} loc("Segformer":0:0), %arg93: tensor<1x256x256xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_563.1"} loc("Segformer":0:0), %arg94: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_563.6"} loc("Segformer":0:0), %arg95: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_563.8"} loc("Segformer":0:0), %arg96: tensor<1x256x256xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_584.1"} loc("Segformer":0:0), %arg97: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_584.6"} loc("Segformer":0:0), %arg98: tensor<1x256x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_584.8"} loc("Segformer":0:0), %arg99: tensor<32x3x7x7xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.0.proj.weight"} loc("Segformer":0:0), %arg100: tensor<1x1x1x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.0.proj.bias"} loc("Segformer":0:0), %arg101: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.0.layer_norm.weight"} loc("Segformer":0:0), %arg102: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.0.layer_norm.bias"} loc("Segformer":0:0), %arg103: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.layer_norm_1.weight"} loc("Segformer":0:0), %arg104: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.layer_norm_1.bias"} loc("Segformer":0:0), %arg105: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.query.weight"} loc("Segformer":0:0), %arg106: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.query.bias"} loc("Segformer":0:0), %arg107: tensor<32x32x8x8xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.sr.weight"} loc("Segformer":0:0), %arg108: tensor<1x1x1x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.sr.bias"} loc("Segformer":0:0), %arg109: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.layer_norm.weight"} loc("Segformer":0:0), %arg110: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.layer_norm.bias"} loc("Segformer":0:0), %arg111: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.key.weight"} loc("Segformer":0:0), %arg112: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.key.bias"} loc("Segformer":0:0), %arg113: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.value.weight"} loc("Segformer":0:0), %arg114: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.self.value.bias"} loc("Segformer":0:0), %arg115: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.output.dense.weight"} loc("Segformer":0:0), %arg116: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.attention.output.dense.bias"} loc("Segformer":0:0), %arg117: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.layer_norm_2.weight"} loc("Segformer":0:0), %arg118: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.layer_norm_2.bias"} loc("Segformer":0:0), %arg119: tensor<32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.mlp.dense1.weight"} loc("Segformer":0:0), %arg120: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.mlp.dense1.bias"} loc("Segformer":0:0), %arg121: tensor<128x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg122: tensor<1x1x1x128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg123: tensor<128x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.mlp.dense2.weight"} loc("Segformer":0:0), %arg124: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.0.mlp.dense2.bias"} loc("Segformer":0:0), %arg125: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.layer_norm_1.weight"} loc("Segformer":0:0), %arg126: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.layer_norm_1.bias"} loc("Segformer":0:0), %arg127: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.query.weight"} loc("Segformer":0:0), %arg128: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.query.bias"} loc("Segformer":0:0), %arg129: tensor<32x32x8x8xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.sr.weight"} loc("Segformer":0:0), %arg130: tensor<1x1x1x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.sr.bias"} loc("Segformer":0:0), %arg131: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.layer_norm.weight"} loc("Segformer":0:0), %arg132: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.layer_norm.bias"} loc("Segformer":0:0), %arg133: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.key.weight"} loc("Segformer":0:0), %arg134: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.key.bias"} loc("Segformer":0:0), %arg135: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.value.weight"} loc("Segformer":0:0), %arg136: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.self.value.bias"} loc("Segformer":0:0), %arg137: tensor<32x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.output.dense.weight"} loc("Segformer":0:0), %arg138: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.attention.output.dense.bias"} loc("Segformer":0:0), %arg139: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.layer_norm_2.weight"} loc("Segformer":0:0), %arg140: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.layer_norm_2.bias"} loc("Segformer":0:0), %arg141: tensor<32x128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.mlp.dense1.weight"} loc("Segformer":0:0), %arg142: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.mlp.dense1.bias"} loc("Segformer":0:0), %arg143: tensor<128x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg144: tensor<1x1x1x128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg145: tensor<128x32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.mlp.dense2.weight"} loc("Segformer":0:0), %arg146: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.0.1.mlp.dense2.bias"} loc("Segformer":0:0), %arg147: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.0.weight"} loc("Segformer":0:0), %arg148: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.0.bias"} loc("Segformer":0:0), %arg149: tensor<64x32x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.1.proj.weight"} loc("Segformer":0:0), %arg150: tensor<1x1x1x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.1.proj.bias"} loc("Segformer":0:0), %arg151: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.1.layer_norm.weight"} loc("Segformer":0:0), %arg152: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.1.layer_norm.bias"} loc("Segformer":0:0), %arg153: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.layer_norm_1.weight"} loc("Segformer":0:0), %arg154: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.layer_norm_1.bias"} loc("Segformer":0:0), %arg155: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.query.weight"} loc("Segformer":0:0), %arg156: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.query.bias"} loc("Segformer":0:0), %arg157: tensor<64x64x4x4xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.sr.weight"} loc("Segformer":0:0), %arg158: tensor<1x1x1x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.sr.bias"} loc("Segformer":0:0), %arg159: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.layer_norm.weight"} loc("Segformer":0:0), %arg160: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.layer_norm.bias"} loc("Segformer":0:0), %arg161: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.key.weight"} loc("Segformer":0:0), %arg162: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.key.bias"} loc("Segformer":0:0), %arg163: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.value.weight"} loc("Segformer":0:0), %arg164: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.self.value.bias"} loc("Segformer":0:0), %arg165: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.output.dense.weight"} loc("Segformer":0:0), %arg166: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.attention.output.dense.bias"} loc("Segformer":0:0), %arg167: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.layer_norm_2.weight"} loc("Segformer":0:0), %arg168: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.layer_norm_2.bias"} loc("Segformer":0:0), %arg169: tensor<64x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.mlp.dense1.weight"} loc("Segformer":0:0), %arg170: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.mlp.dense1.bias"} loc("Segformer":0:0), %arg171: tensor<256x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg172: tensor<1x1x1x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg173: tensor<256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.mlp.dense2.weight"} loc("Segformer":0:0), %arg174: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.0.mlp.dense2.bias"} loc("Segformer":0:0), %arg175: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.layer_norm_1.weight"} loc("Segformer":0:0), %arg176: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.layer_norm_1.bias"} loc("Segformer":0:0), %arg177: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.query.weight"} loc("Segformer":0:0), %arg178: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.query.bias"} loc("Segformer":0:0), %arg179: tensor<64x64x4x4xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.sr.weight"} loc("Segformer":0:0), %arg180: tensor<1x1x1x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.sr.bias"} loc("Segformer":0:0), %arg181: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.layer_norm.weight"} loc("Segformer":0:0), %arg182: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.layer_norm.bias"} loc("Segformer":0:0), %arg183: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.key.weight"} loc("Segformer":0:0), %arg184: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.key.bias"} loc("Segformer":0:0), %arg185: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.value.weight"} loc("Segformer":0:0), %arg186: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.self.value.bias"} loc("Segformer":0:0), %arg187: tensor<64x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.output.dense.weight"} loc("Segformer":0:0), %arg188: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.attention.output.dense.bias"} loc("Segformer":0:0), %arg189: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.layer_norm_2.weight"} loc("Segformer":0:0), %arg190: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.layer_norm_2.bias"} loc("Segformer":0:0), %arg191: tensor<64x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.mlp.dense1.weight"} loc("Segformer":0:0), %arg192: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.mlp.dense1.bias"} loc("Segformer":0:0), %arg193: tensor<256x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg194: tensor<1x1x1x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg195: tensor<256x64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.mlp.dense2.weight"} loc("Segformer":0:0), %arg196: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.1.1.mlp.dense2.bias"} loc("Segformer":0:0), %arg197: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.1.weight"} loc("Segformer":0:0), %arg198: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.1.bias"} loc("Segformer":0:0), %arg199: tensor<160x64x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.2.proj.weight"} loc("Segformer":0:0), %arg200: tensor<1x1x1x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.2.proj.bias"} loc("Segformer":0:0), %arg201: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.2.layer_norm.weight"} loc("Segformer":0:0), %arg202: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.2.layer_norm.bias"} loc("Segformer":0:0), %arg203: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.layer_norm_1.weight"} loc("Segformer":0:0), %arg204: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.layer_norm_1.bias"} loc("Segformer":0:0), %arg205: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.query.weight"} loc("Segformer":0:0), %arg206: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.query.bias"} loc("Segformer":0:0), %arg207: tensor<160x160x2x2xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.sr.weight"} loc("Segformer":0:0), %arg208: tensor<1x1x1x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.sr.bias"} loc("Segformer":0:0), %arg209: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.layer_norm.weight"} loc("Segformer":0:0), %arg210: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.layer_norm.bias"} loc("Segformer":0:0), %arg211: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.key.weight"} loc("Segformer":0:0), %arg212: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.key.bias"} loc("Segformer":0:0), %arg213: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.value.weight"} loc("Segformer":0:0), %arg214: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.self.value.bias"} loc("Segformer":0:0), %arg215: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.output.dense.weight"} loc("Segformer":0:0), %arg216: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.attention.output.dense.bias"} loc("Segformer":0:0), %arg217: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.layer_norm_2.weight"} loc("Segformer":0:0), %arg218: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.layer_norm_2.bias"} loc("Segformer":0:0), %arg219: tensor<160x640xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.mlp.dense1.weight"} loc("Segformer":0:0), %arg220: tensor<640xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.mlp.dense1.bias"} loc("Segformer":0:0), %arg221: tensor<640x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg222: tensor<1x1x1x640xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg223: tensor<640x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.mlp.dense2.weight"} loc("Segformer":0:0), %arg224: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.0.mlp.dense2.bias"} loc("Segformer":0:0), %arg225: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.layer_norm_1.weight"} loc("Segformer":0:0), %arg226: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.layer_norm_1.bias"} loc("Segformer":0:0), %arg227: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.query.weight"} loc("Segformer":0:0), %arg228: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.query.bias"} loc("Segformer":0:0), %arg229: tensor<160x160x2x2xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.sr.weight"} loc("Segformer":0:0), %arg230: tensor<1x1x1x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.sr.bias"} loc("Segformer":0:0), %arg231: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.layer_norm.weight"} loc("Segformer":0:0), %arg232: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.layer_norm.bias"} loc("Segformer":0:0), %arg233: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.key.weight"} loc("Segformer":0:0), %arg234: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.key.bias"} loc("Segformer":0:0), %arg235: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.value.weight"} loc("Segformer":0:0), %arg236: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.self.value.bias"} loc("Segformer":0:0), %arg237: tensor<160x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.output.dense.weight"} loc("Segformer":0:0), %arg238: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.attention.output.dense.bias"} loc("Segformer":0:0), %arg239: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.layer_norm_2.weight"} loc("Segformer":0:0), %arg240: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.layer_norm_2.bias"} loc("Segformer":0:0), %arg241: tensor<160x640xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.mlp.dense1.weight"} loc("Segformer":0:0), %arg242: tensor<640xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.mlp.dense1.bias"} loc("Segformer":0:0), %arg243: tensor<640x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg244: tensor<1x1x1x640xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg245: tensor<640x160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.mlp.dense2.weight"} loc("Segformer":0:0), %arg246: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.2.1.mlp.dense2.bias"} loc("Segformer":0:0), %arg247: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.2.weight"} loc("Segformer":0:0), %arg248: tensor<160xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.2.bias"} loc("Segformer":0:0), %arg249: tensor<256x160x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.3.proj.weight"} loc("Segformer":0:0), %arg250: tensor<1x1x1x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.3.proj.bias"} loc("Segformer":0:0), %arg251: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.3.layer_norm.weight"} loc("Segformer":0:0), %arg252: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.patch_embeddings.3.layer_norm.bias"} loc("Segformer":0:0), %arg253: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.layer_norm_1.weight"} loc("Segformer":0:0), %arg254: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.layer_norm_1.bias"} loc("Segformer":0:0), %arg255: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.self.query.weight"} loc("Segformer":0:0), %arg256: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.self.query.bias"} loc("Segformer":0:0), %arg257: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.self.key.weight"} loc("Segformer":0:0), %arg258: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.self.key.bias"} loc("Segformer":0:0), %arg259: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.self.value.weight"} loc("Segformer":0:0), %arg260: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.self.value.bias"} loc("Segformer":0:0), %arg261: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.output.dense.weight"} loc("Segformer":0:0), %arg262: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.attention.output.dense.bias"} loc("Segformer":0:0), %arg263: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.layer_norm_2.weight"} loc("Segformer":0:0), %arg264: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.layer_norm_2.bias"} loc("Segformer":0:0), %arg265: tensor<256x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.mlp.dense1.weight"} loc("Segformer":0:0), %arg266: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.mlp.dense1.bias"} loc("Segformer":0:0), %arg267: tensor<1024x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg268: tensor<1x1x1x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg269: tensor<1024x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.mlp.dense2.weight"} loc("Segformer":0:0), %arg270: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.0.mlp.dense2.bias"} loc("Segformer":0:0), %arg271: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.layer_norm_1.weight"} loc("Segformer":0:0), %arg272: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.layer_norm_1.bias"} loc("Segformer":0:0), %arg273: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.self.query.weight"} loc("Segformer":0:0), %arg274: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.self.query.bias"} loc("Segformer":0:0), %arg275: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.self.key.weight"} loc("Segformer":0:0), %arg276: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.self.key.bias"} loc("Segformer":0:0), %arg277: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.self.value.weight"} loc("Segformer":0:0), %arg278: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.self.value.bias"} loc("Segformer":0:0), %arg279: tensor<256x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.output.dense.weight"} loc("Segformer":0:0), %arg280: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.attention.output.dense.bias"} loc("Segformer":0:0), %arg281: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.layer_norm_2.weight"} loc("Segformer":0:0), %arg282: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.layer_norm_2.bias"} loc("Segformer":0:0), %arg283: tensor<256x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.mlp.dense1.weight"} loc("Segformer":0:0), %arg284: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.mlp.dense1.bias"} loc("Segformer":0:0), %arg285: tensor<1024x1x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight"} loc("Segformer":0:0), %arg286: tensor<1x1x1x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias"} loc("Segformer":0:0), %arg287: tensor<1024x256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.mlp.dense2.weight"} loc("Segformer":0:0), %arg288: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.block.3.1.mlp.dense2.bias"} loc("Segformer":0:0), %arg289: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.3.weight"} loc("Segformer":0:0), %arg290: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "segformer.encoder.layer_norm.3.bias"} loc("Segformer":0:0), %arg291: tensor<256x1000xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "classifier.weight"} loc("Segformer":0:0), %arg292: tensor<1000xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "classifier.bias"} loc("Segformer":0:0)) -> (tensor<1x1000xbf16> {ttir.name = "Segformer.output_add_590"}) {
    %0 = ttir.empty() : tensor<1x512x3x512xbf16> loc(#loc185)
    %1 = "ttir.transpose"(%arg0, %0) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x3x512x512xbf16>, tensor<1x512x3x512xbf16>) -> tensor<1x512x3x512xbf16> loc(#loc185)
    %2 = ttir.empty() : tensor<1x512x512x3xbf16> loc(#loc186)
    %3 = "ttir.transpose"(%1, %2) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x512x3x512xbf16>, tensor<1x512x512x3xbf16>) -> tensor<1x512x512x3xbf16> loc(#loc186)
    %4 = ttir.empty() : tensor<1x128x128x32xbf16> loc(#loc187)
    %5 = "ttir.conv2d"(%3, %arg99, %arg100, %4) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 3, 3, 3, 3>, stride = array<i32: 4, 4>}> {channel_last = 1 : si32} : (tensor<1x512x512x3xbf16>, tensor<32x3x7x7xbf16>, tensor<1x1x1x32xbf16>, tensor<1x128x128x32xbf16>) -> tensor<1x128x128x32xbf16> loc(#loc187)
    %6 = ttir.empty() : tensor<1x128x32x128xbf16> loc(#loc188)
    %7 = "ttir.transpose"(%5, %6) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x128x32xbf16>, tensor<1x128x32x128xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc188)
    %8 = ttir.empty() : tensor<1x32x128x128xbf16> loc(#loc189)
    %9 = "ttir.transpose"(%7, %8) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x32x128xbf16>, tensor<1x32x128x128xbf16>) -> tensor<1x32x128x128xbf16> loc(#loc189)
    %10 = ttir.empty() : tensor<1x32x16384x1xbf16> loc(#loc94)
    %11 = "ttir.reshape"(%9, %10) <{shape = [1 : i32, 32 : i32, 16384 : i32, 1 : i32]}> : (tensor<1x32x128x128xbf16>, tensor<1x32x16384x1xbf16>) -> tensor<1x32x16384x1xbf16> loc(#loc94)
    %12 = ttir.empty() : tensor<1x32x16384xbf16> loc(#loc95)
    %13 = "ttir.squeeze"(%11, %12) <{dim = -1 : si32}> : (tensor<1x32x16384x1xbf16>, tensor<1x32x16384xbf16>) -> tensor<1x32x16384xbf16> loc(#loc95)
    %14 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc96)
    %15 = "ttir.transpose"(%13, %14) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x16384xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc96)
    %16 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc190)
    %17 = "ttir.sum"(%15, %16) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc190)
    %18 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc191)
    %19 = "ttir.multiply"(%arg1, %17, %18) : (tensor<1x16384x32xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc191)
    %20 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc192)
    %21 = "ttir.subtract"(%15, %19, %20) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc192)
    %22 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc193)
    %23 = "ttir.multiply"(%21, %21, %22) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc193)
    %24 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc194)
    %25 = "ttir.sum"(%23, %24) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc194)
    %26 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc195)
    %27 = "ttir.multiply"(%arg2, %25, %26) : (tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc195)
    %28 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc196)
    %29 = "ttir.add"(%27, %arg3, %28) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc196)
    %30 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc197)
    %31 = "ttir.sqrt"(%29, %30) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc197)
    %32 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc198)
    %33 = "ttir.reciprocal"(%31, %32) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc198)
    %34 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc199)
    %35 = "ttir.multiply"(%21, %33, %34) : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc199)
    %36 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc200)
    %37 = "ttir.multiply"(%35, %arg101, %36) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc200)
    %38 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc201)
    %39 = "ttir.add"(%37, %arg102, %38) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc201)
    %40 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc202)
    %41 = "ttir.sum"(%39, %40) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc202)
    %42 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc203)
    %43 = "ttir.multiply"(%arg4, %41, %42) : (tensor<1x16384x32xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc203)
    %44 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc204)
    %45 = "ttir.subtract"(%39, %43, %44) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc204)
    %46 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc205)
    %47 = "ttir.multiply"(%45, %45, %46) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc205)
    %48 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc206)
    %49 = "ttir.sum"(%47, %48) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc206)
    %50 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc207)
    %51 = "ttir.multiply"(%arg5, %49, %50) : (tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc207)
    %52 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc208)
    %53 = "ttir.add"(%51, %arg6, %52) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc208)
    %54 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc209)
    %55 = "ttir.sqrt"(%53, %54) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc209)
    %56 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc210)
    %57 = "ttir.reciprocal"(%55, %56) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc210)
    %58 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc211)
    %59 = "ttir.multiply"(%45, %57, %58) : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc211)
    %60 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc212)
    %61 = "ttir.multiply"(%59, %arg103, %60) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc212)
    %62 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc213)
    %63 = "ttir.add"(%61, %arg104, %62) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc213)
    %64 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc695)
    %65 = "ttir.matmul"(%63, %arg105, %64) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<32x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc695)
    %66 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc2)
    %67 = "ttir.add"(%65, %arg106, %66) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc2)
    %68 = ttir.empty() : tensor<1x16384x1x32xbf16> loc(#loc458)
    %69 = "ttir.reshape"(%67, %68) <{shape = [1 : i32, 16384 : i32, 1 : i32, 32 : i32]}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1x32xbf16>) -> tensor<1x16384x1x32xbf16> loc(#loc458)
    %70 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc459)
    %71 = "ttir.squeeze"(%69, %70) <{dim = -2 : si32}> : (tensor<1x16384x1x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc459)
    %72 = ttir.empty() : tensor<1x32x16384xbf16> loc(#loc460)
    %73 = "ttir.transpose"(%63, %72) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16384x32xbf16>, tensor<1x32x16384xbf16>) -> tensor<1x32x16384xbf16> loc(#loc460)
    %74 = ttir.empty() : tensor<1x32x128x128xbf16> loc(#loc461)
    %75 = "ttir.reshape"(%73, %74) <{shape = [1 : i32, 32 : i32, 128 : i32, 128 : i32]}> : (tensor<1x32x16384xbf16>, tensor<1x32x128x128xbf16>) -> tensor<1x32x128x128xbf16> loc(#loc461)
    %76 = ttir.empty() : tensor<1x128x32x128xbf16> loc(#loc696)
    %77 = "ttir.transpose"(%75, %76) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x32x128x128xbf16>, tensor<1x128x32x128xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc696)
    %78 = ttir.empty() : tensor<1x128x128x32xbf16> loc(#loc697)
    %79 = "ttir.transpose"(%77, %78) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x32x128xbf16>, tensor<1x128x128x32xbf16>) -> tensor<1x128x128x32xbf16> loc(#loc697)
    %80 = ttir.empty() : tensor<1x16x16x32xbf16> loc(#loc698)
    %81 = "ttir.conv2d"(%79, %arg107, %arg108, %80) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 8, 8>}> {channel_last = 1 : si32} : (tensor<1x128x128x32xbf16>, tensor<32x32x8x8xbf16>, tensor<1x1x1x32xbf16>, tensor<1x16x16x32xbf16>) -> tensor<1x16x16x32xbf16> loc(#loc698)
    %82 = ttir.empty() : tensor<1x16x32x16xbf16> loc(#loc699)
    %83 = "ttir.transpose"(%81, %82) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x32xbf16>, tensor<1x16x32x16xbf16>) -> tensor<1x16x32x16xbf16> loc(#loc699)
    %84 = ttir.empty() : tensor<1x32x16x16xbf16> loc(#loc700)
    %85 = "ttir.transpose"(%83, %84) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x32x16xbf16>, tensor<1x32x16x16xbf16>) -> tensor<1x32x16x16xbf16> loc(#loc700)
    %86 = ttir.empty() : tensor<1x32x256xbf16> loc(#loc463)
    %87 = "ttir.reshape"(%85, %86) <{shape = [1 : i32, 32 : i32, 256 : i32]}> : (tensor<1x32x16x16xbf16>, tensor<1x32x256xbf16>) -> tensor<1x32x256xbf16> loc(#loc463)
    %88 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc464)
    %89 = "ttir.transpose"(%87, %88) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x256xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc464)
    %90 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc701)
    %91 = "ttir.sum"(%89, %90) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x32xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc701)
    %92 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc702)
    %93 = "ttir.multiply"(%arg7, %91, %92) : (tensor<1x256x32xf32>, tensor<1x256x1xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc702)
    %94 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc703)
    %95 = "ttir.subtract"(%89, %93, %94) : (tensor<1x256x32xbf16>, tensor<1x256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc703)
    %96 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc704)
    %97 = "ttir.multiply"(%95, %95, %96) : (tensor<1x256x32xbf16>, tensor<1x256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc704)
    %98 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc705)
    %99 = "ttir.sum"(%97, %98) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x32xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc705)
    %100 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc706)
    %101 = "ttir.multiply"(%arg8, %99, %100) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc706)
    %102 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc707)
    %103 = "ttir.add"(%101, %arg9, %102) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc707)
    %104 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc708)
    %105 = "ttir.sqrt"(%103, %104) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc708)
    %106 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc709)
    %107 = "ttir.reciprocal"(%105, %106) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc709)
    %108 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc710)
    %109 = "ttir.multiply"(%95, %107, %108) : (tensor<1x256x32xbf16>, tensor<1x256x1xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc710)
    %110 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc711)
    %111 = "ttir.multiply"(%109, %arg109, %110) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc711)
    %112 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc712)
    %113 = "ttir.add"(%111, %arg110, %112) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc712)
    %114 = ttir.empty() : tensor<256x32xbf16> loc(#loc713)
    %115 = "ttir.squeeze"(%113, %114) <{dim = 0 : si32}> : (tensor<1x256x32xbf16>, tensor<256x32xbf16>) -> tensor<256x32xbf16> loc(#loc713)
    %116 = ttir.empty() : tensor<256x32xbf16> loc(#loc714)
    %117 = "ttir.matmul"(%115, %arg111, %116) <{transpose_a = false, transpose_b = false}> : (tensor<256x32xbf16>, tensor<32x32xbf16>, tensor<256x32xbf16>) -> tensor<256x32xbf16> loc(#loc714)
    %118 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc715)
    %119 = "ttir.unsqueeze"(%117, %118) <{dim = 0 : si32}> : (tensor<256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc715)
    %120 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc3)
    %121 = "ttir.add"(%119, %arg112, %120) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc3)
    %122 = ttir.empty() : tensor<1x256x1x32xbf16> loc(#loc467)
    %123 = "ttir.reshape"(%121, %122) <{shape = [1 : i32, 256 : i32, 1 : i32, 32 : i32]}> : (tensor<1x256x32xbf16>, tensor<1x256x1x32xbf16>) -> tensor<1x256x1x32xbf16> loc(#loc467)
    %124 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc468)
    %125 = "ttir.squeeze"(%123, %124) <{dim = -2 : si32}> : (tensor<1x256x1x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc468)
    %126 = ttir.empty() : tensor<1x32x256xbf16> loc(#loc4)
    %127 = "ttir.transpose"(%125, %126) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x256x32xbf16>, tensor<1x32x256xbf16>) -> tensor<1x32x256xbf16> loc(#loc4)
    %128 = ttir.empty() : tensor<1x16384x256xbf16> loc(#loc469)
    %129 = "ttir.matmul"(%71, %127, %128) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<1x32x256xbf16>, tensor<1x16384x256xbf16>) -> tensor<1x16384x256xbf16> loc(#loc469)
    %130 = ttir.empty() : tensor<1x1x16384x256xbf16> loc(#loc470)
    %131 = "ttir.unsqueeze"(%129, %130) <{dim = 0 : si32}> : (tensor<1x16384x256xbf16>, tensor<1x1x16384x256xbf16>) -> tensor<1x1x16384x256xbf16> loc(#loc470)
    %132 = ttir.empty() : tensor<1x1x16384x256xbf16> loc(#loc471)
    %133 = "ttir.div"(%131, %arg10, %132) : (tensor<1x1x16384x256xbf16>, tensor<1xbf16>, tensor<1x1x16384x256xbf16>) -> tensor<1x1x16384x256xbf16> loc(#loc471)
    %134 = ttir.empty() : tensor<1x1x16384x256xbf16> loc(#loc472)
    %135 = "ttir.softmax"(%133, %134) <{dimension = -1 : si32}> : (tensor<1x1x16384x256xbf16>, tensor<1x1x16384x256xbf16>) -> tensor<1x1x16384x256xbf16> loc(#loc472)
    %136 = ttir.empty() : tensor<1x16384x256xbf16> loc(#loc473)
    %137 = "ttir.squeeze"(%135, %136) <{dim = 0 : si32}> : (tensor<1x1x16384x256xbf16>, tensor<1x16384x256xbf16>) -> tensor<1x16384x256xbf16> loc(#loc473)
    %138 = ttir.empty() : tensor<256x32xbf16> loc(#loc716)
    %139 = "ttir.matmul"(%115, %arg113, %138) <{transpose_a = false, transpose_b = false}> : (tensor<256x32xbf16>, tensor<32x32xbf16>, tensor<256x32xbf16>) -> tensor<256x32xbf16> loc(#loc716)
    %140 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc717)
    %141 = "ttir.unsqueeze"(%139, %140) <{dim = 0 : si32}> : (tensor<256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc717)
    %142 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc5)
    %143 = "ttir.add"(%141, %arg114, %142) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc5)
    %144 = ttir.empty() : tensor<1x256x1x32xbf16> loc(#loc475)
    %145 = "ttir.reshape"(%143, %144) <{shape = [1 : i32, 256 : i32, 1 : i32, 32 : i32]}> : (tensor<1x256x32xbf16>, tensor<1x256x1x32xbf16>) -> tensor<1x256x1x32xbf16> loc(#loc475)
    %146 = ttir.empty() : tensor<1x1x256x32xbf16> loc(#loc476)
    %147 = "ttir.transpose"(%145, %146) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x1x32xbf16>, tensor<1x1x256x32xbf16>) -> tensor<1x1x256x32xbf16> loc(#loc476)
    %148 = ttir.empty() : tensor<1x1x32x256xbf16> loc(#loc477)
    %149 = "ttir.transpose"(%147, %148) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1x256x32xbf16>, tensor<1x1x32x256xbf16>) -> tensor<1x1x32x256xbf16> loc(#loc477)
    %150 = ttir.empty() : tensor<1x32x256xbf16> loc(#loc478)
    %151 = "ttir.squeeze"(%149, %150) <{dim = 0 : si32}> : (tensor<1x1x32x256xbf16>, tensor<1x32x256xbf16>) -> tensor<1x32x256xbf16> loc(#loc478)
    %152 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc6)
    %153 = "ttir.transpose"(%151, %152) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x256xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc6)
    %154 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc479)
    %155 = "ttir.matmul"(%137, %153, %154) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x256xbf16>, tensor<1x256x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc479)
    %156 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc718)
    %157 = "ttir.matmul"(%155, %arg115, %156) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<32x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc718)
    %158 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc7)
    %159 = "ttir.add"(%157, %arg116, %158) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc7)
    %160 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc100)
    %161 = "ttir.add"(%159, %39, %160) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc100)
    %162 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc216)
    %163 = "ttir.sum"(%161, %162) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc216)
    %164 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc217)
    %165 = "ttir.multiply"(%arg11, %163, %164) : (tensor<1x16384x32xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc217)
    %166 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc218)
    %167 = "ttir.subtract"(%161, %165, %166) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc218)
    %168 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc219)
    %169 = "ttir.multiply"(%167, %167, %168) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc219)
    %170 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc220)
    %171 = "ttir.sum"(%169, %170) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc220)
    %172 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc221)
    %173 = "ttir.multiply"(%arg12, %171, %172) : (tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc221)
    %174 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc222)
    %175 = "ttir.add"(%173, %arg13, %174) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc222)
    %176 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc223)
    %177 = "ttir.sqrt"(%175, %176) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc223)
    %178 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc224)
    %179 = "ttir.reciprocal"(%177, %178) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc224)
    %180 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc225)
    %181 = "ttir.multiply"(%167, %179, %180) : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc225)
    %182 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc226)
    %183 = "ttir.multiply"(%181, %arg117, %182) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc226)
    %184 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc227)
    %185 = "ttir.add"(%183, %arg118, %184) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc227)
    %186 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc481)
    %187 = "ttir.matmul"(%185, %arg119, %186) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<32x128xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc481)
    %188 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc8)
    %189 = "ttir.add"(%187, %arg120, %188) : (tensor<1x16384x128xbf16>, tensor<128xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc8)
    %190 = ttir.empty() : tensor<1x128x16384xbf16> loc(#loc482)
    %191 = "ttir.transpose"(%189, %190) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16384x128xbf16>, tensor<1x128x16384xbf16>) -> tensor<1x128x16384xbf16> loc(#loc482)
    %192 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc483)
    %193 = "ttir.reshape"(%191, %192) <{shape = [1 : i32, 128 : i32, 128 : i32, 128 : i32]}> : (tensor<1x128x16384xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc483)
    %194 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc719)
    %195 = "ttir.transpose"(%193, %194) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc719)
    %196 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc720)
    %197 = "ttir.transpose"(%195, %196) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc720)
    %198 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc721)
    %199 = "ttir.conv2d"(%197, %arg121, %arg122, %198) <{dilation = array<i32: 1, 1>, groups = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x128x128x128xbf16>, tensor<128x1x3x3xbf16>, tensor<1x1x1x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc721)
    %200 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc722)
    %201 = "ttir.transpose"(%199, %200) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc722)
    %202 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc723)
    %203 = "ttir.transpose"(%201, %202) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc723)
    %204 = ttir.empty() : tensor<1x128x16384x1xbf16> loc(#loc485)
    %205 = "ttir.reshape"(%203, %204) <{shape = [1 : i32, 128 : i32, 16384 : i32, 1 : i32]}> : (tensor<1x128x128x128xbf16>, tensor<1x128x16384x1xbf16>) -> tensor<1x128x16384x1xbf16> loc(#loc485)
    %206 = ttir.empty() : tensor<1x128x16384xbf16> loc(#loc486)
    %207 = "ttir.squeeze"(%205, %206) <{dim = -1 : si32}> : (tensor<1x128x16384x1xbf16>, tensor<1x128x16384xbf16>) -> tensor<1x128x16384xbf16> loc(#loc486)
    %208 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc487)
    %209 = "ttir.transpose"(%207, %208) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x16384xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc487)
    %210 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc488)
    %211 = "ttir.gelu"(%209, %210) : (tensor<1x16384x128xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc488)
    %212 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc489)
    %213 = "ttir.matmul"(%211, %arg123, %212) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x128xbf16>, tensor<128x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc489)
    %214 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc9)
    %215 = "ttir.add"(%213, %arg124, %214) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc9)
    %216 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc103)
    %217 = "ttir.add"(%215, %161, %216) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc103)
    %218 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc232)
    %219 = "ttir.sum"(%217, %218) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc232)
    %220 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc233)
    %221 = "ttir.multiply"(%arg14, %219, %220) : (tensor<1x16384x32xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc233)
    %222 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc234)
    %223 = "ttir.subtract"(%217, %221, %222) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc234)
    %224 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc235)
    %225 = "ttir.multiply"(%223, %223, %224) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc235)
    %226 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc236)
    %227 = "ttir.sum"(%225, %226) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc236)
    %228 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc237)
    %229 = "ttir.multiply"(%arg15, %227, %228) : (tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc237)
    %230 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc238)
    %231 = "ttir.add"(%229, %arg16, %230) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc238)
    %232 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc239)
    %233 = "ttir.sqrt"(%231, %232) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc239)
    %234 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc240)
    %235 = "ttir.reciprocal"(%233, %234) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc240)
    %236 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc241)
    %237 = "ttir.multiply"(%223, %235, %236) : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc241)
    %238 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc242)
    %239 = "ttir.multiply"(%237, %arg125, %238) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc242)
    %240 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc243)
    %241 = "ttir.add"(%239, %arg126, %240) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc243)
    %242 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc724)
    %243 = "ttir.matmul"(%241, %arg127, %242) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<32x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc724)
    %244 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc10)
    %245 = "ttir.add"(%243, %arg128, %244) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc10)
    %246 = ttir.empty() : tensor<1x16384x1x32xbf16> loc(#loc491)
    %247 = "ttir.reshape"(%245, %246) <{shape = [1 : i32, 16384 : i32, 1 : i32, 32 : i32]}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1x32xbf16>) -> tensor<1x16384x1x32xbf16> loc(#loc491)
    %248 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc492)
    %249 = "ttir.squeeze"(%247, %248) <{dim = -2 : si32}> : (tensor<1x16384x1x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc492)
    %250 = ttir.empty() : tensor<1x32x16384xbf16> loc(#loc493)
    %251 = "ttir.transpose"(%241, %250) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16384x32xbf16>, tensor<1x32x16384xbf16>) -> tensor<1x32x16384xbf16> loc(#loc493)
    %252 = ttir.empty() : tensor<1x32x128x128xbf16> loc(#loc494)
    %253 = "ttir.reshape"(%251, %252) <{shape = [1 : i32, 32 : i32, 128 : i32, 128 : i32]}> : (tensor<1x32x16384xbf16>, tensor<1x32x128x128xbf16>) -> tensor<1x32x128x128xbf16> loc(#loc494)
    %254 = ttir.empty() : tensor<1x128x32x128xbf16> loc(#loc725)
    %255 = "ttir.transpose"(%253, %254) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x32x128x128xbf16>, tensor<1x128x32x128xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc725)
    %256 = ttir.empty() : tensor<1x128x128x32xbf16> loc(#loc726)
    %257 = "ttir.transpose"(%255, %256) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x32x128xbf16>, tensor<1x128x128x32xbf16>) -> tensor<1x128x128x32xbf16> loc(#loc726)
    %258 = ttir.empty() : tensor<1x16x16x32xbf16> loc(#loc727)
    %259 = "ttir.conv2d"(%257, %arg129, %arg130, %258) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 8, 8>}> {channel_last = 1 : si32} : (tensor<1x128x128x32xbf16>, tensor<32x32x8x8xbf16>, tensor<1x1x1x32xbf16>, tensor<1x16x16x32xbf16>) -> tensor<1x16x16x32xbf16> loc(#loc727)
    %260 = ttir.empty() : tensor<1x16x32x16xbf16> loc(#loc728)
    %261 = "ttir.transpose"(%259, %260) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x32xbf16>, tensor<1x16x32x16xbf16>) -> tensor<1x16x32x16xbf16> loc(#loc728)
    %262 = ttir.empty() : tensor<1x32x16x16xbf16> loc(#loc729)
    %263 = "ttir.transpose"(%261, %262) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x32x16xbf16>, tensor<1x32x16x16xbf16>) -> tensor<1x32x16x16xbf16> loc(#loc729)
    %264 = ttir.empty() : tensor<1x32x256xbf16> loc(#loc496)
    %265 = "ttir.reshape"(%263, %264) <{shape = [1 : i32, 32 : i32, 256 : i32]}> : (tensor<1x32x16x16xbf16>, tensor<1x32x256xbf16>) -> tensor<1x32x256xbf16> loc(#loc496)
    %266 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc497)
    %267 = "ttir.transpose"(%265, %266) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x256xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc497)
    %268 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc730)
    %269 = "ttir.sum"(%267, %268) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x32xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc730)
    %270 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc731)
    %271 = "ttir.multiply"(%arg17, %269, %270) : (tensor<1x256x32xf32>, tensor<1x256x1xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc731)
    %272 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc732)
    %273 = "ttir.subtract"(%267, %271, %272) : (tensor<1x256x32xbf16>, tensor<1x256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc732)
    %274 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc733)
    %275 = "ttir.multiply"(%273, %273, %274) : (tensor<1x256x32xbf16>, tensor<1x256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc733)
    %276 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc734)
    %277 = "ttir.sum"(%275, %276) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x32xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc734)
    %278 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc735)
    %279 = "ttir.multiply"(%arg18, %277, %278) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc735)
    %280 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc736)
    %281 = "ttir.add"(%279, %arg19, %280) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc736)
    %282 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc737)
    %283 = "ttir.sqrt"(%281, %282) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc737)
    %284 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc738)
    %285 = "ttir.reciprocal"(%283, %284) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc738)
    %286 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc739)
    %287 = "ttir.multiply"(%273, %285, %286) : (tensor<1x256x32xbf16>, tensor<1x256x1xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc739)
    %288 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc740)
    %289 = "ttir.multiply"(%287, %arg131, %288) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc740)
    %290 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc741)
    %291 = "ttir.add"(%289, %arg132, %290) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc741)
    %292 = ttir.empty() : tensor<256x32xbf16> loc(#loc742)
    %293 = "ttir.squeeze"(%291, %292) <{dim = 0 : si32}> : (tensor<1x256x32xbf16>, tensor<256x32xbf16>) -> tensor<256x32xbf16> loc(#loc742)
    %294 = ttir.empty() : tensor<256x32xbf16> loc(#loc743)
    %295 = "ttir.matmul"(%293, %arg133, %294) <{transpose_a = false, transpose_b = false}> : (tensor<256x32xbf16>, tensor<32x32xbf16>, tensor<256x32xbf16>) -> tensor<256x32xbf16> loc(#loc743)
    %296 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc744)
    %297 = "ttir.unsqueeze"(%295, %296) <{dim = 0 : si32}> : (tensor<256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc744)
    %298 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc11)
    %299 = "ttir.add"(%297, %arg134, %298) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc11)
    %300 = ttir.empty() : tensor<1x256x1x32xbf16> loc(#loc500)
    %301 = "ttir.reshape"(%299, %300) <{shape = [1 : i32, 256 : i32, 1 : i32, 32 : i32]}> : (tensor<1x256x32xbf16>, tensor<1x256x1x32xbf16>) -> tensor<1x256x1x32xbf16> loc(#loc500)
    %302 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc501)
    %303 = "ttir.squeeze"(%301, %302) <{dim = -2 : si32}> : (tensor<1x256x1x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc501)
    %304 = ttir.empty() : tensor<1x32x256xbf16> loc(#loc12)
    %305 = "ttir.transpose"(%303, %304) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x256x32xbf16>, tensor<1x32x256xbf16>) -> tensor<1x32x256xbf16> loc(#loc12)
    %306 = ttir.empty() : tensor<1x16384x256xbf16> loc(#loc502)
    %307 = "ttir.matmul"(%249, %305, %306) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<1x32x256xbf16>, tensor<1x16384x256xbf16>) -> tensor<1x16384x256xbf16> loc(#loc502)
    %308 = ttir.empty() : tensor<1x1x16384x256xbf16> loc(#loc503)
    %309 = "ttir.unsqueeze"(%307, %308) <{dim = 0 : si32}> : (tensor<1x16384x256xbf16>, tensor<1x1x16384x256xbf16>) -> tensor<1x1x16384x256xbf16> loc(#loc503)
    %310 = ttir.empty() : tensor<1x1x16384x256xbf16> loc(#loc504)
    %311 = "ttir.div"(%309, %arg20, %310) : (tensor<1x1x16384x256xbf16>, tensor<1xbf16>, tensor<1x1x16384x256xbf16>) -> tensor<1x1x16384x256xbf16> loc(#loc504)
    %312 = ttir.empty() : tensor<1x1x16384x256xbf16> loc(#loc505)
    %313 = "ttir.softmax"(%311, %312) <{dimension = -1 : si32}> : (tensor<1x1x16384x256xbf16>, tensor<1x1x16384x256xbf16>) -> tensor<1x1x16384x256xbf16> loc(#loc505)
    %314 = ttir.empty() : tensor<1x16384x256xbf16> loc(#loc506)
    %315 = "ttir.squeeze"(%313, %314) <{dim = 0 : si32}> : (tensor<1x1x16384x256xbf16>, tensor<1x16384x256xbf16>) -> tensor<1x16384x256xbf16> loc(#loc506)
    %316 = ttir.empty() : tensor<256x32xbf16> loc(#loc745)
    %317 = "ttir.matmul"(%293, %arg135, %316) <{transpose_a = false, transpose_b = false}> : (tensor<256x32xbf16>, tensor<32x32xbf16>, tensor<256x32xbf16>) -> tensor<256x32xbf16> loc(#loc745)
    %318 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc746)
    %319 = "ttir.unsqueeze"(%317, %318) <{dim = 0 : si32}> : (tensor<256x32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc746)
    %320 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc13)
    %321 = "ttir.add"(%319, %arg136, %320) : (tensor<1x256x32xbf16>, tensor<32xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc13)
    %322 = ttir.empty() : tensor<1x256x1x32xbf16> loc(#loc508)
    %323 = "ttir.reshape"(%321, %322) <{shape = [1 : i32, 256 : i32, 1 : i32, 32 : i32]}> : (tensor<1x256x32xbf16>, tensor<1x256x1x32xbf16>) -> tensor<1x256x1x32xbf16> loc(#loc508)
    %324 = ttir.empty() : tensor<1x1x256x32xbf16> loc(#loc509)
    %325 = "ttir.transpose"(%323, %324) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x1x32xbf16>, tensor<1x1x256x32xbf16>) -> tensor<1x1x256x32xbf16> loc(#loc509)
    %326 = ttir.empty() : tensor<1x1x32x256xbf16> loc(#loc510)
    %327 = "ttir.transpose"(%325, %326) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1x256x32xbf16>, tensor<1x1x32x256xbf16>) -> tensor<1x1x32x256xbf16> loc(#loc510)
    %328 = ttir.empty() : tensor<1x32x256xbf16> loc(#loc511)
    %329 = "ttir.squeeze"(%327, %328) <{dim = 0 : si32}> : (tensor<1x1x32x256xbf16>, tensor<1x32x256xbf16>) -> tensor<1x32x256xbf16> loc(#loc511)
    %330 = ttir.empty() : tensor<1x256x32xbf16> loc(#loc14)
    %331 = "ttir.transpose"(%329, %330) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x256xbf16>, tensor<1x256x32xbf16>) -> tensor<1x256x32xbf16> loc(#loc14)
    %332 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc512)
    %333 = "ttir.matmul"(%315, %331, %332) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x256xbf16>, tensor<1x256x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc512)
    %334 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc747)
    %335 = "ttir.matmul"(%333, %arg137, %334) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<32x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc747)
    %336 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc15)
    %337 = "ttir.add"(%335, %arg138, %336) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc15)
    %338 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc106)
    %339 = "ttir.add"(%337, %217, %338) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc106)
    %340 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc246)
    %341 = "ttir.sum"(%339, %340) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc246)
    %342 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc247)
    %343 = "ttir.multiply"(%arg21, %341, %342) : (tensor<1x16384x32xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc247)
    %344 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc248)
    %345 = "ttir.subtract"(%339, %343, %344) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc248)
    %346 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc249)
    %347 = "ttir.multiply"(%345, %345, %346) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc249)
    %348 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc250)
    %349 = "ttir.sum"(%347, %348) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc250)
    %350 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc251)
    %351 = "ttir.multiply"(%arg22, %349, %350) : (tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc251)
    %352 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc252)
    %353 = "ttir.add"(%351, %arg23, %352) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc252)
    %354 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc253)
    %355 = "ttir.sqrt"(%353, %354) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc253)
    %356 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc254)
    %357 = "ttir.reciprocal"(%355, %356) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc254)
    %358 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc255)
    %359 = "ttir.multiply"(%345, %357, %358) : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc255)
    %360 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc256)
    %361 = "ttir.multiply"(%359, %arg139, %360) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc256)
    %362 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc257)
    %363 = "ttir.add"(%361, %arg140, %362) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc257)
    %364 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc514)
    %365 = "ttir.matmul"(%363, %arg141, %364) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x32xbf16>, tensor<32x128xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc514)
    %366 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc16)
    %367 = "ttir.add"(%365, %arg142, %366) : (tensor<1x16384x128xbf16>, tensor<128xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc16)
    %368 = ttir.empty() : tensor<1x128x16384xbf16> loc(#loc515)
    %369 = "ttir.transpose"(%367, %368) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16384x128xbf16>, tensor<1x128x16384xbf16>) -> tensor<1x128x16384xbf16> loc(#loc515)
    %370 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc516)
    %371 = "ttir.reshape"(%369, %370) <{shape = [1 : i32, 128 : i32, 128 : i32, 128 : i32]}> : (tensor<1x128x16384xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc516)
    %372 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc748)
    %373 = "ttir.transpose"(%371, %372) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc748)
    %374 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc749)
    %375 = "ttir.transpose"(%373, %374) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc749)
    %376 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc750)
    %377 = "ttir.conv2d"(%375, %arg143, %arg144, %376) <{dilation = array<i32: 1, 1>, groups = 128 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x128x128x128xbf16>, tensor<128x1x3x3xbf16>, tensor<1x1x1x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc750)
    %378 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc751)
    %379 = "ttir.transpose"(%377, %378) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc751)
    %380 = ttir.empty() : tensor<1x128x128x128xbf16> loc(#loc752)
    %381 = "ttir.transpose"(%379, %380) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x128x128xbf16>, tensor<1x128x128x128xbf16>) -> tensor<1x128x128x128xbf16> loc(#loc752)
    %382 = ttir.empty() : tensor<1x128x16384x1xbf16> loc(#loc518)
    %383 = "ttir.reshape"(%381, %382) <{shape = [1 : i32, 128 : i32, 16384 : i32, 1 : i32]}> : (tensor<1x128x128x128xbf16>, tensor<1x128x16384x1xbf16>) -> tensor<1x128x16384x1xbf16> loc(#loc518)
    %384 = ttir.empty() : tensor<1x128x16384xbf16> loc(#loc519)
    %385 = "ttir.squeeze"(%383, %384) <{dim = -1 : si32}> : (tensor<1x128x16384x1xbf16>, tensor<1x128x16384xbf16>) -> tensor<1x128x16384xbf16> loc(#loc519)
    %386 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc520)
    %387 = "ttir.transpose"(%385, %386) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x16384xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc520)
    %388 = ttir.empty() : tensor<1x16384x128xbf16> loc(#loc521)
    %389 = "ttir.gelu"(%387, %388) : (tensor<1x16384x128xbf16>, tensor<1x16384x128xbf16>) -> tensor<1x16384x128xbf16> loc(#loc521)
    %390 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc522)
    %391 = "ttir.matmul"(%389, %arg145, %390) <{transpose_a = false, transpose_b = false}> : (tensor<1x16384x128xbf16>, tensor<128x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc522)
    %392 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc17)
    %393 = "ttir.add"(%391, %arg146, %392) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc17)
    %394 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc109)
    %395 = "ttir.add"(%393, %339, %394) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc109)
    %396 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc110)
    %397 = "ttir.sum"(%395, %396) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc110)
    %398 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc111)
    %399 = "ttir.multiply"(%arg24, %397, %398) : (tensor<1x16384x32xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc111)
    %400 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc112)
    %401 = "ttir.subtract"(%395, %399, %400) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc112)
    %402 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc113)
    %403 = "ttir.multiply"(%401, %401, %402) : (tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc113)
    %404 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc114)
    %405 = "ttir.sum"(%403, %404) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc114)
    %406 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc115)
    %407 = "ttir.multiply"(%arg25, %405, %406) : (tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc115)
    %408 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc116)
    %409 = "ttir.add"(%407, %arg26, %408) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xf32>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc116)
    %410 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc117)
    %411 = "ttir.sqrt"(%409, %410) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc117)
    %412 = ttir.empty() : tensor<1x16384x1xbf16> loc(#loc118)
    %413 = "ttir.reciprocal"(%411, %412) : (tensor<1x16384x1xbf16>, tensor<1x16384x1xbf16>) -> tensor<1x16384x1xbf16> loc(#loc118)
    %414 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc119)
    %415 = "ttir.multiply"(%401, %413, %414) : (tensor<1x16384x32xbf16>, tensor<1x16384x1xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc119)
    %416 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc120)
    %417 = "ttir.multiply"(%415, %arg147, %416) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc120)
    %418 = ttir.empty() : tensor<1x16384x32xbf16> loc(#loc121)
    %419 = "ttir.add"(%417, %arg148, %418) : (tensor<1x16384x32xbf16>, tensor<32xbf16>, tensor<1x16384x32xbf16>) -> tensor<1x16384x32xbf16> loc(#loc121)
    %420 = ttir.empty() : tensor<1x128x128x32xbf16> loc(#loc81)
    %421 = "ttir.reshape"(%419, %420) <{shape = [1 : i32, 128 : i32, 128 : i32, 32 : i32]}> : (tensor<1x16384x32xbf16>, tensor<1x128x128x32xbf16>) -> tensor<1x128x128x32xbf16> loc(#loc81)
    %422 = ttir.empty() : tensor<1x32x128x128xbf16> loc(#loc18)
    %423 = "ttir.transpose"(%421, %422) <{dim0 = -3 : si32, dim1 = -1 : si32}> : (tensor<1x128x128x32xbf16>, tensor<1x32x128x128xbf16>) -> tensor<1x32x128x128xbf16> loc(#loc18)
    %424 = ttir.empty() : tensor<1x32x128x128xbf16> loc(#loc82)
    %425 = "ttir.transpose"(%423, %424) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x128x128xbf16>, tensor<1x32x128x128xbf16>) -> tensor<1x32x128x128xbf16> loc(#loc82)
    %426 = ttir.empty() : tensor<1x128x32x128xbf16> loc(#loc262)
    %427 = "ttir.transpose"(%425, %426) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x32x128x128xbf16>, tensor<1x128x32x128xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc262)
    %428 = ttir.empty() : tensor<1x128x128x32xbf16> loc(#loc263)
    %429 = "ttir.transpose"(%427, %428) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x128x32x128xbf16>, tensor<1x128x128x32xbf16>) -> tensor<1x128x128x32xbf16> loc(#loc263)
    %430 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc264)
    %431 = "ttir.conv2d"(%429, %arg149, %arg150, %430) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> {channel_last = 1 : si32} : (tensor<1x128x128x32xbf16>, tensor<64x32x3x3xbf16>, tensor<1x1x1x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc264)
    %432 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc265)
    %433 = "ttir.transpose"(%431, %432) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc265)
    %434 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc266)
    %435 = "ttir.transpose"(%433, %434) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc266)
    %436 = ttir.empty() : tensor<1x64x4096x1xbf16> loc(#loc123)
    %437 = "ttir.reshape"(%435, %436) <{shape = [1 : i32, 64 : i32, 4096 : i32, 1 : i32]}> : (tensor<1x64x64x64xbf16>, tensor<1x64x4096x1xbf16>) -> tensor<1x64x4096x1xbf16> loc(#loc123)
    %438 = ttir.empty() : tensor<1x64x4096xbf16> loc(#loc124)
    %439 = "ttir.squeeze"(%437, %438) <{dim = -1 : si32}> : (tensor<1x64x4096x1xbf16>, tensor<1x64x4096xbf16>) -> tensor<1x64x4096xbf16> loc(#loc124)
    %440 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc125)
    %441 = "ttir.transpose"(%439, %440) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x4096xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc125)
    %442 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc267)
    %443 = "ttir.sum"(%441, %442) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc267)
    %444 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc268)
    %445 = "ttir.multiply"(%arg27, %443, %444) : (tensor<1x4096x64xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc268)
    %446 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc269)
    %447 = "ttir.subtract"(%441, %445, %446) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc269)
    %448 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc270)
    %449 = "ttir.multiply"(%447, %447, %448) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc270)
    %450 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc271)
    %451 = "ttir.sum"(%449, %450) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc271)
    %452 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc272)
    %453 = "ttir.multiply"(%arg28, %451, %452) : (tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc272)
    %454 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc273)
    %455 = "ttir.add"(%453, %arg29, %454) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc273)
    %456 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc274)
    %457 = "ttir.sqrt"(%455, %456) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc274)
    %458 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc275)
    %459 = "ttir.reciprocal"(%457, %458) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc275)
    %460 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc276)
    %461 = "ttir.multiply"(%447, %459, %460) : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc276)
    %462 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc277)
    %463 = "ttir.multiply"(%461, %arg151, %462) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc277)
    %464 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc278)
    %465 = "ttir.add"(%463, %arg152, %464) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc278)
    %466 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc279)
    %467 = "ttir.sum"(%465, %466) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc279)
    %468 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc280)
    %469 = "ttir.multiply"(%arg30, %467, %468) : (tensor<1x4096x64xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc280)
    %470 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc281)
    %471 = "ttir.subtract"(%465, %469, %470) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc281)
    %472 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc282)
    %473 = "ttir.multiply"(%471, %471, %472) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc282)
    %474 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc283)
    %475 = "ttir.sum"(%473, %474) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc283)
    %476 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc284)
    %477 = "ttir.multiply"(%arg31, %475, %476) : (tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc284)
    %478 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc285)
    %479 = "ttir.add"(%477, %arg32, %478) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc285)
    %480 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc286)
    %481 = "ttir.sqrt"(%479, %480) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc286)
    %482 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc287)
    %483 = "ttir.reciprocal"(%481, %482) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc287)
    %484 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc288)
    %485 = "ttir.multiply"(%471, %483, %484) : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc288)
    %486 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc289)
    %487 = "ttir.multiply"(%485, %arg153, %486) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc289)
    %488 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc290)
    %489 = "ttir.add"(%487, %arg154, %488) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc290)
    %490 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc753)
    %491 = "ttir.matmul"(%489, %arg155, %490) <{transpose_a = false, transpose_b = false}> : (tensor<1x4096x64xbf16>, tensor<64x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc753)
    %492 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc19)
    %493 = "ttir.add"(%491, %arg156, %492) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc19)
    %494 = ttir.empty() : tensor<1x4096x2x32xbf16> loc(#loc523)
    %495 = "ttir.reshape"(%493, %494) <{shape = [1 : i32, 4096 : i32, 2 : i32, 32 : i32]}> : (tensor<1x4096x64xbf16>, tensor<1x4096x2x32xbf16>) -> tensor<1x4096x2x32xbf16> loc(#loc523)
    %496 = ttir.empty() : tensor<1x2x4096x32xbf16> loc(#loc524)
    %497 = "ttir.transpose"(%495, %496) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x4096x2x32xbf16>, tensor<1x2x4096x32xbf16>) -> tensor<1x2x4096x32xbf16> loc(#loc524)
    %498 = ttir.empty() : tensor<2x4096x32xbf16> loc(#loc525)
    %499 = "ttir.squeeze"(%497, %498) <{dim = 0 : si32}> : (tensor<1x2x4096x32xbf16>, tensor<2x4096x32xbf16>) -> tensor<2x4096x32xbf16> loc(#loc525)
    %500 = ttir.empty() : tensor<1x64x4096xbf16> loc(#loc526)
    %501 = "ttir.transpose"(%489, %500) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x4096x64xbf16>, tensor<1x64x4096xbf16>) -> tensor<1x64x4096xbf16> loc(#loc526)
    %502 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc527)
    %503 = "ttir.reshape"(%501, %502) <{shape = [1 : i32, 64 : i32, 64 : i32, 64 : i32]}> : (tensor<1x64x4096xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc527)
    %504 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc754)
    %505 = "ttir.transpose"(%503, %504) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc754)
    %506 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc755)
    %507 = "ttir.transpose"(%505, %506) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc755)
    %508 = ttir.empty() : tensor<1x16x16x64xbf16> loc(#loc756)
    %509 = "ttir.conv2d"(%507, %arg157, %arg158, %508) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 4, 4>}> {channel_last = 1 : si32} : (tensor<1x64x64x64xbf16>, tensor<64x64x4x4xbf16>, tensor<1x1x1x64xbf16>, tensor<1x16x16x64xbf16>) -> tensor<1x16x16x64xbf16> loc(#loc756)
    %510 = ttir.empty() : tensor<1x16x64x16xbf16> loc(#loc757)
    %511 = "ttir.transpose"(%509, %510) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x64xbf16>, tensor<1x16x64x16xbf16>) -> tensor<1x16x64x16xbf16> loc(#loc757)
    %512 = ttir.empty() : tensor<1x64x16x16xbf16> loc(#loc758)
    %513 = "ttir.transpose"(%511, %512) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x64x16xbf16>, tensor<1x64x16x16xbf16>) -> tensor<1x64x16x16xbf16> loc(#loc758)
    %514 = ttir.empty() : tensor<1x64x256xbf16> loc(#loc528)
    %515 = "ttir.reshape"(%513, %514) <{shape = [1 : i32, 64 : i32, 256 : i32]}> : (tensor<1x64x16x16xbf16>, tensor<1x64x256xbf16>) -> tensor<1x64x256xbf16> loc(#loc528)
    %516 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc529)
    %517 = "ttir.transpose"(%515, %516) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x256xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc529)
    %518 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc759)
    %519 = "ttir.sum"(%517, %518) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x64xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc759)
    %520 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc760)
    %521 = "ttir.multiply"(%arg33, %519, %520) : (tensor<1x256x64xf32>, tensor<1x256x1xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc760)
    %522 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc761)
    %523 = "ttir.subtract"(%517, %521, %522) : (tensor<1x256x64xbf16>, tensor<1x256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc761)
    %524 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc762)
    %525 = "ttir.multiply"(%523, %523, %524) : (tensor<1x256x64xbf16>, tensor<1x256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc762)
    %526 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc763)
    %527 = "ttir.sum"(%525, %526) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x64xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc763)
    %528 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc764)
    %529 = "ttir.multiply"(%arg34, %527, %528) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc764)
    %530 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc765)
    %531 = "ttir.add"(%529, %arg35, %530) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc765)
    %532 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc766)
    %533 = "ttir.sqrt"(%531, %532) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc766)
    %534 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc767)
    %535 = "ttir.reciprocal"(%533, %534) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc767)
    %536 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc768)
    %537 = "ttir.multiply"(%523, %535, %536) : (tensor<1x256x64xbf16>, tensor<1x256x1xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc768)
    %538 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc769)
    %539 = "ttir.multiply"(%537, %arg159, %538) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc769)
    %540 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc770)
    %541 = "ttir.add"(%539, %arg160, %540) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc770)
    %542 = ttir.empty() : tensor<256x64xbf16> loc(#loc771)
    %543 = "ttir.squeeze"(%541, %542) <{dim = 0 : si32}> : (tensor<1x256x64xbf16>, tensor<256x64xbf16>) -> tensor<256x64xbf16> loc(#loc771)
    %544 = ttir.empty() : tensor<256x64xbf16> loc(#loc772)
    %545 = "ttir.matmul"(%543, %arg161, %544) <{transpose_a = false, transpose_b = false}> : (tensor<256x64xbf16>, tensor<64x64xbf16>, tensor<256x64xbf16>) -> tensor<256x64xbf16> loc(#loc772)
    %546 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc773)
    %547 = "ttir.unsqueeze"(%545, %546) <{dim = 0 : si32}> : (tensor<256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc773)
    %548 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc20)
    %549 = "ttir.add"(%547, %arg162, %548) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc20)
    %550 = ttir.empty() : tensor<1x256x2x32xbf16> loc(#loc530)
    %551 = "ttir.reshape"(%549, %550) <{shape = [1 : i32, 256 : i32, 2 : i32, 32 : i32]}> : (tensor<1x256x64xbf16>, tensor<1x256x2x32xbf16>) -> tensor<1x256x2x32xbf16> loc(#loc530)
    %552 = ttir.empty() : tensor<1x2x256x32xbf16> loc(#loc531)
    %553 = "ttir.transpose"(%551, %552) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x2x32xbf16>, tensor<1x2x256x32xbf16>) -> tensor<1x2x256x32xbf16> loc(#loc531)
    %554 = ttir.empty() : tensor<2x256x32xbf16> loc(#loc532)
    %555 = "ttir.squeeze"(%553, %554) <{dim = 0 : si32}> : (tensor<1x2x256x32xbf16>, tensor<2x256x32xbf16>) -> tensor<2x256x32xbf16> loc(#loc532)
    %556 = ttir.empty() : tensor<2x32x256xbf16> loc(#loc21)
    %557 = "ttir.transpose"(%555, %556) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<2x256x32xbf16>, tensor<2x32x256xbf16>) -> tensor<2x32x256xbf16> loc(#loc21)
    %558 = ttir.empty() : tensor<2x4096x256xbf16> loc(#loc533)
    %559 = "ttir.matmul"(%499, %557, %558) <{transpose_a = false, transpose_b = false}> : (tensor<2x4096x32xbf16>, tensor<2x32x256xbf16>, tensor<2x4096x256xbf16>) -> tensor<2x4096x256xbf16> loc(#loc533)
    %560 = ttir.empty() : tensor<1x2x4096x256xbf16> loc(#loc534)
    %561 = "ttir.unsqueeze"(%559, %560) <{dim = 0 : si32}> : (tensor<2x4096x256xbf16>, tensor<1x2x4096x256xbf16>) -> tensor<1x2x4096x256xbf16> loc(#loc534)
    %562 = ttir.empty() : tensor<1x2x4096x256xbf16> loc(#loc535)
    %563 = "ttir.div"(%561, %arg36, %562) : (tensor<1x2x4096x256xbf16>, tensor<1xbf16>, tensor<1x2x4096x256xbf16>) -> tensor<1x2x4096x256xbf16> loc(#loc535)
    %564 = ttir.empty() : tensor<1x2x4096x256xbf16> loc(#loc536)
    %565 = "ttir.softmax"(%563, %564) <{dimension = -1 : si32}> : (tensor<1x2x4096x256xbf16>, tensor<1x2x4096x256xbf16>) -> tensor<1x2x4096x256xbf16> loc(#loc536)
    %566 = ttir.empty() : tensor<2x4096x256xbf16> loc(#loc537)
    %567 = "ttir.squeeze"(%565, %566) <{dim = 0 : si32}> : (tensor<1x2x4096x256xbf16>, tensor<2x4096x256xbf16>) -> tensor<2x4096x256xbf16> loc(#loc537)
    %568 = ttir.empty() : tensor<256x64xbf16> loc(#loc774)
    %569 = "ttir.matmul"(%543, %arg163, %568) <{transpose_a = false, transpose_b = false}> : (tensor<256x64xbf16>, tensor<64x64xbf16>, tensor<256x64xbf16>) -> tensor<256x64xbf16> loc(#loc774)
    %570 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc775)
    %571 = "ttir.unsqueeze"(%569, %570) <{dim = 0 : si32}> : (tensor<256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc775)
    %572 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc22)
    %573 = "ttir.add"(%571, %arg164, %572) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc22)
    %574 = ttir.empty() : tensor<1x256x2x32xbf16> loc(#loc538)
    %575 = "ttir.reshape"(%573, %574) <{shape = [1 : i32, 256 : i32, 2 : i32, 32 : i32]}> : (tensor<1x256x64xbf16>, tensor<1x256x2x32xbf16>) -> tensor<1x256x2x32xbf16> loc(#loc538)
    %576 = ttir.empty() : tensor<1x2x256x32xbf16> loc(#loc539)
    %577 = "ttir.transpose"(%575, %576) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x2x32xbf16>, tensor<1x2x256x32xbf16>) -> tensor<1x2x256x32xbf16> loc(#loc539)
    %578 = ttir.empty() : tensor<1x2x32x256xbf16> loc(#loc540)
    %579 = "ttir.transpose"(%577, %578) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x2x256x32xbf16>, tensor<1x2x32x256xbf16>) -> tensor<1x2x32x256xbf16> loc(#loc540)
    %580 = ttir.empty() : tensor<2x32x256xbf16> loc(#loc541)
    %581 = "ttir.squeeze"(%579, %580) <{dim = 0 : si32}> : (tensor<1x2x32x256xbf16>, tensor<2x32x256xbf16>) -> tensor<2x32x256xbf16> loc(#loc541)
    %582 = ttir.empty() : tensor<2x256x32xbf16> loc(#loc23)
    %583 = "ttir.transpose"(%581, %582) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<2x32x256xbf16>, tensor<2x256x32xbf16>) -> tensor<2x256x32xbf16> loc(#loc23)
    %584 = ttir.empty() : tensor<2x4096x32xbf16> loc(#loc542)
    %585 = "ttir.matmul"(%567, %583, %584) <{transpose_a = false, transpose_b = false}> : (tensor<2x4096x256xbf16>, tensor<2x256x32xbf16>, tensor<2x4096x32xbf16>) -> tensor<2x4096x32xbf16> loc(#loc542)
    %586 = ttir.empty() : tensor<1x2x4096x32xbf16> loc(#loc543)
    %587 = "ttir.unsqueeze"(%585, %586) <{dim = 0 : si32}> : (tensor<2x4096x32xbf16>, tensor<1x2x4096x32xbf16>) -> tensor<1x2x4096x32xbf16> loc(#loc543)
    %588 = ttir.empty() : tensor<1x4096x2x32xbf16> loc(#loc544)
    %589 = "ttir.transpose"(%587, %588) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x2x4096x32xbf16>, tensor<1x4096x2x32xbf16>) -> tensor<1x4096x2x32xbf16> loc(#loc544)
    %590 = ttir.empty() : tensor<4096x64xbf16> loc(#loc776)
    %591 = "ttir.reshape"(%589, %590) <{shape = [4096 : i32, 64 : i32]}> : (tensor<1x4096x2x32xbf16>, tensor<4096x64xbf16>) -> tensor<4096x64xbf16> loc(#loc776)
    %592 = ttir.empty() : tensor<4096x64xbf16> loc(#loc777)
    %593 = "ttir.matmul"(%591, %arg165, %592) <{transpose_a = false, transpose_b = false}> : (tensor<4096x64xbf16>, tensor<64x64xbf16>, tensor<4096x64xbf16>) -> tensor<4096x64xbf16> loc(#loc777)
    %594 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc778)
    %595 = "ttir.unsqueeze"(%593, %594) <{dim = 0 : si32}> : (tensor<4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc778)
    %596 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc24)
    %597 = "ttir.add"(%595, %arg166, %596) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc24)
    %598 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc127)
    %599 = "ttir.add"(%597, %465, %598) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc127)
    %600 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc291)
    %601 = "ttir.sum"(%599, %600) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc291)
    %602 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc292)
    %603 = "ttir.multiply"(%arg37, %601, %602) : (tensor<1x4096x64xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc292)
    %604 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc293)
    %605 = "ttir.subtract"(%599, %603, %604) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc293)
    %606 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc294)
    %607 = "ttir.multiply"(%605, %605, %606) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc294)
    %608 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc295)
    %609 = "ttir.sum"(%607, %608) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc295)
    %610 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc296)
    %611 = "ttir.multiply"(%arg38, %609, %610) : (tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc296)
    %612 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc297)
    %613 = "ttir.add"(%611, %arg39, %612) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc297)
    %614 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc298)
    %615 = "ttir.sqrt"(%613, %614) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc298)
    %616 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc299)
    %617 = "ttir.reciprocal"(%615, %616) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc299)
    %618 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc300)
    %619 = "ttir.multiply"(%605, %617, %618) : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc300)
    %620 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc301)
    %621 = "ttir.multiply"(%619, %arg167, %620) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc301)
    %622 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc302)
    %623 = "ttir.add"(%621, %arg168, %622) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc302)
    %624 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc545)
    %625 = "ttir.matmul"(%623, %arg169, %624) <{transpose_a = false, transpose_b = false}> : (tensor<1x4096x64xbf16>, tensor<64x256xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc545)
    %626 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc25)
    %627 = "ttir.add"(%625, %arg170, %626) : (tensor<1x4096x256xbf16>, tensor<256xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc25)
    %628 = ttir.empty() : tensor<1x256x4096xbf16> loc(#loc546)
    %629 = "ttir.transpose"(%627, %628) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x4096x256xbf16>, tensor<1x256x4096xbf16>) -> tensor<1x256x4096xbf16> loc(#loc546)
    %630 = ttir.empty() : tensor<1x256x64x64xbf16> loc(#loc547)
    %631 = "ttir.reshape"(%629, %630) <{shape = [1 : i32, 256 : i32, 64 : i32, 64 : i32]}> : (tensor<1x256x4096xbf16>, tensor<1x256x64x64xbf16>) -> tensor<1x256x64x64xbf16> loc(#loc547)
    %632 = ttir.empty() : tensor<1x64x256x64xbf16> loc(#loc779)
    %633 = "ttir.transpose"(%631, %632) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x64x64xbf16>, tensor<1x64x256x64xbf16>) -> tensor<1x64x256x64xbf16> loc(#loc779)
    %634 = ttir.empty() : tensor<1x64x64x256xbf16> loc(#loc780)
    %635 = "ttir.transpose"(%633, %634) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x256x64xbf16>, tensor<1x64x64x256xbf16>) -> tensor<1x64x64x256xbf16> loc(#loc780)
    %636 = ttir.empty() : tensor<1x64x64x256xbf16> loc(#loc781)
    %637 = "ttir.conv2d"(%635, %arg171, %arg172, %636) <{dilation = array<i32: 1, 1>, groups = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x64x64x256xbf16>, tensor<256x1x3x3xbf16>, tensor<1x1x1x256xbf16>, tensor<1x64x64x256xbf16>) -> tensor<1x64x64x256xbf16> loc(#loc781)
    %638 = ttir.empty() : tensor<1x64x256x64xbf16> loc(#loc782)
    %639 = "ttir.transpose"(%637, %638) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x256xbf16>, tensor<1x64x256x64xbf16>) -> tensor<1x64x256x64xbf16> loc(#loc782)
    %640 = ttir.empty() : tensor<1x256x64x64xbf16> loc(#loc783)
    %641 = "ttir.transpose"(%639, %640) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x256x64xbf16>, tensor<1x256x64x64xbf16>) -> tensor<1x256x64x64xbf16> loc(#loc783)
    %642 = ttir.empty() : tensor<1x256x4096x1xbf16> loc(#loc548)
    %643 = "ttir.reshape"(%641, %642) <{shape = [1 : i32, 256 : i32, 4096 : i32, 1 : i32]}> : (tensor<1x256x64x64xbf16>, tensor<1x256x4096x1xbf16>) -> tensor<1x256x4096x1xbf16> loc(#loc548)
    %644 = ttir.empty() : tensor<1x256x4096xbf16> loc(#loc549)
    %645 = "ttir.squeeze"(%643, %644) <{dim = -1 : si32}> : (tensor<1x256x4096x1xbf16>, tensor<1x256x4096xbf16>) -> tensor<1x256x4096xbf16> loc(#loc549)
    %646 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc550)
    %647 = "ttir.transpose"(%645, %646) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x256x4096xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc550)
    %648 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc551)
    %649 = "ttir.gelu"(%647, %648) : (tensor<1x4096x256xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc551)
    %650 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc552)
    %651 = "ttir.matmul"(%649, %arg173, %650) <{transpose_a = false, transpose_b = false}> : (tensor<1x4096x256xbf16>, tensor<256x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc552)
    %652 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc26)
    %653 = "ttir.add"(%651, %arg174, %652) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc26)
    %654 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc128)
    %655 = "ttir.add"(%653, %599, %654) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc128)
    %656 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc303)
    %657 = "ttir.sum"(%655, %656) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc303)
    %658 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc304)
    %659 = "ttir.multiply"(%arg40, %657, %658) : (tensor<1x4096x64xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc304)
    %660 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc305)
    %661 = "ttir.subtract"(%655, %659, %660) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc305)
    %662 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc306)
    %663 = "ttir.multiply"(%661, %661, %662) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc306)
    %664 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc307)
    %665 = "ttir.sum"(%663, %664) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc307)
    %666 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc308)
    %667 = "ttir.multiply"(%arg41, %665, %666) : (tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc308)
    %668 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc309)
    %669 = "ttir.add"(%667, %arg42, %668) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc309)
    %670 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc310)
    %671 = "ttir.sqrt"(%669, %670) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc310)
    %672 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc311)
    %673 = "ttir.reciprocal"(%671, %672) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc311)
    %674 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc312)
    %675 = "ttir.multiply"(%661, %673, %674) : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc312)
    %676 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc313)
    %677 = "ttir.multiply"(%675, %arg175, %676) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc313)
    %678 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc314)
    %679 = "ttir.add"(%677, %arg176, %678) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc314)
    %680 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc784)
    %681 = "ttir.matmul"(%679, %arg177, %680) <{transpose_a = false, transpose_b = false}> : (tensor<1x4096x64xbf16>, tensor<64x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc784)
    %682 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc27)
    %683 = "ttir.add"(%681, %arg178, %682) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc27)
    %684 = ttir.empty() : tensor<1x4096x2x32xbf16> loc(#loc553)
    %685 = "ttir.reshape"(%683, %684) <{shape = [1 : i32, 4096 : i32, 2 : i32, 32 : i32]}> : (tensor<1x4096x64xbf16>, tensor<1x4096x2x32xbf16>) -> tensor<1x4096x2x32xbf16> loc(#loc553)
    %686 = ttir.empty() : tensor<1x2x4096x32xbf16> loc(#loc554)
    %687 = "ttir.transpose"(%685, %686) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x4096x2x32xbf16>, tensor<1x2x4096x32xbf16>) -> tensor<1x2x4096x32xbf16> loc(#loc554)
    %688 = ttir.empty() : tensor<2x4096x32xbf16> loc(#loc555)
    %689 = "ttir.squeeze"(%687, %688) <{dim = 0 : si32}> : (tensor<1x2x4096x32xbf16>, tensor<2x4096x32xbf16>) -> tensor<2x4096x32xbf16> loc(#loc555)
    %690 = ttir.empty() : tensor<1x64x4096xbf16> loc(#loc556)
    %691 = "ttir.transpose"(%679, %690) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x4096x64xbf16>, tensor<1x64x4096xbf16>) -> tensor<1x64x4096xbf16> loc(#loc556)
    %692 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc557)
    %693 = "ttir.reshape"(%691, %692) <{shape = [1 : i32, 64 : i32, 64 : i32, 64 : i32]}> : (tensor<1x64x4096xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc557)
    %694 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc785)
    %695 = "ttir.transpose"(%693, %694) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc785)
    %696 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc786)
    %697 = "ttir.transpose"(%695, %696) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc786)
    %698 = ttir.empty() : tensor<1x16x16x64xbf16> loc(#loc787)
    %699 = "ttir.conv2d"(%697, %arg179, %arg180, %698) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 4, 4>}> {channel_last = 1 : si32} : (tensor<1x64x64x64xbf16>, tensor<64x64x4x4xbf16>, tensor<1x1x1x64xbf16>, tensor<1x16x16x64xbf16>) -> tensor<1x16x16x64xbf16> loc(#loc787)
    %700 = ttir.empty() : tensor<1x16x64x16xbf16> loc(#loc788)
    %701 = "ttir.transpose"(%699, %700) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x64xbf16>, tensor<1x16x64x16xbf16>) -> tensor<1x16x64x16xbf16> loc(#loc788)
    %702 = ttir.empty() : tensor<1x64x16x16xbf16> loc(#loc789)
    %703 = "ttir.transpose"(%701, %702) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x64x16xbf16>, tensor<1x64x16x16xbf16>) -> tensor<1x64x16x16xbf16> loc(#loc789)
    %704 = ttir.empty() : tensor<1x64x256xbf16> loc(#loc558)
    %705 = "ttir.reshape"(%703, %704) <{shape = [1 : i32, 64 : i32, 256 : i32]}> : (tensor<1x64x16x16xbf16>, tensor<1x64x256xbf16>) -> tensor<1x64x256xbf16> loc(#loc558)
    %706 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc559)
    %707 = "ttir.transpose"(%705, %706) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x256xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc559)
    %708 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc790)
    %709 = "ttir.sum"(%707, %708) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x64xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc790)
    %710 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc791)
    %711 = "ttir.multiply"(%arg43, %709, %710) : (tensor<1x256x64xf32>, tensor<1x256x1xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc791)
    %712 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc792)
    %713 = "ttir.subtract"(%707, %711, %712) : (tensor<1x256x64xbf16>, tensor<1x256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc792)
    %714 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc793)
    %715 = "ttir.multiply"(%713, %713, %714) : (tensor<1x256x64xbf16>, tensor<1x256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc793)
    %716 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc794)
    %717 = "ttir.sum"(%715, %716) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x64xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc794)
    %718 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc795)
    %719 = "ttir.multiply"(%arg44, %717, %718) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc795)
    %720 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc796)
    %721 = "ttir.add"(%719, %arg45, %720) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc796)
    %722 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc797)
    %723 = "ttir.sqrt"(%721, %722) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc797)
    %724 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc798)
    %725 = "ttir.reciprocal"(%723, %724) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc798)
    %726 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc799)
    %727 = "ttir.multiply"(%713, %725, %726) : (tensor<1x256x64xbf16>, tensor<1x256x1xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc799)
    %728 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc800)
    %729 = "ttir.multiply"(%727, %arg181, %728) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc800)
    %730 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc801)
    %731 = "ttir.add"(%729, %arg182, %730) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc801)
    %732 = ttir.empty() : tensor<256x64xbf16> loc(#loc802)
    %733 = "ttir.squeeze"(%731, %732) <{dim = 0 : si32}> : (tensor<1x256x64xbf16>, tensor<256x64xbf16>) -> tensor<256x64xbf16> loc(#loc802)
    %734 = ttir.empty() : tensor<256x64xbf16> loc(#loc803)
    %735 = "ttir.matmul"(%733, %arg183, %734) <{transpose_a = false, transpose_b = false}> : (tensor<256x64xbf16>, tensor<64x64xbf16>, tensor<256x64xbf16>) -> tensor<256x64xbf16> loc(#loc803)
    %736 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc804)
    %737 = "ttir.unsqueeze"(%735, %736) <{dim = 0 : si32}> : (tensor<256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc804)
    %738 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc28)
    %739 = "ttir.add"(%737, %arg184, %738) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc28)
    %740 = ttir.empty() : tensor<1x256x2x32xbf16> loc(#loc560)
    %741 = "ttir.reshape"(%739, %740) <{shape = [1 : i32, 256 : i32, 2 : i32, 32 : i32]}> : (tensor<1x256x64xbf16>, tensor<1x256x2x32xbf16>) -> tensor<1x256x2x32xbf16> loc(#loc560)
    %742 = ttir.empty() : tensor<1x2x256x32xbf16> loc(#loc561)
    %743 = "ttir.transpose"(%741, %742) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x2x32xbf16>, tensor<1x2x256x32xbf16>) -> tensor<1x2x256x32xbf16> loc(#loc561)
    %744 = ttir.empty() : tensor<2x256x32xbf16> loc(#loc562)
    %745 = "ttir.squeeze"(%743, %744) <{dim = 0 : si32}> : (tensor<1x2x256x32xbf16>, tensor<2x256x32xbf16>) -> tensor<2x256x32xbf16> loc(#loc562)
    %746 = ttir.empty() : tensor<2x32x256xbf16> loc(#loc29)
    %747 = "ttir.transpose"(%745, %746) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<2x256x32xbf16>, tensor<2x32x256xbf16>) -> tensor<2x32x256xbf16> loc(#loc29)
    %748 = ttir.empty() : tensor<2x4096x256xbf16> loc(#loc563)
    %749 = "ttir.matmul"(%689, %747, %748) <{transpose_a = false, transpose_b = false}> : (tensor<2x4096x32xbf16>, tensor<2x32x256xbf16>, tensor<2x4096x256xbf16>) -> tensor<2x4096x256xbf16> loc(#loc563)
    %750 = ttir.empty() : tensor<1x2x4096x256xbf16> loc(#loc564)
    %751 = "ttir.unsqueeze"(%749, %750) <{dim = 0 : si32}> : (tensor<2x4096x256xbf16>, tensor<1x2x4096x256xbf16>) -> tensor<1x2x4096x256xbf16> loc(#loc564)
    %752 = ttir.empty() : tensor<1x2x4096x256xbf16> loc(#loc565)
    %753 = "ttir.div"(%751, %arg46, %752) : (tensor<1x2x4096x256xbf16>, tensor<1xbf16>, tensor<1x2x4096x256xbf16>) -> tensor<1x2x4096x256xbf16> loc(#loc565)
    %754 = ttir.empty() : tensor<1x2x4096x256xbf16> loc(#loc566)
    %755 = "ttir.softmax"(%753, %754) <{dimension = -1 : si32}> : (tensor<1x2x4096x256xbf16>, tensor<1x2x4096x256xbf16>) -> tensor<1x2x4096x256xbf16> loc(#loc566)
    %756 = ttir.empty() : tensor<2x4096x256xbf16> loc(#loc567)
    %757 = "ttir.squeeze"(%755, %756) <{dim = 0 : si32}> : (tensor<1x2x4096x256xbf16>, tensor<2x4096x256xbf16>) -> tensor<2x4096x256xbf16> loc(#loc567)
    %758 = ttir.empty() : tensor<256x64xbf16> loc(#loc805)
    %759 = "ttir.matmul"(%733, %arg185, %758) <{transpose_a = false, transpose_b = false}> : (tensor<256x64xbf16>, tensor<64x64xbf16>, tensor<256x64xbf16>) -> tensor<256x64xbf16> loc(#loc805)
    %760 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc806)
    %761 = "ttir.unsqueeze"(%759, %760) <{dim = 0 : si32}> : (tensor<256x64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc806)
    %762 = ttir.empty() : tensor<1x256x64xbf16> loc(#loc30)
    %763 = "ttir.add"(%761, %arg186, %762) : (tensor<1x256x64xbf16>, tensor<64xbf16>, tensor<1x256x64xbf16>) -> tensor<1x256x64xbf16> loc(#loc30)
    %764 = ttir.empty() : tensor<1x256x2x32xbf16> loc(#loc568)
    %765 = "ttir.reshape"(%763, %764) <{shape = [1 : i32, 256 : i32, 2 : i32, 32 : i32]}> : (tensor<1x256x64xbf16>, tensor<1x256x2x32xbf16>) -> tensor<1x256x2x32xbf16> loc(#loc568)
    %766 = ttir.empty() : tensor<1x2x256x32xbf16> loc(#loc569)
    %767 = "ttir.transpose"(%765, %766) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x2x32xbf16>, tensor<1x2x256x32xbf16>) -> tensor<1x2x256x32xbf16> loc(#loc569)
    %768 = ttir.empty() : tensor<1x2x32x256xbf16> loc(#loc570)
    %769 = "ttir.transpose"(%767, %768) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x2x256x32xbf16>, tensor<1x2x32x256xbf16>) -> tensor<1x2x32x256xbf16> loc(#loc570)
    %770 = ttir.empty() : tensor<2x32x256xbf16> loc(#loc571)
    %771 = "ttir.squeeze"(%769, %770) <{dim = 0 : si32}> : (tensor<1x2x32x256xbf16>, tensor<2x32x256xbf16>) -> tensor<2x32x256xbf16> loc(#loc571)
    %772 = ttir.empty() : tensor<2x256x32xbf16> loc(#loc31)
    %773 = "ttir.transpose"(%771, %772) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<2x32x256xbf16>, tensor<2x256x32xbf16>) -> tensor<2x256x32xbf16> loc(#loc31)
    %774 = ttir.empty() : tensor<2x4096x32xbf16> loc(#loc572)
    %775 = "ttir.matmul"(%757, %773, %774) <{transpose_a = false, transpose_b = false}> : (tensor<2x4096x256xbf16>, tensor<2x256x32xbf16>, tensor<2x4096x32xbf16>) -> tensor<2x4096x32xbf16> loc(#loc572)
    %776 = ttir.empty() : tensor<1x2x4096x32xbf16> loc(#loc573)
    %777 = "ttir.unsqueeze"(%775, %776) <{dim = 0 : si32}> : (tensor<2x4096x32xbf16>, tensor<1x2x4096x32xbf16>) -> tensor<1x2x4096x32xbf16> loc(#loc573)
    %778 = ttir.empty() : tensor<1x4096x2x32xbf16> loc(#loc574)
    %779 = "ttir.transpose"(%777, %778) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x2x4096x32xbf16>, tensor<1x4096x2x32xbf16>) -> tensor<1x4096x2x32xbf16> loc(#loc574)
    %780 = ttir.empty() : tensor<4096x64xbf16> loc(#loc807)
    %781 = "ttir.reshape"(%779, %780) <{shape = [4096 : i32, 64 : i32]}> : (tensor<1x4096x2x32xbf16>, tensor<4096x64xbf16>) -> tensor<4096x64xbf16> loc(#loc807)
    %782 = ttir.empty() : tensor<4096x64xbf16> loc(#loc808)
    %783 = "ttir.matmul"(%781, %arg187, %782) <{transpose_a = false, transpose_b = false}> : (tensor<4096x64xbf16>, tensor<64x64xbf16>, tensor<4096x64xbf16>) -> tensor<4096x64xbf16> loc(#loc808)
    %784 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc809)
    %785 = "ttir.unsqueeze"(%783, %784) <{dim = 0 : si32}> : (tensor<4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc809)
    %786 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc32)
    %787 = "ttir.add"(%785, %arg188, %786) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc32)
    %788 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc129)
    %789 = "ttir.add"(%787, %655, %788) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc129)
    %790 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc315)
    %791 = "ttir.sum"(%789, %790) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc315)
    %792 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc316)
    %793 = "ttir.multiply"(%arg47, %791, %792) : (tensor<1x4096x64xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc316)
    %794 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc317)
    %795 = "ttir.subtract"(%789, %793, %794) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc317)
    %796 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc318)
    %797 = "ttir.multiply"(%795, %795, %796) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc318)
    %798 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc319)
    %799 = "ttir.sum"(%797, %798) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc319)
    %800 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc320)
    %801 = "ttir.multiply"(%arg48, %799, %800) : (tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc320)
    %802 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc321)
    %803 = "ttir.add"(%801, %arg49, %802) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc321)
    %804 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc322)
    %805 = "ttir.sqrt"(%803, %804) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc322)
    %806 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc323)
    %807 = "ttir.reciprocal"(%805, %806) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc323)
    %808 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc324)
    %809 = "ttir.multiply"(%795, %807, %808) : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc324)
    %810 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc325)
    %811 = "ttir.multiply"(%809, %arg189, %810) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc325)
    %812 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc326)
    %813 = "ttir.add"(%811, %arg190, %812) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc326)
    %814 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc575)
    %815 = "ttir.matmul"(%813, %arg191, %814) <{transpose_a = false, transpose_b = false}> : (tensor<1x4096x64xbf16>, tensor<64x256xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc575)
    %816 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc33)
    %817 = "ttir.add"(%815, %arg192, %816) : (tensor<1x4096x256xbf16>, tensor<256xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc33)
    %818 = ttir.empty() : tensor<1x256x4096xbf16> loc(#loc576)
    %819 = "ttir.transpose"(%817, %818) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x4096x256xbf16>, tensor<1x256x4096xbf16>) -> tensor<1x256x4096xbf16> loc(#loc576)
    %820 = ttir.empty() : tensor<1x256x64x64xbf16> loc(#loc577)
    %821 = "ttir.reshape"(%819, %820) <{shape = [1 : i32, 256 : i32, 64 : i32, 64 : i32]}> : (tensor<1x256x4096xbf16>, tensor<1x256x64x64xbf16>) -> tensor<1x256x64x64xbf16> loc(#loc577)
    %822 = ttir.empty() : tensor<1x64x256x64xbf16> loc(#loc810)
    %823 = "ttir.transpose"(%821, %822) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x64x64xbf16>, tensor<1x64x256x64xbf16>) -> tensor<1x64x256x64xbf16> loc(#loc810)
    %824 = ttir.empty() : tensor<1x64x64x256xbf16> loc(#loc811)
    %825 = "ttir.transpose"(%823, %824) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x256x64xbf16>, tensor<1x64x64x256xbf16>) -> tensor<1x64x64x256xbf16> loc(#loc811)
    %826 = ttir.empty() : tensor<1x64x64x256xbf16> loc(#loc812)
    %827 = "ttir.conv2d"(%825, %arg193, %arg194, %826) <{dilation = array<i32: 1, 1>, groups = 256 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x64x64x256xbf16>, tensor<256x1x3x3xbf16>, tensor<1x1x1x256xbf16>, tensor<1x64x64x256xbf16>) -> tensor<1x64x64x256xbf16> loc(#loc812)
    %828 = ttir.empty() : tensor<1x64x256x64xbf16> loc(#loc813)
    %829 = "ttir.transpose"(%827, %828) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x256xbf16>, tensor<1x64x256x64xbf16>) -> tensor<1x64x256x64xbf16> loc(#loc813)
    %830 = ttir.empty() : tensor<1x256x64x64xbf16> loc(#loc814)
    %831 = "ttir.transpose"(%829, %830) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x256x64xbf16>, tensor<1x256x64x64xbf16>) -> tensor<1x256x64x64xbf16> loc(#loc814)
    %832 = ttir.empty() : tensor<1x256x4096x1xbf16> loc(#loc578)
    %833 = "ttir.reshape"(%831, %832) <{shape = [1 : i32, 256 : i32, 4096 : i32, 1 : i32]}> : (tensor<1x256x64x64xbf16>, tensor<1x256x4096x1xbf16>) -> tensor<1x256x4096x1xbf16> loc(#loc578)
    %834 = ttir.empty() : tensor<1x256x4096xbf16> loc(#loc579)
    %835 = "ttir.squeeze"(%833, %834) <{dim = -1 : si32}> : (tensor<1x256x4096x1xbf16>, tensor<1x256x4096xbf16>) -> tensor<1x256x4096xbf16> loc(#loc579)
    %836 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc580)
    %837 = "ttir.transpose"(%835, %836) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x256x4096xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc580)
    %838 = ttir.empty() : tensor<1x4096x256xbf16> loc(#loc581)
    %839 = "ttir.gelu"(%837, %838) : (tensor<1x4096x256xbf16>, tensor<1x4096x256xbf16>) -> tensor<1x4096x256xbf16> loc(#loc581)
    %840 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc582)
    %841 = "ttir.matmul"(%839, %arg195, %840) <{transpose_a = false, transpose_b = false}> : (tensor<1x4096x256xbf16>, tensor<256x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc582)
    %842 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc34)
    %843 = "ttir.add"(%841, %arg196, %842) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc34)
    %844 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc130)
    %845 = "ttir.add"(%843, %789, %844) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc130)
    %846 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc131)
    %847 = "ttir.sum"(%845, %846) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc131)
    %848 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc132)
    %849 = "ttir.multiply"(%arg50, %847, %848) : (tensor<1x4096x64xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc132)
    %850 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc133)
    %851 = "ttir.subtract"(%845, %849, %850) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc133)
    %852 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc134)
    %853 = "ttir.multiply"(%851, %851, %852) : (tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc134)
    %854 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc135)
    %855 = "ttir.sum"(%853, %854) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc135)
    %856 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc136)
    %857 = "ttir.multiply"(%arg51, %855, %856) : (tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc136)
    %858 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc137)
    %859 = "ttir.add"(%857, %arg52, %858) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xf32>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc137)
    %860 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc138)
    %861 = "ttir.sqrt"(%859, %860) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc138)
    %862 = ttir.empty() : tensor<1x4096x1xbf16> loc(#loc139)
    %863 = "ttir.reciprocal"(%861, %862) : (tensor<1x4096x1xbf16>, tensor<1x4096x1xbf16>) -> tensor<1x4096x1xbf16> loc(#loc139)
    %864 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc140)
    %865 = "ttir.multiply"(%851, %863, %864) : (tensor<1x4096x64xbf16>, tensor<1x4096x1xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc140)
    %866 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc141)
    %867 = "ttir.multiply"(%865, %arg197, %866) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc141)
    %868 = ttir.empty() : tensor<1x4096x64xbf16> loc(#loc142)
    %869 = "ttir.add"(%867, %arg198, %868) : (tensor<1x4096x64xbf16>, tensor<64xbf16>, tensor<1x4096x64xbf16>) -> tensor<1x4096x64xbf16> loc(#loc142)
    %870 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc85)
    %871 = "ttir.reshape"(%869, %870) <{shape = [1 : i32, 64 : i32, 64 : i32, 64 : i32]}> : (tensor<1x4096x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc85)
    %872 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc35)
    %873 = "ttir.transpose"(%871, %872) <{dim0 = -3 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc35)
    %874 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc86)
    %875 = "ttir.transpose"(%873, %874) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc86)
    %876 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc327)
    %877 = "ttir.transpose"(%875, %876) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc327)
    %878 = ttir.empty() : tensor<1x64x64x64xbf16> loc(#loc328)
    %879 = "ttir.transpose"(%877, %878) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x64x64x64xbf16>, tensor<1x64x64x64xbf16>) -> tensor<1x64x64x64xbf16> loc(#loc328)
    %880 = ttir.empty() : tensor<1x32x32x160xbf16> loc(#loc329)
    %881 = "ttir.conv2d"(%879, %arg199, %arg200, %880) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> {channel_last = 1 : si32} : (tensor<1x64x64x64xbf16>, tensor<160x64x3x3xbf16>, tensor<1x1x1x160xbf16>, tensor<1x32x32x160xbf16>) -> tensor<1x32x32x160xbf16> loc(#loc329)
    %882 = ttir.empty() : tensor<1x32x160x32xbf16> loc(#loc330)
    %883 = "ttir.transpose"(%881, %882) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x32x160xbf16>, tensor<1x32x160x32xbf16>) -> tensor<1x32x160x32xbf16> loc(#loc330)
    %884 = ttir.empty() : tensor<1x160x32x32xbf16> loc(#loc331)
    %885 = "ttir.transpose"(%883, %884) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x32x160x32xbf16>, tensor<1x160x32x32xbf16>) -> tensor<1x160x32x32xbf16> loc(#loc331)
    %886 = ttir.empty() : tensor<1x160x1024x1xbf16> loc(#loc144)
    %887 = "ttir.reshape"(%885, %886) <{shape = [1 : i32, 160 : i32, 1024 : i32, 1 : i32]}> : (tensor<1x160x32x32xbf16>, tensor<1x160x1024x1xbf16>) -> tensor<1x160x1024x1xbf16> loc(#loc144)
    %888 = ttir.empty() : tensor<1x160x1024xbf16> loc(#loc145)
    %889 = "ttir.squeeze"(%887, %888) <{dim = -1 : si32}> : (tensor<1x160x1024x1xbf16>, tensor<1x160x1024xbf16>) -> tensor<1x160x1024xbf16> loc(#loc145)
    %890 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc146)
    %891 = "ttir.transpose"(%889, %890) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x160x1024xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc146)
    %892 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc332)
    %893 = "ttir.sum"(%891, %892) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc332)
    %894 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc333)
    %895 = "ttir.multiply"(%arg53, %893, %894) : (tensor<1x1024x160xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc333)
    %896 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc334)
    %897 = "ttir.subtract"(%891, %895, %896) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc334)
    %898 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc335)
    %899 = "ttir.multiply"(%897, %897, %898) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc335)
    %900 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc336)
    %901 = "ttir.sum"(%899, %900) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc336)
    %902 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc337)
    %903 = "ttir.multiply"(%arg54, %901, %902) : (tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc337)
    %904 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc338)
    %905 = "ttir.add"(%903, %arg55, %904) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc338)
    %906 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc339)
    %907 = "ttir.sqrt"(%905, %906) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc339)
    %908 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc340)
    %909 = "ttir.reciprocal"(%907, %908) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc340)
    %910 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc341)
    %911 = "ttir.multiply"(%897, %909, %910) : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc341)
    %912 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc342)
    %913 = "ttir.multiply"(%911, %arg201, %912) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc342)
    %914 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc343)
    %915 = "ttir.add"(%913, %arg202, %914) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc343)
    %916 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc344)
    %917 = "ttir.sum"(%915, %916) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc344)
    %918 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc345)
    %919 = "ttir.multiply"(%arg56, %917, %918) : (tensor<1x1024x160xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc345)
    %920 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc346)
    %921 = "ttir.subtract"(%915, %919, %920) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc346)
    %922 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc347)
    %923 = "ttir.multiply"(%921, %921, %922) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc347)
    %924 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc348)
    %925 = "ttir.sum"(%923, %924) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc348)
    %926 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc349)
    %927 = "ttir.multiply"(%arg57, %925, %926) : (tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc349)
    %928 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc350)
    %929 = "ttir.add"(%927, %arg58, %928) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc350)
    %930 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc351)
    %931 = "ttir.sqrt"(%929, %930) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc351)
    %932 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc352)
    %933 = "ttir.reciprocal"(%931, %932) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc352)
    %934 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc353)
    %935 = "ttir.multiply"(%921, %933, %934) : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc353)
    %936 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc354)
    %937 = "ttir.multiply"(%935, %arg203, %936) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc354)
    %938 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc355)
    %939 = "ttir.add"(%937, %arg204, %938) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc355)
    %940 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc815)
    %941 = "ttir.matmul"(%939, %arg205, %940) <{transpose_a = false, transpose_b = false}> : (tensor<1x1024x160xbf16>, tensor<160x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc815)
    %942 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc36)
    %943 = "ttir.add"(%941, %arg206, %942) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc36)
    %944 = ttir.empty() : tensor<1x1024x5x32xbf16> loc(#loc583)
    %945 = "ttir.reshape"(%943, %944) <{shape = [1 : i32, 1024 : i32, 5 : i32, 32 : i32]}> : (tensor<1x1024x160xbf16>, tensor<1x1024x5x32xbf16>) -> tensor<1x1024x5x32xbf16> loc(#loc583)
    %946 = ttir.empty() : tensor<1x5x1024x32xbf16> loc(#loc584)
    %947 = "ttir.transpose"(%945, %946) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x5x32xbf16>, tensor<1x5x1024x32xbf16>) -> tensor<1x5x1024x32xbf16> loc(#loc584)
    %948 = ttir.empty() : tensor<5x1024x32xbf16> loc(#loc585)
    %949 = "ttir.squeeze"(%947, %948) <{dim = 0 : si32}> : (tensor<1x5x1024x32xbf16>, tensor<5x1024x32xbf16>) -> tensor<5x1024x32xbf16> loc(#loc585)
    %950 = ttir.empty() : tensor<1x160x1024xbf16> loc(#loc586)
    %951 = "ttir.transpose"(%939, %950) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1024x160xbf16>, tensor<1x160x1024xbf16>) -> tensor<1x160x1024xbf16> loc(#loc586)
    %952 = ttir.empty() : tensor<1x160x32x32xbf16> loc(#loc587)
    %953 = "ttir.reshape"(%951, %952) <{shape = [1 : i32, 160 : i32, 32 : i32, 32 : i32]}> : (tensor<1x160x1024xbf16>, tensor<1x160x32x32xbf16>) -> tensor<1x160x32x32xbf16> loc(#loc587)
    %954 = ttir.empty() : tensor<1x32x160x32xbf16> loc(#loc816)
    %955 = "ttir.transpose"(%953, %954) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x160x32x32xbf16>, tensor<1x32x160x32xbf16>) -> tensor<1x32x160x32xbf16> loc(#loc816)
    %956 = ttir.empty() : tensor<1x32x32x160xbf16> loc(#loc817)
    %957 = "ttir.transpose"(%955, %956) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x160x32xbf16>, tensor<1x32x32x160xbf16>) -> tensor<1x32x32x160xbf16> loc(#loc817)
    %958 = ttir.empty() : tensor<1x16x16x160xbf16> loc(#loc818)
    %959 = "ttir.conv2d"(%957, %arg207, %arg208, %958) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> {channel_last = 1 : si32} : (tensor<1x32x32x160xbf16>, tensor<160x160x2x2xbf16>, tensor<1x1x1x160xbf16>, tensor<1x16x16x160xbf16>) -> tensor<1x16x16x160xbf16> loc(#loc818)
    %960 = ttir.empty() : tensor<1x16x160x16xbf16> loc(#loc819)
    %961 = "ttir.transpose"(%959, %960) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x160xbf16>, tensor<1x16x160x16xbf16>) -> tensor<1x16x160x16xbf16> loc(#loc819)
    %962 = ttir.empty() : tensor<1x160x16x16xbf16> loc(#loc820)
    %963 = "ttir.transpose"(%961, %962) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x160x16xbf16>, tensor<1x160x16x16xbf16>) -> tensor<1x160x16x16xbf16> loc(#loc820)
    %964 = ttir.empty() : tensor<1x160x256xbf16> loc(#loc588)
    %965 = "ttir.reshape"(%963, %964) <{shape = [1 : i32, 160 : i32, 256 : i32]}> : (tensor<1x160x16x16xbf16>, tensor<1x160x256xbf16>) -> tensor<1x160x256xbf16> loc(#loc588)
    %966 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc589)
    %967 = "ttir.transpose"(%965, %966) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x160x256xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc589)
    %968 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc821)
    %969 = "ttir.sum"(%967, %968) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x160xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc821)
    %970 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc822)
    %971 = "ttir.multiply"(%arg59, %969, %970) : (tensor<1x256x160xf32>, tensor<1x256x1xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc822)
    %972 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc823)
    %973 = "ttir.subtract"(%967, %971, %972) : (tensor<1x256x160xbf16>, tensor<1x256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc823)
    %974 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc824)
    %975 = "ttir.multiply"(%973, %973, %974) : (tensor<1x256x160xbf16>, tensor<1x256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc824)
    %976 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc825)
    %977 = "ttir.sum"(%975, %976) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x160xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc825)
    %978 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc826)
    %979 = "ttir.multiply"(%arg60, %977, %978) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc826)
    %980 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc827)
    %981 = "ttir.add"(%979, %arg61, %980) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc827)
    %982 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc828)
    %983 = "ttir.sqrt"(%981, %982) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc828)
    %984 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc829)
    %985 = "ttir.reciprocal"(%983, %984) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc829)
    %986 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc830)
    %987 = "ttir.multiply"(%973, %985, %986) : (tensor<1x256x160xbf16>, tensor<1x256x1xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc830)
    %988 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc831)
    %989 = "ttir.multiply"(%987, %arg209, %988) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc831)
    %990 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc832)
    %991 = "ttir.add"(%989, %arg210, %990) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc832)
    %992 = ttir.empty() : tensor<256x160xbf16> loc(#loc833)
    %993 = "ttir.squeeze"(%991, %992) <{dim = 0 : si32}> : (tensor<1x256x160xbf16>, tensor<256x160xbf16>) -> tensor<256x160xbf16> loc(#loc833)
    %994 = ttir.empty() : tensor<256x160xbf16> loc(#loc834)
    %995 = "ttir.matmul"(%993, %arg211, %994) <{transpose_a = false, transpose_b = false}> : (tensor<256x160xbf16>, tensor<160x160xbf16>, tensor<256x160xbf16>) -> tensor<256x160xbf16> loc(#loc834)
    %996 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc835)
    %997 = "ttir.unsqueeze"(%995, %996) <{dim = 0 : si32}> : (tensor<256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc835)
    %998 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc37)
    %999 = "ttir.add"(%997, %arg212, %998) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc37)
    %1000 = ttir.empty() : tensor<1x256x5x32xbf16> loc(#loc590)
    %1001 = "ttir.reshape"(%999, %1000) <{shape = [1 : i32, 256 : i32, 5 : i32, 32 : i32]}> : (tensor<1x256x160xbf16>, tensor<1x256x5x32xbf16>) -> tensor<1x256x5x32xbf16> loc(#loc590)
    %1002 = ttir.empty() : tensor<1x5x256x32xbf16> loc(#loc591)
    %1003 = "ttir.transpose"(%1001, %1002) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x5x32xbf16>, tensor<1x5x256x32xbf16>) -> tensor<1x5x256x32xbf16> loc(#loc591)
    %1004 = ttir.empty() : tensor<5x256x32xbf16> loc(#loc592)
    %1005 = "ttir.squeeze"(%1003, %1004) <{dim = 0 : si32}> : (tensor<1x5x256x32xbf16>, tensor<5x256x32xbf16>) -> tensor<5x256x32xbf16> loc(#loc592)
    %1006 = ttir.empty() : tensor<5x32x256xbf16> loc(#loc38)
    %1007 = "ttir.transpose"(%1005, %1006) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<5x256x32xbf16>, tensor<5x32x256xbf16>) -> tensor<5x32x256xbf16> loc(#loc38)
    %1008 = ttir.empty() : tensor<5x1024x256xbf16> loc(#loc593)
    %1009 = "ttir.matmul"(%949, %1007, %1008) <{transpose_a = false, transpose_b = false}> : (tensor<5x1024x32xbf16>, tensor<5x32x256xbf16>, tensor<5x1024x256xbf16>) -> tensor<5x1024x256xbf16> loc(#loc593)
    %1010 = ttir.empty() : tensor<1x5x1024x256xbf16> loc(#loc594)
    %1011 = "ttir.unsqueeze"(%1009, %1010) <{dim = 0 : si32}> : (tensor<5x1024x256xbf16>, tensor<1x5x1024x256xbf16>) -> tensor<1x5x1024x256xbf16> loc(#loc594)
    %1012 = ttir.empty() : tensor<1x5x1024x256xbf16> loc(#loc595)
    %1013 = "ttir.div"(%1011, %arg62, %1012) : (tensor<1x5x1024x256xbf16>, tensor<1xbf16>, tensor<1x5x1024x256xbf16>) -> tensor<1x5x1024x256xbf16> loc(#loc595)
    %1014 = ttir.empty() : tensor<1x5x1024x256xbf16> loc(#loc596)
    %1015 = "ttir.softmax"(%1013, %1014) <{dimension = -1 : si32}> : (tensor<1x5x1024x256xbf16>, tensor<1x5x1024x256xbf16>) -> tensor<1x5x1024x256xbf16> loc(#loc596)
    %1016 = ttir.empty() : tensor<5x1024x256xbf16> loc(#loc597)
    %1017 = "ttir.squeeze"(%1015, %1016) <{dim = 0 : si32}> : (tensor<1x5x1024x256xbf16>, tensor<5x1024x256xbf16>) -> tensor<5x1024x256xbf16> loc(#loc597)
    %1018 = ttir.empty() : tensor<256x160xbf16> loc(#loc836)
    %1019 = "ttir.matmul"(%993, %arg213, %1018) <{transpose_a = false, transpose_b = false}> : (tensor<256x160xbf16>, tensor<160x160xbf16>, tensor<256x160xbf16>) -> tensor<256x160xbf16> loc(#loc836)
    %1020 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc837)
    %1021 = "ttir.unsqueeze"(%1019, %1020) <{dim = 0 : si32}> : (tensor<256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc837)
    %1022 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc39)
    %1023 = "ttir.add"(%1021, %arg214, %1022) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc39)
    %1024 = ttir.empty() : tensor<1x256x5x32xbf16> loc(#loc598)
    %1025 = "ttir.reshape"(%1023, %1024) <{shape = [1 : i32, 256 : i32, 5 : i32, 32 : i32]}> : (tensor<1x256x160xbf16>, tensor<1x256x5x32xbf16>) -> tensor<1x256x5x32xbf16> loc(#loc598)
    %1026 = ttir.empty() : tensor<1x5x256x32xbf16> loc(#loc599)
    %1027 = "ttir.transpose"(%1025, %1026) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x5x32xbf16>, tensor<1x5x256x32xbf16>) -> tensor<1x5x256x32xbf16> loc(#loc599)
    %1028 = ttir.empty() : tensor<1x5x32x256xbf16> loc(#loc600)
    %1029 = "ttir.transpose"(%1027, %1028) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x5x256x32xbf16>, tensor<1x5x32x256xbf16>) -> tensor<1x5x32x256xbf16> loc(#loc600)
    %1030 = ttir.empty() : tensor<5x32x256xbf16> loc(#loc601)
    %1031 = "ttir.squeeze"(%1029, %1030) <{dim = 0 : si32}> : (tensor<1x5x32x256xbf16>, tensor<5x32x256xbf16>) -> tensor<5x32x256xbf16> loc(#loc601)
    %1032 = ttir.empty() : tensor<5x256x32xbf16> loc(#loc40)
    %1033 = "ttir.transpose"(%1031, %1032) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<5x32x256xbf16>, tensor<5x256x32xbf16>) -> tensor<5x256x32xbf16> loc(#loc40)
    %1034 = ttir.empty() : tensor<5x1024x32xbf16> loc(#loc602)
    %1035 = "ttir.matmul"(%1017, %1033, %1034) <{transpose_a = false, transpose_b = false}> : (tensor<5x1024x256xbf16>, tensor<5x256x32xbf16>, tensor<5x1024x32xbf16>) -> tensor<5x1024x32xbf16> loc(#loc602)
    %1036 = ttir.empty() : tensor<1x5x1024x32xbf16> loc(#loc603)
    %1037 = "ttir.unsqueeze"(%1035, %1036) <{dim = 0 : si32}> : (tensor<5x1024x32xbf16>, tensor<1x5x1024x32xbf16>) -> tensor<1x5x1024x32xbf16> loc(#loc603)
    %1038 = ttir.empty() : tensor<1x1024x5x32xbf16> loc(#loc604)
    %1039 = "ttir.transpose"(%1037, %1038) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x5x1024x32xbf16>, tensor<1x1024x5x32xbf16>) -> tensor<1x1024x5x32xbf16> loc(#loc604)
    %1040 = ttir.empty() : tensor<1024x160xbf16> loc(#loc838)
    %1041 = "ttir.reshape"(%1039, %1040) <{shape = [1024 : i32, 160 : i32]}> : (tensor<1x1024x5x32xbf16>, tensor<1024x160xbf16>) -> tensor<1024x160xbf16> loc(#loc838)
    %1042 = ttir.empty() : tensor<1024x160xbf16> loc(#loc839)
    %1043 = "ttir.matmul"(%1041, %arg215, %1042) <{transpose_a = false, transpose_b = false}> : (tensor<1024x160xbf16>, tensor<160x160xbf16>, tensor<1024x160xbf16>) -> tensor<1024x160xbf16> loc(#loc839)
    %1044 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc840)
    %1045 = "ttir.unsqueeze"(%1043, %1044) <{dim = 0 : si32}> : (tensor<1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc840)
    %1046 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc41)
    %1047 = "ttir.add"(%1045, %arg216, %1046) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc41)
    %1048 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc148)
    %1049 = "ttir.add"(%1047, %915, %1048) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc148)
    %1050 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc356)
    %1051 = "ttir.sum"(%1049, %1050) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc356)
    %1052 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc357)
    %1053 = "ttir.multiply"(%arg63, %1051, %1052) : (tensor<1x1024x160xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc357)
    %1054 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc358)
    %1055 = "ttir.subtract"(%1049, %1053, %1054) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc358)
    %1056 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc359)
    %1057 = "ttir.multiply"(%1055, %1055, %1056) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc359)
    %1058 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc360)
    %1059 = "ttir.sum"(%1057, %1058) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc360)
    %1060 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc361)
    %1061 = "ttir.multiply"(%arg64, %1059, %1060) : (tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc361)
    %1062 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc362)
    %1063 = "ttir.add"(%1061, %arg65, %1062) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc362)
    %1064 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc363)
    %1065 = "ttir.sqrt"(%1063, %1064) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc363)
    %1066 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc364)
    %1067 = "ttir.reciprocal"(%1065, %1066) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc364)
    %1068 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc365)
    %1069 = "ttir.multiply"(%1055, %1067, %1068) : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc365)
    %1070 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc366)
    %1071 = "ttir.multiply"(%1069, %arg217, %1070) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc366)
    %1072 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc367)
    %1073 = "ttir.add"(%1071, %arg218, %1072) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc367)
    %1074 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc605)
    %1075 = "ttir.matmul"(%1073, %arg219, %1074) <{transpose_a = false, transpose_b = false}> : (tensor<1x1024x160xbf16>, tensor<160x640xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc605)
    %1076 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc42)
    %1077 = "ttir.add"(%1075, %arg220, %1076) : (tensor<1x1024x640xbf16>, tensor<640xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc42)
    %1078 = ttir.empty() : tensor<1x640x1024xbf16> loc(#loc606)
    %1079 = "ttir.transpose"(%1077, %1078) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1024x640xbf16>, tensor<1x640x1024xbf16>) -> tensor<1x640x1024xbf16> loc(#loc606)
    %1080 = ttir.empty() : tensor<1x640x32x32xbf16> loc(#loc607)
    %1081 = "ttir.reshape"(%1079, %1080) <{shape = [1 : i32, 640 : i32, 32 : i32, 32 : i32]}> : (tensor<1x640x1024xbf16>, tensor<1x640x32x32xbf16>) -> tensor<1x640x32x32xbf16> loc(#loc607)
    %1082 = ttir.empty() : tensor<1x32x640x32xbf16> loc(#loc841)
    %1083 = "ttir.transpose"(%1081, %1082) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x640x32x32xbf16>, tensor<1x32x640x32xbf16>) -> tensor<1x32x640x32xbf16> loc(#loc841)
    %1084 = ttir.empty() : tensor<1x32x32x640xbf16> loc(#loc842)
    %1085 = "ttir.transpose"(%1083, %1084) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x640x32xbf16>, tensor<1x32x32x640xbf16>) -> tensor<1x32x32x640xbf16> loc(#loc842)
    %1086 = ttir.empty() : tensor<1x32x32x640xbf16> loc(#loc843)
    %1087 = "ttir.conv2d"(%1085, %arg221, %arg222, %1086) <{dilation = array<i32: 1, 1>, groups = 640 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x32x32x640xbf16>, tensor<640x1x3x3xbf16>, tensor<1x1x1x640xbf16>, tensor<1x32x32x640xbf16>) -> tensor<1x32x32x640xbf16> loc(#loc843)
    %1088 = ttir.empty() : tensor<1x32x640x32xbf16> loc(#loc844)
    %1089 = "ttir.transpose"(%1087, %1088) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x32x640xbf16>, tensor<1x32x640x32xbf16>) -> tensor<1x32x640x32xbf16> loc(#loc844)
    %1090 = ttir.empty() : tensor<1x640x32x32xbf16> loc(#loc845)
    %1091 = "ttir.transpose"(%1089, %1090) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x32x640x32xbf16>, tensor<1x640x32x32xbf16>) -> tensor<1x640x32x32xbf16> loc(#loc845)
    %1092 = ttir.empty() : tensor<1x640x1024x1xbf16> loc(#loc608)
    %1093 = "ttir.reshape"(%1091, %1092) <{shape = [1 : i32, 640 : i32, 1024 : i32, 1 : i32]}> : (tensor<1x640x32x32xbf16>, tensor<1x640x1024x1xbf16>) -> tensor<1x640x1024x1xbf16> loc(#loc608)
    %1094 = ttir.empty() : tensor<1x640x1024xbf16> loc(#loc609)
    %1095 = "ttir.squeeze"(%1093, %1094) <{dim = -1 : si32}> : (tensor<1x640x1024x1xbf16>, tensor<1x640x1024xbf16>) -> tensor<1x640x1024xbf16> loc(#loc609)
    %1096 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc610)
    %1097 = "ttir.transpose"(%1095, %1096) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x640x1024xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc610)
    %1098 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc611)
    %1099 = "ttir.gelu"(%1097, %1098) : (tensor<1x1024x640xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc611)
    %1100 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc612)
    %1101 = "ttir.matmul"(%1099, %arg223, %1100) <{transpose_a = false, transpose_b = false}> : (tensor<1x1024x640xbf16>, tensor<640x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc612)
    %1102 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc43)
    %1103 = "ttir.add"(%1101, %arg224, %1102) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc43)
    %1104 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc149)
    %1105 = "ttir.add"(%1103, %1049, %1104) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc149)
    %1106 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc368)
    %1107 = "ttir.sum"(%1105, %1106) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc368)
    %1108 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc369)
    %1109 = "ttir.multiply"(%arg66, %1107, %1108) : (tensor<1x1024x160xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc369)
    %1110 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc370)
    %1111 = "ttir.subtract"(%1105, %1109, %1110) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc370)
    %1112 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc371)
    %1113 = "ttir.multiply"(%1111, %1111, %1112) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc371)
    %1114 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc372)
    %1115 = "ttir.sum"(%1113, %1114) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc372)
    %1116 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc373)
    %1117 = "ttir.multiply"(%arg67, %1115, %1116) : (tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc373)
    %1118 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc374)
    %1119 = "ttir.add"(%1117, %arg68, %1118) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc374)
    %1120 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc375)
    %1121 = "ttir.sqrt"(%1119, %1120) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc375)
    %1122 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc376)
    %1123 = "ttir.reciprocal"(%1121, %1122) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc376)
    %1124 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc377)
    %1125 = "ttir.multiply"(%1111, %1123, %1124) : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc377)
    %1126 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc378)
    %1127 = "ttir.multiply"(%1125, %arg225, %1126) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc378)
    %1128 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc379)
    %1129 = "ttir.add"(%1127, %arg226, %1128) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc379)
    %1130 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc846)
    %1131 = "ttir.matmul"(%1129, %arg227, %1130) <{transpose_a = false, transpose_b = false}> : (tensor<1x1024x160xbf16>, tensor<160x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc846)
    %1132 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc44)
    %1133 = "ttir.add"(%1131, %arg228, %1132) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc44)
    %1134 = ttir.empty() : tensor<1x1024x5x32xbf16> loc(#loc613)
    %1135 = "ttir.reshape"(%1133, %1134) <{shape = [1 : i32, 1024 : i32, 5 : i32, 32 : i32]}> : (tensor<1x1024x160xbf16>, tensor<1x1024x5x32xbf16>) -> tensor<1x1024x5x32xbf16> loc(#loc613)
    %1136 = ttir.empty() : tensor<1x5x1024x32xbf16> loc(#loc614)
    %1137 = "ttir.transpose"(%1135, %1136) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x5x32xbf16>, tensor<1x5x1024x32xbf16>) -> tensor<1x5x1024x32xbf16> loc(#loc614)
    %1138 = ttir.empty() : tensor<5x1024x32xbf16> loc(#loc615)
    %1139 = "ttir.squeeze"(%1137, %1138) <{dim = 0 : si32}> : (tensor<1x5x1024x32xbf16>, tensor<5x1024x32xbf16>) -> tensor<5x1024x32xbf16> loc(#loc615)
    %1140 = ttir.empty() : tensor<1x160x1024xbf16> loc(#loc616)
    %1141 = "ttir.transpose"(%1129, %1140) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1024x160xbf16>, tensor<1x160x1024xbf16>) -> tensor<1x160x1024xbf16> loc(#loc616)
    %1142 = ttir.empty() : tensor<1x160x32x32xbf16> loc(#loc617)
    %1143 = "ttir.reshape"(%1141, %1142) <{shape = [1 : i32, 160 : i32, 32 : i32, 32 : i32]}> : (tensor<1x160x1024xbf16>, tensor<1x160x32x32xbf16>) -> tensor<1x160x32x32xbf16> loc(#loc617)
    %1144 = ttir.empty() : tensor<1x32x160x32xbf16> loc(#loc847)
    %1145 = "ttir.transpose"(%1143, %1144) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x160x32x32xbf16>, tensor<1x32x160x32xbf16>) -> tensor<1x32x160x32xbf16> loc(#loc847)
    %1146 = ttir.empty() : tensor<1x32x32x160xbf16> loc(#loc848)
    %1147 = "ttir.transpose"(%1145, %1146) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x160x32xbf16>, tensor<1x32x32x160xbf16>) -> tensor<1x32x32x160xbf16> loc(#loc848)
    %1148 = ttir.empty() : tensor<1x16x16x160xbf16> loc(#loc849)
    %1149 = "ttir.conv2d"(%1147, %arg229, %arg230, %1148) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 2, 2>}> {channel_last = 1 : si32} : (tensor<1x32x32x160xbf16>, tensor<160x160x2x2xbf16>, tensor<1x1x1x160xbf16>, tensor<1x16x16x160xbf16>) -> tensor<1x16x16x160xbf16> loc(#loc849)
    %1150 = ttir.empty() : tensor<1x16x160x16xbf16> loc(#loc850)
    %1151 = "ttir.transpose"(%1149, %1150) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x160xbf16>, tensor<1x16x160x16xbf16>) -> tensor<1x16x160x16xbf16> loc(#loc850)
    %1152 = ttir.empty() : tensor<1x160x16x16xbf16> loc(#loc851)
    %1153 = "ttir.transpose"(%1151, %1152) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x160x16xbf16>, tensor<1x160x16x16xbf16>) -> tensor<1x160x16x16xbf16> loc(#loc851)
    %1154 = ttir.empty() : tensor<1x160x256xbf16> loc(#loc618)
    %1155 = "ttir.reshape"(%1153, %1154) <{shape = [1 : i32, 160 : i32, 256 : i32]}> : (tensor<1x160x16x16xbf16>, tensor<1x160x256xbf16>) -> tensor<1x160x256xbf16> loc(#loc618)
    %1156 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc619)
    %1157 = "ttir.transpose"(%1155, %1156) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x160x256xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc619)
    %1158 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc852)
    %1159 = "ttir.sum"(%1157, %1158) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x160xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc852)
    %1160 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc853)
    %1161 = "ttir.multiply"(%arg69, %1159, %1160) : (tensor<1x256x160xf32>, tensor<1x256x1xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc853)
    %1162 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc854)
    %1163 = "ttir.subtract"(%1157, %1161, %1162) : (tensor<1x256x160xbf16>, tensor<1x256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc854)
    %1164 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc855)
    %1165 = "ttir.multiply"(%1163, %1163, %1164) : (tensor<1x256x160xbf16>, tensor<1x256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc855)
    %1166 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc856)
    %1167 = "ttir.sum"(%1165, %1166) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x160xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc856)
    %1168 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc857)
    %1169 = "ttir.multiply"(%arg70, %1167, %1168) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc857)
    %1170 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc858)
    %1171 = "ttir.add"(%1169, %arg71, %1170) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc858)
    %1172 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc859)
    %1173 = "ttir.sqrt"(%1171, %1172) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc859)
    %1174 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc860)
    %1175 = "ttir.reciprocal"(%1173, %1174) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc860)
    %1176 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc861)
    %1177 = "ttir.multiply"(%1163, %1175, %1176) : (tensor<1x256x160xbf16>, tensor<1x256x1xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc861)
    %1178 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc862)
    %1179 = "ttir.multiply"(%1177, %arg231, %1178) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc862)
    %1180 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc863)
    %1181 = "ttir.add"(%1179, %arg232, %1180) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc863)
    %1182 = ttir.empty() : tensor<256x160xbf16> loc(#loc864)
    %1183 = "ttir.squeeze"(%1181, %1182) <{dim = 0 : si32}> : (tensor<1x256x160xbf16>, tensor<256x160xbf16>) -> tensor<256x160xbf16> loc(#loc864)
    %1184 = ttir.empty() : tensor<256x160xbf16> loc(#loc865)
    %1185 = "ttir.matmul"(%1183, %arg233, %1184) <{transpose_a = false, transpose_b = false}> : (tensor<256x160xbf16>, tensor<160x160xbf16>, tensor<256x160xbf16>) -> tensor<256x160xbf16> loc(#loc865)
    %1186 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc866)
    %1187 = "ttir.unsqueeze"(%1185, %1186) <{dim = 0 : si32}> : (tensor<256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc866)
    %1188 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc45)
    %1189 = "ttir.add"(%1187, %arg234, %1188) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc45)
    %1190 = ttir.empty() : tensor<1x256x5x32xbf16> loc(#loc620)
    %1191 = "ttir.reshape"(%1189, %1190) <{shape = [1 : i32, 256 : i32, 5 : i32, 32 : i32]}> : (tensor<1x256x160xbf16>, tensor<1x256x5x32xbf16>) -> tensor<1x256x5x32xbf16> loc(#loc620)
    %1192 = ttir.empty() : tensor<1x5x256x32xbf16> loc(#loc621)
    %1193 = "ttir.transpose"(%1191, %1192) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x5x32xbf16>, tensor<1x5x256x32xbf16>) -> tensor<1x5x256x32xbf16> loc(#loc621)
    %1194 = ttir.empty() : tensor<5x256x32xbf16> loc(#loc622)
    %1195 = "ttir.squeeze"(%1193, %1194) <{dim = 0 : si32}> : (tensor<1x5x256x32xbf16>, tensor<5x256x32xbf16>) -> tensor<5x256x32xbf16> loc(#loc622)
    %1196 = ttir.empty() : tensor<5x32x256xbf16> loc(#loc46)
    %1197 = "ttir.transpose"(%1195, %1196) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<5x256x32xbf16>, tensor<5x32x256xbf16>) -> tensor<5x32x256xbf16> loc(#loc46)
    %1198 = ttir.empty() : tensor<5x1024x256xbf16> loc(#loc623)
    %1199 = "ttir.matmul"(%1139, %1197, %1198) <{transpose_a = false, transpose_b = false}> : (tensor<5x1024x32xbf16>, tensor<5x32x256xbf16>, tensor<5x1024x256xbf16>) -> tensor<5x1024x256xbf16> loc(#loc623)
    %1200 = ttir.empty() : tensor<1x5x1024x256xbf16> loc(#loc624)
    %1201 = "ttir.unsqueeze"(%1199, %1200) <{dim = 0 : si32}> : (tensor<5x1024x256xbf16>, tensor<1x5x1024x256xbf16>) -> tensor<1x5x1024x256xbf16> loc(#loc624)
    %1202 = ttir.empty() : tensor<1x5x1024x256xbf16> loc(#loc625)
    %1203 = "ttir.div"(%1201, %arg72, %1202) : (tensor<1x5x1024x256xbf16>, tensor<1xbf16>, tensor<1x5x1024x256xbf16>) -> tensor<1x5x1024x256xbf16> loc(#loc625)
    %1204 = ttir.empty() : tensor<1x5x1024x256xbf16> loc(#loc626)
    %1205 = "ttir.softmax"(%1203, %1204) <{dimension = -1 : si32}> : (tensor<1x5x1024x256xbf16>, tensor<1x5x1024x256xbf16>) -> tensor<1x5x1024x256xbf16> loc(#loc626)
    %1206 = ttir.empty() : tensor<5x1024x256xbf16> loc(#loc627)
    %1207 = "ttir.squeeze"(%1205, %1206) <{dim = 0 : si32}> : (tensor<1x5x1024x256xbf16>, tensor<5x1024x256xbf16>) -> tensor<5x1024x256xbf16> loc(#loc627)
    %1208 = ttir.empty() : tensor<256x160xbf16> loc(#loc867)
    %1209 = "ttir.matmul"(%1183, %arg235, %1208) <{transpose_a = false, transpose_b = false}> : (tensor<256x160xbf16>, tensor<160x160xbf16>, tensor<256x160xbf16>) -> tensor<256x160xbf16> loc(#loc867)
    %1210 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc868)
    %1211 = "ttir.unsqueeze"(%1209, %1210) <{dim = 0 : si32}> : (tensor<256x160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc868)
    %1212 = ttir.empty() : tensor<1x256x160xbf16> loc(#loc47)
    %1213 = "ttir.add"(%1211, %arg236, %1212) : (tensor<1x256x160xbf16>, tensor<160xbf16>, tensor<1x256x160xbf16>) -> tensor<1x256x160xbf16> loc(#loc47)
    %1214 = ttir.empty() : tensor<1x256x5x32xbf16> loc(#loc628)
    %1215 = "ttir.reshape"(%1213, %1214) <{shape = [1 : i32, 256 : i32, 5 : i32, 32 : i32]}> : (tensor<1x256x160xbf16>, tensor<1x256x5x32xbf16>) -> tensor<1x256x5x32xbf16> loc(#loc628)
    %1216 = ttir.empty() : tensor<1x5x256x32xbf16> loc(#loc629)
    %1217 = "ttir.transpose"(%1215, %1216) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x5x32xbf16>, tensor<1x5x256x32xbf16>) -> tensor<1x5x256x32xbf16> loc(#loc629)
    %1218 = ttir.empty() : tensor<1x5x32x256xbf16> loc(#loc630)
    %1219 = "ttir.transpose"(%1217, %1218) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x5x256x32xbf16>, tensor<1x5x32x256xbf16>) -> tensor<1x5x32x256xbf16> loc(#loc630)
    %1220 = ttir.empty() : tensor<5x32x256xbf16> loc(#loc631)
    %1221 = "ttir.squeeze"(%1219, %1220) <{dim = 0 : si32}> : (tensor<1x5x32x256xbf16>, tensor<5x32x256xbf16>) -> tensor<5x32x256xbf16> loc(#loc631)
    %1222 = ttir.empty() : tensor<5x256x32xbf16> loc(#loc48)
    %1223 = "ttir.transpose"(%1221, %1222) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<5x32x256xbf16>, tensor<5x256x32xbf16>) -> tensor<5x256x32xbf16> loc(#loc48)
    %1224 = ttir.empty() : tensor<5x1024x32xbf16> loc(#loc632)
    %1225 = "ttir.matmul"(%1207, %1223, %1224) <{transpose_a = false, transpose_b = false}> : (tensor<5x1024x256xbf16>, tensor<5x256x32xbf16>, tensor<5x1024x32xbf16>) -> tensor<5x1024x32xbf16> loc(#loc632)
    %1226 = ttir.empty() : tensor<1x5x1024x32xbf16> loc(#loc633)
    %1227 = "ttir.unsqueeze"(%1225, %1226) <{dim = 0 : si32}> : (tensor<5x1024x32xbf16>, tensor<1x5x1024x32xbf16>) -> tensor<1x5x1024x32xbf16> loc(#loc633)
    %1228 = ttir.empty() : tensor<1x1024x5x32xbf16> loc(#loc634)
    %1229 = "ttir.transpose"(%1227, %1228) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x5x1024x32xbf16>, tensor<1x1024x5x32xbf16>) -> tensor<1x1024x5x32xbf16> loc(#loc634)
    %1230 = ttir.empty() : tensor<1024x160xbf16> loc(#loc869)
    %1231 = "ttir.reshape"(%1229, %1230) <{shape = [1024 : i32, 160 : i32]}> : (tensor<1x1024x5x32xbf16>, tensor<1024x160xbf16>) -> tensor<1024x160xbf16> loc(#loc869)
    %1232 = ttir.empty() : tensor<1024x160xbf16> loc(#loc870)
    %1233 = "ttir.matmul"(%1231, %arg237, %1232) <{transpose_a = false, transpose_b = false}> : (tensor<1024x160xbf16>, tensor<160x160xbf16>, tensor<1024x160xbf16>) -> tensor<1024x160xbf16> loc(#loc870)
    %1234 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc871)
    %1235 = "ttir.unsqueeze"(%1233, %1234) <{dim = 0 : si32}> : (tensor<1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc871)
    %1236 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc49)
    %1237 = "ttir.add"(%1235, %arg238, %1236) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc49)
    %1238 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc150)
    %1239 = "ttir.add"(%1237, %1105, %1238) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc150)
    %1240 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc380)
    %1241 = "ttir.sum"(%1239, %1240) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc380)
    %1242 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc381)
    %1243 = "ttir.multiply"(%arg73, %1241, %1242) : (tensor<1x1024x160xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc381)
    %1244 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc382)
    %1245 = "ttir.subtract"(%1239, %1243, %1244) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc382)
    %1246 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc383)
    %1247 = "ttir.multiply"(%1245, %1245, %1246) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc383)
    %1248 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc384)
    %1249 = "ttir.sum"(%1247, %1248) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc384)
    %1250 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc385)
    %1251 = "ttir.multiply"(%arg74, %1249, %1250) : (tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc385)
    %1252 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc386)
    %1253 = "ttir.add"(%1251, %arg75, %1252) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc386)
    %1254 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc387)
    %1255 = "ttir.sqrt"(%1253, %1254) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc387)
    %1256 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc388)
    %1257 = "ttir.reciprocal"(%1255, %1256) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc388)
    %1258 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc389)
    %1259 = "ttir.multiply"(%1245, %1257, %1258) : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc389)
    %1260 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc390)
    %1261 = "ttir.multiply"(%1259, %arg239, %1260) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc390)
    %1262 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc391)
    %1263 = "ttir.add"(%1261, %arg240, %1262) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc391)
    %1264 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc635)
    %1265 = "ttir.matmul"(%1263, %arg241, %1264) <{transpose_a = false, transpose_b = false}> : (tensor<1x1024x160xbf16>, tensor<160x640xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc635)
    %1266 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc50)
    %1267 = "ttir.add"(%1265, %arg242, %1266) : (tensor<1x1024x640xbf16>, tensor<640xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc50)
    %1268 = ttir.empty() : tensor<1x640x1024xbf16> loc(#loc636)
    %1269 = "ttir.transpose"(%1267, %1268) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1024x640xbf16>, tensor<1x640x1024xbf16>) -> tensor<1x640x1024xbf16> loc(#loc636)
    %1270 = ttir.empty() : tensor<1x640x32x32xbf16> loc(#loc637)
    %1271 = "ttir.reshape"(%1269, %1270) <{shape = [1 : i32, 640 : i32, 32 : i32, 32 : i32]}> : (tensor<1x640x1024xbf16>, tensor<1x640x32x32xbf16>) -> tensor<1x640x32x32xbf16> loc(#loc637)
    %1272 = ttir.empty() : tensor<1x32x640x32xbf16> loc(#loc872)
    %1273 = "ttir.transpose"(%1271, %1272) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x640x32x32xbf16>, tensor<1x32x640x32xbf16>) -> tensor<1x32x640x32xbf16> loc(#loc872)
    %1274 = ttir.empty() : tensor<1x32x32x640xbf16> loc(#loc873)
    %1275 = "ttir.transpose"(%1273, %1274) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x640x32xbf16>, tensor<1x32x32x640xbf16>) -> tensor<1x32x32x640xbf16> loc(#loc873)
    %1276 = ttir.empty() : tensor<1x32x32x640xbf16> loc(#loc874)
    %1277 = "ttir.conv2d"(%1275, %arg243, %arg244, %1276) <{dilation = array<i32: 1, 1>, groups = 640 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x32x32x640xbf16>, tensor<640x1x3x3xbf16>, tensor<1x1x1x640xbf16>, tensor<1x32x32x640xbf16>) -> tensor<1x32x32x640xbf16> loc(#loc874)
    %1278 = ttir.empty() : tensor<1x32x640x32xbf16> loc(#loc875)
    %1279 = "ttir.transpose"(%1277, %1278) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x32x640xbf16>, tensor<1x32x640x32xbf16>) -> tensor<1x32x640x32xbf16> loc(#loc875)
    %1280 = ttir.empty() : tensor<1x640x32x32xbf16> loc(#loc876)
    %1281 = "ttir.transpose"(%1279, %1280) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x32x640x32xbf16>, tensor<1x640x32x32xbf16>) -> tensor<1x640x32x32xbf16> loc(#loc876)
    %1282 = ttir.empty() : tensor<1x640x1024x1xbf16> loc(#loc638)
    %1283 = "ttir.reshape"(%1281, %1282) <{shape = [1 : i32, 640 : i32, 1024 : i32, 1 : i32]}> : (tensor<1x640x32x32xbf16>, tensor<1x640x1024x1xbf16>) -> tensor<1x640x1024x1xbf16> loc(#loc638)
    %1284 = ttir.empty() : tensor<1x640x1024xbf16> loc(#loc639)
    %1285 = "ttir.squeeze"(%1283, %1284) <{dim = -1 : si32}> : (tensor<1x640x1024x1xbf16>, tensor<1x640x1024xbf16>) -> tensor<1x640x1024xbf16> loc(#loc639)
    %1286 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc640)
    %1287 = "ttir.transpose"(%1285, %1286) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x640x1024xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc640)
    %1288 = ttir.empty() : tensor<1x1024x640xbf16> loc(#loc641)
    %1289 = "ttir.gelu"(%1287, %1288) : (tensor<1x1024x640xbf16>, tensor<1x1024x640xbf16>) -> tensor<1x1024x640xbf16> loc(#loc641)
    %1290 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc642)
    %1291 = "ttir.matmul"(%1289, %arg245, %1290) <{transpose_a = false, transpose_b = false}> : (tensor<1x1024x640xbf16>, tensor<640x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc642)
    %1292 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc51)
    %1293 = "ttir.add"(%1291, %arg246, %1292) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc51)
    %1294 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc151)
    %1295 = "ttir.add"(%1293, %1239, %1294) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc151)
    %1296 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc152)
    %1297 = "ttir.sum"(%1295, %1296) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc152)
    %1298 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc153)
    %1299 = "ttir.multiply"(%arg76, %1297, %1298) : (tensor<1x1024x160xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc153)
    %1300 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc154)
    %1301 = "ttir.subtract"(%1295, %1299, %1300) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc154)
    %1302 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc155)
    %1303 = "ttir.multiply"(%1301, %1301, %1302) : (tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc155)
    %1304 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc156)
    %1305 = "ttir.sum"(%1303, %1304) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc156)
    %1306 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc157)
    %1307 = "ttir.multiply"(%arg77, %1305, %1306) : (tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc157)
    %1308 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc158)
    %1309 = "ttir.add"(%1307, %arg78, %1308) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xf32>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc158)
    %1310 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc159)
    %1311 = "ttir.sqrt"(%1309, %1310) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc159)
    %1312 = ttir.empty() : tensor<1x1024x1xbf16> loc(#loc160)
    %1313 = "ttir.reciprocal"(%1311, %1312) : (tensor<1x1024x1xbf16>, tensor<1x1024x1xbf16>) -> tensor<1x1024x1xbf16> loc(#loc160)
    %1314 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc161)
    %1315 = "ttir.multiply"(%1301, %1313, %1314) : (tensor<1x1024x160xbf16>, tensor<1x1024x1xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc161)
    %1316 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc162)
    %1317 = "ttir.multiply"(%1315, %arg247, %1316) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc162)
    %1318 = ttir.empty() : tensor<1x1024x160xbf16> loc(#loc163)
    %1319 = "ttir.add"(%1317, %arg248, %1318) : (tensor<1x1024x160xbf16>, tensor<160xbf16>, tensor<1x1024x160xbf16>) -> tensor<1x1024x160xbf16> loc(#loc163)
    %1320 = ttir.empty() : tensor<1x32x32x160xbf16> loc(#loc89)
    %1321 = "ttir.reshape"(%1319, %1320) <{shape = [1 : i32, 32 : i32, 32 : i32, 160 : i32]}> : (tensor<1x1024x160xbf16>, tensor<1x32x32x160xbf16>) -> tensor<1x32x32x160xbf16> loc(#loc89)
    %1322 = ttir.empty() : tensor<1x160x32x32xbf16> loc(#loc52)
    %1323 = "ttir.transpose"(%1321, %1322) <{dim0 = -3 : si32, dim1 = -1 : si32}> : (tensor<1x32x32x160xbf16>, tensor<1x160x32x32xbf16>) -> tensor<1x160x32x32xbf16> loc(#loc52)
    %1324 = ttir.empty() : tensor<1x160x32x32xbf16> loc(#loc90)
    %1325 = "ttir.transpose"(%1323, %1324) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x160x32x32xbf16>, tensor<1x160x32x32xbf16>) -> tensor<1x160x32x32xbf16> loc(#loc90)
    %1326 = ttir.empty() : tensor<1x32x160x32xbf16> loc(#loc392)
    %1327 = "ttir.transpose"(%1325, %1326) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x160x32x32xbf16>, tensor<1x32x160x32xbf16>) -> tensor<1x32x160x32xbf16> loc(#loc392)
    %1328 = ttir.empty() : tensor<1x32x32x160xbf16> loc(#loc393)
    %1329 = "ttir.transpose"(%1327, %1328) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x32x160x32xbf16>, tensor<1x32x32x160xbf16>) -> tensor<1x32x32x160xbf16> loc(#loc393)
    %1330 = ttir.empty() : tensor<1x16x16x256xbf16> loc(#loc394)
    %1331 = "ttir.conv2d"(%1329, %arg249, %arg250, %1330) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 2, 2>}> {channel_last = 1 : si32} : (tensor<1x32x32x160xbf16>, tensor<256x160x3x3xbf16>, tensor<1x1x1x256xbf16>, tensor<1x16x16x256xbf16>) -> tensor<1x16x16x256xbf16> loc(#loc394)
    %1332 = ttir.empty() : tensor<1x16x256x16xbf16> loc(#loc395)
    %1333 = "ttir.transpose"(%1331, %1332) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x256xbf16>, tensor<1x16x256x16xbf16>) -> tensor<1x16x256x16xbf16> loc(#loc395)
    %1334 = ttir.empty() : tensor<1x256x16x16xbf16> loc(#loc396)
    %1335 = "ttir.transpose"(%1333, %1334) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x256x16xbf16>, tensor<1x256x16x16xbf16>) -> tensor<1x256x16x16xbf16> loc(#loc396)
    %1336 = ttir.empty() : tensor<1x256x256x1xbf16> loc(#loc165)
    %1337 = "ttir.reshape"(%1335, %1336) <{shape = [1 : i32, 256 : i32, 256 : i32, 1 : i32]}> : (tensor<1x256x16x16xbf16>, tensor<1x256x256x1xbf16>) -> tensor<1x256x256x1xbf16> loc(#loc165)
    %1338 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc166)
    %1339 = "ttir.squeeze"(%1337, %1338) <{dim = -1 : si32}> : (tensor<1x256x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc166)
    %1340 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc167)
    %1341 = "ttir.transpose"(%1339, %1340) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc167)
    %1342 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc397)
    %1343 = "ttir.sum"(%1341, %1342) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc397)
    %1344 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc398)
    %1345 = "ttir.multiply"(%arg79, %1343, %1344) : (tensor<1x256x256xf32>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc398)
    %1346 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc399)
    %1347 = "ttir.subtract"(%1341, %1345, %1346) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc399)
    %1348 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc400)
    %1349 = "ttir.multiply"(%1347, %1347, %1348) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc400)
    %1350 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc401)
    %1351 = "ttir.sum"(%1349, %1350) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc401)
    %1352 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc402)
    %1353 = "ttir.multiply"(%arg80, %1351, %1352) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc402)
    %1354 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc403)
    %1355 = "ttir.add"(%1353, %arg81, %1354) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc403)
    %1356 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc404)
    %1357 = "ttir.sqrt"(%1355, %1356) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc404)
    %1358 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc405)
    %1359 = "ttir.reciprocal"(%1357, %1358) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc405)
    %1360 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc406)
    %1361 = "ttir.multiply"(%1347, %1359, %1360) : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc406)
    %1362 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc407)
    %1363 = "ttir.multiply"(%1361, %arg251, %1362) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc407)
    %1364 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc408)
    %1365 = "ttir.add"(%1363, %arg252, %1364) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc408)
    %1366 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc409)
    %1367 = "ttir.sum"(%1365, %1366) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc409)
    %1368 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc410)
    %1369 = "ttir.multiply"(%arg82, %1367, %1368) : (tensor<1x256x256xf32>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc410)
    %1370 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc411)
    %1371 = "ttir.subtract"(%1365, %1369, %1370) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc411)
    %1372 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc412)
    %1373 = "ttir.multiply"(%1371, %1371, %1372) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc412)
    %1374 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc413)
    %1375 = "ttir.sum"(%1373, %1374) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc413)
    %1376 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc414)
    %1377 = "ttir.multiply"(%arg83, %1375, %1376) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc414)
    %1378 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc415)
    %1379 = "ttir.add"(%1377, %arg84, %1378) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc415)
    %1380 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc416)
    %1381 = "ttir.sqrt"(%1379, %1380) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc416)
    %1382 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc417)
    %1383 = "ttir.reciprocal"(%1381, %1382) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc417)
    %1384 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc418)
    %1385 = "ttir.multiply"(%1371, %1383, %1384) : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc418)
    %1386 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc419)
    %1387 = "ttir.multiply"(%1385, %arg253, %1386) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc419)
    %1388 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc420)
    %1389 = "ttir.add"(%1387, %arg254, %1388) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc420)
    %1390 = ttir.empty() : tensor<256x256xbf16> loc(#loc877)
    %1391 = "ttir.squeeze"(%1389, %1390) <{dim = 0 : si32}> : (tensor<1x256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc877)
    %1392 = ttir.empty() : tensor<256x256xbf16> loc(#loc878)
    %1393 = "ttir.matmul"(%1391, %arg255, %1392) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc878)
    %1394 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc879)
    %1395 = "ttir.unsqueeze"(%1393, %1394) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc879)
    %1396 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc53)
    %1397 = "ttir.add"(%1395, %arg256, %1396) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc53)
    %1398 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc643)
    %1399 = "ttir.reshape"(%1397, %1398) <{shape = [1 : i32, 256 : i32, 8 : i32, 32 : i32]}> : (tensor<1x256x256xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc643)
    %1400 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc644)
    %1401 = "ttir.transpose"(%1399, %1400) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x8x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc644)
    %1402 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc645)
    %1403 = "ttir.squeeze"(%1401, %1402) <{dim = 0 : si32}> : (tensor<1x8x256x32xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc645)
    %1404 = ttir.empty() : tensor<256x256xbf16> loc(#loc880)
    %1405 = "ttir.matmul"(%1391, %arg257, %1404) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc880)
    %1406 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc881)
    %1407 = "ttir.unsqueeze"(%1405, %1406) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc881)
    %1408 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc54)
    %1409 = "ttir.add"(%1407, %arg258, %1408) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc54)
    %1410 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc646)
    %1411 = "ttir.reshape"(%1409, %1410) <{shape = [1 : i32, 256 : i32, 8 : i32, 32 : i32]}> : (tensor<1x256x256xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc646)
    %1412 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc647)
    %1413 = "ttir.transpose"(%1411, %1412) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x8x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc647)
    %1414 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc648)
    %1415 = "ttir.squeeze"(%1413, %1414) <{dim = 0 : si32}> : (tensor<1x8x256x32xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc648)
    %1416 = ttir.empty() : tensor<8x32x256xbf16> loc(#loc55)
    %1417 = "ttir.transpose"(%1415, %1416) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<8x256x32xbf16>, tensor<8x32x256xbf16>) -> tensor<8x32x256xbf16> loc(#loc55)
    %1418 = ttir.empty() : tensor<8x256x256xbf16> loc(#loc649)
    %1419 = "ttir.matmul"(%1403, %1417, %1418) <{transpose_a = false, transpose_b = false}> : (tensor<8x256x32xbf16>, tensor<8x32x256xbf16>, tensor<8x256x256xbf16>) -> tensor<8x256x256xbf16> loc(#loc649)
    %1420 = ttir.empty() : tensor<1x8x256x256xbf16> loc(#loc650)
    %1421 = "ttir.unsqueeze"(%1419, %1420) <{dim = 0 : si32}> : (tensor<8x256x256xbf16>, tensor<1x8x256x256xbf16>) -> tensor<1x8x256x256xbf16> loc(#loc650)
    %1422 = ttir.empty() : tensor<1x8x256x256xbf16> loc(#loc651)
    %1423 = "ttir.div"(%1421, %arg85, %1422) : (tensor<1x8x256x256xbf16>, tensor<1xbf16>, tensor<1x8x256x256xbf16>) -> tensor<1x8x256x256xbf16> loc(#loc651)
    %1424 = ttir.empty() : tensor<1x8x256x256xbf16> loc(#loc652)
    %1425 = "ttir.softmax"(%1423, %1424) <{dimension = -1 : si32}> : (tensor<1x8x256x256xbf16>, tensor<1x8x256x256xbf16>) -> tensor<1x8x256x256xbf16> loc(#loc652)
    %1426 = ttir.empty() : tensor<8x256x256xbf16> loc(#loc653)
    %1427 = "ttir.squeeze"(%1425, %1426) <{dim = 0 : si32}> : (tensor<1x8x256x256xbf16>, tensor<8x256x256xbf16>) -> tensor<8x256x256xbf16> loc(#loc653)
    %1428 = ttir.empty() : tensor<256x256xbf16> loc(#loc882)
    %1429 = "ttir.matmul"(%1391, %arg259, %1428) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc882)
    %1430 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc883)
    %1431 = "ttir.unsqueeze"(%1429, %1430) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc883)
    %1432 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc56)
    %1433 = "ttir.add"(%1431, %arg260, %1432) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc56)
    %1434 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc654)
    %1435 = "ttir.reshape"(%1433, %1434) <{shape = [1 : i32, 256 : i32, 8 : i32, 32 : i32]}> : (tensor<1x256x256xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc654)
    %1436 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc655)
    %1437 = "ttir.transpose"(%1435, %1436) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x8x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc655)
    %1438 = ttir.empty() : tensor<1x8x32x256xbf16> loc(#loc656)
    %1439 = "ttir.transpose"(%1437, %1438) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x8x256x32xbf16>, tensor<1x8x32x256xbf16>) -> tensor<1x8x32x256xbf16> loc(#loc656)
    %1440 = ttir.empty() : tensor<8x32x256xbf16> loc(#loc657)
    %1441 = "ttir.squeeze"(%1439, %1440) <{dim = 0 : si32}> : (tensor<1x8x32x256xbf16>, tensor<8x32x256xbf16>) -> tensor<8x32x256xbf16> loc(#loc657)
    %1442 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc57)
    %1443 = "ttir.transpose"(%1441, %1442) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<8x32x256xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc57)
    %1444 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc658)
    %1445 = "ttir.matmul"(%1427, %1443, %1444) <{transpose_a = false, transpose_b = false}> : (tensor<8x256x256xbf16>, tensor<8x256x32xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc658)
    %1446 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc659)
    %1447 = "ttir.unsqueeze"(%1445, %1446) <{dim = 0 : si32}> : (tensor<8x256x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc659)
    %1448 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc660)
    %1449 = "ttir.transpose"(%1447, %1448) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x8x256x32xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc660)
    %1450 = ttir.empty() : tensor<256x256xbf16> loc(#loc884)
    %1451 = "ttir.reshape"(%1449, %1450) <{shape = [256 : i32, 256 : i32]}> : (tensor<1x256x8x32xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc884)
    %1452 = ttir.empty() : tensor<256x256xbf16> loc(#loc885)
    %1453 = "ttir.matmul"(%1451, %arg261, %1452) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc885)
    %1454 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc886)
    %1455 = "ttir.unsqueeze"(%1453, %1454) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc886)
    %1456 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc58)
    %1457 = "ttir.add"(%1455, %arg262, %1456) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc58)
    %1458 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc169)
    %1459 = "ttir.add"(%1457, %1365, %1458) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc169)
    %1460 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc421)
    %1461 = "ttir.sum"(%1459, %1460) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc421)
    %1462 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc422)
    %1463 = "ttir.multiply"(%arg86, %1461, %1462) : (tensor<1x256x256xf32>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc422)
    %1464 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc423)
    %1465 = "ttir.subtract"(%1459, %1463, %1464) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc423)
    %1466 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc424)
    %1467 = "ttir.multiply"(%1465, %1465, %1466) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc424)
    %1468 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc425)
    %1469 = "ttir.sum"(%1467, %1468) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc425)
    %1470 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc426)
    %1471 = "ttir.multiply"(%arg87, %1469, %1470) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc426)
    %1472 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc427)
    %1473 = "ttir.add"(%1471, %arg88, %1472) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc427)
    %1474 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc428)
    %1475 = "ttir.sqrt"(%1473, %1474) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc428)
    %1476 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc429)
    %1477 = "ttir.reciprocal"(%1475, %1476) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc429)
    %1478 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc430)
    %1479 = "ttir.multiply"(%1465, %1477, %1478) : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc430)
    %1480 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc431)
    %1481 = "ttir.multiply"(%1479, %arg263, %1480) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc431)
    %1482 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc432)
    %1483 = "ttir.add"(%1481, %arg264, %1482) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc432)
    %1484 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc661)
    %1485 = "ttir.matmul"(%1483, %arg265, %1484) <{transpose_a = false, transpose_b = false}> : (tensor<1x256x256xbf16>, tensor<256x1024xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc661)
    %1486 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc59)
    %1487 = "ttir.add"(%1485, %arg266, %1486) : (tensor<1x256x1024xbf16>, tensor<1024xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc59)
    %1488 = ttir.empty() : tensor<1x1024x256xbf16> loc(#loc662)
    %1489 = "ttir.transpose"(%1487, %1488) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x256x1024xbf16>, tensor<1x1024x256xbf16>) -> tensor<1x1024x256xbf16> loc(#loc662)
    %1490 = ttir.empty() : tensor<1x1024x16x16xbf16> loc(#loc663)
    %1491 = "ttir.reshape"(%1489, %1490) <{shape = [1 : i32, 1024 : i32, 16 : i32, 16 : i32]}> : (tensor<1x1024x256xbf16>, tensor<1x1024x16x16xbf16>) -> tensor<1x1024x16x16xbf16> loc(#loc663)
    %1492 = ttir.empty() : tensor<1x16x1024x16xbf16> loc(#loc887)
    %1493 = "ttir.transpose"(%1491, %1492) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x16x16xbf16>, tensor<1x16x1024x16xbf16>) -> tensor<1x16x1024x16xbf16> loc(#loc887)
    %1494 = ttir.empty() : tensor<1x16x16x1024xbf16> loc(#loc888)
    %1495 = "ttir.transpose"(%1493, %1494) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x1024x16xbf16>, tensor<1x16x16x1024xbf16>) -> tensor<1x16x16x1024xbf16> loc(#loc888)
    %1496 = ttir.empty() : tensor<1x16x16x1024xbf16> loc(#loc889)
    %1497 = "ttir.conv2d"(%1495, %arg267, %arg268, %1496) <{dilation = array<i32: 1, 1>, groups = 1024 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x16x16x1024xbf16>, tensor<1024x1x3x3xbf16>, tensor<1x1x1x1024xbf16>, tensor<1x16x16x1024xbf16>) -> tensor<1x16x16x1024xbf16> loc(#loc889)
    %1498 = ttir.empty() : tensor<1x16x1024x16xbf16> loc(#loc890)
    %1499 = "ttir.transpose"(%1497, %1498) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x1024xbf16>, tensor<1x16x1024x16xbf16>) -> tensor<1x16x1024x16xbf16> loc(#loc890)
    %1500 = ttir.empty() : tensor<1x1024x16x16xbf16> loc(#loc891)
    %1501 = "ttir.transpose"(%1499, %1500) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x1024x16xbf16>, tensor<1x1024x16x16xbf16>) -> tensor<1x1024x16x16xbf16> loc(#loc891)
    %1502 = ttir.empty() : tensor<1x1024x256x1xbf16> loc(#loc664)
    %1503 = "ttir.reshape"(%1501, %1502) <{shape = [1 : i32, 1024 : i32, 256 : i32, 1 : i32]}> : (tensor<1x1024x16x16xbf16>, tensor<1x1024x256x1xbf16>) -> tensor<1x1024x256x1xbf16> loc(#loc664)
    %1504 = ttir.empty() : tensor<1x1024x256xbf16> loc(#loc665)
    %1505 = "ttir.squeeze"(%1503, %1504) <{dim = -1 : si32}> : (tensor<1x1024x256x1xbf16>, tensor<1x1024x256xbf16>) -> tensor<1x1024x256xbf16> loc(#loc665)
    %1506 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc666)
    %1507 = "ttir.transpose"(%1505, %1506) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1024x256xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc666)
    %1508 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc667)
    %1509 = "ttir.gelu"(%1507, %1508) : (tensor<1x256x1024xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc667)
    %1510 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc668)
    %1511 = "ttir.matmul"(%1509, %arg269, %1510) <{transpose_a = false, transpose_b = false}> : (tensor<1x256x1024xbf16>, tensor<1024x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc668)
    %1512 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc60)
    %1513 = "ttir.add"(%1511, %arg270, %1512) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc60)
    %1514 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc170)
    %1515 = "ttir.add"(%1513, %1459, %1514) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc170)
    %1516 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc433)
    %1517 = "ttir.sum"(%1515, %1516) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc433)
    %1518 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc434)
    %1519 = "ttir.multiply"(%arg89, %1517, %1518) : (tensor<1x256x256xf32>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc434)
    %1520 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc435)
    %1521 = "ttir.subtract"(%1515, %1519, %1520) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc435)
    %1522 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc436)
    %1523 = "ttir.multiply"(%1521, %1521, %1522) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc436)
    %1524 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc437)
    %1525 = "ttir.sum"(%1523, %1524) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc437)
    %1526 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc438)
    %1527 = "ttir.multiply"(%arg90, %1525, %1526) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc438)
    %1528 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc439)
    %1529 = "ttir.add"(%1527, %arg91, %1528) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc439)
    %1530 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc440)
    %1531 = "ttir.sqrt"(%1529, %1530) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc440)
    %1532 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc441)
    %1533 = "ttir.reciprocal"(%1531, %1532) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc441)
    %1534 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc442)
    %1535 = "ttir.multiply"(%1521, %1533, %1534) : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc442)
    %1536 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc443)
    %1537 = "ttir.multiply"(%1535, %arg271, %1536) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc443)
    %1538 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc444)
    %1539 = "ttir.add"(%1537, %arg272, %1538) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc444)
    %1540 = ttir.empty() : tensor<256x256xbf16> loc(#loc892)
    %1541 = "ttir.squeeze"(%1539, %1540) <{dim = 0 : si32}> : (tensor<1x256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc892)
    %1542 = ttir.empty() : tensor<256x256xbf16> loc(#loc893)
    %1543 = "ttir.matmul"(%1541, %arg273, %1542) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc893)
    %1544 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc894)
    %1545 = "ttir.unsqueeze"(%1543, %1544) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc894)
    %1546 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc61)
    %1547 = "ttir.add"(%1545, %arg274, %1546) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc61)
    %1548 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc669)
    %1549 = "ttir.reshape"(%1547, %1548) <{shape = [1 : i32, 256 : i32, 8 : i32, 32 : i32]}> : (tensor<1x256x256xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc669)
    %1550 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc670)
    %1551 = "ttir.transpose"(%1549, %1550) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x8x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc670)
    %1552 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc671)
    %1553 = "ttir.squeeze"(%1551, %1552) <{dim = 0 : si32}> : (tensor<1x8x256x32xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc671)
    %1554 = ttir.empty() : tensor<256x256xbf16> loc(#loc895)
    %1555 = "ttir.matmul"(%1541, %arg275, %1554) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc895)
    %1556 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc896)
    %1557 = "ttir.unsqueeze"(%1555, %1556) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc896)
    %1558 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc62)
    %1559 = "ttir.add"(%1557, %arg276, %1558) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc62)
    %1560 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc672)
    %1561 = "ttir.reshape"(%1559, %1560) <{shape = [1 : i32, 256 : i32, 8 : i32, 32 : i32]}> : (tensor<1x256x256xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc672)
    %1562 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc673)
    %1563 = "ttir.transpose"(%1561, %1562) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x8x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc673)
    %1564 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc674)
    %1565 = "ttir.squeeze"(%1563, %1564) <{dim = 0 : si32}> : (tensor<1x8x256x32xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc674)
    %1566 = ttir.empty() : tensor<8x32x256xbf16> loc(#loc63)
    %1567 = "ttir.transpose"(%1565, %1566) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<8x256x32xbf16>, tensor<8x32x256xbf16>) -> tensor<8x32x256xbf16> loc(#loc63)
    %1568 = ttir.empty() : tensor<8x256x256xbf16> loc(#loc675)
    %1569 = "ttir.matmul"(%1553, %1567, %1568) <{transpose_a = false, transpose_b = false}> : (tensor<8x256x32xbf16>, tensor<8x32x256xbf16>, tensor<8x256x256xbf16>) -> tensor<8x256x256xbf16> loc(#loc675)
    %1570 = ttir.empty() : tensor<1x8x256x256xbf16> loc(#loc676)
    %1571 = "ttir.unsqueeze"(%1569, %1570) <{dim = 0 : si32}> : (tensor<8x256x256xbf16>, tensor<1x8x256x256xbf16>) -> tensor<1x8x256x256xbf16> loc(#loc676)
    %1572 = ttir.empty() : tensor<1x8x256x256xbf16> loc(#loc677)
    %1573 = "ttir.div"(%1571, %arg92, %1572) : (tensor<1x8x256x256xbf16>, tensor<1xbf16>, tensor<1x8x256x256xbf16>) -> tensor<1x8x256x256xbf16> loc(#loc677)
    %1574 = ttir.empty() : tensor<1x8x256x256xbf16> loc(#loc678)
    %1575 = "ttir.softmax"(%1573, %1574) <{dimension = -1 : si32}> : (tensor<1x8x256x256xbf16>, tensor<1x8x256x256xbf16>) -> tensor<1x8x256x256xbf16> loc(#loc678)
    %1576 = ttir.empty() : tensor<8x256x256xbf16> loc(#loc679)
    %1577 = "ttir.squeeze"(%1575, %1576) <{dim = 0 : si32}> : (tensor<1x8x256x256xbf16>, tensor<8x256x256xbf16>) -> tensor<8x256x256xbf16> loc(#loc679)
    %1578 = ttir.empty() : tensor<256x256xbf16> loc(#loc897)
    %1579 = "ttir.matmul"(%1541, %arg277, %1578) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc897)
    %1580 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc898)
    %1581 = "ttir.unsqueeze"(%1579, %1580) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc898)
    %1582 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc64)
    %1583 = "ttir.add"(%1581, %arg278, %1582) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc64)
    %1584 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc680)
    %1585 = "ttir.reshape"(%1583, %1584) <{shape = [1 : i32, 256 : i32, 8 : i32, 32 : i32]}> : (tensor<1x256x256xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc680)
    %1586 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc681)
    %1587 = "ttir.transpose"(%1585, %1586) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x8x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc681)
    %1588 = ttir.empty() : tensor<1x8x32x256xbf16> loc(#loc682)
    %1589 = "ttir.transpose"(%1587, %1588) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x8x256x32xbf16>, tensor<1x8x32x256xbf16>) -> tensor<1x8x32x256xbf16> loc(#loc682)
    %1590 = ttir.empty() : tensor<8x32x256xbf16> loc(#loc683)
    %1591 = "ttir.squeeze"(%1589, %1590) <{dim = 0 : si32}> : (tensor<1x8x32x256xbf16>, tensor<8x32x256xbf16>) -> tensor<8x32x256xbf16> loc(#loc683)
    %1592 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc65)
    %1593 = "ttir.transpose"(%1591, %1592) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<8x32x256xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc65)
    %1594 = ttir.empty() : tensor<8x256x32xbf16> loc(#loc684)
    %1595 = "ttir.matmul"(%1577, %1593, %1594) <{transpose_a = false, transpose_b = false}> : (tensor<8x256x256xbf16>, tensor<8x256x32xbf16>, tensor<8x256x32xbf16>) -> tensor<8x256x32xbf16> loc(#loc684)
    %1596 = ttir.empty() : tensor<1x8x256x32xbf16> loc(#loc685)
    %1597 = "ttir.unsqueeze"(%1595, %1596) <{dim = 0 : si32}> : (tensor<8x256x32xbf16>, tensor<1x8x256x32xbf16>) -> tensor<1x8x256x32xbf16> loc(#loc685)
    %1598 = ttir.empty() : tensor<1x256x8x32xbf16> loc(#loc686)
    %1599 = "ttir.transpose"(%1597, %1598) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x8x256x32xbf16>, tensor<1x256x8x32xbf16>) -> tensor<1x256x8x32xbf16> loc(#loc686)
    %1600 = ttir.empty() : tensor<256x256xbf16> loc(#loc899)
    %1601 = "ttir.reshape"(%1599, %1600) <{shape = [256 : i32, 256 : i32]}> : (tensor<1x256x8x32xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc899)
    %1602 = ttir.empty() : tensor<256x256xbf16> loc(#loc900)
    %1603 = "ttir.matmul"(%1601, %arg279, %1602) <{transpose_a = false, transpose_b = false}> : (tensor<256x256xbf16>, tensor<256x256xbf16>, tensor<256x256xbf16>) -> tensor<256x256xbf16> loc(#loc900)
    %1604 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc901)
    %1605 = "ttir.unsqueeze"(%1603, %1604) <{dim = 0 : si32}> : (tensor<256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc901)
    %1606 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc66)
    %1607 = "ttir.add"(%1605, %arg280, %1606) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc66)
    %1608 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc171)
    %1609 = "ttir.add"(%1607, %1515, %1608) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc171)
    %1610 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc445)
    %1611 = "ttir.sum"(%1609, %1610) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc445)
    %1612 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc446)
    %1613 = "ttir.multiply"(%arg93, %1611, %1612) : (tensor<1x256x256xf32>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc446)
    %1614 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc447)
    %1615 = "ttir.subtract"(%1609, %1613, %1614) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc447)
    %1616 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc448)
    %1617 = "ttir.multiply"(%1615, %1615, %1616) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc448)
    %1618 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc449)
    %1619 = "ttir.sum"(%1617, %1618) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc449)
    %1620 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc450)
    %1621 = "ttir.multiply"(%arg94, %1619, %1620) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc450)
    %1622 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc451)
    %1623 = "ttir.add"(%1621, %arg95, %1622) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc451)
    %1624 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc452)
    %1625 = "ttir.sqrt"(%1623, %1624) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc452)
    %1626 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc453)
    %1627 = "ttir.reciprocal"(%1625, %1626) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc453)
    %1628 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc454)
    %1629 = "ttir.multiply"(%1615, %1627, %1628) : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc454)
    %1630 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc455)
    %1631 = "ttir.multiply"(%1629, %arg281, %1630) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc455)
    %1632 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc456)
    %1633 = "ttir.add"(%1631, %arg282, %1632) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc456)
    %1634 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc687)
    %1635 = "ttir.matmul"(%1633, %arg283, %1634) <{transpose_a = false, transpose_b = false}> : (tensor<1x256x256xbf16>, tensor<256x1024xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc687)
    %1636 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc67)
    %1637 = "ttir.add"(%1635, %arg284, %1636) : (tensor<1x256x1024xbf16>, tensor<1024xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc67)
    %1638 = ttir.empty() : tensor<1x1024x256xbf16> loc(#loc688)
    %1639 = "ttir.transpose"(%1637, %1638) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x256x1024xbf16>, tensor<1x1024x256xbf16>) -> tensor<1x1024x256xbf16> loc(#loc688)
    %1640 = ttir.empty() : tensor<1x1024x16x16xbf16> loc(#loc689)
    %1641 = "ttir.reshape"(%1639, %1640) <{shape = [1 : i32, 1024 : i32, 16 : i32, 16 : i32]}> : (tensor<1x1024x256xbf16>, tensor<1x1024x16x16xbf16>) -> tensor<1x1024x16x16xbf16> loc(#loc689)
    %1642 = ttir.empty() : tensor<1x16x1024x16xbf16> loc(#loc902)
    %1643 = "ttir.transpose"(%1641, %1642) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x16x16xbf16>, tensor<1x16x1024x16xbf16>) -> tensor<1x16x1024x16xbf16> loc(#loc902)
    %1644 = ttir.empty() : tensor<1x16x16x1024xbf16> loc(#loc903)
    %1645 = "ttir.transpose"(%1643, %1644) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x1024x16xbf16>, tensor<1x16x16x1024xbf16>) -> tensor<1x16x16x1024xbf16> loc(#loc903)
    %1646 = ttir.empty() : tensor<1x16x16x1024xbf16> loc(#loc904)
    %1647 = "ttir.conv2d"(%1645, %arg285, %arg286, %1646) <{dilation = array<i32: 1, 1>, groups = 1024 : i32, padding = array<i32: 1, 1, 1, 1>, stride = array<i32: 1, 1>}> {channel_last = 1 : si32} : (tensor<1x16x16x1024xbf16>, tensor<1024x1x3x3xbf16>, tensor<1x1x1x1024xbf16>, tensor<1x16x16x1024xbf16>) -> tensor<1x16x16x1024xbf16> loc(#loc904)
    %1648 = ttir.empty() : tensor<1x16x1024x16xbf16> loc(#loc905)
    %1649 = "ttir.transpose"(%1647, %1648) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x16x16x1024xbf16>, tensor<1x16x1024x16xbf16>) -> tensor<1x16x1024x16xbf16> loc(#loc905)
    %1650 = ttir.empty() : tensor<1x1024x16x16xbf16> loc(#loc906)
    %1651 = "ttir.transpose"(%1649, %1650) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x16x1024x16xbf16>, tensor<1x1024x16x16xbf16>) -> tensor<1x1024x16x16xbf16> loc(#loc906)
    %1652 = ttir.empty() : tensor<1x1024x256x1xbf16> loc(#loc690)
    %1653 = "ttir.reshape"(%1651, %1652) <{shape = [1 : i32, 1024 : i32, 256 : i32, 1 : i32]}> : (tensor<1x1024x16x16xbf16>, tensor<1x1024x256x1xbf16>) -> tensor<1x1024x256x1xbf16> loc(#loc690)
    %1654 = ttir.empty() : tensor<1x1024x256xbf16> loc(#loc691)
    %1655 = "ttir.squeeze"(%1653, %1654) <{dim = -1 : si32}> : (tensor<1x1024x256x1xbf16>, tensor<1x1024x256xbf16>) -> tensor<1x1024x256xbf16> loc(#loc691)
    %1656 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc692)
    %1657 = "ttir.transpose"(%1655, %1656) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1024x256xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc692)
    %1658 = ttir.empty() : tensor<1x256x1024xbf16> loc(#loc693)
    %1659 = "ttir.gelu"(%1657, %1658) : (tensor<1x256x1024xbf16>, tensor<1x256x1024xbf16>) -> tensor<1x256x1024xbf16> loc(#loc693)
    %1660 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc694)
    %1661 = "ttir.matmul"(%1659, %arg287, %1660) <{transpose_a = false, transpose_b = false}> : (tensor<1x256x1024xbf16>, tensor<1024x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc694)
    %1662 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc68)
    %1663 = "ttir.add"(%1661, %arg288, %1662) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc68)
    %1664 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc172)
    %1665 = "ttir.add"(%1663, %1609, %1664) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc172)
    %1666 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc173)
    %1667 = "ttir.sum"(%1665, %1666) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc173)
    %1668 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc174)
    %1669 = "ttir.multiply"(%arg96, %1667, %1668) : (tensor<1x256x256xf32>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc174)
    %1670 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc175)
    %1671 = "ttir.subtract"(%1665, %1669, %1670) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc175)
    %1672 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc176)
    %1673 = "ttir.multiply"(%1671, %1671, %1672) : (tensor<1x256x256xbf16>, tensor<1x256x256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc176)
    %1674 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc177)
    %1675 = "ttir.sum"(%1673, %1674) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc177)
    %1676 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc178)
    %1677 = "ttir.multiply"(%arg97, %1675, %1676) : (tensor<1x256x1xf32>, tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc178)
    %1678 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc179)
    %1679 = "ttir.add"(%1677, %arg98, %1678) : (tensor<1x256x1xbf16>, tensor<1x256x1xf32>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc179)
    %1680 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc180)
    %1681 = "ttir.sqrt"(%1679, %1680) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc180)
    %1682 = ttir.empty() : tensor<1x256x1xbf16> loc(#loc181)
    %1683 = "ttir.reciprocal"(%1681, %1682) : (tensor<1x256x1xbf16>, tensor<1x256x1xbf16>) -> tensor<1x256x1xbf16> loc(#loc181)
    %1684 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc182)
    %1685 = "ttir.multiply"(%1671, %1683, %1684) : (tensor<1x256x256xbf16>, tensor<1x256x1xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc182)
    %1686 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc183)
    %1687 = "ttir.multiply"(%1685, %arg289, %1686) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc183)
    %1688 = ttir.empty() : tensor<1x256x256xbf16> loc(#loc184)
    %1689 = "ttir.add"(%1687, %arg290, %1688) : (tensor<1x256x256xbf16>, tensor<256xbf16>, tensor<1x256x256xbf16>) -> tensor<1x256x256xbf16> loc(#loc184)
    %1690 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc69)
    %1691 = "ttir.mean"(%1689, %1690) <{dim_arg = [-2 : i32], keep_dim = true}> : (tensor<1x256x256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc69)
    %1692 = ttir.empty() : tensor<1x256xbf16> loc(#loc73)
    %1693 = "ttir.squeeze"(%1691, %1692) <{dim = 1 : si32}> : (tensor<1x1x256xbf16>, tensor<1x256xbf16>) -> tensor<1x256xbf16> loc(#loc73)
    %1694 = ttir.empty() : tensor<1x1000xbf16> loc(#loc76)
    %1695 = "ttir.matmul"(%1693, %arg291, %1694) <{transpose_a = false, transpose_b = false}> : (tensor<1x256xbf16>, tensor<256x1000xbf16>, tensor<1x1000xbf16>) -> tensor<1x1000xbf16> loc(#loc76)
    %1696 = ttir.empty() : tensor<1x1000xbf16> loc(#loc70)
    %1697 = "ttir.add"(%1695, %arg292, %1696) : (tensor<1x1000xbf16>, tensor<1000xbf16>, tensor<1x1000xbf16>) -> tensor<1x1000xbf16> loc(#loc70)
    return %1697 : tensor<1x1000xbf16> loc(#loc71)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("transformers.models.segformer.modeling_segformer.SegformerForImageClassification::")
#loc2 = loc("add_11")
#loc3 = loc("add_27")
#loc4 = loc("transpose_30")
#loc5 = loc("add_40")
#loc6 = loc("transpose_45")
#loc7 = loc("add_49")
#loc8 = loc("add_55")
#loc9 = loc("add_70")
#loc10 = loc("add_76")
#loc11 = loc("add_92")
#loc12 = loc("transpose_95")
#loc13 = loc("add_105")
#loc14 = loc("transpose_110")
#loc15 = loc("add_114")
#loc16 = loc("add_120")
#loc17 = loc("add_135")
#loc18 = loc("transpose_140")
#loc19 = loc("add_153")
#loc20 = loc("add_170")
#loc21 = loc("transpose_174")
#loc22 = loc("add_184")
#loc23 = loc("transpose_189")
#loc24 = loc("add_197")
#loc25 = loc("add_203")
#loc26 = loc("add_218")
#loc27 = loc("add_224")
#loc28 = loc("add_241")
#loc29 = loc("transpose_245")
#loc30 = loc("add_255")
#loc31 = loc("transpose_260")
#loc32 = loc("add_268")
#loc33 = loc("add_274")
#loc34 = loc("add_289")
#loc35 = loc("transpose_294")
#loc36 = loc("add_307")
#loc37 = loc("add_324")
#loc38 = loc("transpose_328")
#loc39 = loc("add_338")
#loc40 = loc("transpose_343")
#loc41 = loc("add_351")
#loc42 = loc("add_357")
#loc43 = loc("add_372")
#loc44 = loc("add_378")
#loc45 = loc("add_395")
#loc46 = loc("transpose_399")
#loc47 = loc("add_409")
#loc48 = loc("transpose_414")
#loc49 = loc("add_422")
#loc50 = loc("add_428")
#loc51 = loc("add_443")
#loc52 = loc("transpose_448")
#loc53 = loc("add_463")
#loc54 = loc("add_470")
#loc55 = loc("transpose_474")
#loc56 = loc("add_484")
#loc57 = loc("transpose_489")
#loc58 = loc("add_497")
#loc59 = loc("add_503")
#loc60 = loc("add_518")
#loc61 = loc("add_526")
#loc62 = loc("add_533")
#loc63 = loc("transpose_537")
#loc64 = loc("add_547")
#loc65 = loc("transpose_552")
#loc66 = loc("add_560")
#loc67 = loc("add_566")
#loc68 = loc("add_581")
#loc69 = loc("reduce_avg_586")
#loc70 = loc("add_590")
#loc71 = loc(unknown)
#loc72 = loc("transformers.models.segformer.modeling_segformer.SegformerModel::segformer"(#loc1))
#loc73 = loc("squeeze_587"(#loc1))
#loc74 = loc("torch.nn.modules.linear.Linear::classifier"(#loc1))
#loc75 = loc("transformers.models.segformer.modeling_segformer.SegformerEncoder::encoder"(#loc72))
#loc76 = loc("matmul_589"(#loc74))
#loc77 = loc("transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings::patch_embeddings.0"(#loc75))
#loc78 = loc("transformers.models.segformer.modeling_segformer.SegformerLayer::0"(#loc75))
#loc79 = loc("transformers.models.segformer.modeling_segformer.SegformerLayer::1"(#loc75))
#loc80 = loc("torch.nn.modules.normalization.LayerNorm::0"(#loc75))
#loc81 = loc("reshape_139"(#loc75))
#loc82 = loc("transpose_141"(#loc75))
#loc83 = loc("transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings::patch_embeddings.1"(#loc75))
#loc84 = loc("torch.nn.modules.normalization.LayerNorm::1"(#loc75))
#loc85 = loc("reshape_293"(#loc75))
#loc86 = loc("transpose_295"(#loc75))
#loc87 = loc("transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings::patch_embeddings.2"(#loc75))
#loc88 = loc("torch.nn.modules.normalization.LayerNorm::2"(#loc75))
#loc89 = loc("reshape_447"(#loc75))
#loc90 = loc("transpose_449"(#loc75))
#loc91 = loc("transformers.models.segformer.modeling_segformer.SegformerOverlapPatchEmbeddings::patch_embeddings.3"(#loc75))
#loc92 = loc("torch.nn.modules.normalization.LayerNorm::3"(#loc75))
#loc93 = loc("torch.nn.modules.conv.Conv2d::proj"(#loc77))
#loc94 = loc("reshape_4"(#loc77))
#loc95 = loc("squeeze_5"(#loc77))
#loc96 = loc("transpose_6"(#loc77))
#loc97 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm"(#loc77))
#loc98 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm_1"(#loc78))
#loc99 = loc("transformers.models.segformer.modeling_segformer.SegformerAttention::attention"(#loc78))
#loc100 = loc("add_51"(#loc78))
#loc101 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm_2"(#loc78))
#loc102 = loc("transformers.models.segformer.modeling_segformer.SegformerMixFFN::mlp"(#loc78))
#loc103 = loc("add_72"(#loc78))
#loc104 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm_1"(#loc79))
#loc105 = loc("transformers.models.segformer.modeling_segformer.SegformerAttention::attention"(#loc79))
#loc106 = loc("add_116"(#loc79))
#loc107 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm_2"(#loc79))
#loc108 = loc("transformers.models.segformer.modeling_segformer.SegformerMixFFN::mlp"(#loc79))
#loc109 = loc("add_137"(#loc79))
#loc110 = loc("layernorm_138.dc.reduce_sum.0"(#loc80))
#loc111 = loc("layernorm_138.dc.multiply.2"(#loc80))
#loc112 = loc("layernorm_138.dc.subtract.3"(#loc80))
#loc113 = loc("layernorm_138.dc.multiply.4"(#loc80))
#loc114 = loc("layernorm_138.dc.reduce_sum.5"(#loc80))
#loc115 = loc("layernorm_138.dc.multiply.7"(#loc80))
#loc116 = loc("layernorm_138.dc.add.9"(#loc80))
#loc117 = loc("layernorm_138.dc.sqrt.10"(#loc80))
#loc118 = loc("layernorm_138.dc.reciprocal.11"(#loc80))
#loc119 = loc("layernorm_138.dc.multiply.12"(#loc80))
#loc120 = loc("layernorm_138.dc.multiply.13"(#loc80))
#loc121 = loc("layernorm_138.dc.add.14"(#loc80))
#loc122 = loc("torch.nn.modules.conv.Conv2d::proj"(#loc83))
#loc123 = loc("reshape_146"(#loc83))
#loc124 = loc("squeeze_147"(#loc83))
#loc125 = loc("transpose_148"(#loc83))
#loc126 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm"(#loc83))
#loc127 = loc("add_199"(#loc78))
#loc128 = loc("add_220"(#loc78))
#loc129 = loc("add_270"(#loc79))
#loc130 = loc("add_291"(#loc79))
#loc131 = loc("layernorm_292.dc.reduce_sum.0"(#loc84))
#loc132 = loc("layernorm_292.dc.multiply.2"(#loc84))
#loc133 = loc("layernorm_292.dc.subtract.3"(#loc84))
#loc134 = loc("layernorm_292.dc.multiply.4"(#loc84))
#loc135 = loc("layernorm_292.dc.reduce_sum.5"(#loc84))
#loc136 = loc("layernorm_292.dc.multiply.7"(#loc84))
#loc137 = loc("layernorm_292.dc.add.9"(#loc84))
#loc138 = loc("layernorm_292.dc.sqrt.10"(#loc84))
#loc139 = loc("layernorm_292.dc.reciprocal.11"(#loc84))
#loc140 = loc("layernorm_292.dc.multiply.12"(#loc84))
#loc141 = loc("layernorm_292.dc.multiply.13"(#loc84))
#loc142 = loc("layernorm_292.dc.add.14"(#loc84))
#loc143 = loc("torch.nn.modules.conv.Conv2d::proj"(#loc87))
#loc144 = loc("reshape_300"(#loc87))
#loc145 = loc("squeeze_301"(#loc87))
#loc146 = loc("transpose_302"(#loc87))
#loc147 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm"(#loc87))
#loc148 = loc("add_353"(#loc78))
#loc149 = loc("add_374"(#loc78))
#loc150 = loc("add_424"(#loc79))
#loc151 = loc("add_445"(#loc79))
#loc152 = loc("layernorm_446.dc.reduce_sum.0"(#loc88))
#loc153 = loc("layernorm_446.dc.multiply.2"(#loc88))
#loc154 = loc("layernorm_446.dc.subtract.3"(#loc88))
#loc155 = loc("layernorm_446.dc.multiply.4"(#loc88))
#loc156 = loc("layernorm_446.dc.reduce_sum.5"(#loc88))
#loc157 = loc("layernorm_446.dc.multiply.7"(#loc88))
#loc158 = loc("layernorm_446.dc.add.9"(#loc88))
#loc159 = loc("layernorm_446.dc.sqrt.10"(#loc88))
#loc160 = loc("layernorm_446.dc.reciprocal.11"(#loc88))
#loc161 = loc("layernorm_446.dc.multiply.12"(#loc88))
#loc162 = loc("layernorm_446.dc.multiply.13"(#loc88))
#loc163 = loc("layernorm_446.dc.add.14"(#loc88))
#loc164 = loc("torch.nn.modules.conv.Conv2d::proj"(#loc91))
#loc165 = loc("reshape_454"(#loc91))
#loc166 = loc("squeeze_455"(#loc91))
#loc167 = loc("transpose_456"(#loc91))
#loc168 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm"(#loc91))
#loc169 = loc("add_499"(#loc78))
#loc170 = loc("add_520"(#loc78))
#loc171 = loc("add_562"(#loc79))
#loc172 = loc("add_583"(#loc79))
#loc173 = loc("layernorm_584.dc.reduce_sum.0"(#loc92))
#loc174 = loc("layernorm_584.dc.multiply.2"(#loc92))
#loc175 = loc("layernorm_584.dc.subtract.3"(#loc92))
#loc176 = loc("layernorm_584.dc.multiply.4"(#loc92))
#loc177 = loc("layernorm_584.dc.reduce_sum.5"(#loc92))
#loc178 = loc("layernorm_584.dc.multiply.7"(#loc92))
#loc179 = loc("layernorm_584.dc.add.9"(#loc92))
#loc180 = loc("layernorm_584.dc.sqrt.10"(#loc92))
#loc181 = loc("layernorm_584.dc.reciprocal.11"(#loc92))
#loc182 = loc("layernorm_584.dc.multiply.12"(#loc92))
#loc183 = loc("layernorm_584.dc.multiply.13"(#loc92))
#loc184 = loc("layernorm_584.dc.add.14"(#loc92))
#loc185 = loc("conv2d_0.dc.transpose.0"(#loc93))
#loc186 = loc("conv2d_0.dc.transpose.1"(#loc93))
#loc187 = loc("conv2d_0.dc.conv2d.4"(#loc93))
#loc188 = loc("conv2d_0.dc.transpose.5"(#loc93))
#loc189 = loc("conv2d_0.dc.transpose.6"(#loc93))
#loc190 = loc("layernorm_7.dc.reduce_sum.0"(#loc97))
#loc191 = loc("layernorm_7.dc.multiply.2"(#loc97))
#loc192 = loc("layernorm_7.dc.subtract.3"(#loc97))
#loc193 = loc("layernorm_7.dc.multiply.4"(#loc97))
#loc194 = loc("layernorm_7.dc.reduce_sum.5"(#loc97))
#loc195 = loc("layernorm_7.dc.multiply.7"(#loc97))
#loc196 = loc("layernorm_7.dc.add.9"(#loc97))
#loc197 = loc("layernorm_7.dc.sqrt.10"(#loc97))
#loc198 = loc("layernorm_7.dc.reciprocal.11"(#loc97))
#loc199 = loc("layernorm_7.dc.multiply.12"(#loc97))
#loc200 = loc("layernorm_7.dc.multiply.13"(#loc97))
#loc201 = loc("layernorm_7.dc.add.14"(#loc97))
#loc202 = loc("layernorm_8.dc.reduce_sum.0"(#loc98))
#loc203 = loc("layernorm_8.dc.multiply.2"(#loc98))
#loc204 = loc("layernorm_8.dc.subtract.3"(#loc98))
#loc205 = loc("layernorm_8.dc.multiply.4"(#loc98))
#loc206 = loc("layernorm_8.dc.reduce_sum.5"(#loc98))
#loc207 = loc("layernorm_8.dc.multiply.7"(#loc98))
#loc208 = loc("layernorm_8.dc.add.9"(#loc98))
#loc209 = loc("layernorm_8.dc.sqrt.10"(#loc98))
#loc210 = loc("layernorm_8.dc.reciprocal.11"(#loc98))
#loc211 = loc("layernorm_8.dc.multiply.12"(#loc98))
#loc212 = loc("layernorm_8.dc.multiply.13"(#loc98))
#loc213 = loc("layernorm_8.dc.add.14"(#loc98))
#loc214 = loc("transformers.models.segformer.modeling_segformer.SegformerEfficientSelfAttention::self"(#loc99))
#loc215 = loc("transformers.models.segformer.modeling_segformer.SegformerSelfOutput::output"(#loc99))
#loc216 = loc("layernorm_52.dc.reduce_sum.0"(#loc101))
#loc217 = loc("layernorm_52.dc.multiply.2"(#loc101))
#loc218 = loc("layernorm_52.dc.subtract.3"(#loc101))
#loc219 = loc("layernorm_52.dc.multiply.4"(#loc101))
#loc220 = loc("layernorm_52.dc.reduce_sum.5"(#loc101))
#loc221 = loc("layernorm_52.dc.multiply.7"(#loc101))
#loc222 = loc("layernorm_52.dc.add.9"(#loc101))
#loc223 = loc("layernorm_52.dc.sqrt.10"(#loc101))
#loc224 = loc("layernorm_52.dc.reciprocal.11"(#loc101))
#loc225 = loc("layernorm_52.dc.multiply.12"(#loc101))
#loc226 = loc("layernorm_52.dc.multiply.13"(#loc101))
#loc227 = loc("layernorm_52.dc.add.14"(#loc101))
#loc228 = loc("torch.nn.modules.linear.Linear::dense1"(#loc102))
#loc229 = loc("transformers.models.segformer.modeling_segformer.SegformerDWConv::dwconv"(#loc102))
#loc230 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc102))
#loc231 = loc("torch.nn.modules.linear.Linear::dense2"(#loc102))
#loc232 = loc("layernorm_73.dc.reduce_sum.0"(#loc104))
#loc233 = loc("layernorm_73.dc.multiply.2"(#loc104))
#loc234 = loc("layernorm_73.dc.subtract.3"(#loc104))
#loc235 = loc("layernorm_73.dc.multiply.4"(#loc104))
#loc236 = loc("layernorm_73.dc.reduce_sum.5"(#loc104))
#loc237 = loc("layernorm_73.dc.multiply.7"(#loc104))
#loc238 = loc("layernorm_73.dc.add.9"(#loc104))
#loc239 = loc("layernorm_73.dc.sqrt.10"(#loc104))
#loc240 = loc("layernorm_73.dc.reciprocal.11"(#loc104))
#loc241 = loc("layernorm_73.dc.multiply.12"(#loc104))
#loc242 = loc("layernorm_73.dc.multiply.13"(#loc104))
#loc243 = loc("layernorm_73.dc.add.14"(#loc104))
#loc244 = loc("transformers.models.segformer.modeling_segformer.SegformerEfficientSelfAttention::self"(#loc105))
#loc245 = loc("transformers.models.segformer.modeling_segformer.SegformerSelfOutput::output"(#loc105))
#loc246 = loc("layernorm_117.dc.reduce_sum.0"(#loc107))
#loc247 = loc("layernorm_117.dc.multiply.2"(#loc107))
#loc248 = loc("layernorm_117.dc.subtract.3"(#loc107))
#loc249 = loc("layernorm_117.dc.multiply.4"(#loc107))
#loc250 = loc("layernorm_117.dc.reduce_sum.5"(#loc107))
#loc251 = loc("layernorm_117.dc.multiply.7"(#loc107))
#loc252 = loc("layernorm_117.dc.add.9"(#loc107))
#loc253 = loc("layernorm_117.dc.sqrt.10"(#loc107))
#loc254 = loc("layernorm_117.dc.reciprocal.11"(#loc107))
#loc255 = loc("layernorm_117.dc.multiply.12"(#loc107))
#loc256 = loc("layernorm_117.dc.multiply.13"(#loc107))
#loc257 = loc("layernorm_117.dc.add.14"(#loc107))
#loc258 = loc("torch.nn.modules.linear.Linear::dense1"(#loc108))
#loc259 = loc("transformers.models.segformer.modeling_segformer.SegformerDWConv::dwconv"(#loc108))
#loc260 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc108))
#loc261 = loc("torch.nn.modules.linear.Linear::dense2"(#loc108))
#loc262 = loc("conv2d_142.dc.transpose.0"(#loc122))
#loc263 = loc("conv2d_142.dc.transpose.1"(#loc122))
#loc264 = loc("conv2d_142.dc.conv2d.4"(#loc122))
#loc265 = loc("conv2d_142.dc.transpose.5"(#loc122))
#loc266 = loc("conv2d_142.dc.transpose.6"(#loc122))
#loc267 = loc("layernorm_149.dc.reduce_sum.0"(#loc126))
#loc268 = loc("layernorm_149.dc.multiply.2"(#loc126))
#loc269 = loc("layernorm_149.dc.subtract.3"(#loc126))
#loc270 = loc("layernorm_149.dc.multiply.4"(#loc126))
#loc271 = loc("layernorm_149.dc.reduce_sum.5"(#loc126))
#loc272 = loc("layernorm_149.dc.multiply.7"(#loc126))
#loc273 = loc("layernorm_149.dc.add.9"(#loc126))
#loc274 = loc("layernorm_149.dc.sqrt.10"(#loc126))
#loc275 = loc("layernorm_149.dc.reciprocal.11"(#loc126))
#loc276 = loc("layernorm_149.dc.multiply.12"(#loc126))
#loc277 = loc("layernorm_149.dc.multiply.13"(#loc126))
#loc278 = loc("layernorm_149.dc.add.14"(#loc126))
#loc279 = loc("layernorm_150.dc.reduce_sum.0"(#loc98))
#loc280 = loc("layernorm_150.dc.multiply.2"(#loc98))
#loc281 = loc("layernorm_150.dc.subtract.3"(#loc98))
#loc282 = loc("layernorm_150.dc.multiply.4"(#loc98))
#loc283 = loc("layernorm_150.dc.reduce_sum.5"(#loc98))
#loc284 = loc("layernorm_150.dc.multiply.7"(#loc98))
#loc285 = loc("layernorm_150.dc.add.9"(#loc98))
#loc286 = loc("layernorm_150.dc.sqrt.10"(#loc98))
#loc287 = loc("layernorm_150.dc.reciprocal.11"(#loc98))
#loc288 = loc("layernorm_150.dc.multiply.12"(#loc98))
#loc289 = loc("layernorm_150.dc.multiply.13"(#loc98))
#loc290 = loc("layernorm_150.dc.add.14"(#loc98))
#loc291 = loc("layernorm_200.dc.reduce_sum.0"(#loc101))
#loc292 = loc("layernorm_200.dc.multiply.2"(#loc101))
#loc293 = loc("layernorm_200.dc.subtract.3"(#loc101))
#loc294 = loc("layernorm_200.dc.multiply.4"(#loc101))
#loc295 = loc("layernorm_200.dc.reduce_sum.5"(#loc101))
#loc296 = loc("layernorm_200.dc.multiply.7"(#loc101))
#loc297 = loc("layernorm_200.dc.add.9"(#loc101))
#loc298 = loc("layernorm_200.dc.sqrt.10"(#loc101))
#loc299 = loc("layernorm_200.dc.reciprocal.11"(#loc101))
#loc300 = loc("layernorm_200.dc.multiply.12"(#loc101))
#loc301 = loc("layernorm_200.dc.multiply.13"(#loc101))
#loc302 = loc("layernorm_200.dc.add.14"(#loc101))
#loc303 = loc("layernorm_221.dc.reduce_sum.0"(#loc104))
#loc304 = loc("layernorm_221.dc.multiply.2"(#loc104))
#loc305 = loc("layernorm_221.dc.subtract.3"(#loc104))
#loc306 = loc("layernorm_221.dc.multiply.4"(#loc104))
#loc307 = loc("layernorm_221.dc.reduce_sum.5"(#loc104))
#loc308 = loc("layernorm_221.dc.multiply.7"(#loc104))
#loc309 = loc("layernorm_221.dc.add.9"(#loc104))
#loc310 = loc("layernorm_221.dc.sqrt.10"(#loc104))
#loc311 = loc("layernorm_221.dc.reciprocal.11"(#loc104))
#loc312 = loc("layernorm_221.dc.multiply.12"(#loc104))
#loc313 = loc("layernorm_221.dc.multiply.13"(#loc104))
#loc314 = loc("layernorm_221.dc.add.14"(#loc104))
#loc315 = loc("layernorm_271.dc.reduce_sum.0"(#loc107))
#loc316 = loc("layernorm_271.dc.multiply.2"(#loc107))
#loc317 = loc("layernorm_271.dc.subtract.3"(#loc107))
#loc318 = loc("layernorm_271.dc.multiply.4"(#loc107))
#loc319 = loc("layernorm_271.dc.reduce_sum.5"(#loc107))
#loc320 = loc("layernorm_271.dc.multiply.7"(#loc107))
#loc321 = loc("layernorm_271.dc.add.9"(#loc107))
#loc322 = loc("layernorm_271.dc.sqrt.10"(#loc107))
#loc323 = loc("layernorm_271.dc.reciprocal.11"(#loc107))
#loc324 = loc("layernorm_271.dc.multiply.12"(#loc107))
#loc325 = loc("layernorm_271.dc.multiply.13"(#loc107))
#loc326 = loc("layernorm_271.dc.add.14"(#loc107))
#loc327 = loc("conv2d_296.dc.transpose.0"(#loc143))
#loc328 = loc("conv2d_296.dc.transpose.1"(#loc143))
#loc329 = loc("conv2d_296.dc.conv2d.4"(#loc143))
#loc330 = loc("conv2d_296.dc.transpose.5"(#loc143))
#loc331 = loc("conv2d_296.dc.transpose.6"(#loc143))
#loc332 = loc("layernorm_303.dc.reduce_sum.0"(#loc147))
#loc333 = loc("layernorm_303.dc.multiply.2"(#loc147))
#loc334 = loc("layernorm_303.dc.subtract.3"(#loc147))
#loc335 = loc("layernorm_303.dc.multiply.4"(#loc147))
#loc336 = loc("layernorm_303.dc.reduce_sum.5"(#loc147))
#loc337 = loc("layernorm_303.dc.multiply.7"(#loc147))
#loc338 = loc("layernorm_303.dc.add.9"(#loc147))
#loc339 = loc("layernorm_303.dc.sqrt.10"(#loc147))
#loc340 = loc("layernorm_303.dc.reciprocal.11"(#loc147))
#loc341 = loc("layernorm_303.dc.multiply.12"(#loc147))
#loc342 = loc("layernorm_303.dc.multiply.13"(#loc147))
#loc343 = loc("layernorm_303.dc.add.14"(#loc147))
#loc344 = loc("layernorm_304.dc.reduce_sum.0"(#loc98))
#loc345 = loc("layernorm_304.dc.multiply.2"(#loc98))
#loc346 = loc("layernorm_304.dc.subtract.3"(#loc98))
#loc347 = loc("layernorm_304.dc.multiply.4"(#loc98))
#loc348 = loc("layernorm_304.dc.reduce_sum.5"(#loc98))
#loc349 = loc("layernorm_304.dc.multiply.7"(#loc98))
#loc350 = loc("layernorm_304.dc.add.9"(#loc98))
#loc351 = loc("layernorm_304.dc.sqrt.10"(#loc98))
#loc352 = loc("layernorm_304.dc.reciprocal.11"(#loc98))
#loc353 = loc("layernorm_304.dc.multiply.12"(#loc98))
#loc354 = loc("layernorm_304.dc.multiply.13"(#loc98))
#loc355 = loc("layernorm_304.dc.add.14"(#loc98))
#loc356 = loc("layernorm_354.dc.reduce_sum.0"(#loc101))
#loc357 = loc("layernorm_354.dc.multiply.2"(#loc101))
#loc358 = loc("layernorm_354.dc.subtract.3"(#loc101))
#loc359 = loc("layernorm_354.dc.multiply.4"(#loc101))
#loc360 = loc("layernorm_354.dc.reduce_sum.5"(#loc101))
#loc361 = loc("layernorm_354.dc.multiply.7"(#loc101))
#loc362 = loc("layernorm_354.dc.add.9"(#loc101))
#loc363 = loc("layernorm_354.dc.sqrt.10"(#loc101))
#loc364 = loc("layernorm_354.dc.reciprocal.11"(#loc101))
#loc365 = loc("layernorm_354.dc.multiply.12"(#loc101))
#loc366 = loc("layernorm_354.dc.multiply.13"(#loc101))
#loc367 = loc("layernorm_354.dc.add.14"(#loc101))
#loc368 = loc("layernorm_375.dc.reduce_sum.0"(#loc104))
#loc369 = loc("layernorm_375.dc.multiply.2"(#loc104))
#loc370 = loc("layernorm_375.dc.subtract.3"(#loc104))
#loc371 = loc("layernorm_375.dc.multiply.4"(#loc104))
#loc372 = loc("layernorm_375.dc.reduce_sum.5"(#loc104))
#loc373 = loc("layernorm_375.dc.multiply.7"(#loc104))
#loc374 = loc("layernorm_375.dc.add.9"(#loc104))
#loc375 = loc("layernorm_375.dc.sqrt.10"(#loc104))
#loc376 = loc("layernorm_375.dc.reciprocal.11"(#loc104))
#loc377 = loc("layernorm_375.dc.multiply.12"(#loc104))
#loc378 = loc("layernorm_375.dc.multiply.13"(#loc104))
#loc379 = loc("layernorm_375.dc.add.14"(#loc104))
#loc380 = loc("layernorm_425.dc.reduce_sum.0"(#loc107))
#loc381 = loc("layernorm_425.dc.multiply.2"(#loc107))
#loc382 = loc("layernorm_425.dc.subtract.3"(#loc107))
#loc383 = loc("layernorm_425.dc.multiply.4"(#loc107))
#loc384 = loc("layernorm_425.dc.reduce_sum.5"(#loc107))
#loc385 = loc("layernorm_425.dc.multiply.7"(#loc107))
#loc386 = loc("layernorm_425.dc.add.9"(#loc107))
#loc387 = loc("layernorm_425.dc.sqrt.10"(#loc107))
#loc388 = loc("layernorm_425.dc.reciprocal.11"(#loc107))
#loc389 = loc("layernorm_425.dc.multiply.12"(#loc107))
#loc390 = loc("layernorm_425.dc.multiply.13"(#loc107))
#loc391 = loc("layernorm_425.dc.add.14"(#loc107))
#loc392 = loc("conv2d_450.dc.transpose.0"(#loc164))
#loc393 = loc("conv2d_450.dc.transpose.1"(#loc164))
#loc394 = loc("conv2d_450.dc.conv2d.4"(#loc164))
#loc395 = loc("conv2d_450.dc.transpose.5"(#loc164))
#loc396 = loc("conv2d_450.dc.transpose.6"(#loc164))
#loc397 = loc("layernorm_457.dc.reduce_sum.0"(#loc168))
#loc398 = loc("layernorm_457.dc.multiply.2"(#loc168))
#loc399 = loc("layernorm_457.dc.subtract.3"(#loc168))
#loc400 = loc("layernorm_457.dc.multiply.4"(#loc168))
#loc401 = loc("layernorm_457.dc.reduce_sum.5"(#loc168))
#loc402 = loc("layernorm_457.dc.multiply.7"(#loc168))
#loc403 = loc("layernorm_457.dc.add.9"(#loc168))
#loc404 = loc("layernorm_457.dc.sqrt.10"(#loc168))
#loc405 = loc("layernorm_457.dc.reciprocal.11"(#loc168))
#loc406 = loc("layernorm_457.dc.multiply.12"(#loc168))
#loc407 = loc("layernorm_457.dc.multiply.13"(#loc168))
#loc408 = loc("layernorm_457.dc.add.14"(#loc168))
#loc409 = loc("layernorm_458.dc.reduce_sum.0"(#loc98))
#loc410 = loc("layernorm_458.dc.multiply.2"(#loc98))
#loc411 = loc("layernorm_458.dc.subtract.3"(#loc98))
#loc412 = loc("layernorm_458.dc.multiply.4"(#loc98))
#loc413 = loc("layernorm_458.dc.reduce_sum.5"(#loc98))
#loc414 = loc("layernorm_458.dc.multiply.7"(#loc98))
#loc415 = loc("layernorm_458.dc.add.9"(#loc98))
#loc416 = loc("layernorm_458.dc.sqrt.10"(#loc98))
#loc417 = loc("layernorm_458.dc.reciprocal.11"(#loc98))
#loc418 = loc("layernorm_458.dc.multiply.12"(#loc98))
#loc419 = loc("layernorm_458.dc.multiply.13"(#loc98))
#loc420 = loc("layernorm_458.dc.add.14"(#loc98))
#loc421 = loc("layernorm_500.dc.reduce_sum.0"(#loc101))
#loc422 = loc("layernorm_500.dc.multiply.2"(#loc101))
#loc423 = loc("layernorm_500.dc.subtract.3"(#loc101))
#loc424 = loc("layernorm_500.dc.multiply.4"(#loc101))
#loc425 = loc("layernorm_500.dc.reduce_sum.5"(#loc101))
#loc426 = loc("layernorm_500.dc.multiply.7"(#loc101))
#loc427 = loc("layernorm_500.dc.add.9"(#loc101))
#loc428 = loc("layernorm_500.dc.sqrt.10"(#loc101))
#loc429 = loc("layernorm_500.dc.reciprocal.11"(#loc101))
#loc430 = loc("layernorm_500.dc.multiply.12"(#loc101))
#loc431 = loc("layernorm_500.dc.multiply.13"(#loc101))
#loc432 = loc("layernorm_500.dc.add.14"(#loc101))
#loc433 = loc("layernorm_521.dc.reduce_sum.0"(#loc104))
#loc434 = loc("layernorm_521.dc.multiply.2"(#loc104))
#loc435 = loc("layernorm_521.dc.subtract.3"(#loc104))
#loc436 = loc("layernorm_521.dc.multiply.4"(#loc104))
#loc437 = loc("layernorm_521.dc.reduce_sum.5"(#loc104))
#loc438 = loc("layernorm_521.dc.multiply.7"(#loc104))
#loc439 = loc("layernorm_521.dc.add.9"(#loc104))
#loc440 = loc("layernorm_521.dc.sqrt.10"(#loc104))
#loc441 = loc("layernorm_521.dc.reciprocal.11"(#loc104))
#loc442 = loc("layernorm_521.dc.multiply.12"(#loc104))
#loc443 = loc("layernorm_521.dc.multiply.13"(#loc104))
#loc444 = loc("layernorm_521.dc.add.14"(#loc104))
#loc445 = loc("layernorm_563.dc.reduce_sum.0"(#loc107))
#loc446 = loc("layernorm_563.dc.multiply.2"(#loc107))
#loc447 = loc("layernorm_563.dc.subtract.3"(#loc107))
#loc448 = loc("layernorm_563.dc.multiply.4"(#loc107))
#loc449 = loc("layernorm_563.dc.reduce_sum.5"(#loc107))
#loc450 = loc("layernorm_563.dc.multiply.7"(#loc107))
#loc451 = loc("layernorm_563.dc.add.9"(#loc107))
#loc452 = loc("layernorm_563.dc.sqrt.10"(#loc107))
#loc453 = loc("layernorm_563.dc.reciprocal.11"(#loc107))
#loc454 = loc("layernorm_563.dc.multiply.12"(#loc107))
#loc455 = loc("layernorm_563.dc.multiply.13"(#loc107))
#loc456 = loc("layernorm_563.dc.add.14"(#loc107))
#loc457 = loc("torch.nn.modules.linear.Linear::query"(#loc214))
#loc458 = loc("reshape_12"(#loc214))
#loc459 = loc("squeeze_13"(#loc214))
#loc460 = loc("transpose_14"(#loc214))
#loc461 = loc("reshape_15"(#loc214))
#loc462 = loc("torch.nn.modules.conv.Conv2d::sr"(#loc214))
#loc463 = loc("reshape_20"(#loc214))
#loc464 = loc("transpose_21"(#loc214))
#loc465 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm"(#loc214))
#loc466 = loc("torch.nn.modules.linear.Linear::key"(#loc214))
#loc467 = loc("reshape_28"(#loc214))
#loc468 = loc("squeeze_29"(#loc214))
#loc469 = loc("matmul_31"(#loc214))
#loc470 = loc("reshape_32.dc.unsqueeze.0"(#loc214))
#loc471 = loc("divide_33"(#loc214))
#loc472 = loc("softmax_34"(#loc214))
#loc473 = loc("reshape_36.dc.squeeze.0"(#loc214))
#loc474 = loc("torch.nn.modules.linear.Linear::value"(#loc214))
#loc475 = loc("reshape_41"(#loc214))
#loc476 = loc("transpose_42"(#loc214))
#loc477 = loc("transpose_43"(#loc214))
#loc478 = loc("reshape_44.dc.squeeze.0"(#loc214))
#loc479 = loc("matmul_46"(#loc214))
#loc480 = loc("torch.nn.modules.linear.Linear::dense"(#loc215))
#loc481 = loc("matmul_54"(#loc228))
#loc482 = loc("transpose_56"(#loc229))
#loc483 = loc("reshape_57"(#loc229))
#loc484 = loc("torch.nn.modules.conv.Conv2d::dwconv"(#loc229))
#loc485 = loc("reshape_63"(#loc229))
#loc486 = loc("squeeze_64"(#loc229))
#loc487 = loc("transpose_65"(#loc229))
#loc488 = loc("gelu_66"(#loc230))
#loc489 = loc("matmul_69"(#loc231))
#loc490 = loc("torch.nn.modules.linear.Linear::query"(#loc244))
#loc491 = loc("reshape_77"(#loc244))
#loc492 = loc("squeeze_78"(#loc244))
#loc493 = loc("transpose_79"(#loc244))
#loc494 = loc("reshape_80"(#loc244))
#loc495 = loc("torch.nn.modules.conv.Conv2d::sr"(#loc244))
#loc496 = loc("reshape_85"(#loc244))
#loc497 = loc("transpose_86"(#loc244))
#loc498 = loc("torch.nn.modules.normalization.LayerNorm::layer_norm"(#loc244))
#loc499 = loc("torch.nn.modules.linear.Linear::key"(#loc244))
#loc500 = loc("reshape_93"(#loc244))
#loc501 = loc("squeeze_94"(#loc244))
#loc502 = loc("matmul_96"(#loc244))
#loc503 = loc("reshape_97.dc.unsqueeze.0"(#loc244))
#loc504 = loc("divide_98"(#loc244))
#loc505 = loc("softmax_99"(#loc244))
#loc506 = loc("reshape_101.dc.squeeze.0"(#loc244))
#loc507 = loc("torch.nn.modules.linear.Linear::value"(#loc244))
#loc508 = loc("reshape_106"(#loc244))
#loc509 = loc("transpose_107"(#loc244))
#loc510 = loc("transpose_108"(#loc244))
#loc511 = loc("reshape_109.dc.squeeze.0"(#loc244))
#loc512 = loc("matmul_111"(#loc244))
#loc513 = loc("torch.nn.modules.linear.Linear::dense"(#loc245))
#loc514 = loc("matmul_119"(#loc258))
#loc515 = loc("transpose_121"(#loc259))
#loc516 = loc("reshape_122"(#loc259))
#loc517 = loc("torch.nn.modules.conv.Conv2d::dwconv"(#loc259))
#loc518 = loc("reshape_128"(#loc259))
#loc519 = loc("squeeze_129"(#loc259))
#loc520 = loc("transpose_130"(#loc259))
#loc521 = loc("gelu_131"(#loc260))
#loc522 = loc("matmul_134"(#loc261))
#loc523 = loc("reshape_154"(#loc214))
#loc524 = loc("transpose_155"(#loc214))
#loc525 = loc("reshape_156.dc.squeeze.0"(#loc214))
#loc526 = loc("transpose_157"(#loc214))
#loc527 = loc("reshape_158"(#loc214))
#loc528 = loc("reshape_163"(#loc214))
#loc529 = loc("transpose_164"(#loc214))
#loc530 = loc("reshape_171"(#loc214))
#loc531 = loc("transpose_172"(#loc214))
#loc532 = loc("reshape_173.dc.squeeze.0"(#loc214))
#loc533 = loc("matmul_175"(#loc214))
#loc534 = loc("reshape_176.dc.unsqueeze.0"(#loc214))
#loc535 = loc("divide_177"(#loc214))
#loc536 = loc("softmax_178"(#loc214))
#loc537 = loc("reshape_180.dc.squeeze.0"(#loc214))
#loc538 = loc("reshape_185"(#loc214))
#loc539 = loc("transpose_186"(#loc214))
#loc540 = loc("transpose_187"(#loc214))
#loc541 = loc("reshape_188.dc.squeeze.0"(#loc214))
#loc542 = loc("matmul_190"(#loc214))
#loc543 = loc("reshape_191.dc.unsqueeze.0"(#loc214))
#loc544 = loc("transpose_192"(#loc214))
#loc545 = loc("matmul_202"(#loc228))
#loc546 = loc("transpose_204"(#loc229))
#loc547 = loc("reshape_205"(#loc229))
#loc548 = loc("reshape_211"(#loc229))
#loc549 = loc("squeeze_212"(#loc229))
#loc550 = loc("transpose_213"(#loc229))
#loc551 = loc("gelu_214"(#loc230))
#loc552 = loc("matmul_217"(#loc231))
#loc553 = loc("reshape_225"(#loc244))
#loc554 = loc("transpose_226"(#loc244))
#loc555 = loc("reshape_227.dc.squeeze.0"(#loc244))
#loc556 = loc("transpose_228"(#loc244))
#loc557 = loc("reshape_229"(#loc244))
#loc558 = loc("reshape_234"(#loc244))
#loc559 = loc("transpose_235"(#loc244))
#loc560 = loc("reshape_242"(#loc244))
#loc561 = loc("transpose_243"(#loc244))
#loc562 = loc("reshape_244.dc.squeeze.0"(#loc244))
#loc563 = loc("matmul_246"(#loc244))
#loc564 = loc("reshape_247.dc.unsqueeze.0"(#loc244))
#loc565 = loc("divide_248"(#loc244))
#loc566 = loc("softmax_249"(#loc244))
#loc567 = loc("reshape_251.dc.squeeze.0"(#loc244))
#loc568 = loc("reshape_256"(#loc244))
#loc569 = loc("transpose_257"(#loc244))
#loc570 = loc("transpose_258"(#loc244))
#loc571 = loc("reshape_259.dc.squeeze.0"(#loc244))
#loc572 = loc("matmul_261"(#loc244))
#loc573 = loc("reshape_262.dc.unsqueeze.0"(#loc244))
#loc574 = loc("transpose_263"(#loc244))
#loc575 = loc("matmul_273"(#loc258))
#loc576 = loc("transpose_275"(#loc259))
#loc577 = loc("reshape_276"(#loc259))
#loc578 = loc("reshape_282"(#loc259))
#loc579 = loc("squeeze_283"(#loc259))
#loc580 = loc("transpose_284"(#loc259))
#loc581 = loc("gelu_285"(#loc260))
#loc582 = loc("matmul_288"(#loc261))
#loc583 = loc("reshape_308"(#loc214))
#loc584 = loc("transpose_309"(#loc214))
#loc585 = loc("reshape_310.dc.squeeze.0"(#loc214))
#loc586 = loc("transpose_311"(#loc214))
#loc587 = loc("reshape_312"(#loc214))
#loc588 = loc("reshape_317"(#loc214))
#loc589 = loc("transpose_318"(#loc214))
#loc590 = loc("reshape_325"(#loc214))
#loc591 = loc("transpose_326"(#loc214))
#loc592 = loc("reshape_327.dc.squeeze.0"(#loc214))
#loc593 = loc("matmul_329"(#loc214))
#loc594 = loc("reshape_330.dc.unsqueeze.0"(#loc214))
#loc595 = loc("divide_331"(#loc214))
#loc596 = loc("softmax_332"(#loc214))
#loc597 = loc("reshape_334.dc.squeeze.0"(#loc214))
#loc598 = loc("reshape_339"(#loc214))
#loc599 = loc("transpose_340"(#loc214))
#loc600 = loc("transpose_341"(#loc214))
#loc601 = loc("reshape_342.dc.squeeze.0"(#loc214))
#loc602 = loc("matmul_344"(#loc214))
#loc603 = loc("reshape_345.dc.unsqueeze.0"(#loc214))
#loc604 = loc("transpose_346"(#loc214))
#loc605 = loc("matmul_356"(#loc228))
#loc606 = loc("transpose_358"(#loc229))
#loc607 = loc("reshape_359"(#loc229))
#loc608 = loc("reshape_365"(#loc229))
#loc609 = loc("squeeze_366"(#loc229))
#loc610 = loc("transpose_367"(#loc229))
#loc611 = loc("gelu_368"(#loc230))
#loc612 = loc("matmul_371"(#loc231))
#loc613 = loc("reshape_379"(#loc244))
#loc614 = loc("transpose_380"(#loc244))
#loc615 = loc("reshape_381.dc.squeeze.0"(#loc244))
#loc616 = loc("transpose_382"(#loc244))
#loc617 = loc("reshape_383"(#loc244))
#loc618 = loc("reshape_388"(#loc244))
#loc619 = loc("transpose_389"(#loc244))
#loc620 = loc("reshape_396"(#loc244))
#loc621 = loc("transpose_397"(#loc244))
#loc622 = loc("reshape_398.dc.squeeze.0"(#loc244))
#loc623 = loc("matmul_400"(#loc244))
#loc624 = loc("reshape_401.dc.unsqueeze.0"(#loc244))
#loc625 = loc("divide_402"(#loc244))
#loc626 = loc("softmax_403"(#loc244))
#loc627 = loc("reshape_405.dc.squeeze.0"(#loc244))
#loc628 = loc("reshape_410"(#loc244))
#loc629 = loc("transpose_411"(#loc244))
#loc630 = loc("transpose_412"(#loc244))
#loc631 = loc("reshape_413.dc.squeeze.0"(#loc244))
#loc632 = loc("matmul_415"(#loc244))
#loc633 = loc("reshape_416.dc.unsqueeze.0"(#loc244))
#loc634 = loc("transpose_417"(#loc244))
#loc635 = loc("matmul_427"(#loc258))
#loc636 = loc("transpose_429"(#loc259))
#loc637 = loc("reshape_430"(#loc259))
#loc638 = loc("reshape_436"(#loc259))
#loc639 = loc("squeeze_437"(#loc259))
#loc640 = loc("transpose_438"(#loc259))
#loc641 = loc("gelu_439"(#loc260))
#loc642 = loc("matmul_442"(#loc261))
#loc643 = loc("reshape_464"(#loc214))
#loc644 = loc("transpose_465"(#loc214))
#loc645 = loc("reshape_466.dc.squeeze.0"(#loc214))
#loc646 = loc("reshape_471"(#loc214))
#loc647 = loc("transpose_472"(#loc214))
#loc648 = loc("reshape_473.dc.squeeze.0"(#loc214))
#loc649 = loc("matmul_475"(#loc214))
#loc650 = loc("reshape_476.dc.unsqueeze.0"(#loc214))
#loc651 = loc("divide_477"(#loc214))
#loc652 = loc("softmax_478"(#loc214))
#loc653 = loc("reshape_480.dc.squeeze.0"(#loc214))
#loc654 = loc("reshape_485"(#loc214))
#loc655 = loc("transpose_486"(#loc214))
#loc656 = loc("transpose_487"(#loc214))
#loc657 = loc("reshape_488.dc.squeeze.0"(#loc214))
#loc658 = loc("matmul_490"(#loc214))
#loc659 = loc("reshape_491.dc.unsqueeze.0"(#loc214))
#loc660 = loc("transpose_492"(#loc214))
#loc661 = loc("matmul_502"(#loc228))
#loc662 = loc("transpose_504"(#loc229))
#loc663 = loc("reshape_505"(#loc229))
#loc664 = loc("reshape_511"(#loc229))
#loc665 = loc("squeeze_512"(#loc229))
#loc666 = loc("transpose_513"(#loc229))
#loc667 = loc("gelu_514"(#loc230))
#loc668 = loc("matmul_517"(#loc231))
#loc669 = loc("reshape_527"(#loc244))
#loc670 = loc("transpose_528"(#loc244))
#loc671 = loc("reshape_529.dc.squeeze.0"(#loc244))
#loc672 = loc("reshape_534"(#loc244))
#loc673 = loc("transpose_535"(#loc244))
#loc674 = loc("reshape_536.dc.squeeze.0"(#loc244))
#loc675 = loc("matmul_538"(#loc244))
#loc676 = loc("reshape_539.dc.unsqueeze.0"(#loc244))
#loc677 = loc("divide_540"(#loc244))
#loc678 = loc("softmax_541"(#loc244))
#loc679 = loc("reshape_543.dc.squeeze.0"(#loc244))
#loc680 = loc("reshape_548"(#loc244))
#loc681 = loc("transpose_549"(#loc244))
#loc682 = loc("transpose_550"(#loc244))
#loc683 = loc("reshape_551.dc.squeeze.0"(#loc244))
#loc684 = loc("matmul_553"(#loc244))
#loc685 = loc("reshape_554.dc.unsqueeze.0"(#loc244))
#loc686 = loc("transpose_555"(#loc244))
#loc687 = loc("matmul_565"(#loc258))
#loc688 = loc("transpose_567"(#loc259))
#loc689 = loc("reshape_568"(#loc259))
#loc690 = loc("reshape_574"(#loc259))
#loc691 = loc("squeeze_575"(#loc259))
#loc692 = loc("transpose_576"(#loc259))
#loc693 = loc("gelu_577"(#loc260))
#loc694 = loc("matmul_580"(#loc261))
#loc695 = loc("matmul_10"(#loc457))
#loc696 = loc("conv2d_16.dc.transpose.0"(#loc462))
#loc697 = loc("conv2d_16.dc.transpose.1"(#loc462))
#loc698 = loc("conv2d_16.dc.conv2d.4"(#loc462))
#loc699 = loc("conv2d_16.dc.transpose.5"(#loc462))
#loc700 = loc("conv2d_16.dc.transpose.6"(#loc462))
#loc701 = loc("layernorm_22.dc.reduce_sum.0"(#loc465))
#loc702 = loc("layernorm_22.dc.multiply.2"(#loc465))
#loc703 = loc("layernorm_22.dc.subtract.3"(#loc465))
#loc704 = loc("layernorm_22.dc.multiply.4"(#loc465))
#loc705 = loc("layernorm_22.dc.reduce_sum.5"(#loc465))
#loc706 = loc("layernorm_22.dc.multiply.7"(#loc465))
#loc707 = loc("layernorm_22.dc.add.9"(#loc465))
#loc708 = loc("layernorm_22.dc.sqrt.10"(#loc465))
#loc709 = loc("layernorm_22.dc.reciprocal.11"(#loc465))
#loc710 = loc("layernorm_22.dc.multiply.12"(#loc465))
#loc711 = loc("layernorm_22.dc.multiply.13"(#loc465))
#loc712 = loc("layernorm_22.dc.add.14"(#loc465))
#loc713 = loc("reshape_23.dc.squeeze.0"(#loc466))
#loc714 = loc("matmul_25"(#loc466))
#loc715 = loc("reshape_26.dc.unsqueeze.0"(#loc466))
#loc716 = loc("matmul_38"(#loc474))
#loc717 = loc("reshape_39.dc.unsqueeze.0"(#loc474))
#loc718 = loc("matmul_48"(#loc480))
#loc719 = loc("conv2d_59.dc.transpose.0"(#loc484))
#loc720 = loc("conv2d_59.dc.transpose.1"(#loc484))
#loc721 = loc("conv2d_59.dc.conv2d.4"(#loc484))
#loc722 = loc("conv2d_59.dc.transpose.5"(#loc484))
#loc723 = loc("conv2d_59.dc.transpose.6"(#loc484))
#loc724 = loc("matmul_75"(#loc490))
#loc725 = loc("conv2d_81.dc.transpose.0"(#loc495))
#loc726 = loc("conv2d_81.dc.transpose.1"(#loc495))
#loc727 = loc("conv2d_81.dc.conv2d.4"(#loc495))
#loc728 = loc("conv2d_81.dc.transpose.5"(#loc495))
#loc729 = loc("conv2d_81.dc.transpose.6"(#loc495))
#loc730 = loc("layernorm_87.dc.reduce_sum.0"(#loc498))
#loc731 = loc("layernorm_87.dc.multiply.2"(#loc498))
#loc732 = loc("layernorm_87.dc.subtract.3"(#loc498))
#loc733 = loc("layernorm_87.dc.multiply.4"(#loc498))
#loc734 = loc("layernorm_87.dc.reduce_sum.5"(#loc498))
#loc735 = loc("layernorm_87.dc.multiply.7"(#loc498))
#loc736 = loc("layernorm_87.dc.add.9"(#loc498))
#loc737 = loc("layernorm_87.dc.sqrt.10"(#loc498))
#loc738 = loc("layernorm_87.dc.reciprocal.11"(#loc498))
#loc739 = loc("layernorm_87.dc.multiply.12"(#loc498))
#loc740 = loc("layernorm_87.dc.multiply.13"(#loc498))
#loc741 = loc("layernorm_87.dc.add.14"(#loc498))
#loc742 = loc("reshape_88.dc.squeeze.0"(#loc499))
#loc743 = loc("matmul_90"(#loc499))
#loc744 = loc("reshape_91.dc.unsqueeze.0"(#loc499))
#loc745 = loc("matmul_103"(#loc507))
#loc746 = loc("reshape_104.dc.unsqueeze.0"(#loc507))
#loc747 = loc("matmul_113"(#loc513))
#loc748 = loc("conv2d_124.dc.transpose.0"(#loc517))
#loc749 = loc("conv2d_124.dc.transpose.1"(#loc517))
#loc750 = loc("conv2d_124.dc.conv2d.4"(#loc517))
#loc751 = loc("conv2d_124.dc.transpose.5"(#loc517))
#loc752 = loc("conv2d_124.dc.transpose.6"(#loc517))
#loc753 = loc("matmul_152"(#loc457))
#loc754 = loc("conv2d_159.dc.transpose.0"(#loc462))
#loc755 = loc("conv2d_159.dc.transpose.1"(#loc462))
#loc756 = loc("conv2d_159.dc.conv2d.4"(#loc462))
#loc757 = loc("conv2d_159.dc.transpose.5"(#loc462))
#loc758 = loc("conv2d_159.dc.transpose.6"(#loc462))
#loc759 = loc("layernorm_165.dc.reduce_sum.0"(#loc465))
#loc760 = loc("layernorm_165.dc.multiply.2"(#loc465))
#loc761 = loc("layernorm_165.dc.subtract.3"(#loc465))
#loc762 = loc("layernorm_165.dc.multiply.4"(#loc465))
#loc763 = loc("layernorm_165.dc.reduce_sum.5"(#loc465))
#loc764 = loc("layernorm_165.dc.multiply.7"(#loc465))
#loc765 = loc("layernorm_165.dc.add.9"(#loc465))
#loc766 = loc("layernorm_165.dc.sqrt.10"(#loc465))
#loc767 = loc("layernorm_165.dc.reciprocal.11"(#loc465))
#loc768 = loc("layernorm_165.dc.multiply.12"(#loc465))
#loc769 = loc("layernorm_165.dc.multiply.13"(#loc465))
#loc770 = loc("layernorm_165.dc.add.14"(#loc465))
#loc771 = loc("reshape_166.dc.squeeze.0"(#loc466))
#loc772 = loc("matmul_168"(#loc466))
#loc773 = loc("reshape_169.dc.unsqueeze.0"(#loc466))
#loc774 = loc("matmul_182"(#loc474))
#loc775 = loc("reshape_183.dc.unsqueeze.0"(#loc474))
#loc776 = loc("reshape_193"(#loc480))
#loc777 = loc("matmul_195"(#loc480))
#loc778 = loc("reshape_196.dc.unsqueeze.0"(#loc480))
#loc779 = loc("conv2d_207.dc.transpose.0"(#loc484))
#loc780 = loc("conv2d_207.dc.transpose.1"(#loc484))
#loc781 = loc("conv2d_207.dc.conv2d.4"(#loc484))
#loc782 = loc("conv2d_207.dc.transpose.5"(#loc484))
#loc783 = loc("conv2d_207.dc.transpose.6"(#loc484))
#loc784 = loc("matmul_223"(#loc490))
#loc785 = loc("conv2d_230.dc.transpose.0"(#loc495))
#loc786 = loc("conv2d_230.dc.transpose.1"(#loc495))
#loc787 = loc("conv2d_230.dc.conv2d.4"(#loc495))
#loc788 = loc("conv2d_230.dc.transpose.5"(#loc495))
#loc789 = loc("conv2d_230.dc.transpose.6"(#loc495))
#loc790 = loc("layernorm_236.dc.reduce_sum.0"(#loc498))
#loc791 = loc("layernorm_236.dc.multiply.2"(#loc498))
#loc792 = loc("layernorm_236.dc.subtract.3"(#loc498))
#loc793 = loc("layernorm_236.dc.multiply.4"(#loc498))
#loc794 = loc("layernorm_236.dc.reduce_sum.5"(#loc498))
#loc795 = loc("layernorm_236.dc.multiply.7"(#loc498))
#loc796 = loc("layernorm_236.dc.add.9"(#loc498))
#loc797 = loc("layernorm_236.dc.sqrt.10"(#loc498))
#loc798 = loc("layernorm_236.dc.reciprocal.11"(#loc498))
#loc799 = loc("layernorm_236.dc.multiply.12"(#loc498))
#loc800 = loc("layernorm_236.dc.multiply.13"(#loc498))
#loc801 = loc("layernorm_236.dc.add.14"(#loc498))
#loc802 = loc("reshape_237.dc.squeeze.0"(#loc499))
#loc803 = loc("matmul_239"(#loc499))
#loc804 = loc("reshape_240.dc.unsqueeze.0"(#loc499))
#loc805 = loc("matmul_253"(#loc507))
#loc806 = loc("reshape_254.dc.unsqueeze.0"(#loc507))
#loc807 = loc("reshape_264"(#loc513))
#loc808 = loc("matmul_266"(#loc513))
#loc809 = loc("reshape_267.dc.unsqueeze.0"(#loc513))
#loc810 = loc("conv2d_278.dc.transpose.0"(#loc517))
#loc811 = loc("conv2d_278.dc.transpose.1"(#loc517))
#loc812 = loc("conv2d_278.dc.conv2d.4"(#loc517))
#loc813 = loc("conv2d_278.dc.transpose.5"(#loc517))
#loc814 = loc("conv2d_278.dc.transpose.6"(#loc517))
#loc815 = loc("matmul_306"(#loc457))
#loc816 = loc("conv2d_313.dc.transpose.0"(#loc462))
#loc817 = loc("conv2d_313.dc.transpose.1"(#loc462))
#loc818 = loc("conv2d_313.dc.conv2d.4"(#loc462))
#loc819 = loc("conv2d_313.dc.transpose.5"(#loc462))
#loc820 = loc("conv2d_313.dc.transpose.6"(#loc462))
#loc821 = loc("layernorm_319.dc.reduce_sum.0"(#loc465))
#loc822 = loc("layernorm_319.dc.multiply.2"(#loc465))
#loc823 = loc("layernorm_319.dc.subtract.3"(#loc465))
#loc824 = loc("layernorm_319.dc.multiply.4"(#loc465))
#loc825 = loc("layernorm_319.dc.reduce_sum.5"(#loc465))
#loc826 = loc("layernorm_319.dc.multiply.7"(#loc465))
#loc827 = loc("layernorm_319.dc.add.9"(#loc465))
#loc828 = loc("layernorm_319.dc.sqrt.10"(#loc465))
#loc829 = loc("layernorm_319.dc.reciprocal.11"(#loc465))
#loc830 = loc("layernorm_319.dc.multiply.12"(#loc465))
#loc831 = loc("layernorm_319.dc.multiply.13"(#loc465))
#loc832 = loc("layernorm_319.dc.add.14"(#loc465))
#loc833 = loc("reshape_320.dc.squeeze.0"(#loc466))
#loc834 = loc("matmul_322"(#loc466))
#loc835 = loc("reshape_323.dc.unsqueeze.0"(#loc466))
#loc836 = loc("matmul_336"(#loc474))
#loc837 = loc("reshape_337.dc.unsqueeze.0"(#loc474))
#loc838 = loc("reshape_347"(#loc480))
#loc839 = loc("matmul_349"(#loc480))
#loc840 = loc("reshape_350.dc.unsqueeze.0"(#loc480))
#loc841 = loc("conv2d_361.dc.transpose.0"(#loc484))
#loc842 = loc("conv2d_361.dc.transpose.1"(#loc484))
#loc843 = loc("conv2d_361.dc.conv2d.4"(#loc484))
#loc844 = loc("conv2d_361.dc.transpose.5"(#loc484))
#loc845 = loc("conv2d_361.dc.transpose.6"(#loc484))
#loc846 = loc("matmul_377"(#loc490))
#loc847 = loc("conv2d_384.dc.transpose.0"(#loc495))
#loc848 = loc("conv2d_384.dc.transpose.1"(#loc495))
#loc849 = loc("conv2d_384.dc.conv2d.4"(#loc495))
#loc850 = loc("conv2d_384.dc.transpose.5"(#loc495))
#loc851 = loc("conv2d_384.dc.transpose.6"(#loc495))
#loc852 = loc("layernorm_390.dc.reduce_sum.0"(#loc498))
#loc853 = loc("layernorm_390.dc.multiply.2"(#loc498))
#loc854 = loc("layernorm_390.dc.subtract.3"(#loc498))
#loc855 = loc("layernorm_390.dc.multiply.4"(#loc498))
#loc856 = loc("layernorm_390.dc.reduce_sum.5"(#loc498))
#loc857 = loc("layernorm_390.dc.multiply.7"(#loc498))
#loc858 = loc("layernorm_390.dc.add.9"(#loc498))
#loc859 = loc("layernorm_390.dc.sqrt.10"(#loc498))
#loc860 = loc("layernorm_390.dc.reciprocal.11"(#loc498))
#loc861 = loc("layernorm_390.dc.multiply.12"(#loc498))
#loc862 = loc("layernorm_390.dc.multiply.13"(#loc498))
#loc863 = loc("layernorm_390.dc.add.14"(#loc498))
#loc864 = loc("reshape_391.dc.squeeze.0"(#loc499))
#loc865 = loc("matmul_393"(#loc499))
#loc866 = loc("reshape_394.dc.unsqueeze.0"(#loc499))
#loc867 = loc("matmul_407"(#loc507))
#loc868 = loc("reshape_408.dc.unsqueeze.0"(#loc507))
#loc869 = loc("reshape_418"(#loc513))
#loc870 = loc("matmul_420"(#loc513))
#loc871 = loc("reshape_421.dc.unsqueeze.0"(#loc513))
#loc872 = loc("conv2d_432.dc.transpose.0"(#loc517))
#loc873 = loc("conv2d_432.dc.transpose.1"(#loc517))
#loc874 = loc("conv2d_432.dc.conv2d.4"(#loc517))
#loc875 = loc("conv2d_432.dc.transpose.5"(#loc517))
#loc876 = loc("conv2d_432.dc.transpose.6"(#loc517))
#loc877 = loc("reshape_459.dc.squeeze.0"(#loc457))
#loc878 = loc("matmul_461"(#loc457))
#loc879 = loc("reshape_462.dc.unsqueeze.0"(#loc457))
#loc880 = loc("matmul_468"(#loc466))
#loc881 = loc("reshape_469.dc.unsqueeze.0"(#loc466))
#loc882 = loc("matmul_482"(#loc474))
#loc883 = loc("reshape_483.dc.unsqueeze.0"(#loc474))
#loc884 = loc("reshape_493"(#loc480))
#loc885 = loc("matmul_495"(#loc480))
#loc886 = loc("reshape_496.dc.unsqueeze.0"(#loc480))
#loc887 = loc("conv2d_507.dc.transpose.0"(#loc484))
#loc888 = loc("conv2d_507.dc.transpose.1"(#loc484))
#loc889 = loc("conv2d_507.dc.conv2d.4"(#loc484))
#loc890 = loc("conv2d_507.dc.transpose.5"(#loc484))
#loc891 = loc("conv2d_507.dc.transpose.6"(#loc484))
#loc892 = loc("reshape_522.dc.squeeze.0"(#loc490))
#loc893 = loc("matmul_524"(#loc490))
#loc894 = loc("reshape_525.dc.unsqueeze.0"(#loc490))
#loc895 = loc("matmul_531"(#loc499))
#loc896 = loc("reshape_532.dc.unsqueeze.0"(#loc499))
#loc897 = loc("matmul_545"(#loc507))
#loc898 = loc("reshape_546.dc.unsqueeze.0"(#loc507))
#loc899 = loc("reshape_556"(#loc513))
#loc900 = loc("matmul_558"(#loc513))
#loc901 = loc("reshape_559.dc.unsqueeze.0"(#loc513))
#loc902 = loc("conv2d_570.dc.transpose.0"(#loc517))
#loc903 = loc("conv2d_570.dc.transpose.1"(#loc517))
#loc904 = loc("conv2d_570.dc.conv2d.4"(#loc517))
#loc905 = loc("conv2d_570.dc.transpose.5"(#loc517))
#loc906 = loc("conv2d_570.dc.transpose.6"(#loc517))
