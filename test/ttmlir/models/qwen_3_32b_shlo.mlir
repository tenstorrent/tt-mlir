#loc1 = loc("p0.3")
#loc2 = loc("p1.11")
#loc3 = loc("p2.31")
#loc4 = loc("p3.39")
#loc5 = loc("p4.44")
#loc6 = loc("p5.80")
#loc7 = loc("p6.119")
#loc8 = loc("p7.157")
#loc9 = loc("p8.165")
#loc10 = loc("p9.187")
#loc11 = loc("p10.200")
#loc12 = loc("p11.209")
#loc13 = loc("p12.214")
#loc14 = loc("p13.223")
#loc15 = loc("p14.281")
#loc16 = loc("p15.319")
#loc17 = loc("p16.442")
#loc18 = loc("p17.451")
#loc19 = loc("p18.497")
#loc20 = loc("p19.536")
#loc21 = loc("p20.569")
#loc22 = loc("p21.577")
#loc23 = loc("p22.599")
#loc24 = loc("p23.612")
#loc25 = loc("p24.621")
#loc26 = loc("p25.626")
#loc27 = loc("p26.635")
#loc28 = loc("p27.676")
#loc29 = loc("p28.714")
#loc30 = loc("p29.837")
#loc31 = loc("p30.846")
#loc32 = loc("p31.892")
#loc33 = loc("p32.931")
#loc34 = loc("p33.964")
#loc35 = loc("p34.972")
#loc36 = loc("p35.994")
#loc37 = loc("p36.1007")
#loc38 = loc("p37.1016")
#loc39 = loc("p38.1021")
#loc40 = loc("p39.1030")
#loc41 = loc("p40.1071")
#loc42 = loc("p41.1109")
#loc43 = loc("p42.1232")
#loc44 = loc("p43.1241")
#loc45 = loc("p44.1287")
#loc46 = loc("p45.1326")
#loc47 = loc("p46.1359")
#loc48 = loc("p47.1367")
#loc49 = loc("p48.1389")
#loc50 = loc("p49.1402")
#loc51 = loc("p50.1411")
#loc52 = loc("p51.1416")
#loc53 = loc("p52.1425")
#loc54 = loc("p53.1466")
#loc55 = loc("p54.1504")
#loc56 = loc("p55.1627")
#loc57 = loc("p56.1636")
#loc58 = loc("p57.1682")
#loc59 = loc("p58.1721")
#loc60 = loc("p59.1754")
#loc61 = loc("p60.1762")
#loc62 = loc("p61.1784")
#loc63 = loc("p62.1797")
#loc64 = loc("p63.1806")
#loc65 = loc("p64.1811")
#loc66 = loc("p65.1820")
#loc67 = loc("p66.1861")
#loc68 = loc("p67.1899")
#loc69 = loc("p68.2022")
#loc70 = loc("p69.2031")
#loc71 = loc("p70.2077")
#loc72 = loc("p71.2116")
#loc73 = loc("p72.2149")
#loc74 = loc("p73.2157")
#loc75 = loc("p74.2179")
#loc76 = loc("p75.2192")
#loc77 = loc("p76.2201")
#loc78 = loc("p77.2206")
#loc79 = loc("p78.2215")
#loc80 = loc("p79.2256")
#loc81 = loc("p80.2294")
#loc82 = loc("p81.2417")
#loc83 = loc("p82.2426")
#loc84 = loc("p83.2472")
#loc85 = loc("p84.2511")
#loc86 = loc("p85.2544")
#loc87 = loc("p86.2552")
#loc88 = loc("p87.2574")
#loc89 = loc("p88.2587")
#loc90 = loc("p89.2596")
#loc91 = loc("p90.2601")
#loc92 = loc("p91.2610")
#loc93 = loc("p92.2651")
#loc94 = loc("p93.2689")
#loc95 = loc("p94.2812")
#loc96 = loc("p95.2821")
#loc97 = loc("p96.2867")
#loc98 = loc("p97.2906")
#loc99 = loc("p98.2939")
#loc100 = loc("p99.2947")
#loc101 = loc("p100.2969")
#loc102 = loc("p101.2982")
#loc103 = loc("p102.2991")
#loc104 = loc("p103.2996")
#loc105 = loc("p104.3005")
#loc106 = loc("p105.3046")
#loc107 = loc("p106.3084")
#loc108 = loc("p107.3207")
#loc109 = loc("p108.3216")
#loc110 = loc("p109.3262")
#loc111 = loc("p110.3301")
#loc112 = loc("p111.3334")
#loc113 = loc("p112.3342")
#loc114 = loc("p113.3364")
#loc115 = loc("p114.3377")
#loc116 = loc("p115.3386")
#loc117 = loc("p116.3391")
#loc118 = loc("p117.3400")
#loc119 = loc("p118.3441")
#loc120 = loc("p119.3479")
#loc121 = loc("p120.3602")
#loc122 = loc("p121.3611")
#loc123 = loc("p122.3657")
#loc124 = loc("p123.3696")
#loc125 = loc("p124.3729")
#loc126 = loc("p125.3737")
#loc127 = loc("p126.3759")
#loc128 = loc("p127.3772")
#loc129 = loc("p128.3781")
#loc130 = loc("p129.3786")
#loc131 = loc("p130.3795")
#loc132 = loc("p131.3836")
#loc133 = loc("p132.3874")
#loc134 = loc("p133.3997")
#loc135 = loc("p134.4006")
#loc136 = loc("p135.4052")
#loc137 = loc("p136.4091")
#loc138 = loc("p137.4124")
#loc139 = loc("p138.4132")
#loc140 = loc("p139.4154")
#loc141 = loc("p140.4167")
#loc142 = loc("p141.4176")
#loc143 = loc("p142.4181")
#loc144 = loc("p143.4190")
#loc145 = loc("p144.4231")
#loc146 = loc("p145.4269")
#loc147 = loc("p146.4392")
#loc148 = loc("p147.4401")
#loc149 = loc("p148.4447")
#loc150 = loc("p149.4486")
#loc151 = loc("p150.4519")
#loc152 = loc("p151.4527")
#loc153 = loc("p152.4549")
#loc154 = loc("p153.4562")
#loc155 = loc("p154.4571")
#loc156 = loc("p155.4576")
#loc157 = loc("p156.4585")
#loc158 = loc("p157.4626")
#loc159 = loc("p158.4664")
#loc160 = loc("p159.4787")
#loc161 = loc("p160.4796")
#loc162 = loc("p161.4842")
#loc163 = loc("p162.4881")
#loc164 = loc("p163.4914")
#loc165 = loc("p164.4922")
#loc166 = loc("p165.4944")
#loc167 = loc("p166.4957")
#loc168 = loc("p167.4966")
#loc169 = loc("p168.4971")
#loc170 = loc("p169.4980")
#loc171 = loc("p170.5021")
#loc172 = loc("p171.5059")
#loc173 = loc("p172.5182")
#loc174 = loc("p173.5191")
#loc175 = loc("p174.5237")
#loc176 = loc("p175.5276")
#loc177 = loc("p176.5309")
#loc178 = loc("p177.5317")
#loc179 = loc("p178.5339")
#loc180 = loc("p179.5352")
#loc181 = loc("p180.5361")
#loc182 = loc("p181.5366")
#loc183 = loc("p182.5375")
#loc184 = loc("p183.5416")
#loc185 = loc("p184.5454")
#loc186 = loc("p185.5577")
#loc187 = loc("p186.5586")
#loc188 = loc("p187.5632")
#loc189 = loc("p188.5671")
#loc190 = loc("p189.5704")
#loc191 = loc("p190.5712")
#loc192 = loc("p191.5734")
#loc193 = loc("p192.5747")
#loc194 = loc("p193.5756")
#loc195 = loc("p194.5761")
#loc196 = loc("p195.5770")
#loc197 = loc("p196.5811")
#loc198 = loc("p197.5849")
#loc199 = loc("p198.5972")
#loc200 = loc("p199.5981")
#loc201 = loc("p200.6027")
#loc202 = loc("p201.6066")
#loc203 = loc("p202.6099")
#loc204 = loc("p203.6107")
#loc205 = loc("p204.6129")
#loc206 = loc("p205.6142")
#loc207 = loc("p206.6151")
#loc208 = loc("p207.6156")
#loc209 = loc("p208.6165")
#loc210 = loc("p209.6206")
#loc211 = loc("p210.6244")
#loc212 = loc("p211.6367")
#loc213 = loc("p212.6376")
#loc214 = loc("p213.6422")
#loc215 = loc("p214.6461")
#loc216 = loc("p215.6494")
#loc217 = loc("p216.6502")
#loc218 = loc("p217.6524")
#loc219 = loc("p218.6537")
#loc220 = loc("p219.6546")
#loc221 = loc("p220.6551")
#loc222 = loc("p221.6560")
#loc223 = loc("p222.6601")
#loc224 = loc("p223.6639")
#loc225 = loc("p224.6762")
#loc226 = loc("p225.6771")
#loc227 = loc("p226.6817")
#loc228 = loc("p227.6856")
#loc229 = loc("p228.6889")
#loc230 = loc("p229.6897")
#loc231 = loc("p230.6919")
#loc232 = loc("p231.6932")
#loc233 = loc("p232.6941")
#loc234 = loc("p233.6946")
#loc235 = loc("p234.6955")
#loc236 = loc("p235.6996")
#loc237 = loc("p236.7034")
#loc238 = loc("p237.7157")
#loc239 = loc("p238.7166")
#loc240 = loc("p239.7212")
#loc241 = loc("p240.7251")
#loc242 = loc("p241.7284")
#loc243 = loc("p242.7292")
#loc244 = loc("p243.7314")
#loc245 = loc("p244.7327")
#loc246 = loc("p245.7336")
#loc247 = loc("p246.7341")
#loc248 = loc("p247.7350")
#loc249 = loc("p248.7391")
#loc250 = loc("p249.7429")
#loc251 = loc("p250.7552")
#loc252 = loc("p251.7561")
#loc253 = loc("p252.7607")
#loc254 = loc("p253.7646")
#loc255 = loc("p254.7679")
#loc256 = loc("p255.7687")
#loc257 = loc("p256.7709")
#loc258 = loc("p257.7722")
#loc259 = loc("p258.7731")
#loc260 = loc("p259.7736")
#loc261 = loc("p260.7745")
#loc262 = loc("p261.7786")
#loc263 = loc("p262.7824")
#loc264 = loc("p263.7947")
#loc265 = loc("p264.7956")
#loc266 = loc("p265.8002")
#loc267 = loc("p266.8041")
#loc268 = loc("p267.8074")
#loc269 = loc("p268.8082")
#loc270 = loc("p269.8104")
#loc271 = loc("p270.8117")
#loc272 = loc("p271.8126")
#loc273 = loc("p272.8131")
#loc274 = loc("p273.8140")
#loc275 = loc("p274.8181")
#loc276 = loc("p275.8219")
#loc277 = loc("p276.8342")
#loc278 = loc("p277.8351")
#loc279 = loc("p278.8397")
#loc280 = loc("p279.8436")
#loc281 = loc("p280.8469")
#loc282 = loc("p281.8477")
#loc283 = loc("p282.8499")
#loc284 = loc("p283.8512")
#loc285 = loc("p284.8521")
#loc286 = loc("p285.8526")
#loc287 = loc("p286.8535")
#loc288 = loc("p287.8576")
#loc289 = loc("p288.8614")
#loc290 = loc("p289.8737")
#loc291 = loc("p290.8746")
#loc292 = loc("p291.8792")
#loc293 = loc("p292.8831")
#loc294 = loc("p293.8864")
#loc295 = loc("p294.8872")
#loc296 = loc("p295.8894")
#loc297 = loc("p296.8907")
#loc298 = loc("p297.8916")
#loc299 = loc("p298.8921")
#loc300 = loc("p299.8930")
#loc301 = loc("p300.8971")
#loc302 = loc("p301.9009")
#loc303 = loc("p302.9132")
#loc304 = loc("p303.9141")
#loc305 = loc("p304.9187")
#loc306 = loc("p305.9226")
#loc307 = loc("p306.9259")
#loc308 = loc("p307.9267")
#loc309 = loc("p308.9289")
#loc310 = loc("p309.9302")
#loc311 = loc("p310.9311")
#loc312 = loc("p311.9316")
#loc313 = loc("p312.9325")
#loc314 = loc("p313.9366")
#loc315 = loc("p314.9404")
#loc316 = loc("p315.9527")
#loc317 = loc("p316.9536")
#loc318 = loc("p317.9582")
#loc319 = loc("p318.9621")
#loc320 = loc("p319.9654")
#loc321 = loc("p320.9662")
#loc322 = loc("p321.9684")
#loc323 = loc("p322.9697")
#loc324 = loc("p323.9706")
#loc325 = loc("p324.9711")
#loc326 = loc("p325.9720")
#loc327 = loc("p326.9761")
#loc328 = loc("p327.9799")
#loc329 = loc("p328.9922")
#loc330 = loc("p329.9931")
#loc331 = loc("p330.9977")
#loc332 = loc("p331.10016")
#loc333 = loc("p332.10049")
#loc334 = loc("p333.10057")
#loc335 = loc("p334.10079")
#loc336 = loc("p335.10092")
#loc337 = loc("p336.10101")
#loc338 = loc("p337.10106")
#loc339 = loc("p338.10115")
#loc340 = loc("p339.10156")
#loc341 = loc("p340.10194")
#loc342 = loc("p341.10317")
#loc343 = loc("p342.10326")
#loc344 = loc("p343.10372")
#loc345 = loc("p344.10411")
#loc346 = loc("p345.10444")
#loc347 = loc("p346.10452")
#loc348 = loc("p347.10474")
#loc349 = loc("p348.10487")
#loc350 = loc("p349.10496")
#loc351 = loc("p350.10501")
#loc352 = loc("p351.10510")
#loc353 = loc("p352.10551")
#loc354 = loc("p353.10589")
#loc355 = loc("p354.10712")
#loc356 = loc("p355.10721")
#loc357 = loc("p356.10767")
#loc358 = loc("p357.10806")
#loc359 = loc("p358.10839")
#loc360 = loc("p359.10847")
#loc361 = loc("p360.10869")
#loc362 = loc("p361.10882")
#loc363 = loc("p362.10891")
#loc364 = loc("p363.10896")
#loc365 = loc("p364.10905")
#loc366 = loc("p365.10946")
#loc367 = loc("p366.10984")
#loc368 = loc("p367.11107")
#loc369 = loc("p368.11116")
#loc370 = loc("p369.11162")
#loc371 = loc("p370.11201")
#loc372 = loc("p371.11234")
#loc373 = loc("p372.11242")
#loc374 = loc("p373.11264")
#loc375 = loc("p374.11277")
#loc376 = loc("p375.11286")
#loc377 = loc("p376.11291")
#loc378 = loc("p377.11300")
#loc379 = loc("p378.11341")
#loc380 = loc("p379.11379")
#loc381 = loc("p380.11502")
#loc382 = loc("p381.11511")
#loc383 = loc("p382.11557")
#loc384 = loc("p383.11596")
#loc385 = loc("p384.11629")
#loc386 = loc("p385.11637")
#loc387 = loc("p386.11659")
#loc388 = loc("p387.11672")
#loc389 = loc("p388.11681")
#loc390 = loc("p389.11686")
#loc391 = loc("p390.11695")
#loc392 = loc("p391.11736")
#loc393 = loc("p392.11774")
#loc394 = loc("p393.11897")
#loc395 = loc("p394.11906")
#loc396 = loc("p395.11952")
#loc397 = loc("p396.11991")
#loc398 = loc("p397.12024")
#loc399 = loc("p398.12032")
#loc400 = loc("p399.12054")
#loc401 = loc("p400.12067")
#loc402 = loc("p401.12076")
#loc403 = loc("p402.12081")
#loc404 = loc("p403.12090")
#loc405 = loc("p404.12131")
#loc406 = loc("p405.12169")
#loc407 = loc("p406.12292")
#loc408 = loc("p407.12301")
#loc409 = loc("p408.12347")
#loc410 = loc("p409.12386")
#loc411 = loc("p410.12419")
#loc412 = loc("p411.12427")
#loc413 = loc("p412.12449")
#loc414 = loc("p413.12462")
#loc415 = loc("p414.12471")
#loc416 = loc("p415.12476")
#loc417 = loc("p416.12485")
#loc418 = loc("p417.12526")
#loc419 = loc("p418.12564")
#loc420 = loc("p419.12687")
#loc421 = loc("p420.12696")
#loc422 = loc("p421.12742")
#loc423 = loc("p422.12781")
#loc424 = loc("p423.12814")
#loc425 = loc("p424.12822")
#loc426 = loc("p425.12844")
#loc427 = loc("p426.12857")
#loc428 = loc("p427.12866")
#loc429 = loc("p428.12871")
#loc430 = loc("p429.12880")
#loc431 = loc("p430.12921")
#loc432 = loc("p431.12959")
#loc433 = loc("p432.13082")
#loc434 = loc("p433.13091")
#loc435 = loc("p434.13137")
#loc436 = loc("p435.13176")
#loc437 = loc("p436.13209")
#loc438 = loc("p437.13217")
#loc439 = loc("p438.13239")
#loc440 = loc("p439.13252")
#loc441 = loc("p440.13261")
#loc442 = loc("p441.13266")
#loc443 = loc("p442.13275")
#loc444 = loc("p443.13316")
#loc445 = loc("p444.13354")
#loc446 = loc("p445.13477")
#loc447 = loc("p446.13486")
#loc448 = loc("p447.13532")
#loc449 = loc("p448.13571")
#loc450 = loc("p449.13604")
#loc451 = loc("p450.13612")
#loc452 = loc("p451.13634")
#loc453 = loc("p452.13647")
#loc454 = loc("p453.13656")
#loc455 = loc("p454.13661")
#loc456 = loc("p455.13670")
#loc457 = loc("p456.13711")
#loc458 = loc("p457.13749")
#loc459 = loc("p458.13872")
#loc460 = loc("p459.13881")
#loc461 = loc("p460.13927")
#loc462 = loc("p461.13966")
#loc463 = loc("p462.13999")
#loc464 = loc("p463.14007")
#loc465 = loc("p464.14029")
#loc466 = loc("p465.14042")
#loc467 = loc("p466.14051")
#loc468 = loc("p467.14056")
#loc469 = loc("p468.14065")
#loc470 = loc("p469.14106")
#loc471 = loc("p470.14144")
#loc472 = loc("p471.14267")
#loc473 = loc("p472.14276")
#loc474 = loc("p473.14322")
#loc475 = loc("p474.14361")
#loc476 = loc("p475.14394")
#loc477 = loc("p476.14402")
#loc478 = loc("p477.14424")
#loc479 = loc("p478.14437")
#loc480 = loc("p479.14446")
#loc481 = loc("p480.14451")
#loc482 = loc("p481.14460")
#loc483 = loc("p482.14501")
#loc484 = loc("p483.14539")
#loc485 = loc("p484.14662")
#loc486 = loc("p485.14671")
#loc487 = loc("p486.14717")
#loc488 = loc("p487.14756")
#loc489 = loc("p488.14789")
#loc490 = loc("p489.14797")
#loc491 = loc("p490.14819")
#loc492 = loc("p491.14832")
#loc493 = loc("p492.14841")
#loc494 = loc("p493.14846")
#loc495 = loc("p494.14855")
#loc496 = loc("p495.14896")
#loc497 = loc("p496.14934")
#loc498 = loc("p497.15057")
#loc499 = loc("p498.15066")
#loc500 = loc("p499.15112")
#loc501 = loc("p500.15151")
#loc502 = loc("p501.15184")
#loc503 = loc("p502.15192")
#loc504 = loc("p503.15214")
#loc505 = loc("p504.15227")
#loc506 = loc("p505.15236")
#loc507 = loc("p506.15241")
#loc508 = loc("p507.15250")
#loc509 = loc("p508.15291")
#loc510 = loc("p509.15329")
#loc511 = loc("p510.15452")
#loc512 = loc("p511.15461")
#loc513 = loc("p512.15507")
#loc514 = loc("p513.15546")
#loc515 = loc("p514.15579")
#loc516 = loc("p515.15587")
#loc517 = loc("p516.15609")
#loc518 = loc("p517.15622")
#loc519 = loc("p518.15631")
#loc520 = loc("p519.15636")
#loc521 = loc("p520.15645")
#loc522 = loc("p521.15686")
#loc523 = loc("p522.15724")
#loc524 = loc("p523.15847")
#loc525 = loc("p524.15856")
#loc526 = loc("p525.15902")
#loc527 = loc("p526.15941")
#loc528 = loc("p527.15974")
#loc529 = loc("p528.15982")
#loc530 = loc("p529.16004")
#loc531 = loc("p530.16017")
#loc532 = loc("p531.16026")
#loc533 = loc("p532.16031")
#loc534 = loc("p533.16040")
#loc535 = loc("p534.16081")
#loc536 = loc("p535.16119")
#loc537 = loc("p536.16242")
#loc538 = loc("p537.16251")
#loc539 = loc("p538.16297")
#loc540 = loc("p539.16336")
#loc541 = loc("p540.16369")
#loc542 = loc("p541.16377")
#loc543 = loc("p542.16399")
#loc544 = loc("p543.16412")
#loc545 = loc("p544.16421")
#loc546 = loc("p545.16426")
#loc547 = loc("p546.16435")
#loc548 = loc("p547.16476")
#loc549 = loc("p548.16514")
#loc550 = loc("p549.16637")
#loc551 = loc("p550.16646")
#loc552 = loc("p551.16692")
#loc553 = loc("p552.16731")
#loc554 = loc("p553.16764")
#loc555 = loc("p554.16772")
#loc556 = loc("p555.16794")
#loc557 = loc("p556.16807")
#loc558 = loc("p557.16816")
#loc559 = loc("p558.16821")
#loc560 = loc("p559.16830")
#loc561 = loc("p560.16871")
#loc562 = loc("p561.16909")
#loc563 = loc("p562.17032")
#loc564 = loc("p563.17041")
#loc565 = loc("p564.17087")
#loc566 = loc("p565.17126")
#loc567 = loc("p566.17159")
#loc568 = loc("p567.17167")
#loc569 = loc("p568.17189")
#loc570 = loc("p569.17202")
#loc571 = loc("p570.17211")
#loc572 = loc("p571.17216")
#loc573 = loc("p572.17225")
#loc574 = loc("p573.17266")
#loc575 = loc("p574.17304")
#loc576 = loc("p575.17427")
#loc577 = loc("p576.17436")
#loc578 = loc("p577.17482")
#loc579 = loc("p578.17521")
#loc580 = loc("p579.17554")
#loc581 = loc("p580.17562")
#loc582 = loc("p581.17584")
#loc583 = loc("p582.17597")
#loc584 = loc("p583.17606")
#loc585 = loc("p584.17611")
#loc586 = loc("p585.17620")
#loc587 = loc("p586.17661")
#loc588 = loc("p587.17699")
#loc589 = loc("p588.17822")
#loc590 = loc("p589.17831")
#loc591 = loc("p590.17877")
#loc592 = loc("p591.17916")
#loc593 = loc("p592.17949")
#loc594 = loc("p593.17957")
#loc595 = loc("p594.17979")
#loc596 = loc("p595.17992")
#loc597 = loc("p596.18001")
#loc598 = loc("p597.18006")
#loc599 = loc("p598.18015")
#loc600 = loc("p599.18056")
#loc601 = loc("p600.18094")
#loc602 = loc("p601.18217")
#loc603 = loc("p602.18226")
#loc604 = loc("p603.18272")
#loc605 = loc("p604.18311")
#loc606 = loc("p605.18344")
#loc607 = loc("p606.18352")
#loc608 = loc("p607.18374")
#loc609 = loc("p608.18387")
#loc610 = loc("p609.18396")
#loc611 = loc("p610.18401")
#loc612 = loc("p611.18410")
#loc613 = loc("p612.18451")
#loc614 = loc("p613.18489")
#loc615 = loc("p614.18612")
#loc616 = loc("p615.18621")
#loc617 = loc("p616.18667")
#loc618 = loc("p617.18706")
#loc619 = loc("p618.18739")
#loc620 = loc("p619.18747")
#loc621 = loc("p620.18769")
#loc622 = loc("p621.18782")
#loc623 = loc("p622.18791")
#loc624 = loc("p623.18796")
#loc625 = loc("p624.18805")
#loc626 = loc("p625.18846")
#loc627 = loc("p626.18884")
#loc628 = loc("p627.19007")
#loc629 = loc("p628.19016")
#loc630 = loc("p629.19062")
#loc631 = loc("p630.19101")
#loc632 = loc("p631.19134")
#loc633 = loc("p632.19142")
#loc634 = loc("p633.19164")
#loc635 = loc("p634.19177")
#loc636 = loc("p635.19186")
#loc637 = loc("p636.19191")
#loc638 = loc("p637.19200")
#loc639 = loc("p638.19241")
#loc640 = loc("p639.19279")
#loc641 = loc("p640.19402")
#loc642 = loc("p641.19411")
#loc643 = loc("p642.19457")
#loc644 = loc("p643.19496")
#loc645 = loc("p644.19529")
#loc646 = loc("p645.19537")
#loc647 = loc("p646.19559")
#loc648 = loc("p647.19572")
#loc649 = loc("p648.19581")
#loc650 = loc("p649.19586")
#loc651 = loc("p650.19595")
#loc652 = loc("p651.19636")
#loc653 = loc("p652.19674")
#loc654 = loc("p653.19797")
#loc655 = loc("p654.19806")
#loc656 = loc("p655.19852")
#loc657 = loc("p656.19891")
#loc658 = loc("p657.19924")
#loc659 = loc("p658.19932")
#loc660 = loc("p659.19954")
#loc661 = loc("p660.19967")
#loc662 = loc("p661.19976")
#loc663 = loc("p662.19981")
#loc664 = loc("p663.19990")
#loc665 = loc("p664.20031")
#loc666 = loc("p665.20069")
#loc667 = loc("p666.20192")
#loc668 = loc("p667.20201")
#loc669 = loc("p668.20247")
#loc670 = loc("p669.20286")
#loc671 = loc("p670.20319")
#loc672 = loc("p671.20327")
#loc673 = loc("p672.20349")
#loc674 = loc("p673.20362")
#loc675 = loc("p674.20371")
#loc676 = loc("p675.20376")
#loc677 = loc("p676.20385")
#loc678 = loc("p677.20426")
#loc679 = loc("p678.20464")
#loc680 = loc("p679.20587")
#loc681 = loc("p680.20596")
#loc682 = loc("p681.20642")
#loc683 = loc("p682.20681")
#loc684 = loc("p683.20714")
#loc685 = loc("p684.20722")
#loc686 = loc("p685.20744")
#loc687 = loc("p686.20757")
#loc688 = loc("p687.20766")
#loc689 = loc("p688.20771")
#loc690 = loc("p689.20780")
#loc691 = loc("p690.20821")
#loc692 = loc("p691.20859")
#loc693 = loc("p692.20982")
#loc694 = loc("p693.20991")
#loc695 = loc("p694.21037")
#loc696 = loc("p695.21076")
#loc697 = loc("p696.21109")
#loc698 = loc("p697.21117")
#loc699 = loc("p698.21139")
#loc700 = loc("p699.21152")
#loc701 = loc("p700.21161")
#loc702 = loc("p701.21166")
#loc703 = loc("p702.21175")
#loc704 = loc("p703.21216")
#loc705 = loc("p704.21254")
#loc706 = loc("p705.21377")
#loc707 = loc("p706.21386")
#loc708 = loc("p707.21432")
#loc709 = loc("p708.21471")
#loc710 = loc("p709.21504")
#loc711 = loc("p710.21512")
#loc712 = loc("p711.21534")
#loc713 = loc("p712.21547")
#loc714 = loc("p713.21556")
#loc715 = loc("p714.21561")
#loc716 = loc("p715.21570")
#loc717 = loc("p716.21611")
#loc718 = loc("p717.21649")
#loc719 = loc("p718.21772")
#loc720 = loc("p719.21781")
#loc721 = loc("p720.21827")
#loc722 = loc("p721.21866")
#loc723 = loc("p722.21899")
#loc724 = loc("p723.21907")
#loc725 = loc("p724.21929")
#loc726 = loc("p725.21942")
#loc727 = loc("p726.21951")
#loc728 = loc("p727.21956")
#loc729 = loc("p728.21965")
#loc730 = loc("p729.22006")
#loc731 = loc("p730.22044")
#loc732 = loc("p731.22167")
#loc733 = loc("p732.22176")
#loc734 = loc("p733.22222")
#loc735 = loc("p734.22261")
#loc736 = loc("p735.22294")
#loc737 = loc("p736.22302")
#loc738 = loc("p737.22324")
#loc739 = loc("p738.22337")
#loc740 = loc("p739.22346")
#loc741 = loc("p740.22351")
#loc742 = loc("p741.22360")
#loc743 = loc("p742.22401")
#loc744 = loc("p743.22439")
#loc745 = loc("p744.22562")
#loc746 = loc("p745.22571")
#loc747 = loc("p746.22617")
#loc748 = loc("p747.22656")
#loc749 = loc("p748.22689")
#loc750 = loc("p749.22697")
#loc751 = loc("p750.22719")
#loc752 = loc("p751.22732")
#loc753 = loc("p752.22741")
#loc754 = loc("p753.22746")
#loc755 = loc("p754.22755")
#loc756 = loc("p755.22796")
#loc757 = loc("p756.22834")
#loc758 = loc("p757.22957")
#loc759 = loc("p758.22966")
#loc760 = loc("p759.23012")
#loc761 = loc("p760.23051")
#loc762 = loc("p761.23084")
#loc763 = loc("p762.23092")
#loc764 = loc("p763.23114")
#loc765 = loc("p764.23127")
#loc766 = loc("p765.23136")
#loc767 = loc("p766.23141")
#loc768 = loc("p767.23150")
#loc769 = loc("p768.23191")
#loc770 = loc("p769.23229")
#loc771 = loc("p770.23352")
#loc772 = loc("p771.23361")
#loc773 = loc("p772.23407")
#loc774 = loc("p773.23446")
#loc775 = loc("p774.23479")
#loc776 = loc("p775.23487")
#loc777 = loc("p776.23509")
#loc778 = loc("p777.23522")
#loc779 = loc("p778.23531")
#loc780 = loc("p779.23536")
#loc781 = loc("p780.23545")
#loc782 = loc("p781.23586")
#loc783 = loc("p782.23624")
#loc784 = loc("p783.23747")
#loc785 = loc("p784.23756")
#loc786 = loc("p785.23802")
#loc787 = loc("p786.23841")
#loc788 = loc("p787.23874")
#loc789 = loc("p788.23882")
#loc790 = loc("p789.23904")
#loc791 = loc("p790.23917")
#loc792 = loc("p791.23926")
#loc793 = loc("p792.23931")
#loc794 = loc("p793.23940")
#loc795 = loc("p794.23981")
#loc796 = loc("p795.24019")
#loc797 = loc("p796.24142")
#loc798 = loc("p797.24151")
#loc799 = loc("p798.24197")
#loc800 = loc("p799.24236")
#loc801 = loc("p800.24269")
#loc802 = loc("p801.24277")
#loc803 = loc("p802.24299")
#loc804 = loc("p803.24312")
#loc805 = loc("p804.24321")
#loc806 = loc("p805.24326")
#loc807 = loc("p806.24335")
#loc808 = loc("p807.24376")
#loc809 = loc("p808.24414")
#loc810 = loc("p809.24537")
#loc811 = loc("p810.24546")
#loc812 = loc("p811.24592")
#loc813 = loc("p812.24631")
#loc814 = loc("p813.24664")
#loc815 = loc("p814.24672")
#loc816 = loc("p815.24694")
#loc817 = loc("p816.24707")
#loc818 = loc("p817.24716")
#loc819 = loc("p818.24721")
#loc820 = loc("p819.24730")
#loc821 = loc("p820.24771")
#loc822 = loc("p821.24809")
#loc823 = loc("p822.24932")
#loc824 = loc("p823.24941")
#loc825 = loc("p824.24987")
#loc826 = loc("p825.25026")
#loc827 = loc("p826.25059")
#loc828 = loc("p827.25067")
#loc829 = loc("p828.25089")
#loc830 = loc("p829.25097")
#loc831 = loc("p830.25106")
#loc832 = loc("p831.25111")
#loc833 = loc("p832.25120")
#loc834 = loc("p833.25161")
#loc835 = loc("p834.25199")
#loc836 = loc("p835.25322")
#loc837 = loc("p836.25331")
#loc838 = loc("p837.25377")
#loc911 = loc("scatter.163")
#loc918 = loc("scatter.193")
#loc976 = loc("reduce.391")
#loc1001 = loc("dot.408")
#loc1037 = loc("dot.463")
#loc1078 = loc("scatter.575")
#loc1085 = loc("scatter.605")
#loc1132 = loc("reduce.786")
#loc1157 = loc("dot.803")
#loc1193 = loc("dot.858")
#loc1234 = loc("scatter.970")
#loc1241 = loc("scatter.1000")
#loc1288 = loc("reduce.1181")
#loc1313 = loc("dot.1198")
#loc1349 = loc("dot.1253")
#loc1390 = loc("scatter.1365")
#loc1397 = loc("scatter.1395")
#loc1444 = loc("reduce.1576")
#loc1469 = loc("dot.1593")
#loc1505 = loc("dot.1648")
#loc1546 = loc("scatter.1760")
#loc1553 = loc("scatter.1790")
#loc1600 = loc("reduce.1971")
#loc1625 = loc("dot.1988")
#loc1661 = loc("dot.2043")
#loc1702 = loc("scatter.2155")
#loc1709 = loc("scatter.2185")
#loc1756 = loc("reduce.2366")
#loc1781 = loc("dot.2383")
#loc1817 = loc("dot.2438")
#loc1858 = loc("scatter.2550")
#loc1865 = loc("scatter.2580")
#loc1912 = loc("reduce.2761")
#loc1937 = loc("dot.2778")
#loc1973 = loc("dot.2833")
#loc2014 = loc("scatter.2945")
#loc2021 = loc("scatter.2975")
#loc2068 = loc("reduce.3156")
#loc2093 = loc("dot.3173")
#loc2129 = loc("dot.3228")
#loc2170 = loc("scatter.3340")
#loc2177 = loc("scatter.3370")
#loc2224 = loc("reduce.3551")
#loc2249 = loc("dot.3568")
#loc2285 = loc("dot.3623")
#loc2326 = loc("scatter.3735")
#loc2333 = loc("scatter.3765")
#loc2380 = loc("reduce.3946")
#loc2405 = loc("dot.3963")
#loc2441 = loc("dot.4018")
#loc2482 = loc("scatter.4130")
#loc2489 = loc("scatter.4160")
#loc2536 = loc("reduce.4341")
#loc2561 = loc("dot.4358")
#loc2597 = loc("dot.4413")
#loc2638 = loc("scatter.4525")
#loc2645 = loc("scatter.4555")
#loc2692 = loc("reduce.4736")
#loc2717 = loc("dot.4753")
#loc2753 = loc("dot.4808")
#loc2794 = loc("scatter.4920")
#loc2801 = loc("scatter.4950")
#loc2848 = loc("reduce.5131")
#loc2873 = loc("dot.5148")
#loc2909 = loc("dot.5203")
#loc2950 = loc("scatter.5315")
#loc2957 = loc("scatter.5345")
#loc3004 = loc("reduce.5526")
#loc3029 = loc("dot.5543")
#loc3065 = loc("dot.5598")
#loc3106 = loc("scatter.5710")
#loc3113 = loc("scatter.5740")
#loc3160 = loc("reduce.5921")
#loc3185 = loc("dot.5938")
#loc3221 = loc("dot.5993")
#loc3262 = loc("scatter.6105")
#loc3269 = loc("scatter.6135")
#loc3316 = loc("reduce.6316")
#loc3341 = loc("dot.6333")
#loc3377 = loc("dot.6388")
#loc3418 = loc("scatter.6500")
#loc3425 = loc("scatter.6530")
#loc3472 = loc("reduce.6711")
#loc3497 = loc("dot.6728")
#loc3533 = loc("dot.6783")
#loc3574 = loc("scatter.6895")
#loc3581 = loc("scatter.6925")
#loc3628 = loc("reduce.7106")
#loc3653 = loc("dot.7123")
#loc3689 = loc("dot.7178")
#loc3730 = loc("scatter.7290")
#loc3737 = loc("scatter.7320")
#loc3784 = loc("reduce.7501")
#loc3809 = loc("dot.7518")
#loc3845 = loc("dot.7573")
#loc3886 = loc("scatter.7685")
#loc3893 = loc("scatter.7715")
#loc3940 = loc("reduce.7896")
#loc3965 = loc("dot.7913")
#loc4001 = loc("dot.7968")
#loc4042 = loc("scatter.8080")
#loc4049 = loc("scatter.8110")
#loc4096 = loc("reduce.8291")
#loc4121 = loc("dot.8308")
#loc4157 = loc("dot.8363")
#loc4198 = loc("scatter.8475")
#loc4205 = loc("scatter.8505")
#loc4252 = loc("reduce.8686")
#loc4277 = loc("dot.8703")
#loc4313 = loc("dot.8758")
#loc4354 = loc("scatter.8870")
#loc4361 = loc("scatter.8900")
#loc4408 = loc("reduce.9081")
#loc4433 = loc("dot.9098")
#loc4469 = loc("dot.9153")
#loc4510 = loc("scatter.9265")
#loc4517 = loc("scatter.9295")
#loc4564 = loc("reduce.9476")
#loc4589 = loc("dot.9493")
#loc4625 = loc("dot.9548")
#loc4666 = loc("scatter.9660")
#loc4673 = loc("scatter.9690")
#loc4720 = loc("reduce.9871")
#loc4745 = loc("dot.9888")
#loc4781 = loc("dot.9943")
#loc4822 = loc("scatter.10055")
#loc4829 = loc("scatter.10085")
#loc4876 = loc("reduce.10266")
#loc4901 = loc("dot.10283")
#loc4937 = loc("dot.10338")
#loc4978 = loc("scatter.10450")
#loc4985 = loc("scatter.10480")
#loc5032 = loc("reduce.10661")
#loc5057 = loc("dot.10678")
#loc5093 = loc("dot.10733")
#loc5134 = loc("scatter.10845")
#loc5141 = loc("scatter.10875")
#loc5188 = loc("reduce.11056")
#loc5213 = loc("dot.11073")
#loc5249 = loc("dot.11128")
#loc5290 = loc("scatter.11240")
#loc5297 = loc("scatter.11270")
#loc5344 = loc("reduce.11451")
#loc5369 = loc("dot.11468")
#loc5405 = loc("dot.11523")
#loc5446 = loc("scatter.11635")
#loc5453 = loc("scatter.11665")
#loc5500 = loc("reduce.11846")
#loc5525 = loc("dot.11863")
#loc5561 = loc("dot.11918")
#loc5602 = loc("scatter.12030")
#loc5609 = loc("scatter.12060")
#loc5656 = loc("reduce.12241")
#loc5681 = loc("dot.12258")
#loc5717 = loc("dot.12313")
#loc5758 = loc("scatter.12425")
#loc5765 = loc("scatter.12455")
#loc5812 = loc("reduce.12636")
#loc5837 = loc("dot.12653")
#loc5873 = loc("dot.12708")
#loc5914 = loc("scatter.12820")
#loc5921 = loc("scatter.12850")
#loc5968 = loc("reduce.13031")
#loc5993 = loc("dot.13048")
#loc6029 = loc("dot.13103")
#loc6070 = loc("scatter.13215")
#loc6077 = loc("scatter.13245")
#loc6124 = loc("reduce.13426")
#loc6149 = loc("dot.13443")
#loc6185 = loc("dot.13498")
#loc6226 = loc("scatter.13610")
#loc6233 = loc("scatter.13640")
#loc6280 = loc("reduce.13821")
#loc6305 = loc("dot.13838")
#loc6341 = loc("dot.13893")
#loc6382 = loc("scatter.14005")
#loc6389 = loc("scatter.14035")
#loc6436 = loc("reduce.14216")
#loc6461 = loc("dot.14233")
#loc6497 = loc("dot.14288")
#loc6538 = loc("scatter.14400")
#loc6545 = loc("scatter.14430")
#loc6592 = loc("reduce.14611")
#loc6617 = loc("dot.14628")
#loc6653 = loc("dot.14683")
#loc6694 = loc("scatter.14795")
#loc6701 = loc("scatter.14825")
#loc6748 = loc("reduce.15006")
#loc6773 = loc("dot.15023")
#loc6809 = loc("dot.15078")
#loc6850 = loc("scatter.15190")
#loc6857 = loc("scatter.15220")
#loc6904 = loc("reduce.15401")
#loc6929 = loc("dot.15418")
#loc6965 = loc("dot.15473")
#loc7006 = loc("scatter.15585")
#loc7013 = loc("scatter.15615")
#loc7060 = loc("reduce.15796")
#loc7085 = loc("dot.15813")
#loc7121 = loc("dot.15868")
#loc7162 = loc("scatter.15980")
#loc7169 = loc("scatter.16010")
#loc7216 = loc("reduce.16191")
#loc7241 = loc("dot.16208")
#loc7277 = loc("dot.16263")
#loc7318 = loc("scatter.16375")
#loc7325 = loc("scatter.16405")
#loc7372 = loc("reduce.16586")
#loc7397 = loc("dot.16603")
#loc7433 = loc("dot.16658")
#loc7474 = loc("scatter.16770")
#loc7481 = loc("scatter.16800")
#loc7528 = loc("reduce.16981")
#loc7553 = loc("dot.16998")
#loc7589 = loc("dot.17053")
#loc7630 = loc("scatter.17165")
#loc7637 = loc("scatter.17195")
#loc7684 = loc("reduce.17376")
#loc7709 = loc("dot.17393")
#loc7745 = loc("dot.17448")
#loc7786 = loc("scatter.17560")
#loc7793 = loc("scatter.17590")
#loc7840 = loc("reduce.17771")
#loc7865 = loc("dot.17788")
#loc7901 = loc("dot.17843")
#loc7942 = loc("scatter.17955")
#loc7949 = loc("scatter.17985")
#loc7996 = loc("reduce.18166")
#loc8021 = loc("dot.18183")
#loc8057 = loc("dot.18238")
#loc8098 = loc("scatter.18350")
#loc8105 = loc("scatter.18380")
#loc8152 = loc("reduce.18561")
#loc8177 = loc("dot.18578")
#loc8213 = loc("dot.18633")
#loc8254 = loc("scatter.18745")
#loc8261 = loc("scatter.18775")
#loc8308 = loc("reduce.18956")
#loc8333 = loc("dot.18973")
#loc8369 = loc("dot.19028")
#loc8410 = loc("scatter.19140")
#loc8417 = loc("scatter.19170")
#loc8464 = loc("reduce.19351")
#loc8489 = loc("dot.19368")
#loc8525 = loc("dot.19423")
#loc8566 = loc("scatter.19535")
#loc8573 = loc("scatter.19565")
#loc8620 = loc("reduce.19746")
#loc8645 = loc("dot.19763")
#loc8681 = loc("dot.19818")
#loc8722 = loc("scatter.19930")
#loc8729 = loc("scatter.19960")
#loc8776 = loc("reduce.20141")
#loc8801 = loc("dot.20158")
#loc8837 = loc("dot.20213")
#loc8878 = loc("scatter.20325")
#loc8885 = loc("scatter.20355")
#loc8932 = loc("reduce.20536")
#loc8957 = loc("dot.20553")
#loc8993 = loc("dot.20608")
#loc9034 = loc("scatter.20720")
#loc9041 = loc("scatter.20750")
#loc9088 = loc("reduce.20931")
#loc9113 = loc("dot.20948")
#loc9149 = loc("dot.21003")
#loc9190 = loc("scatter.21115")
#loc9197 = loc("scatter.21145")
#loc9244 = loc("reduce.21326")
#loc9269 = loc("dot.21343")
#loc9305 = loc("dot.21398")
#loc9346 = loc("scatter.21510")
#loc9353 = loc("scatter.21540")
#loc9400 = loc("reduce.21721")
#loc9425 = loc("dot.21738")
#loc9461 = loc("dot.21793")
#loc9502 = loc("scatter.21905")
#loc9509 = loc("scatter.21935")
#loc9556 = loc("reduce.22116")
#loc9581 = loc("dot.22133")
#loc9617 = loc("dot.22188")
#loc9658 = loc("scatter.22300")
#loc9665 = loc("scatter.22330")
#loc9712 = loc("reduce.22511")
#loc9737 = loc("dot.22528")
#loc9773 = loc("dot.22583")
#loc9814 = loc("scatter.22695")
#loc9821 = loc("scatter.22725")
#loc9868 = loc("reduce.22906")
#loc9893 = loc("dot.22923")
#loc9929 = loc("dot.22978")
#loc9970 = loc("scatter.23090")
#loc9977 = loc("scatter.23120")
#loc10024 = loc("reduce.23301")
#loc10049 = loc("dot.23318")
#loc10085 = loc("dot.23373")
#loc10126 = loc("scatter.23485")
#loc10133 = loc("scatter.23515")
#loc10180 = loc("reduce.23696")
#loc10205 = loc("dot.23713")
#loc10241 = loc("dot.23768")
#loc10282 = loc("scatter.23880")
#loc10289 = loc("scatter.23910")
#loc10336 = loc("reduce.24091")
#loc10361 = loc("dot.24108")
#loc10397 = loc("dot.24163")
#loc10438 = loc("scatter.24275")
#loc10445 = loc("scatter.24305")
#loc10492 = loc("reduce.24486")
#loc10517 = loc("dot.24503")
#loc10553 = loc("dot.24558")
#loc10594 = loc("scatter.24670")
#loc10601 = loc("scatter.24700")
#loc10648 = loc("reduce.24881")
#loc10673 = loc("dot.24898")
#loc10709 = loc("dot.24953")
#loc10750 = loc("scatter.25065")
#loc10757 = loc("scatter.25095")
#loc10801 = loc("reduce.25271")
#loc10826 = loc("dot.25288")
#loc10862 = loc("dot.25343")
module @SyncTensorsGraph.25387 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]> loc(#loc)
  func.func @main(
  %arg0: tensor<17xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"} loc("p0.3"),
  %arg1: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p1.11"),
  %arg2: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"} loc("p2.31"),
  %arg3: tensor<32x17xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"} loc("p3.39"),
  %arg4: tensor<151936x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p4.44"),
  %arg5: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"} loc("p5.80"),
  %arg6: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_norm_weight"} loc("p6.119"),
  %arg7: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"} loc("p7.157"),
  %arg8: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"} loc("p8.165"),
  %arg9: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"} loc("p9.187"),
  %arg10: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___self_attn_k_proj_weight"} loc("p10.200"),
  %arg11: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"} loc("p11.209"),
  %arg12: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"} loc("p12.214"),
  %arg13: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"} loc("p13.223"),
  %arg14: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"} loc("p14.281"),
  %arg15: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_norm_weight"} loc("p15.319"),
  %arg16: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"} loc("p16.442"),
  %arg17: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"} loc("p17.451"),
  %arg18: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___input_layernorm_weight"} loc("p18.497"),
  %arg19: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___self_attn_k_norm_weight"} loc("p19.536"),
  %arg20: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_4"} loc("p20.569"),
  %arg21: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___self_attn_v_proj_weight"} loc("p21.577"),
  %arg22: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_5"} loc("p22.599"),
  %arg23: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___self_attn_k_proj_weight"} loc("p23.612"),
  %arg24: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___mlp_down_proj_weight"} loc("p24.621"),
  %arg25: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___mlp_up_proj_weight"} loc("p25.626"),
  %arg26: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___self_attn_o_proj_weight"} loc("p26.635"),
  %arg27: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___self_attn_q_proj_weight"} loc("p27.676"),
  %arg28: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___self_attn_q_norm_weight"} loc("p28.714"),
  %arg29: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___post_attention_layernorm_weight"} loc("p29.837"),
  %arg30: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__1___mlp_gate_proj_weight"} loc("p30.846"),
  %arg31: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___input_layernorm_weight"} loc("p31.892"),
  %arg32: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___self_attn_k_norm_weight"} loc("p32.931"),
  %arg33: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_6"} loc("p33.964"),
  %arg34: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___self_attn_v_proj_weight"} loc("p34.972"),
  %arg35: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_7"} loc("p35.994"),
  %arg36: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___self_attn_k_proj_weight"} loc("p36.1007"),
  %arg37: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___mlp_down_proj_weight"} loc("p37.1016"),
  %arg38: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___mlp_up_proj_weight"} loc("p38.1021"),
  %arg39: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___self_attn_o_proj_weight"} loc("p39.1030"),
  %arg40: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___self_attn_q_proj_weight"} loc("p40.1071"),
  %arg41: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___self_attn_q_norm_weight"} loc("p41.1109"),
  %arg42: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___post_attention_layernorm_weight"} loc("p42.1232"),
  %arg43: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__2___mlp_gate_proj_weight"} loc("p43.1241"),
  %arg44: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___input_layernorm_weight"} loc("p44.1287"),
  %arg45: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___self_attn_k_norm_weight"} loc("p45.1326"),
  %arg46: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_8"} loc("p46.1359"),
  %arg47: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___self_attn_v_proj_weight"} loc("p47.1367"),
  %arg48: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_9"} loc("p48.1389"),
  %arg49: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___self_attn_k_proj_weight"} loc("p49.1402"),
  %arg50: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___mlp_down_proj_weight"} loc("p50.1411"),
  %arg51: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___mlp_up_proj_weight"} loc("p51.1416"),
  %arg52: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___self_attn_o_proj_weight"} loc("p52.1425"),
  %arg53: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___self_attn_q_proj_weight"} loc("p53.1466"),
  %arg54: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___self_attn_q_norm_weight"} loc("p54.1504"),
  %arg55: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___post_attention_layernorm_weight"} loc("p55.1627"),
  %arg56: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__3___mlp_gate_proj_weight"} loc("p56.1636"),
  %arg57: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___input_layernorm_weight"} loc("p57.1682"),
  %arg58: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___self_attn_k_norm_weight"} loc("p58.1721"),
  %arg59: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_10"} loc("p59.1754"),
  %arg60: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___self_attn_v_proj_weight"} loc("p60.1762"),
  %arg61: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_11"} loc("p61.1784"),
  %arg62: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___self_attn_k_proj_weight"} loc("p62.1797"),
  %arg63: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___mlp_down_proj_weight"} loc("p63.1806"),
  %arg64: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___mlp_up_proj_weight"} loc("p64.1811"),
  %arg65: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___self_attn_o_proj_weight"} loc("p65.1820"),
  %arg66: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___self_attn_q_proj_weight"} loc("p66.1861"),
  %arg67: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___self_attn_q_norm_weight"} loc("p67.1899"),
  %arg68: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___post_attention_layernorm_weight"} loc("p68.2022"),
  %arg69: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__4___mlp_gate_proj_weight"} loc("p69.2031"),
  %arg70: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___input_layernorm_weight"} loc("p70.2077"),
  %arg71: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___self_attn_k_norm_weight"} loc("p71.2116"),
  %arg72: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_12"} loc("p72.2149"),
  %arg73: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___self_attn_v_proj_weight"} loc("p73.2157"),
  %arg74: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_13"} loc("p74.2179"),
  %arg75: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___self_attn_k_proj_weight"} loc("p75.2192"),
  %arg76: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___mlp_down_proj_weight"} loc("p76.2201"),
  %arg77: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___mlp_up_proj_weight"} loc("p77.2206"),
  %arg78: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___self_attn_o_proj_weight"} loc("p78.2215"),
  %arg79: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___self_attn_q_proj_weight"} loc("p79.2256"),
  %arg80: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___self_attn_q_norm_weight"} loc("p80.2294"),
  %arg81: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___post_attention_layernorm_weight"} loc("p81.2417"),
  %arg82: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__5___mlp_gate_proj_weight"} loc("p82.2426"),
  %arg83: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___input_layernorm_weight"} loc("p83.2472"),
  %arg84: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___self_attn_k_norm_weight"} loc("p84.2511"),
  %arg85: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_14"} loc("p85.2544"),
  %arg86: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___self_attn_v_proj_weight"} loc("p86.2552"),
  %arg87: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_15"} loc("p87.2574"),
  %arg88: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___self_attn_k_proj_weight"} loc("p88.2587"),
  %arg89: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___mlp_down_proj_weight"} loc("p89.2596"),
  %arg90: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___mlp_up_proj_weight"} loc("p90.2601"),
  %arg91: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___self_attn_o_proj_weight"} loc("p91.2610"),
  %arg92: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___self_attn_q_proj_weight"} loc("p92.2651"),
  %arg93: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___self_attn_q_norm_weight"} loc("p93.2689"),
  %arg94: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___post_attention_layernorm_weight"} loc("p94.2812"),
  %arg95: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__6___mlp_gate_proj_weight"} loc("p95.2821"),
  %arg96: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___input_layernorm_weight"} loc("p96.2867"),
  %arg97: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___self_attn_k_norm_weight"} loc("p97.2906"),
  %arg98: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_16"} loc("p98.2939"),
  %arg99: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___self_attn_v_proj_weight"} loc("p99.2947"),
  %arg100: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_17"} loc("p100.2969"),
  %arg101: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___self_attn_k_proj_weight"} loc("p101.2982"),
  %arg102: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___mlp_down_proj_weight"} loc("p102.2991"),
  %arg103: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___mlp_up_proj_weight"} loc("p103.2996"),
  %arg104: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___self_attn_o_proj_weight"} loc("p104.3005"),
  %arg105: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___self_attn_q_proj_weight"} loc("p105.3046"),
  %arg106: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___self_attn_q_norm_weight"} loc("p106.3084"),
  %arg107: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___post_attention_layernorm_weight"} loc("p107.3207"),
  %arg108: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__7___mlp_gate_proj_weight"} loc("p108.3216"),
  %arg109: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___input_layernorm_weight"} loc("p109.3262"),
  %arg110: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___self_attn_k_norm_weight"} loc("p110.3301"),
  %arg111: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_18"} loc("p111.3334"),
  %arg112: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___self_attn_v_proj_weight"} loc("p112.3342"),
  %arg113: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_19"} loc("p113.3364"),
  %arg114: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___self_attn_k_proj_weight"} loc("p114.3377"),
  %arg115: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___mlp_down_proj_weight"} loc("p115.3386"),
  %arg116: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___mlp_up_proj_weight"} loc("p116.3391"),
  %arg117: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___self_attn_o_proj_weight"} loc("p117.3400"),
  %arg118: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___self_attn_q_proj_weight"} loc("p118.3441"),
  %arg119: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___self_attn_q_norm_weight"} loc("p119.3479"),
  %arg120: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___post_attention_layernorm_weight"} loc("p120.3602"),
  %arg121: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__8___mlp_gate_proj_weight"} loc("p121.3611"),
  %arg122: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___input_layernorm_weight"} loc("p122.3657"),
  %arg123: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___self_attn_k_norm_weight"} loc("p123.3696"),
  %arg124: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_20"} loc("p124.3729"),
  %arg125: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___self_attn_v_proj_weight"} loc("p125.3737"),
  %arg126: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_21"} loc("p126.3759"),
  %arg127: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___self_attn_k_proj_weight"} loc("p127.3772"),
  %arg128: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___mlp_down_proj_weight"} loc("p128.3781"),
  %arg129: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___mlp_up_proj_weight"} loc("p129.3786"),
  %arg130: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___self_attn_o_proj_weight"} loc("p130.3795"),
  %arg131: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___self_attn_q_proj_weight"} loc("p131.3836"),
  %arg132: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___self_attn_q_norm_weight"} loc("p132.3874"),
  %arg133: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___post_attention_layernorm_weight"} loc("p133.3997"),
  %arg134: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__9___mlp_gate_proj_weight"} loc("p134.4006"),
  %arg135: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___input_layernorm_weight"} loc("p135.4052"),
  %arg136: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___self_attn_k_norm_weight"} loc("p136.4091"),
  %arg137: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_22"} loc("p137.4124"),
  %arg138: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___self_attn_v_proj_weight"} loc("p138.4132"),
  %arg139: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_23"} loc("p139.4154"),
  %arg140: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___self_attn_k_proj_weight"} loc("p140.4167"),
  %arg141: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___mlp_down_proj_weight"} loc("p141.4176"),
  %arg142: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___mlp_up_proj_weight"} loc("p142.4181"),
  %arg143: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___self_attn_o_proj_weight"} loc("p143.4190"),
  %arg144: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___self_attn_q_proj_weight"} loc("p144.4231"),
  %arg145: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___self_attn_q_norm_weight"} loc("p145.4269"),
  %arg146: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___post_attention_layernorm_weight"} loc("p146.4392"),
  %arg147: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__10___mlp_gate_proj_weight"} loc("p147.4401"),
  %arg148: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___input_layernorm_weight"} loc("p148.4447"),
  %arg149: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___self_attn_k_norm_weight"} loc("p149.4486"),
  %arg150: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_24"} loc("p150.4519"),
  %arg151: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___self_attn_v_proj_weight"} loc("p151.4527"),
  %arg152: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_25"} loc("p152.4549"),
  %arg153: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___self_attn_k_proj_weight"} loc("p153.4562"),
  %arg154: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___mlp_down_proj_weight"} loc("p154.4571"),
  %arg155: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___mlp_up_proj_weight"} loc("p155.4576"),
  %arg156: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___self_attn_o_proj_weight"} loc("p156.4585"),
  %arg157: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___self_attn_q_proj_weight"} loc("p157.4626"),
  %arg158: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___self_attn_q_norm_weight"} loc("p158.4664"),
  %arg159: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___post_attention_layernorm_weight"} loc("p159.4787"),
  %arg160: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__11___mlp_gate_proj_weight"} loc("p160.4796"),
  %arg161: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___input_layernorm_weight"} loc("p161.4842"),
  %arg162: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___self_attn_k_norm_weight"} loc("p162.4881"),
  %arg163: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_26"} loc("p163.4914"),
  %arg164: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___self_attn_v_proj_weight"} loc("p164.4922"),
  %arg165: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_27"} loc("p165.4944"),
  %arg166: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___self_attn_k_proj_weight"} loc("p166.4957"),
  %arg167: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___mlp_down_proj_weight"} loc("p167.4966"),
  %arg168: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___mlp_up_proj_weight"} loc("p168.4971"),
  %arg169: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___self_attn_o_proj_weight"} loc("p169.4980"),
  %arg170: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___self_attn_q_proj_weight"} loc("p170.5021"),
  %arg171: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___self_attn_q_norm_weight"} loc("p171.5059"),
  %arg172: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___post_attention_layernorm_weight"} loc("p172.5182"),
  %arg173: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__12___mlp_gate_proj_weight"} loc("p173.5191"),
  %arg174: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___input_layernorm_weight"} loc("p174.5237"),
  %arg175: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___self_attn_k_norm_weight"} loc("p175.5276"),
  %arg176: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_28"} loc("p176.5309"),
  %arg177: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___self_attn_v_proj_weight"} loc("p177.5317"),
  %arg178: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_29"} loc("p178.5339"),
  %arg179: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___self_attn_k_proj_weight"} loc("p179.5352"),
  %arg180: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___mlp_down_proj_weight"} loc("p180.5361"),
  %arg181: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___mlp_up_proj_weight"} loc("p181.5366"),
  %arg182: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___self_attn_o_proj_weight"} loc("p182.5375"),
  %arg183: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___self_attn_q_proj_weight"} loc("p183.5416"),
  %arg184: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___self_attn_q_norm_weight"} loc("p184.5454"),
  %arg185: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___post_attention_layernorm_weight"} loc("p185.5577"),
  %arg186: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__13___mlp_gate_proj_weight"} loc("p186.5586"),
  %arg187: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___input_layernorm_weight"} loc("p187.5632"),
  %arg188: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___self_attn_k_norm_weight"} loc("p188.5671"),
  %arg189: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_30"} loc("p189.5704"),
  %arg190: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___self_attn_v_proj_weight"} loc("p190.5712"),
  %arg191: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_31"} loc("p191.5734"),
  %arg192: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___self_attn_k_proj_weight"} loc("p192.5747"),
  %arg193: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___mlp_down_proj_weight"} loc("p193.5756"),
  %arg194: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___mlp_up_proj_weight"} loc("p194.5761"),
  %arg195: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___self_attn_o_proj_weight"} loc("p195.5770"),
  %arg196: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___self_attn_q_proj_weight"} loc("p196.5811"),
  %arg197: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___self_attn_q_norm_weight"} loc("p197.5849"),
  %arg198: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___post_attention_layernorm_weight"} loc("p198.5972"),
  %arg199: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__14___mlp_gate_proj_weight"} loc("p199.5981"),
  %arg200: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___input_layernorm_weight"} loc("p200.6027"),
  %arg201: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___self_attn_k_norm_weight"} loc("p201.6066"),
  %arg202: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_32"} loc("p202.6099"),
  %arg203: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___self_attn_v_proj_weight"} loc("p203.6107"),
  %arg204: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_33"} loc("p204.6129"),
  %arg205: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___self_attn_k_proj_weight"} loc("p205.6142"),
  %arg206: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___mlp_down_proj_weight"} loc("p206.6151"),
  %arg207: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___mlp_up_proj_weight"} loc("p207.6156"),
  %arg208: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___self_attn_o_proj_weight"} loc("p208.6165"),
  %arg209: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___self_attn_q_proj_weight"} loc("p209.6206"),
  %arg210: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___self_attn_q_norm_weight"} loc("p210.6244"),
  %arg211: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___post_attention_layernorm_weight"} loc("p211.6367"),
  %arg212: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__15___mlp_gate_proj_weight"} loc("p212.6376"),
  %arg213: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___input_layernorm_weight"} loc("p213.6422"),
  %arg214: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___self_attn_k_norm_weight"} loc("p214.6461"),
  %arg215: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_34"} loc("p215.6494"),
  %arg216: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___self_attn_v_proj_weight"} loc("p216.6502"),
  %arg217: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_35"} loc("p217.6524"),
  %arg218: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___self_attn_k_proj_weight"} loc("p218.6537"),
  %arg219: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___mlp_down_proj_weight"} loc("p219.6546"),
  %arg220: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___mlp_up_proj_weight"} loc("p220.6551"),
  %arg221: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___self_attn_o_proj_weight"} loc("p221.6560"),
  %arg222: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___self_attn_q_proj_weight"} loc("p222.6601"),
  %arg223: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___self_attn_q_norm_weight"} loc("p223.6639"),
  %arg224: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___post_attention_layernorm_weight"} loc("p224.6762"),
  %arg225: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__16___mlp_gate_proj_weight"} loc("p225.6771"),
  %arg226: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___input_layernorm_weight"} loc("p226.6817"),
  %arg227: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___self_attn_k_norm_weight"} loc("p227.6856"),
  %arg228: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_36"} loc("p228.6889"),
  %arg229: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___self_attn_v_proj_weight"} loc("p229.6897"),
  %arg230: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_37"} loc("p230.6919"),
  %arg231: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___self_attn_k_proj_weight"} loc("p231.6932"),
  %arg232: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___mlp_down_proj_weight"} loc("p232.6941"),
  %arg233: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___mlp_up_proj_weight"} loc("p233.6946"),
  %arg234: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___self_attn_o_proj_weight"} loc("p234.6955"),
  %arg235: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___self_attn_q_proj_weight"} loc("p235.6996"),
  %arg236: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___self_attn_q_norm_weight"} loc("p236.7034"),
  %arg237: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___post_attention_layernorm_weight"} loc("p237.7157"),
  %arg238: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__17___mlp_gate_proj_weight"} loc("p238.7166"),
  %arg239: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___input_layernorm_weight"} loc("p239.7212"),
  %arg240: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___self_attn_k_norm_weight"} loc("p240.7251"),
  %arg241: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_38"} loc("p241.7284"),
  %arg242: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___self_attn_v_proj_weight"} loc("p242.7292"),
  %arg243: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_39"} loc("p243.7314"),
  %arg244: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___self_attn_k_proj_weight"} loc("p244.7327"),
  %arg245: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___mlp_down_proj_weight"} loc("p245.7336"),
  %arg246: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___mlp_up_proj_weight"} loc("p246.7341"),
  %arg247: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___self_attn_o_proj_weight"} loc("p247.7350"),
  %arg248: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___self_attn_q_proj_weight"} loc("p248.7391"),
  %arg249: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___self_attn_q_norm_weight"} loc("p249.7429"),
  %arg250: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___post_attention_layernorm_weight"} loc("p250.7552"),
  %arg251: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__18___mlp_gate_proj_weight"} loc("p251.7561"),
  %arg252: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___input_layernorm_weight"} loc("p252.7607"),
  %arg253: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___self_attn_k_norm_weight"} loc("p253.7646"),
  %arg254: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_40"} loc("p254.7679"),
  %arg255: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___self_attn_v_proj_weight"} loc("p255.7687"),
  %arg256: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_41"} loc("p256.7709"),
  %arg257: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___self_attn_k_proj_weight"} loc("p257.7722"),
  %arg258: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___mlp_down_proj_weight"} loc("p258.7731"),
  %arg259: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___mlp_up_proj_weight"} loc("p259.7736"),
  %arg260: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___self_attn_o_proj_weight"} loc("p260.7745"),
  %arg261: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___self_attn_q_proj_weight"} loc("p261.7786"),
  %arg262: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___self_attn_q_norm_weight"} loc("p262.7824"),
  %arg263: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___post_attention_layernorm_weight"} loc("p263.7947"),
  %arg264: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__19___mlp_gate_proj_weight"} loc("p264.7956"),
  %arg265: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___input_layernorm_weight"} loc("p265.8002"),
  %arg266: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___self_attn_k_norm_weight"} loc("p266.8041"),
  %arg267: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_42"} loc("p267.8074"),
  %arg268: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___self_attn_v_proj_weight"} loc("p268.8082"),
  %arg269: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_43"} loc("p269.8104"),
  %arg270: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___self_attn_k_proj_weight"} loc("p270.8117"),
  %arg271: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___mlp_down_proj_weight"} loc("p271.8126"),
  %arg272: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___mlp_up_proj_weight"} loc("p272.8131"),
  %arg273: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___self_attn_o_proj_weight"} loc("p273.8140"),
  %arg274: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___self_attn_q_proj_weight"} loc("p274.8181"),
  %arg275: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___self_attn_q_norm_weight"} loc("p275.8219"),
  %arg276: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___post_attention_layernorm_weight"} loc("p276.8342"),
  %arg277: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__20___mlp_gate_proj_weight"} loc("p277.8351"),
  %arg278: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___input_layernorm_weight"} loc("p278.8397"),
  %arg279: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___self_attn_k_norm_weight"} loc("p279.8436"),
  %arg280: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_44"} loc("p280.8469"),
  %arg281: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___self_attn_v_proj_weight"} loc("p281.8477"),
  %arg282: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_45"} loc("p282.8499"),
  %arg283: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___self_attn_k_proj_weight"} loc("p283.8512"),
  %arg284: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___mlp_down_proj_weight"} loc("p284.8521"),
  %arg285: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___mlp_up_proj_weight"} loc("p285.8526"),
  %arg286: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___self_attn_o_proj_weight"} loc("p286.8535"),
  %arg287: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___self_attn_q_proj_weight"} loc("p287.8576"),
  %arg288: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___self_attn_q_norm_weight"} loc("p288.8614"),
  %arg289: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___post_attention_layernorm_weight"} loc("p289.8737"),
  %arg290: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__21___mlp_gate_proj_weight"} loc("p290.8746"),
  %arg291: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___input_layernorm_weight"} loc("p291.8792"),
  %arg292: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___self_attn_k_norm_weight"} loc("p292.8831"),
  %arg293: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_46"} loc("p293.8864"),
  %arg294: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___self_attn_v_proj_weight"} loc("p294.8872"),
  %arg295: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_47"} loc("p295.8894"),
  %arg296: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___self_attn_k_proj_weight"} loc("p296.8907"),
  %arg297: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___mlp_down_proj_weight"} loc("p297.8916"),
  %arg298: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___mlp_up_proj_weight"} loc("p298.8921"),
  %arg299: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___self_attn_o_proj_weight"} loc("p299.8930"),
  %arg300: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___self_attn_q_proj_weight"} loc("p300.8971"),
  %arg301: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___self_attn_q_norm_weight"} loc("p301.9009"),
  %arg302: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___post_attention_layernorm_weight"} loc("p302.9132"),
  %arg303: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__22___mlp_gate_proj_weight"} loc("p303.9141"),
  %arg304: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___input_layernorm_weight"} loc("p304.9187"),
  %arg305: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___self_attn_k_norm_weight"} loc("p305.9226"),
  %arg306: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_48"} loc("p306.9259"),
  %arg307: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___self_attn_v_proj_weight"} loc("p307.9267"),
  %arg308: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_49"} loc("p308.9289"),
  %arg309: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___self_attn_k_proj_weight"} loc("p309.9302"),
  %arg310: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___mlp_down_proj_weight"} loc("p310.9311"),
  %arg311: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___mlp_up_proj_weight"} loc("p311.9316"),
  %arg312: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___self_attn_o_proj_weight"} loc("p312.9325"),
  %arg313: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___self_attn_q_proj_weight"} loc("p313.9366"),
  %arg314: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___self_attn_q_norm_weight"} loc("p314.9404"),
  %arg315: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___post_attention_layernorm_weight"} loc("p315.9527"),
  %arg316: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__23___mlp_gate_proj_weight"} loc("p316.9536"),
  %arg317: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___input_layernorm_weight"} loc("p317.9582"),
  %arg318: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___self_attn_k_norm_weight"} loc("p318.9621"),
  %arg319: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_50"} loc("p319.9654"),
  %arg320: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___self_attn_v_proj_weight"} loc("p320.9662"),
  %arg321: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_51"} loc("p321.9684"),
  %arg322: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___self_attn_k_proj_weight"} loc("p322.9697"),
  %arg323: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___mlp_down_proj_weight"} loc("p323.9706"),
  %arg324: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___mlp_up_proj_weight"} loc("p324.9711"),
  %arg325: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___self_attn_o_proj_weight"} loc("p325.9720"),
  %arg326: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___self_attn_q_proj_weight"} loc("p326.9761"),
  %arg327: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___self_attn_q_norm_weight"} loc("p327.9799"),
  %arg328: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___post_attention_layernorm_weight"} loc("p328.9922"),
  %arg329: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__24___mlp_gate_proj_weight"} loc("p329.9931"),
  %arg330: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___input_layernorm_weight"} loc("p330.9977"),
  %arg331: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___self_attn_k_norm_weight"} loc("p331.10016"),
  %arg332: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_52"} loc("p332.10049"),
  %arg333: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___self_attn_v_proj_weight"} loc("p333.10057"),
  %arg334: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_53"} loc("p334.10079"),
  %arg335: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___self_attn_k_proj_weight"} loc("p335.10092"),
  %arg336: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___mlp_down_proj_weight"} loc("p336.10101"),
  %arg337: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___mlp_up_proj_weight"} loc("p337.10106"),
  %arg338: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___self_attn_o_proj_weight"} loc("p338.10115"),
  %arg339: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___self_attn_q_proj_weight"} loc("p339.10156"),
  %arg340: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___self_attn_q_norm_weight"} loc("p340.10194"),
  %arg341: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___post_attention_layernorm_weight"} loc("p341.10317"),
  %arg342: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__25___mlp_gate_proj_weight"} loc("p342.10326"),
  %arg343: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___input_layernorm_weight"} loc("p343.10372"),
  %arg344: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___self_attn_k_norm_weight"} loc("p344.10411"),
  %arg345: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_54"} loc("p345.10444"),
  %arg346: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___self_attn_v_proj_weight"} loc("p346.10452"),
  %arg347: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_55"} loc("p347.10474"),
  %arg348: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___self_attn_k_proj_weight"} loc("p348.10487"),
  %arg349: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___mlp_down_proj_weight"} loc("p349.10496"),
  %arg350: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___mlp_up_proj_weight"} loc("p350.10501"),
  %arg351: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___self_attn_o_proj_weight"} loc("p351.10510"),
  %arg352: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___self_attn_q_proj_weight"} loc("p352.10551"),
  %arg353: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___self_attn_q_norm_weight"} loc("p353.10589"),
  %arg354: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___post_attention_layernorm_weight"} loc("p354.10712"),
  %arg355: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__26___mlp_gate_proj_weight"} loc("p355.10721"),
  %arg356: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___input_layernorm_weight"} loc("p356.10767"),
  %arg357: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___self_attn_k_norm_weight"} loc("p357.10806"),
  %arg358: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_56"} loc("p358.10839"),
  %arg359: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___self_attn_v_proj_weight"} loc("p359.10847"),
  %arg360: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_57"} loc("p360.10869"),
  %arg361: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___self_attn_k_proj_weight"} loc("p361.10882"),
  %arg362: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___mlp_down_proj_weight"} loc("p362.10891"),
  %arg363: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___mlp_up_proj_weight"} loc("p363.10896"),
  %arg364: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___self_attn_o_proj_weight"} loc("p364.10905"),
  %arg365: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___self_attn_q_proj_weight"} loc("p365.10946"),
  %arg366: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___self_attn_q_norm_weight"} loc("p366.10984"),
  %arg367: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___post_attention_layernorm_weight"} loc("p367.11107"),
  %arg368: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__27___mlp_gate_proj_weight"} loc("p368.11116"),
  %arg369: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___input_layernorm_weight"} loc("p369.11162"),
  %arg370: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___self_attn_k_norm_weight"} loc("p370.11201"),
  %arg371: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_58"} loc("p371.11234"),
  %arg372: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___self_attn_v_proj_weight"} loc("p372.11242"),
  %arg373: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_59"} loc("p373.11264"),
  %arg374: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___self_attn_k_proj_weight"} loc("p374.11277"),
  %arg375: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___mlp_down_proj_weight"} loc("p375.11286"),
  %arg376: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___mlp_up_proj_weight"} loc("p376.11291"),
  %arg377: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___self_attn_o_proj_weight"} loc("p377.11300"),
  %arg378: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___self_attn_q_proj_weight"} loc("p378.11341"),
  %arg379: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___self_attn_q_norm_weight"} loc("p379.11379"),
  %arg380: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___post_attention_layernorm_weight"} loc("p380.11502"),
  %arg381: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__28___mlp_gate_proj_weight"} loc("p381.11511"),
  %arg382: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___input_layernorm_weight"} loc("p382.11557"),
  %arg383: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___self_attn_k_norm_weight"} loc("p383.11596"),
  %arg384: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_60"} loc("p384.11629"),
  %arg385: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___self_attn_v_proj_weight"} loc("p385.11637"),
  %arg386: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_61"} loc("p386.11659"),
  %arg387: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___self_attn_k_proj_weight"} loc("p387.11672"),
  %arg388: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___mlp_down_proj_weight"} loc("p388.11681"),
  %arg389: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___mlp_up_proj_weight"} loc("p389.11686"),
  %arg390: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___self_attn_o_proj_weight"} loc("p390.11695"),
  %arg391: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___self_attn_q_proj_weight"} loc("p391.11736"),
  %arg392: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___self_attn_q_norm_weight"} loc("p392.11774"),
  %arg393: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___post_attention_layernorm_weight"} loc("p393.11897"),
  %arg394: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__29___mlp_gate_proj_weight"} loc("p394.11906"),
  %arg395: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___input_layernorm_weight"} loc("p395.11952"),
  %arg396: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___self_attn_k_norm_weight"} loc("p396.11991"),
  %arg397: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_62"} loc("p397.12024"),
  %arg398: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___self_attn_v_proj_weight"} loc("p398.12032"),
  %arg399: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_63"} loc("p399.12054"),
  %arg400: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___self_attn_k_proj_weight"} loc("p400.12067"),
  %arg401: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___mlp_down_proj_weight"} loc("p401.12076"),
  %arg402: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___mlp_up_proj_weight"} loc("p402.12081"),
  %arg403: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___self_attn_o_proj_weight"} loc("p403.12090"),
  %arg404: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___self_attn_q_proj_weight"} loc("p404.12131"),
  %arg405: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___self_attn_q_norm_weight"} loc("p405.12169"),
  %arg406: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___post_attention_layernorm_weight"} loc("p406.12292"),
  %arg407: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__30___mlp_gate_proj_weight"} loc("p407.12301"),
  %arg408: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___input_layernorm_weight"} loc("p408.12347"),
  %arg409: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___self_attn_k_norm_weight"} loc("p409.12386"),
  %arg410: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_64"} loc("p410.12419"),
  %arg411: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___self_attn_v_proj_weight"} loc("p411.12427"),
  %arg412: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_65"} loc("p412.12449"),
  %arg413: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___self_attn_k_proj_weight"} loc("p413.12462"),
  %arg414: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___mlp_down_proj_weight"} loc("p414.12471"),
  %arg415: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___mlp_up_proj_weight"} loc("p415.12476"),
  %arg416: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___self_attn_o_proj_weight"} loc("p416.12485"),
  %arg417: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___self_attn_q_proj_weight"} loc("p417.12526"),
  %arg418: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___self_attn_q_norm_weight"} loc("p418.12564"),
  %arg419: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___post_attention_layernorm_weight"} loc("p419.12687"),
  %arg420: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__31___mlp_gate_proj_weight"} loc("p420.12696"),
  %arg421: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___input_layernorm_weight"} loc("p421.12742"),
  %arg422: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___self_attn_k_norm_weight"} loc("p422.12781"),
  %arg423: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_66"} loc("p423.12814"),
  %arg424: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___self_attn_v_proj_weight"} loc("p424.12822"),
  %arg425: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_67"} loc("p425.12844"),
  %arg426: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___self_attn_k_proj_weight"} loc("p426.12857"),
  %arg427: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___mlp_down_proj_weight"} loc("p427.12866"),
  %arg428: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___mlp_up_proj_weight"} loc("p428.12871"),
  %arg429: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___self_attn_o_proj_weight"} loc("p429.12880"),
  %arg430: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___self_attn_q_proj_weight"} loc("p430.12921"),
  %arg431: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___self_attn_q_norm_weight"} loc("p431.12959"),
  %arg432: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___post_attention_layernorm_weight"} loc("p432.13082"),
  %arg433: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__32___mlp_gate_proj_weight"} loc("p433.13091"),
  %arg434: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___input_layernorm_weight"} loc("p434.13137"),
  %arg435: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___self_attn_k_norm_weight"} loc("p435.13176"),
  %arg436: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_68"} loc("p436.13209"),
  %arg437: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___self_attn_v_proj_weight"} loc("p437.13217"),
  %arg438: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_69"} loc("p438.13239"),
  %arg439: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___self_attn_k_proj_weight"} loc("p439.13252"),
  %arg440: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___mlp_down_proj_weight"} loc("p440.13261"),
  %arg441: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___mlp_up_proj_weight"} loc("p441.13266"),
  %arg442: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___self_attn_o_proj_weight"} loc("p442.13275"),
  %arg443: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___self_attn_q_proj_weight"} loc("p443.13316"),
  %arg444: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___self_attn_q_norm_weight"} loc("p444.13354"),
  %arg445: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___post_attention_layernorm_weight"} loc("p445.13477"),
  %arg446: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__33___mlp_gate_proj_weight"} loc("p446.13486"),
  %arg447: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___input_layernorm_weight"} loc("p447.13532"),
  %arg448: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___self_attn_k_norm_weight"} loc("p448.13571"),
  %arg449: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_70"} loc("p449.13604"),
  %arg450: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___self_attn_v_proj_weight"} loc("p450.13612"),
  %arg451: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_71"} loc("p451.13634"),
  %arg452: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___self_attn_k_proj_weight"} loc("p452.13647"),
  %arg453: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___mlp_down_proj_weight"} loc("p453.13656"),
  %arg454: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___mlp_up_proj_weight"} loc("p454.13661"),
  %arg455: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___self_attn_o_proj_weight"} loc("p455.13670"),
  %arg456: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___self_attn_q_proj_weight"} loc("p456.13711"),
  %arg457: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___self_attn_q_norm_weight"} loc("p457.13749"),
  %arg458: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___post_attention_layernorm_weight"} loc("p458.13872"),
  %arg459: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__34___mlp_gate_proj_weight"} loc("p459.13881"),
  %arg460: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___input_layernorm_weight"} loc("p460.13927"),
  %arg461: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___self_attn_k_norm_weight"} loc("p461.13966"),
  %arg462: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_72"} loc("p462.13999"),
  %arg463: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___self_attn_v_proj_weight"} loc("p463.14007"),
  %arg464: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_73"} loc("p464.14029"),
  %arg465: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___self_attn_k_proj_weight"} loc("p465.14042"),
  %arg466: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___mlp_down_proj_weight"} loc("p466.14051"),
  %arg467: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___mlp_up_proj_weight"} loc("p467.14056"),
  %arg468: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___self_attn_o_proj_weight"} loc("p468.14065"),
  %arg469: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___self_attn_q_proj_weight"} loc("p469.14106"),
  %arg470: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___self_attn_q_norm_weight"} loc("p470.14144"),
  %arg471: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___post_attention_layernorm_weight"} loc("p471.14267"),
  %arg472: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__35___mlp_gate_proj_weight"} loc("p472.14276"),
  %arg473: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___input_layernorm_weight"} loc("p473.14322"),
  %arg474: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___self_attn_k_norm_weight"} loc("p474.14361"),
  %arg475: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_74"} loc("p475.14394"),
  %arg476: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___self_attn_v_proj_weight"} loc("p476.14402"),
  %arg477: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_75"} loc("p477.14424"),
  %arg478: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___self_attn_k_proj_weight"} loc("p478.14437"),
  %arg479: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___mlp_down_proj_weight"} loc("p479.14446"),
  %arg480: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___mlp_up_proj_weight"} loc("p480.14451"),
  %arg481: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___self_attn_o_proj_weight"} loc("p481.14460"),
  %arg482: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___self_attn_q_proj_weight"} loc("p482.14501"),
  %arg483: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___self_attn_q_norm_weight"} loc("p483.14539"),
  %arg484: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___post_attention_layernorm_weight"} loc("p484.14662"),
  %arg485: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__36___mlp_gate_proj_weight"} loc("p485.14671"),
  %arg486: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___input_layernorm_weight"} loc("p486.14717"),
  %arg487: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___self_attn_k_norm_weight"} loc("p487.14756"),
  %arg488: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_76"} loc("p488.14789"),
  %arg489: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___self_attn_v_proj_weight"} loc("p489.14797"),
  %arg490: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_77"} loc("p490.14819"),
  %arg491: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___self_attn_k_proj_weight"} loc("p491.14832"),
  %arg492: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___mlp_down_proj_weight"} loc("p492.14841"),
  %arg493: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___mlp_up_proj_weight"} loc("p493.14846"),
  %arg494: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___self_attn_o_proj_weight"} loc("p494.14855"),
  %arg495: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___self_attn_q_proj_weight"} loc("p495.14896"),
  %arg496: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___self_attn_q_norm_weight"} loc("p496.14934"),
  %arg497: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___post_attention_layernorm_weight"} loc("p497.15057"),
  %arg498: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__37___mlp_gate_proj_weight"} loc("p498.15066"),
  %arg499: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___input_layernorm_weight"} loc("p499.15112"),
  %arg500: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___self_attn_k_norm_weight"} loc("p500.15151"),
  %arg501: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_78"} loc("p501.15184"),
  %arg502: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___self_attn_v_proj_weight"} loc("p502.15192"),
  %arg503: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_79"} loc("p503.15214"),
  %arg504: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___self_attn_k_proj_weight"} loc("p504.15227"),
  %arg505: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___mlp_down_proj_weight"} loc("p505.15236"),
  %arg506: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___mlp_up_proj_weight"} loc("p506.15241"),
  %arg507: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___self_attn_o_proj_weight"} loc("p507.15250"),
  %arg508: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___self_attn_q_proj_weight"} loc("p508.15291"),
  %arg509: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___self_attn_q_norm_weight"} loc("p509.15329"),
  %arg510: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___post_attention_layernorm_weight"} loc("p510.15452"),
  %arg511: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__38___mlp_gate_proj_weight"} loc("p511.15461"),
  %arg512: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___input_layernorm_weight"} loc("p512.15507"),
  %arg513: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___self_attn_k_norm_weight"} loc("p513.15546"),
  %arg514: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_80"} loc("p514.15579"),
  %arg515: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___self_attn_v_proj_weight"} loc("p515.15587"),
  %arg516: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_81"} loc("p516.15609"),
  %arg517: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___self_attn_k_proj_weight"} loc("p517.15622"),
  %arg518: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___mlp_down_proj_weight"} loc("p518.15631"),
  %arg519: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___mlp_up_proj_weight"} loc("p519.15636"),
  %arg520: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___self_attn_o_proj_weight"} loc("p520.15645"),
  %arg521: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___self_attn_q_proj_weight"} loc("p521.15686"),
  %arg522: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___self_attn_q_norm_weight"} loc("p522.15724"),
  %arg523: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___post_attention_layernorm_weight"} loc("p523.15847"),
  %arg524: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__39___mlp_gate_proj_weight"} loc("p524.15856"),
  %arg525: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___input_layernorm_weight"} loc("p525.15902"),
  %arg526: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___self_attn_k_norm_weight"} loc("p526.15941"),
  %arg527: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_82"} loc("p527.15974"),
  %arg528: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___self_attn_v_proj_weight"} loc("p528.15982"),
  %arg529: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_83"} loc("p529.16004"),
  %arg530: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___self_attn_k_proj_weight"} loc("p530.16017"),
  %arg531: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___mlp_down_proj_weight"} loc("p531.16026"),
  %arg532: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___mlp_up_proj_weight"} loc("p532.16031"),
  %arg533: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___self_attn_o_proj_weight"} loc("p533.16040"),
  %arg534: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___self_attn_q_proj_weight"} loc("p534.16081"),
  %arg535: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___self_attn_q_norm_weight"} loc("p535.16119"),
  %arg536: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___post_attention_layernorm_weight"} loc("p536.16242"),
  %arg537: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__40___mlp_gate_proj_weight"} loc("p537.16251"),
  %arg538: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___input_layernorm_weight"} loc("p538.16297"),
  %arg539: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___self_attn_k_norm_weight"} loc("p539.16336"),
  %arg540: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_84"} loc("p540.16369"),
  %arg541: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___self_attn_v_proj_weight"} loc("p541.16377"),
  %arg542: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_85"} loc("p542.16399"),
  %arg543: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___self_attn_k_proj_weight"} loc("p543.16412"),
  %arg544: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___mlp_down_proj_weight"} loc("p544.16421"),
  %arg545: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___mlp_up_proj_weight"} loc("p545.16426"),
  %arg546: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___self_attn_o_proj_weight"} loc("p546.16435"),
  %arg547: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___self_attn_q_proj_weight"} loc("p547.16476"),
  %arg548: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___self_attn_q_norm_weight"} loc("p548.16514"),
  %arg549: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___post_attention_layernorm_weight"} loc("p549.16637"),
  %arg550: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__41___mlp_gate_proj_weight"} loc("p550.16646"),
  %arg551: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___input_layernorm_weight"} loc("p551.16692"),
  %arg552: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___self_attn_k_norm_weight"} loc("p552.16731"),
  %arg553: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_86"} loc("p553.16764"),
  %arg554: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___self_attn_v_proj_weight"} loc("p554.16772"),
  %arg555: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_87"} loc("p555.16794"),
  %arg556: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___self_attn_k_proj_weight"} loc("p556.16807"),
  %arg557: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___mlp_down_proj_weight"} loc("p557.16816"),
  %arg558: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___mlp_up_proj_weight"} loc("p558.16821"),
  %arg559: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___self_attn_o_proj_weight"} loc("p559.16830"),
  %arg560: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___self_attn_q_proj_weight"} loc("p560.16871"),
  %arg561: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___self_attn_q_norm_weight"} loc("p561.16909"),
  %arg562: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___post_attention_layernorm_weight"} loc("p562.17032"),
  %arg563: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__42___mlp_gate_proj_weight"} loc("p563.17041"),
  %arg564: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___input_layernorm_weight"} loc("p564.17087"),
  %arg565: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___self_attn_k_norm_weight"} loc("p565.17126"),
  %arg566: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_88"} loc("p566.17159"),
  %arg567: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___self_attn_v_proj_weight"} loc("p567.17167"),
  %arg568: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_89"} loc("p568.17189"),
  %arg569: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___self_attn_k_proj_weight"} loc("p569.17202"),
  %arg570: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___mlp_down_proj_weight"} loc("p570.17211"),
  %arg571: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___mlp_up_proj_weight"} loc("p571.17216"),
  %arg572: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___self_attn_o_proj_weight"} loc("p572.17225"),
  %arg573: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___self_attn_q_proj_weight"} loc("p573.17266"),
  %arg574: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___self_attn_q_norm_weight"} loc("p574.17304"),
  %arg575: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___post_attention_layernorm_weight"} loc("p575.17427"),
  %arg576: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__43___mlp_gate_proj_weight"} loc("p576.17436"),
  %arg577: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___input_layernorm_weight"} loc("p577.17482"),
  %arg578: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___self_attn_k_norm_weight"} loc("p578.17521"),
  %arg579: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_90"} loc("p579.17554"),
  %arg580: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___self_attn_v_proj_weight"} loc("p580.17562"),
  %arg581: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_91"} loc("p581.17584"),
  %arg582: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___self_attn_k_proj_weight"} loc("p582.17597"),
  %arg583: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___mlp_down_proj_weight"} loc("p583.17606"),
  %arg584: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___mlp_up_proj_weight"} loc("p584.17611"),
  %arg585: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___self_attn_o_proj_weight"} loc("p585.17620"),
  %arg586: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___self_attn_q_proj_weight"} loc("p586.17661"),
  %arg587: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___self_attn_q_norm_weight"} loc("p587.17699"),
  %arg588: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___post_attention_layernorm_weight"} loc("p588.17822"),
  %arg589: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__44___mlp_gate_proj_weight"} loc("p589.17831"),
  %arg590: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___input_layernorm_weight"} loc("p590.17877"),
  %arg591: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___self_attn_k_norm_weight"} loc("p591.17916"),
  %arg592: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_92"} loc("p592.17949"),
  %arg593: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___self_attn_v_proj_weight"} loc("p593.17957"),
  %arg594: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_93"} loc("p594.17979"),
  %arg595: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___self_attn_k_proj_weight"} loc("p595.17992"),
  %arg596: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___mlp_down_proj_weight"} loc("p596.18001"),
  %arg597: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___mlp_up_proj_weight"} loc("p597.18006"),
  %arg598: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___self_attn_o_proj_weight"} loc("p598.18015"),
  %arg599: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___self_attn_q_proj_weight"} loc("p599.18056"),
  %arg600: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___self_attn_q_norm_weight"} loc("p600.18094"),
  %arg601: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___post_attention_layernorm_weight"} loc("p601.18217"),
  %arg602: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__45___mlp_gate_proj_weight"} loc("p602.18226"),
  %arg603: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___input_layernorm_weight"} loc("p603.18272"),
  %arg604: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___self_attn_k_norm_weight"} loc("p604.18311"),
  %arg605: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_94"} loc("p605.18344"),
  %arg606: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___self_attn_v_proj_weight"} loc("p606.18352"),
  %arg607: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_95"} loc("p607.18374"),
  %arg608: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___self_attn_k_proj_weight"} loc("p608.18387"),
  %arg609: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___mlp_down_proj_weight"} loc("p609.18396"),
  %arg610: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___mlp_up_proj_weight"} loc("p610.18401"),
  %arg611: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___self_attn_o_proj_weight"} loc("p611.18410"),
  %arg612: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___self_attn_q_proj_weight"} loc("p612.18451"),
  %arg613: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___self_attn_q_norm_weight"} loc("p613.18489"),
  %arg614: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___post_attention_layernorm_weight"} loc("p614.18612"),
  %arg615: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__46___mlp_gate_proj_weight"} loc("p615.18621"),
  %arg616: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___input_layernorm_weight"} loc("p616.18667"),
  %arg617: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___self_attn_k_norm_weight"} loc("p617.18706"),
  %arg618: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_96"} loc("p618.18739"),
  %arg619: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___self_attn_v_proj_weight"} loc("p619.18747"),
  %arg620: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_97"} loc("p620.18769"),
  %arg621: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___self_attn_k_proj_weight"} loc("p621.18782"),
  %arg622: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___mlp_down_proj_weight"} loc("p622.18791"),
  %arg623: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___mlp_up_proj_weight"} loc("p623.18796"),
  %arg624: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___self_attn_o_proj_weight"} loc("p624.18805"),
  %arg625: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___self_attn_q_proj_weight"} loc("p625.18846"),
  %arg626: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___self_attn_q_norm_weight"} loc("p626.18884"),
  %arg627: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___post_attention_layernorm_weight"} loc("p627.19007"),
  %arg628: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__47___mlp_gate_proj_weight"} loc("p628.19016"),
  %arg629: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___input_layernorm_weight"} loc("p629.19062"),
  %arg630: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___self_attn_k_norm_weight"} loc("p630.19101"),
  %arg631: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_98"} loc("p631.19134"),
  %arg632: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___self_attn_v_proj_weight"} loc("p632.19142"),
  %arg633: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_99"} loc("p633.19164"),
  %arg634: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___self_attn_k_proj_weight"} loc("p634.19177"),
  %arg635: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___mlp_down_proj_weight"} loc("p635.19186"),
  %arg636: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___mlp_up_proj_weight"} loc("p636.19191"),
  %arg637: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___self_attn_o_proj_weight"} loc("p637.19200"),
  %arg638: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___self_attn_q_proj_weight"} loc("p638.19241"),
  %arg639: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___self_attn_q_norm_weight"} loc("p639.19279"),
  %arg640: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___post_attention_layernorm_weight"} loc("p640.19402"),
  %arg641: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__48___mlp_gate_proj_weight"} loc("p641.19411"),
  %arg642: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___input_layernorm_weight"} loc("p642.19457"),
  %arg643: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___self_attn_k_norm_weight"} loc("p643.19496"),
  %arg644: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_100"} loc("p644.19529"),
  %arg645: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___self_attn_v_proj_weight"} loc("p645.19537"),
  %arg646: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_101"} loc("p646.19559"),
  %arg647: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___self_attn_k_proj_weight"} loc("p647.19572"),
  %arg648: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___mlp_down_proj_weight"} loc("p648.19581"),
  %arg649: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___mlp_up_proj_weight"} loc("p649.19586"),
  %arg650: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___self_attn_o_proj_weight"} loc("p650.19595"),
  %arg651: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___self_attn_q_proj_weight"} loc("p651.19636"),
  %arg652: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___self_attn_q_norm_weight"} loc("p652.19674"),
  %arg653: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___post_attention_layernorm_weight"} loc("p653.19797"),
  %arg654: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__49___mlp_gate_proj_weight"} loc("p654.19806"),
  %arg655: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___input_layernorm_weight"} loc("p655.19852"),
  %arg656: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___self_attn_k_norm_weight"} loc("p656.19891"),
  %arg657: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_102"} loc("p657.19924"),
  %arg658: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___self_attn_v_proj_weight"} loc("p658.19932"),
  %arg659: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_103"} loc("p659.19954"),
  %arg660: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___self_attn_k_proj_weight"} loc("p660.19967"),
  %arg661: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___mlp_down_proj_weight"} loc("p661.19976"),
  %arg662: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___mlp_up_proj_weight"} loc("p662.19981"),
  %arg663: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___self_attn_o_proj_weight"} loc("p663.19990"),
  %arg664: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___self_attn_q_proj_weight"} loc("p664.20031"),
  %arg665: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___self_attn_q_norm_weight"} loc("p665.20069"),
  %arg666: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___post_attention_layernorm_weight"} loc("p666.20192"),
  %arg667: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__50___mlp_gate_proj_weight"} loc("p667.20201"),
  %arg668: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___input_layernorm_weight"} loc("p668.20247"),
  %arg669: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___self_attn_k_norm_weight"} loc("p669.20286"),
  %arg670: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_104"} loc("p670.20319"),
  %arg671: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___self_attn_v_proj_weight"} loc("p671.20327"),
  %arg672: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_105"} loc("p672.20349"),
  %arg673: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___self_attn_k_proj_weight"} loc("p673.20362"),
  %arg674: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___mlp_down_proj_weight"} loc("p674.20371"),
  %arg675: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___mlp_up_proj_weight"} loc("p675.20376"),
  %arg676: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___self_attn_o_proj_weight"} loc("p676.20385"),
  %arg677: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___self_attn_q_proj_weight"} loc("p677.20426"),
  %arg678: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___self_attn_q_norm_weight"} loc("p678.20464"),
  %arg679: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___post_attention_layernorm_weight"} loc("p679.20587"),
  %arg680: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__51___mlp_gate_proj_weight"} loc("p680.20596"),
  %arg681: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___input_layernorm_weight"} loc("p681.20642"),
  %arg682: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___self_attn_k_norm_weight"} loc("p682.20681"),
  %arg683: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_106"} loc("p683.20714"),
  %arg684: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___self_attn_v_proj_weight"} loc("p684.20722"),
  %arg685: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_107"} loc("p685.20744"),
  %arg686: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___self_attn_k_proj_weight"} loc("p686.20757"),
  %arg687: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___mlp_down_proj_weight"} loc("p687.20766"),
  %arg688: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___mlp_up_proj_weight"} loc("p688.20771"),
  %arg689: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___self_attn_o_proj_weight"} loc("p689.20780"),
  %arg690: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___self_attn_q_proj_weight"} loc("p690.20821"),
  %arg691: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___self_attn_q_norm_weight"} loc("p691.20859"),
  %arg692: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___post_attention_layernorm_weight"} loc("p692.20982"),
  %arg693: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__52___mlp_gate_proj_weight"} loc("p693.20991"),
  %arg694: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___input_layernorm_weight"} loc("p694.21037"),
  %arg695: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___self_attn_k_norm_weight"} loc("p695.21076"),
  %arg696: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_108"} loc("p696.21109"),
  %arg697: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___self_attn_v_proj_weight"} loc("p697.21117"),
  %arg698: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_109"} loc("p698.21139"),
  %arg699: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___self_attn_k_proj_weight"} loc("p699.21152"),
  %arg700: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___mlp_down_proj_weight"} loc("p700.21161"),
  %arg701: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___mlp_up_proj_weight"} loc("p701.21166"),
  %arg702: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___self_attn_o_proj_weight"} loc("p702.21175"),
  %arg703: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___self_attn_q_proj_weight"} loc("p703.21216"),
  %arg704: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___self_attn_q_norm_weight"} loc("p704.21254"),
  %arg705: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___post_attention_layernorm_weight"} loc("p705.21377"),
  %arg706: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__53___mlp_gate_proj_weight"} loc("p706.21386"),
  %arg707: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___input_layernorm_weight"} loc("p707.21432"),
  %arg708: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___self_attn_k_norm_weight"} loc("p708.21471"),
  %arg709: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_110"} loc("p709.21504"),
  %arg710: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___self_attn_v_proj_weight"} loc("p710.21512"),
  %arg711: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_111"} loc("p711.21534"),
  %arg712: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___self_attn_k_proj_weight"} loc("p712.21547"),
  %arg713: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___mlp_down_proj_weight"} loc("p713.21556"),
  %arg714: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___mlp_up_proj_weight"} loc("p714.21561"),
  %arg715: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___self_attn_o_proj_weight"} loc("p715.21570"),
  %arg716: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___self_attn_q_proj_weight"} loc("p716.21611"),
  %arg717: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___self_attn_q_norm_weight"} loc("p717.21649"),
  %arg718: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___post_attention_layernorm_weight"} loc("p718.21772"),
  %arg719: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__54___mlp_gate_proj_weight"} loc("p719.21781"),
  %arg720: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___input_layernorm_weight"} loc("p720.21827"),
  %arg721: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___self_attn_k_norm_weight"} loc("p721.21866"),
  %arg722: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_112"} loc("p722.21899"),
  %arg723: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___self_attn_v_proj_weight"} loc("p723.21907"),
  %arg724: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_113"} loc("p724.21929"),
  %arg725: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___self_attn_k_proj_weight"} loc("p725.21942"),
  %arg726: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___mlp_down_proj_weight"} loc("p726.21951"),
  %arg727: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___mlp_up_proj_weight"} loc("p727.21956"),
  %arg728: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___self_attn_o_proj_weight"} loc("p728.21965"),
  %arg729: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___self_attn_q_proj_weight"} loc("p729.22006"),
  %arg730: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___self_attn_q_norm_weight"} loc("p730.22044"),
  %arg731: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___post_attention_layernorm_weight"} loc("p731.22167"),
  %arg732: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__55___mlp_gate_proj_weight"} loc("p732.22176"),
  %arg733: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___input_layernorm_weight"} loc("p733.22222"),
  %arg734: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___self_attn_k_norm_weight"} loc("p734.22261"),
  %arg735: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_114"} loc("p735.22294"),
  %arg736: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___self_attn_v_proj_weight"} loc("p736.22302"),
  %arg737: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_115"} loc("p737.22324"),
  %arg738: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___self_attn_k_proj_weight"} loc("p738.22337"),
  %arg739: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___mlp_down_proj_weight"} loc("p739.22346"),
  %arg740: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___mlp_up_proj_weight"} loc("p740.22351"),
  %arg741: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___self_attn_o_proj_weight"} loc("p741.22360"),
  %arg742: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___self_attn_q_proj_weight"} loc("p742.22401"),
  %arg743: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___self_attn_q_norm_weight"} loc("p743.22439"),
  %arg744: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___post_attention_layernorm_weight"} loc("p744.22562"),
  %arg745: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__56___mlp_gate_proj_weight"} loc("p745.22571"),
  %arg746: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___input_layernorm_weight"} loc("p746.22617"),
  %arg747: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___self_attn_k_norm_weight"} loc("p747.22656"),
  %arg748: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_116"} loc("p748.22689"),
  %arg749: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___self_attn_v_proj_weight"} loc("p749.22697"),
  %arg750: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_117"} loc("p750.22719"),
  %arg751: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___self_attn_k_proj_weight"} loc("p751.22732"),
  %arg752: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___mlp_down_proj_weight"} loc("p752.22741"),
  %arg753: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___mlp_up_proj_weight"} loc("p753.22746"),
  %arg754: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___self_attn_o_proj_weight"} loc("p754.22755"),
  %arg755: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___self_attn_q_proj_weight"} loc("p755.22796"),
  %arg756: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___self_attn_q_norm_weight"} loc("p756.22834"),
  %arg757: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___post_attention_layernorm_weight"} loc("p757.22957"),
  %arg758: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__57___mlp_gate_proj_weight"} loc("p758.22966"),
  %arg759: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___input_layernorm_weight"} loc("p759.23012"),
  %arg760: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___self_attn_k_norm_weight"} loc("p760.23051"),
  %arg761: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_118"} loc("p761.23084"),
  %arg762: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___self_attn_v_proj_weight"} loc("p762.23092"),
  %arg763: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_119"} loc("p763.23114"),
  %arg764: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___self_attn_k_proj_weight"} loc("p764.23127"),
  %arg765: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___mlp_down_proj_weight"} loc("p765.23136"),
  %arg766: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___mlp_up_proj_weight"} loc("p766.23141"),
  %arg767: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___self_attn_o_proj_weight"} loc("p767.23150"),
  %arg768: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___self_attn_q_proj_weight"} loc("p768.23191"),
  %arg769: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___self_attn_q_norm_weight"} loc("p769.23229"),
  %arg770: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___post_attention_layernorm_weight"} loc("p770.23352"),
  %arg771: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__58___mlp_gate_proj_weight"} loc("p771.23361"),
  %arg772: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___input_layernorm_weight"} loc("p772.23407"),
  %arg773: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___self_attn_k_norm_weight"} loc("p773.23446"),
  %arg774: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_120"} loc("p774.23479"),
  %arg775: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___self_attn_v_proj_weight"} loc("p775.23487"),
  %arg776: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_121"} loc("p776.23509"),
  %arg777: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___self_attn_k_proj_weight"} loc("p777.23522"),
  %arg778: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___mlp_down_proj_weight"} loc("p778.23531"),
  %arg779: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___mlp_up_proj_weight"} loc("p779.23536"),
  %arg780: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___self_attn_o_proj_weight"} loc("p780.23545"),
  %arg781: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___self_attn_q_proj_weight"} loc("p781.23586"),
  %arg782: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___self_attn_q_norm_weight"} loc("p782.23624"),
  %arg783: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___post_attention_layernorm_weight"} loc("p783.23747"),
  %arg784: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__59___mlp_gate_proj_weight"} loc("p784.23756"),
  %arg785: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___input_layernorm_weight"} loc("p785.23802"),
  %arg786: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___self_attn_k_norm_weight"} loc("p786.23841"),
  %arg787: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_122"} loc("p787.23874"),
  %arg788: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___self_attn_v_proj_weight"} loc("p788.23882"),
  %arg789: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_123"} loc("p789.23904"),
  %arg790: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___self_attn_k_proj_weight"} loc("p790.23917"),
  %arg791: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___mlp_down_proj_weight"} loc("p791.23926"),
  %arg792: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___mlp_up_proj_weight"} loc("p792.23931"),
  %arg793: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___self_attn_o_proj_weight"} loc("p793.23940"),
  %arg794: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___self_attn_q_proj_weight"} loc("p794.23981"),
  %arg795: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___self_attn_q_norm_weight"} loc("p795.24019"),
  %arg796: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___post_attention_layernorm_weight"} loc("p796.24142"),
  %arg797: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__60___mlp_gate_proj_weight"} loc("p797.24151"),
  %arg798: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___input_layernorm_weight"} loc("p798.24197"),
  %arg799: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___self_attn_k_norm_weight"} loc("p799.24236"),
  %arg800: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_124"} loc("p800.24269"),
  %arg801: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___self_attn_v_proj_weight"} loc("p801.24277"),
  %arg802: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_125"} loc("p802.24299"),
  %arg803: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___self_attn_k_proj_weight"} loc("p803.24312"),
  %arg804: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___mlp_down_proj_weight"} loc("p804.24321"),
  %arg805: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___mlp_up_proj_weight"} loc("p805.24326"),
  %arg806: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___self_attn_o_proj_weight"} loc("p806.24335"),
  %arg807: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___self_attn_q_proj_weight"} loc("p807.24376"),
  %arg808: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___self_attn_q_norm_weight"} loc("p808.24414"),
  %arg809: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___post_attention_layernorm_weight"} loc("p809.24537"),
  %arg810: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__61___mlp_gate_proj_weight"} loc("p810.24546"),
  %arg811: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___input_layernorm_weight"} loc("p811.24592"),
  %arg812: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___self_attn_k_norm_weight"} loc("p812.24631"),
  %arg813: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_126"} loc("p813.24664"),
  %arg814: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___self_attn_v_proj_weight"} loc("p814.24672"),
  %arg815: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_127"} loc("p815.24694"),
  %arg816: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___self_attn_k_proj_weight"} loc("p816.24707"),
  %arg817: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___mlp_down_proj_weight"} loc("p817.24716"),
  %arg818: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___mlp_up_proj_weight"} loc("p818.24721"),
  %arg819: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___self_attn_o_proj_weight"} loc("p819.24730"),
  %arg820: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___self_attn_q_proj_weight"} loc("p820.24771"),
  %arg821: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___self_attn_q_norm_weight"} loc("p821.24809"),
  %arg822: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___post_attention_layernorm_weight"} loc("p822.24932"),
  %arg823: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__62___mlp_gate_proj_weight"} loc("p823.24941"),
  %arg824: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___input_layernorm_weight"} loc("p824.24987"),
  %arg825: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___self_attn_k_norm_weight"} loc("p825.25026"),
  %arg826: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_128"} loc("p826.25059"),
  %arg827: tensor<1024x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___self_attn_v_proj_weight"} loc("p827.25067"),
  %arg828: tensor<32x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_129"} loc("p828.25089"),
  %arg829: tensor<151936x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"} loc("p829.25097"),
  %arg830: tensor<5120x25600xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___mlp_down_proj_weight"} loc("p830.25106"),
  %arg831: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___mlp_up_proj_weight"} loc("p831.25111"),
  %arg832: tensor<5120x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___self_attn_o_proj_weight"} loc("p832.25120"),
  %arg833: tensor<8192x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___self_attn_q_proj_weight"} loc("p833.25161"),
  %arg834: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___self_attn_q_norm_weight"} loc("p834.25199"),
  %arg835: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___post_attention_layernorm_weight"} loc("p835.25322"),
  %arg836: tensor<25600x5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__63___mlp_gate_proj_weight"} loc("p836.25331"),
  %arg837: tensor<5120xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"} loc("p837.25377")) -> (tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x17x151936xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:129 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73, %arg74, %arg75, %arg76, %arg77, %arg78, %arg79, %arg80, %arg81, %arg82, %arg83, %arg84, %arg85, %arg86, %arg87, %arg88, %arg89, %arg90, %arg91, %arg92, %arg93, %arg94, %arg95, %arg96, %arg97, %arg98, %arg99, %arg100, %arg101, %arg102, %arg103, %arg104, %arg105, %arg106, %arg107, %arg108, %arg109, %arg110, %arg111, %arg112, %arg113, %arg114, %arg115, %arg116, %arg117, %arg118, %arg119, %arg120, %arg121, %arg122, %arg123, %arg124, %arg125, %arg126, %arg127, %arg128, %arg129, %arg130, %arg131, %arg132, %arg133, %arg134, %arg135, %arg136, %arg137, %arg138, %arg139, %arg140, %arg141, %arg142, %arg143, %arg144, %arg145, %arg146, %arg147, %arg148, %arg149, %arg150, %arg151, %arg152, %arg153, %arg154, %arg155, %arg156, %arg157, %arg158, %arg159, %arg160, %arg161, %arg162, %arg163, %arg164, %arg165, %arg166, %arg167, %arg168, %arg169, %arg170, %arg171, %arg172, %arg173, %arg174, %arg175, %arg176, %arg177, %arg178, %arg179, %arg180, %arg181, %arg182, %arg183, %arg184, %arg185, %arg186, %arg187, %arg188, %arg189, %arg190, %arg191, %arg192, %arg193, %arg194, %arg195, %arg196, %arg197, %arg198, %arg199, %arg200, %arg201, %arg202, %arg203, %arg204, %arg205, %arg206, %arg207, %arg208, %arg209, %arg210, %arg211, %arg212, %arg213, %arg214, %arg215, %arg216, %arg217, %arg218, %arg219, %arg220, %arg221, %arg222, %arg223, %arg224, %arg225, %arg226, %arg227, %arg228, %arg229, %arg230, %arg231, %arg232, %arg233, %arg234, %arg235, %arg236, %arg237, %arg238, %arg239, %arg240, %arg241, %arg242, %arg243, %arg244, %arg245, %arg246, %arg247, %arg248, %arg249, %arg250, %arg251, %arg252, %arg253, %arg254, %arg255, %arg256, %arg257, %arg258, %arg259, %arg260, %arg261, %arg262, %arg263, %arg264, %arg265, %arg266, %arg267, %arg268, %arg269, %arg270, %arg271, %arg272, %arg273, %arg274, %arg275, %arg276, %arg277, %arg278, %arg279, %arg280, %arg281, %arg282, %arg283, %arg284, %arg285, %arg286, %arg287, %arg288, %arg289, %arg290, %arg291, %arg292, %arg293, %arg294, %arg295, %arg296, %arg297, %arg298, %arg299, %arg300, %arg301, %arg302, %arg303, %arg304, %arg305, %arg306, %arg307, %arg308, %arg309, %arg310, %arg311, %arg312, %arg313, %arg314, %arg315, %arg316, %arg317, %arg318, %arg319, %arg320, %arg321, %arg322, %arg323, %arg324, %arg325, %arg326, %arg327, %arg328, %arg329, %arg330, %arg331, %arg332, %arg333, %arg334, %arg335, %arg336, %arg337, %arg338, %arg339, %arg340, %arg341, %arg342, %arg343, %arg344, %arg345, %arg346, %arg347, %arg348, %arg349, %arg350, %arg351, %arg352, %arg353, %arg354, %arg355, %arg356, %arg357, %arg358, %arg359, %arg360, %arg361, %arg362, %arg363, %arg364, %arg365, %arg366, %arg367, %arg368, %arg369, %arg370, %arg371, %arg372, %arg373, %arg374, %arg375, %arg376, %arg377, %arg378, %arg379, %arg380, %arg381, %arg382, %arg383, %arg384, %arg385, %arg386, %arg387, %arg388, %arg389, %arg390, %arg391, %arg392, %arg393, %arg394, %arg395, %arg396, %arg397, %arg398, %arg399, %arg400, %arg401, %arg402, %arg403, %arg404, %arg405, %arg406, %arg407, %arg408, %arg409, %arg410, %arg411, %arg412, %arg413, %arg414, %arg415, %arg416, %arg417, %arg418, %arg419, %arg420, %arg421, %arg422, %arg423, %arg424, %arg425, %arg426, %arg427, %arg428, %arg429, %arg430, %arg431, %arg432, %arg433, %arg434, %arg435, %arg436, %arg437, %arg438, %arg439, %arg440, %arg441, %arg442, %arg443, %arg444, %arg445, %arg446, %arg447, %arg448, %arg449, %arg450, %arg451, %arg452, %arg453, %arg454, %arg455, %arg456, %arg457, %arg458, %arg459, %arg460, %arg461, %arg462, %arg463, %arg464, %arg465, %arg466, %arg467, %arg468, %arg469, %arg470, %arg471, %arg472, %arg473, %arg474, %arg475, %arg476, %arg477, %arg478, %arg479, %arg480, %arg481, %arg482, %arg483, %arg484, %arg485, %arg486, %arg487, %arg488, %arg489, %arg490, %arg491, %arg492, %arg493, %arg494, %arg495, %arg496, %arg497, %arg498, %arg499, %arg500, %arg501, %arg502, %arg503, %arg504, %arg505, %arg506, %arg507, %arg508, %arg509, %arg510, %arg511, %arg512, %arg513, %arg514, %arg515, %arg516, %arg517, %arg518, %arg519, %arg520, %arg521, %arg522, %arg523, %arg524, %arg525, %arg526, %arg527, %arg528, %arg529, %arg530, %arg531, %arg532, %arg533, %arg534, %arg535, %arg536, %arg537, %arg538, %arg539, %arg540, %arg541, %arg542, %arg543, %arg544, %arg545, %arg546, %arg547, %arg548, %arg549, %arg550, %arg551, %arg552, %arg553, %arg554, %arg555, %arg556, %arg557, %arg558, %arg559, %arg560, %arg561, %arg562, %arg563, %arg564, %arg565, %arg566, %arg567, %arg568, %arg569, %arg570, %arg571, %arg572, %arg573, %arg574, %arg575, %arg576, %arg577, %arg578, %arg579, %arg580, %arg581, %arg582, %arg583, %arg584, %arg585, %arg586, %arg587, %arg588, %arg589, %arg590, %arg591, %arg592, %arg593, %arg594, %arg595, %arg596, %arg597, %arg598, %arg599, %arg600, %arg601, %arg602, %arg603, %arg604, %arg605, %arg606, %arg607, %arg608, %arg609, %arg610, %arg611, %arg612, %arg613, %arg614, %arg615, %arg616, %arg617, %arg618, %arg619, %arg620, %arg621, %arg622, %arg623, %arg624, %arg625, %arg626, %arg627, %arg628, %arg629, %arg630, %arg631, %arg632, %arg633, %arg634, %arg635, %arg636, %arg637, %arg638, %arg639, %arg640, %arg641, %arg642, %arg643, %arg644, %arg645, %arg646, %arg647, %arg648, %arg649, %arg650, %arg651, %arg652, %arg653, %arg654, %arg655, %arg656, %arg657, %arg658, %arg659, %arg660, %arg661, %arg662, %arg663, %arg664, %arg665, %arg666, %arg667, %arg668, %arg669, %arg670, %arg671, %arg672, %arg673, %arg674, %arg675, %arg676, %arg677, %arg678, %arg679, %arg680, %arg681, %arg682, %arg683, %arg684, %arg685, %arg686, %arg687, %arg688, %arg689, %arg690, %arg691, %arg692, %arg693, %arg694, %arg695, %arg696, %arg697, %arg698, %arg699, %arg700, %arg701, %arg702, %arg703, %arg704, %arg705, %arg706, %arg707, %arg708, %arg709, %arg710, %arg711, %arg712, %arg713, %arg714, %arg715, %arg716, %arg717, %arg718, %arg719, %arg720, %arg721, %arg722, %arg723, %arg724, %arg725, %arg726, %arg727, %arg728, %arg729, %arg730, %arg731, %arg732, %arg733, %arg734, %arg735, %arg736, %arg737, %arg738, %arg739, %arg740, %arg741, %arg742, %arg743, %arg744, %arg745, %arg746, %arg747, %arg748, %arg749, %arg750, %arg751, %arg752, %arg753, %arg754, %arg755, %arg756, %arg757, %arg758, %arg759, %arg760, %arg761, %arg762, %arg763, %arg764, %arg765, %arg766, %arg767, %arg768, %arg769, %arg770, %arg771, %arg772, %arg773, %arg774, %arg775, %arg776, %arg777, %arg778, %arg779, %arg780, %arg781, %arg782, %arg783, %arg784, %arg785, %arg786, %arg787, %arg788, %arg789, %arg790, %arg791, %arg792, %arg793, %arg794, %arg795, %arg796, %arg797, %arg798, %arg799, %arg800, %arg801, %arg802, %arg803, %arg804, %arg805, %arg806, %arg807, %arg808, %arg809, %arg810, %arg811, %arg812, %arg813, %arg814, %arg815, %arg816, %arg817, %arg818, %arg819, %arg820, %arg821, %arg822, %arg823, %arg824, %arg825, %arg826, %arg827, %arg828, %arg829, %arg830, %arg831, %arg832, %arg833, %arg834, %arg835, %arg836, %arg837) in_shardings=[<@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {}, {"_axis_0"}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg838: tensor<17xi64> loc("p0.3"), %arg839: tensor<64xbf16> loc("p1.11"), %arg840: tensor<128x5120xbf16> loc("p2.31"), %arg841: tensor<32x17xi64> loc("p3.39"), %arg842: tensor<151936x5120xbf16> loc("p4.44"), %arg843: tensor<5120xbf16> loc("p5.80"), %arg844: tensor<128xbf16> loc("p6.119"), %arg845: tensor<32x1x128x128xbf16> loc("p7.157"), %arg846: tensor<128x5120xbf16> loc("p8.165"), %arg847: tensor<32x1x128x128xbf16> loc("p9.187"), %arg848: tensor<128x5120xbf16> loc("p10.200"), %arg849: tensor<5120x3200xbf16> loc("p11.209"), %arg850: tensor<3200x5120xbf16> loc("p12.214"), %arg851: tensor<5120x1024xbf16> loc("p13.223"), %arg852: tensor<1024x5120xbf16> loc("p14.281"), %arg853: tensor<128xbf16> loc("p15.319"), %arg854: tensor<5120xbf16> loc("p16.442"), %arg855: tensor<3200x5120xbf16> loc("p17.451"), %arg856: tensor<5120xbf16> loc("p18.497"), %arg857: tensor<128xbf16> loc("p19.536"), %arg858: tensor<32x1x128x128xbf16> loc("p20.569"), %arg859: tensor<128x5120xbf16> loc("p21.577"), %arg860: tensor<32x1x128x128xbf16> loc("p22.599"), %arg861: tensor<128x5120xbf16> loc("p23.612"), %arg862: tensor<5120x3200xbf16> loc("p24.621"), %arg863: tensor<3200x5120xbf16> loc("p25.626"), %arg864: tensor<5120x1024xbf16> loc("p26.635"), %arg865: tensor<1024x5120xbf16> loc("p27.676"), %arg866: tensor<128xbf16> loc("p28.714"), %arg867: tensor<5120xbf16> loc("p29.837"), %arg868: tensor<3200x5120xbf16> loc("p30.846"), %arg869: tensor<5120xbf16> loc("p31.892"), %arg870: tensor<128xbf16> loc("p32.931"), %arg871: tensor<32x1x128x128xbf16> loc("p33.964"), %arg872: tensor<128x5120xbf16> loc("p34.972"), %arg873: tensor<32x1x128x128xbf16> loc("p35.994"), %arg874: tensor<128x5120xbf16> loc("p36.1007"), %arg875: tensor<5120x3200xbf16> loc("p37.1016"), %arg876: tensor<3200x5120xbf16> loc("p38.1021"), %arg877: tensor<5120x1024xbf16> loc("p39.1030"), %arg878: tensor<1024x5120xbf16> loc("p40.1071"), %arg879: tensor<128xbf16> loc("p41.1109"), %arg880: tensor<5120xbf16> loc("p42.1232"), %arg881: tensor<3200x5120xbf16> loc("p43.1241"), %arg882: tensor<5120xbf16> loc("p44.1287"), %arg883: tensor<128xbf16> loc("p45.1326"), %arg884: tensor<32x1x128x128xbf16> loc("p46.1359"), %arg885: tensor<128x5120xbf16> loc("p47.1367"), %arg886: tensor<32x1x128x128xbf16> loc("p48.1389"), %arg887: tensor<128x5120xbf16> loc("p49.1402"), %arg888: tensor<5120x3200xbf16> loc("p50.1411"), %arg889: tensor<3200x5120xbf16> loc("p51.1416"), %arg890: tensor<5120x1024xbf16> loc("p52.1425"), %arg891: tensor<1024x5120xbf16> loc("p53.1466"), %arg892: tensor<128xbf16> loc("p54.1504"), %arg893: tensor<5120xbf16> loc("p55.1627"), %arg894: tensor<3200x5120xbf16> loc("p56.1636"), %arg895: tensor<5120xbf16> loc("p57.1682"), %arg896: tensor<128xbf16> loc("p58.1721"), %arg897: tensor<32x1x128x128xbf16> loc("p59.1754"), %arg898: tensor<128x5120xbf16> loc("p60.1762"), %arg899: tensor<32x1x128x128xbf16> loc("p61.1784"), %arg900: tensor<128x5120xbf16> loc("p62.1797"), %arg901: tensor<5120x3200xbf16> loc("p63.1806"), %arg902: tensor<3200x5120xbf16> loc("p64.1811"), %arg903: tensor<5120x1024xbf16> loc("p65.1820"), %arg904: tensor<1024x5120xbf16> loc("p66.1861"), %arg905: tensor<128xbf16> loc("p67.1899"), %arg906: tensor<5120xbf16> loc("p68.2022"), %arg907: tensor<3200x5120xbf16> loc("p69.2031"), %arg908: tensor<5120xbf16> loc("p70.2077"), %arg909: tensor<128xbf16> loc("p71.2116"), %arg910: tensor<32x1x128x128xbf16> loc("p72.2149"), %arg911: tensor<128x5120xbf16> loc("p73.2157"), %arg912: tensor<32x1x128x128xbf16> loc("p74.2179"), %arg913: tensor<128x5120xbf16> loc("p75.2192"), %arg914: tensor<5120x3200xbf16> loc("p76.2201"), %arg915: tensor<3200x5120xbf16> loc("p77.2206"), %arg916: tensor<5120x1024xbf16> loc("p78.2215"), %arg917: tensor<1024x5120xbf16> loc("p79.2256"), %arg918: tensor<128xbf16> loc("p80.2294"), %arg919: tensor<5120xbf16> loc("p81.2417"), %arg920: tensor<3200x5120xbf16> loc("p82.2426"), %arg921: tensor<5120xbf16> loc("p83.2472"), %arg922: tensor<128xbf16> loc("p84.2511"), %arg923: tensor<32x1x128x128xbf16> loc("p85.2544"), %arg924: tensor<128x5120xbf16> loc("p86.2552"), %arg925: tensor<32x1x128x128xbf16> loc("p87.2574"), %arg926: tensor<128x5120xbf16> loc("p88.2587"), %arg927: tensor<5120x3200xbf16> loc("p89.2596"), %arg928: tensor<3200x5120xbf16> loc("p90.2601"), %arg929: tensor<5120x1024xbf16> loc("p91.2610"), %arg930: tensor<1024x5120xbf16> loc("p92.2651"), %arg931: tensor<128xbf16> loc("p93.2689"), %arg932: tensor<5120xbf16> loc("p94.2812"), %arg933: tensor<3200x5120xbf16> loc("p95.2821"), %arg934: tensor<5120xbf16> loc("p96.2867"), %arg935: tensor<128xbf16> loc("p97.2906"), %arg936: tensor<32x1x128x128xbf16> loc("p98.2939"), %arg937: tensor<128x5120xbf16> loc("p99.2947"), %arg938: tensor<32x1x128x128xbf16> loc("p100.2969"), %arg939: tensor<128x5120xbf16> loc("p101.2982"), %arg940: tensor<5120x3200xbf16> loc("p102.2991"), %arg941: tensor<3200x5120xbf16> loc("p103.2996"), %arg942: tensor<5120x1024xbf16> loc("p104.3005"), %arg943: tensor<1024x5120xbf16> loc("p105.3046"), %arg944: tensor<128xbf16> loc("p106.3084"), %arg945: tensor<5120xbf16> loc("p107.3207"), %arg946: tensor<3200x5120xbf16> loc("p108.3216"), %arg947: tensor<5120xbf16> loc("p109.3262"), %arg948: tensor<128xbf16> loc("p110.3301"), %arg949: tensor<32x1x128x128xbf16> loc("p111.3334"), %arg950: tensor<128x5120xbf16> loc("p112.3342"), %arg951: tensor<32x1x128x128xbf16> loc("p113.3364"), %arg952: tensor<128x5120xbf16> loc("p114.3377"), %arg953: tensor<5120x3200xbf16> loc("p115.3386"), %arg954: tensor<3200x5120xbf16> loc("p116.3391"), %arg955: tensor<5120x1024xbf16> loc("p117.3400"), %arg956: tensor<1024x5120xbf16> loc("p118.3441"), %arg957: tensor<128xbf16> loc("p119.3479"), %arg958: tensor<5120xbf16> loc("p120.3602"), %arg959: tensor<3200x5120xbf16> loc("p121.3611"), %arg960: tensor<5120xbf16> loc("p122.3657"), %arg961: tensor<128xbf16> loc("p123.3696"), %arg962: tensor<32x1x128x128xbf16> loc("p124.3729"), %arg963: tensor<128x5120xbf16> loc("p125.3737"), %arg964: tensor<32x1x128x128xbf16> loc("p126.3759"), %arg965: tensor<128x5120xbf16> loc("p127.3772"), %arg966: tensor<5120x3200xbf16> loc("p128.3781"), %arg967: tensor<3200x5120xbf16> loc("p129.3786"), %arg968: tensor<5120x1024xbf16> loc("p130.3795"), %arg969: tensor<1024x5120xbf16> loc("p131.3836"), %arg970: tensor<128xbf16> loc("p132.3874"), %arg971: tensor<5120xbf16> loc("p133.3997"), %arg972: tensor<3200x5120xbf16> loc("p134.4006"), %arg973: tensor<5120xbf16> loc("p135.4052"), %arg974: tensor<128xbf16> loc("p136.4091"), %arg975: tensor<32x1x128x128xbf16> loc("p137.4124"), %arg976: tensor<128x5120xbf16> loc("p138.4132"), %arg977: tensor<32x1x128x128xbf16> loc("p139.4154"), %arg978: tensor<128x5120xbf16> loc("p140.4167"), %arg979: tensor<5120x3200xbf16> loc("p141.4176"), %arg980: tensor<3200x5120xbf16> loc("p142.4181"), %arg981: tensor<5120x1024xbf16> loc("p143.4190"), %arg982: tensor<1024x5120xbf16> loc("p144.4231"), %arg983: tensor<128xbf16> loc("p145.4269"), %arg984: tensor<5120xbf16> loc("p146.4392"), %arg985: tensor<3200x5120xbf16> loc("p147.4401"), %arg986: tensor<5120xbf16> loc("p148.4447"), %arg987: tensor<128xbf16> loc("p149.4486"), %arg988: tensor<32x1x128x128xbf16> loc("p150.4519"), %arg989: tensor<128x5120xbf16> loc("p151.4527"), %arg990: tensor<32x1x128x128xbf16> loc("p152.4549"), %arg991: tensor<128x5120xbf16> loc("p153.4562"), %arg992: tensor<5120x3200xbf16> loc("p154.4571"), %arg993: tensor<3200x5120xbf16> loc("p155.4576"), %arg994: tensor<5120x1024xbf16> loc("p156.4585"), %arg995: tensor<1024x5120xbf16> loc("p157.4626"), %arg996: tensor<128xbf16> loc("p158.4664"), %arg997: tensor<5120xbf16> loc("p159.4787"), %arg998: tensor<3200x5120xbf16> loc("p160.4796"), %arg999: tensor<5120xbf16> loc("p161.4842"), %arg1000: tensor<128xbf16> loc("p162.4881"), %arg1001: tensor<32x1x128x128xbf16> loc("p163.4914"), %arg1002: tensor<128x5120xbf16> loc("p164.4922"), %arg1003: tensor<32x1x128x128xbf16> loc("p165.4944"), %arg1004: tensor<128x5120xbf16> loc("p166.4957"), %arg1005: tensor<5120x3200xbf16> loc("p167.4966"), %arg1006: tensor<3200x5120xbf16> loc("p168.4971"), %arg1007: tensor<5120x1024xbf16> loc("p169.4980"), %arg1008: tensor<1024x5120xbf16> loc("p170.5021"), %arg1009: tensor<128xbf16> loc("p171.5059"), %arg1010: tensor<5120xbf16> loc("p172.5182"), %arg1011: tensor<3200x5120xbf16> loc("p173.5191"), %arg1012: tensor<5120xbf16> loc("p174.5237"), %arg1013: tensor<128xbf16> loc("p175.5276"), %arg1014: tensor<32x1x128x128xbf16> loc("p176.5309"), %arg1015: tensor<128x5120xbf16> loc("p177.5317"), %arg1016: tensor<32x1x128x128xbf16> loc("p178.5339"), %arg1017: tensor<128x5120xbf16> loc("p179.5352"), %arg1018: tensor<5120x3200xbf16> loc("p180.5361"), %arg1019: tensor<3200x5120xbf16> loc("p181.5366"), %arg1020: tensor<5120x1024xbf16> loc("p182.5375"), %arg1021: tensor<1024x5120xbf16> loc("p183.5416"), %arg1022: tensor<128xbf16> loc("p184.5454"), %arg1023: tensor<5120xbf16> loc("p185.5577"), %arg1024: tensor<3200x5120xbf16> loc("p186.5586"), %arg1025: tensor<5120xbf16> loc("p187.5632"), %arg1026: tensor<128xbf16> loc("p188.5671"), %arg1027: tensor<32x1x128x128xbf16> loc("p189.5704"), %arg1028: tensor<128x5120xbf16> loc("p190.5712"), %arg1029: tensor<32x1x128x128xbf16> loc("p191.5734"), %arg1030: tensor<128x5120xbf16> loc("p192.5747"), %arg1031: tensor<5120x3200xbf16> loc("p193.5756"), %arg1032: tensor<3200x5120xbf16> loc("p194.5761"), %arg1033: tensor<5120x1024xbf16> loc("p195.5770"), %arg1034: tensor<1024x5120xbf16> loc("p196.5811"), %arg1035: tensor<128xbf16> loc("p197.5849"), %arg1036: tensor<5120xbf16> loc("p198.5972"), %arg1037: tensor<3200x5120xbf16> loc("p199.5981"), %arg1038: tensor<5120xbf16> loc("p200.6027"), %arg1039: tensor<128xbf16> loc("p201.6066"), %arg1040: tensor<32x1x128x128xbf16> loc("p202.6099"), %arg1041: tensor<128x5120xbf16> loc("p203.6107"), %arg1042: tensor<32x1x128x128xbf16> loc("p204.6129"), %arg1043: tensor<128x5120xbf16> loc("p205.6142"), %arg1044: tensor<5120x3200xbf16> loc("p206.6151"), %arg1045: tensor<3200x5120xbf16> loc("p207.6156"), %arg1046: tensor<5120x1024xbf16> loc("p208.6165"), %arg1047: tensor<1024x5120xbf16> loc("p209.6206"), %arg1048: tensor<128xbf16> loc("p210.6244"), %arg1049: tensor<5120xbf16> loc("p211.6367"), %arg1050: tensor<3200x5120xbf16> loc("p212.6376"), %arg1051: tensor<5120xbf16> loc("p213.6422"), %arg1052: tensor<128xbf16> loc("p214.6461"), %arg1053: tensor<32x1x128x128xbf16> loc("p215.6494"), %arg1054: tensor<128x5120xbf16> loc("p216.6502"), %arg1055: tensor<32x1x128x128xbf16> loc("p217.6524"), %arg1056: tensor<128x5120xbf16> loc("p218.6537"), %arg1057: tensor<5120x3200xbf16> loc("p219.6546"), %arg1058: tensor<3200x5120xbf16> loc("p220.6551"), %arg1059: tensor<5120x1024xbf16> loc("p221.6560"), %arg1060: tensor<1024x5120xbf16> loc("p222.6601"), %arg1061: tensor<128xbf16> loc("p223.6639"), %arg1062: tensor<5120xbf16> loc("p224.6762"), %arg1063: tensor<3200x5120xbf16> loc("p225.6771"), %arg1064: tensor<5120xbf16> loc("p226.6817"), %arg1065: tensor<128xbf16> loc("p227.6856"), %arg1066: tensor<32x1x128x128xbf16> loc("p228.6889"), %arg1067: tensor<128x5120xbf16> loc("p229.6897"), %arg1068: tensor<32x1x128x128xbf16> loc("p230.6919"), %arg1069: tensor<128x5120xbf16> loc("p231.6932"), %arg1070: tensor<5120x3200xbf16> loc("p232.6941"), %arg1071: tensor<3200x5120xbf16> loc("p233.6946"), %arg1072: tensor<5120x1024xbf16> loc("p234.6955"), %arg1073: tensor<1024x5120xbf16> loc("p235.6996"), %arg1074: tensor<128xbf16> loc("p236.7034"), %arg1075: tensor<5120xbf16> loc("p237.7157"), %arg1076: tensor<3200x5120xbf16> loc("p238.7166"), %arg1077: tensor<5120xbf16> loc("p239.7212"), %arg1078: tensor<128xbf16> loc("p240.7251"), %arg1079: tensor<32x1x128x128xbf16> loc("p241.7284"), %arg1080: tensor<128x5120xbf16> loc("p242.7292"), %arg1081: tensor<32x1x128x128xbf16> loc("p243.7314"), %arg1082: tensor<128x5120xbf16> loc("p244.7327"), %arg1083: tensor<5120x3200xbf16> loc("p245.7336"), %arg1084: tensor<3200x5120xbf16> loc("p246.7341"), %arg1085: tensor<5120x1024xbf16> loc("p247.7350"), %arg1086: tensor<1024x5120xbf16> loc("p248.7391"), %arg1087: tensor<128xbf16> loc("p249.7429"), %arg1088: tensor<5120xbf16> loc("p250.7552"), %arg1089: tensor<3200x5120xbf16> loc("p251.7561"), %arg1090: tensor<5120xbf16> loc("p252.7607"), %arg1091: tensor<128xbf16> loc("p253.7646"), %arg1092: tensor<32x1x128x128xbf16> loc("p254.7679"), %arg1093: tensor<128x5120xbf16> loc("p255.7687"), %arg1094: tensor<32x1x128x128xbf16> loc("p256.7709"), %arg1095: tensor<128x5120xbf16> loc("p257.7722"), %arg1096: tensor<5120x3200xbf16> loc("p258.7731"), %arg1097: tensor<3200x5120xbf16> loc("p259.7736"), %arg1098: tensor<5120x1024xbf16> loc("p260.7745"), %arg1099: tensor<1024x5120xbf16> loc("p261.7786"), %arg1100: tensor<128xbf16> loc("p262.7824"), %arg1101: tensor<5120xbf16> loc("p263.7947"), %arg1102: tensor<3200x5120xbf16> loc("p264.7956"), %arg1103: tensor<5120xbf16> loc("p265.8002"), %arg1104: tensor<128xbf16> loc("p266.8041"), %arg1105: tensor<32x1x128x128xbf16> loc("p267.8074"), %arg1106: tensor<128x5120xbf16> loc("p268.8082"), %arg1107: tensor<32x1x128x128xbf16> loc("p269.8104"), %arg1108: tensor<128x5120xbf16> loc("p270.8117"), %arg1109: tensor<5120x3200xbf16> loc("p271.8126"), %arg1110: tensor<3200x5120xbf16> loc("p272.8131"), %arg1111: tensor<5120x1024xbf16> loc("p273.8140"), %arg1112: tensor<1024x5120xbf16> loc("p274.8181"), %arg1113: tensor<128xbf16> loc("p275.8219"), %arg1114: tensor<5120xbf16> loc("p276.8342"), %arg1115: tensor<3200x5120xbf16> loc("p277.8351"), %arg1116: tensor<5120xbf16> loc("p278.8397"), %arg1117: tensor<128xbf16> loc("p279.8436"), %arg1118: tensor<32x1x128x128xbf16> loc("p280.8469"), %arg1119: tensor<128x5120xbf16> loc("p281.8477"), %arg1120: tensor<32x1x128x128xbf16> loc("p282.8499"), %arg1121: tensor<128x5120xbf16> loc("p283.8512"), %arg1122: tensor<5120x3200xbf16> loc("p284.8521"), %arg1123: tensor<3200x5120xbf16> loc("p285.8526"), %arg1124: tensor<5120x1024xbf16> loc("p286.8535"), %arg1125: tensor<1024x5120xbf16> loc("p287.8576"), %arg1126: tensor<128xbf16> loc("p288.8614"), %arg1127: tensor<5120xbf16> loc("p289.8737"), %arg1128: tensor<3200x5120xbf16> loc("p290.8746"), %arg1129: tensor<5120xbf16> loc("p291.8792"), %arg1130: tensor<128xbf16> loc("p292.8831"), %arg1131: tensor<32x1x128x128xbf16> loc("p293.8864"), %arg1132: tensor<128x5120xbf16> loc("p294.8872"), %arg1133: tensor<32x1x128x128xbf16> loc("p295.8894"), %arg1134: tensor<128x5120xbf16> loc("p296.8907"), %arg1135: tensor<5120x3200xbf16> loc("p297.8916"), %arg1136: tensor<3200x5120xbf16> loc("p298.8921"), %arg1137: tensor<5120x1024xbf16> loc("p299.8930"), %arg1138: tensor<1024x5120xbf16> loc("p300.8971"), %arg1139: tensor<128xbf16> loc("p301.9009"), %arg1140: tensor<5120xbf16> loc("p302.9132"), %arg1141: tensor<3200x5120xbf16> loc("p303.9141"), %arg1142: tensor<5120xbf16> loc("p304.9187"), %arg1143: tensor<128xbf16> loc("p305.9226"), %arg1144: tensor<32x1x128x128xbf16> loc("p306.9259"), %arg1145: tensor<128x5120xbf16> loc("p307.9267"), %arg1146: tensor<32x1x128x128xbf16> loc("p308.9289"), %arg1147: tensor<128x5120xbf16> loc("p309.9302"), %arg1148: tensor<5120x3200xbf16> loc("p310.9311"), %arg1149: tensor<3200x5120xbf16> loc("p311.9316"), %arg1150: tensor<5120x1024xbf16> loc("p312.9325"), %arg1151: tensor<1024x5120xbf16> loc("p313.9366"), %arg1152: tensor<128xbf16> loc("p314.9404"), %arg1153: tensor<5120xbf16> loc("p315.9527"), %arg1154: tensor<3200x5120xbf16> loc("p316.9536"), %arg1155: tensor<5120xbf16> loc("p317.9582"), %arg1156: tensor<128xbf16> loc("p318.9621"), %arg1157: tensor<32x1x128x128xbf16> loc("p319.9654"), %arg1158: tensor<128x5120xbf16> loc("p320.9662"), %arg1159: tensor<32x1x128x128xbf16> loc("p321.9684"), %arg1160: tensor<128x5120xbf16> loc("p322.9697"), %arg1161: tensor<5120x3200xbf16> loc("p323.9706"), %arg1162: tensor<3200x5120xbf16> loc("p324.9711"), %arg1163: tensor<5120x1024xbf16> loc("p325.9720"), %arg1164: tensor<1024x5120xbf16> loc("p326.9761"), %arg1165: tensor<128xbf16> loc("p327.9799"), %arg1166: tensor<5120xbf16> loc("p328.9922"), %arg1167: tensor<3200x5120xbf16> loc("p329.9931"), %arg1168: tensor<5120xbf16> loc("p330.9977"), %arg1169: tensor<128xbf16> loc("p331.10016"), %arg1170: tensor<32x1x128x128xbf16> loc("p332.10049"), %arg1171: tensor<128x5120xbf16> loc("p333.10057"), %arg1172: tensor<32x1x128x128xbf16> loc("p334.10079"), %arg1173: tensor<128x5120xbf16> loc("p335.10092"), %arg1174: tensor<5120x3200xbf16> loc("p336.10101"), %arg1175: tensor<3200x5120xbf16> loc("p337.10106"), %arg1176: tensor<5120x1024xbf16> loc("p338.10115"), %arg1177: tensor<1024x5120xbf16> loc("p339.10156"), %arg1178: tensor<128xbf16> loc("p340.10194"), %arg1179: tensor<5120xbf16> loc("p341.10317"), %arg1180: tensor<3200x5120xbf16> loc("p342.10326"), %arg1181: tensor<5120xbf16> loc("p343.10372"), %arg1182: tensor<128xbf16> loc("p344.10411"), %arg1183: tensor<32x1x128x128xbf16> loc("p345.10444"), %arg1184: tensor<128x5120xbf16> loc("p346.10452"), %arg1185: tensor<32x1x128x128xbf16> loc("p347.10474"), %arg1186: tensor<128x5120xbf16> loc("p348.10487"), %arg1187: tensor<5120x3200xbf16> loc("p349.10496"), %arg1188: tensor<3200x5120xbf16> loc("p350.10501"), %arg1189: tensor<5120x1024xbf16> loc("p351.10510"), %arg1190: tensor<1024x5120xbf16> loc("p352.10551"), %arg1191: tensor<128xbf16> loc("p353.10589"), %arg1192: tensor<5120xbf16> loc("p354.10712"), %arg1193: tensor<3200x5120xbf16> loc("p355.10721"), %arg1194: tensor<5120xbf16> loc("p356.10767"), %arg1195: tensor<128xbf16> loc("p357.10806"), %arg1196: tensor<32x1x128x128xbf16> loc("p358.10839"), %arg1197: tensor<128x5120xbf16> loc("p359.10847"), %arg1198: tensor<32x1x128x128xbf16> loc("p360.10869"), %arg1199: tensor<128x5120xbf16> loc("p361.10882"), %arg1200: tensor<5120x3200xbf16> loc("p362.10891"), %arg1201: tensor<3200x5120xbf16> loc("p363.10896"), %arg1202: tensor<5120x1024xbf16> loc("p364.10905"), %arg1203: tensor<1024x5120xbf16> loc("p365.10946"), %arg1204: tensor<128xbf16> loc("p366.10984"), %arg1205: tensor<5120xbf16> loc("p367.11107"), %arg1206: tensor<3200x5120xbf16> loc("p368.11116"), %arg1207: tensor<5120xbf16> loc("p369.11162"), %arg1208: tensor<128xbf16> loc("p370.11201"), %arg1209: tensor<32x1x128x128xbf16> loc("p371.11234"), %arg1210: tensor<128x5120xbf16> loc("p372.11242"), %arg1211: tensor<32x1x128x128xbf16> loc("p373.11264"), %arg1212: tensor<128x5120xbf16> loc("p374.11277"), %arg1213: tensor<5120x3200xbf16> loc("p375.11286"), %arg1214: tensor<3200x5120xbf16> loc("p376.11291"), %arg1215: tensor<5120x1024xbf16> loc("p377.11300"), %arg1216: tensor<1024x5120xbf16> loc("p378.11341"), %arg1217: tensor<128xbf16> loc("p379.11379"), %arg1218: tensor<5120xbf16> loc("p380.11502"), %arg1219: tensor<3200x5120xbf16> loc("p381.11511"), %arg1220: tensor<5120xbf16> loc("p382.11557"), %arg1221: tensor<128xbf16> loc("p383.11596"), %arg1222: tensor<32x1x128x128xbf16> loc("p384.11629"), %arg1223: tensor<128x5120xbf16> loc("p385.11637"), %arg1224: tensor<32x1x128x128xbf16> loc("p386.11659"), %arg1225: tensor<128x5120xbf16> loc("p387.11672"), %arg1226: tensor<5120x3200xbf16> loc("p388.11681"), %arg1227: tensor<3200x5120xbf16> loc("p389.11686"), %arg1228: tensor<5120x1024xbf16> loc("p390.11695"), %arg1229: tensor<1024x5120xbf16> loc("p391.11736"), %arg1230: tensor<128xbf16> loc("p392.11774"), %arg1231: tensor<5120xbf16> loc("p393.11897"), %arg1232: tensor<3200x5120xbf16> loc("p394.11906"), %arg1233: tensor<5120xbf16> loc("p395.11952"), %arg1234: tensor<128xbf16> loc("p396.11991"), %arg1235: tensor<32x1x128x128xbf16> loc("p397.12024"), %arg1236: tensor<128x5120xbf16> loc("p398.12032"), %arg1237: tensor<32x1x128x128xbf16> loc("p399.12054"), %arg1238: tensor<128x5120xbf16> loc("p400.12067"), %arg1239: tensor<5120x3200xbf16> loc("p401.12076"), %arg1240: tensor<3200x5120xbf16> loc("p402.12081"), %arg1241: tensor<5120x1024xbf16> loc("p403.12090"), %arg1242: tensor<1024x5120xbf16> loc("p404.12131"), %arg1243: tensor<128xbf16> loc("p405.12169"), %arg1244: tensor<5120xbf16> loc("p406.12292"), %arg1245: tensor<3200x5120xbf16> loc("p407.12301"), %arg1246: tensor<5120xbf16> loc("p408.12347"), %arg1247: tensor<128xbf16> loc("p409.12386"), %arg1248: tensor<32x1x128x128xbf16> loc("p410.12419"), %arg1249: tensor<128x5120xbf16> loc("p411.12427"), %arg1250: tensor<32x1x128x128xbf16> loc("p412.12449"), %arg1251: tensor<128x5120xbf16> loc("p413.12462"), %arg1252: tensor<5120x3200xbf16> loc("p414.12471"), %arg1253: tensor<3200x5120xbf16> loc("p415.12476"), %arg1254: tensor<5120x1024xbf16> loc("p416.12485"), %arg1255: tensor<1024x5120xbf16> loc("p417.12526"), %arg1256: tensor<128xbf16> loc("p418.12564"), %arg1257: tensor<5120xbf16> loc("p419.12687"), %arg1258: tensor<3200x5120xbf16> loc("p420.12696"), %arg1259: tensor<5120xbf16> loc("p421.12742"), %arg1260: tensor<128xbf16> loc("p422.12781"), %arg1261: tensor<32x1x128x128xbf16> loc("p423.12814"), %arg1262: tensor<128x5120xbf16> loc("p424.12822"), %arg1263: tensor<32x1x128x128xbf16> loc("p425.12844"), %arg1264: tensor<128x5120xbf16> loc("p426.12857"), %arg1265: tensor<5120x3200xbf16> loc("p427.12866"), %arg1266: tensor<3200x5120xbf16> loc("p428.12871"), %arg1267: tensor<5120x1024xbf16> loc("p429.12880"), %arg1268: tensor<1024x5120xbf16> loc("p430.12921"), %arg1269: tensor<128xbf16> loc("p431.12959"), %arg1270: tensor<5120xbf16> loc("p432.13082"), %arg1271: tensor<3200x5120xbf16> loc("p433.13091"), %arg1272: tensor<5120xbf16> loc("p434.13137"), %arg1273: tensor<128xbf16> loc("p435.13176"), %arg1274: tensor<32x1x128x128xbf16> loc("p436.13209"), %arg1275: tensor<128x5120xbf16> loc("p437.13217"), %arg1276: tensor<32x1x128x128xbf16> loc("p438.13239"), %arg1277: tensor<128x5120xbf16> loc("p439.13252"), %arg1278: tensor<5120x3200xbf16> loc("p440.13261"), %arg1279: tensor<3200x5120xbf16> loc("p441.13266"), %arg1280: tensor<5120x1024xbf16> loc("p442.13275"), %arg1281: tensor<1024x5120xbf16> loc("p443.13316"), %arg1282: tensor<128xbf16> loc("p444.13354"), %arg1283: tensor<5120xbf16> loc("p445.13477"), %arg1284: tensor<3200x5120xbf16> loc("p446.13486"), %arg1285: tensor<5120xbf16> loc("p447.13532"), %arg1286: tensor<128xbf16> loc("p448.13571"), %arg1287: tensor<32x1x128x128xbf16> loc("p449.13604"), %arg1288: tensor<128x5120xbf16> loc("p450.13612"), %arg1289: tensor<32x1x128x128xbf16> loc("p451.13634"), %arg1290: tensor<128x5120xbf16> loc("p452.13647"), %arg1291: tensor<5120x3200xbf16> loc("p453.13656"), %arg1292: tensor<3200x5120xbf16> loc("p454.13661"), %arg1293: tensor<5120x1024xbf16> loc("p455.13670"), %arg1294: tensor<1024x5120xbf16> loc("p456.13711"), %arg1295: tensor<128xbf16> loc("p457.13749"), %arg1296: tensor<5120xbf16> loc("p458.13872"), %arg1297: tensor<3200x5120xbf16> loc("p459.13881"), %arg1298: tensor<5120xbf16> loc("p460.13927"), %arg1299: tensor<128xbf16> loc("p461.13966"), %arg1300: tensor<32x1x128x128xbf16> loc("p462.13999"), %arg1301: tensor<128x5120xbf16> loc("p463.14007"), %arg1302: tensor<32x1x128x128xbf16> loc("p464.14029"), %arg1303: tensor<128x5120xbf16> loc("p465.14042"), %arg1304: tensor<5120x3200xbf16> loc("p466.14051"), %arg1305: tensor<3200x5120xbf16> loc("p467.14056"), %arg1306: tensor<5120x1024xbf16> loc("p468.14065"), %arg1307: tensor<1024x5120xbf16> loc("p469.14106"), %arg1308: tensor<128xbf16> loc("p470.14144"), %arg1309: tensor<5120xbf16> loc("p471.14267"), %arg1310: tensor<3200x5120xbf16> loc("p472.14276"), %arg1311: tensor<5120xbf16> loc("p473.14322"), %arg1312: tensor<128xbf16> loc("p474.14361"), %arg1313: tensor<32x1x128x128xbf16> loc("p475.14394"), %arg1314: tensor<128x5120xbf16> loc("p476.14402"), %arg1315: tensor<32x1x128x128xbf16> loc("p477.14424"), %arg1316: tensor<128x5120xbf16> loc("p478.14437"), %arg1317: tensor<5120x3200xbf16> loc("p479.14446"), %arg1318: tensor<3200x5120xbf16> loc("p480.14451"), %arg1319: tensor<5120x1024xbf16> loc("p481.14460"), %arg1320: tensor<1024x5120xbf16> loc("p482.14501"), %arg1321: tensor<128xbf16> loc("p483.14539"), %arg1322: tensor<5120xbf16> loc("p484.14662"), %arg1323: tensor<3200x5120xbf16> loc("p485.14671"), %arg1324: tensor<5120xbf16> loc("p486.14717"), %arg1325: tensor<128xbf16> loc("p487.14756"), %arg1326: tensor<32x1x128x128xbf16> loc("p488.14789"), %arg1327: tensor<128x5120xbf16> loc("p489.14797"), %arg1328: tensor<32x1x128x128xbf16> loc("p490.14819"), %arg1329: tensor<128x5120xbf16> loc("p491.14832"), %arg1330: tensor<5120x3200xbf16> loc("p492.14841"), %arg1331: tensor<3200x5120xbf16> loc("p493.14846"), %arg1332: tensor<5120x1024xbf16> loc("p494.14855"), %arg1333: tensor<1024x5120xbf16> loc("p495.14896"), %arg1334: tensor<128xbf16> loc("p496.14934"), %arg1335: tensor<5120xbf16> loc("p497.15057"), %arg1336: tensor<3200x5120xbf16> loc("p498.15066"), %arg1337: tensor<5120xbf16> loc("p499.15112"), %arg1338: tensor<128xbf16> loc("p500.15151"), %arg1339: tensor<32x1x128x128xbf16> loc("p501.15184"), %arg1340: tensor<128x5120xbf16> loc("p502.15192"), %arg1341: tensor<32x1x128x128xbf16> loc("p503.15214"), %arg1342: tensor<128x5120xbf16> loc("p504.15227"), %arg1343: tensor<5120x3200xbf16> loc("p505.15236"), %arg1344: tensor<3200x5120xbf16> loc("p506.15241"), %arg1345: tensor<5120x1024xbf16> loc("p507.15250"), %arg1346: tensor<1024x5120xbf16> loc("p508.15291"), %arg1347: tensor<128xbf16> loc("p509.15329"), %arg1348: tensor<5120xbf16> loc("p510.15452"), %arg1349: tensor<3200x5120xbf16> loc("p511.15461"), %arg1350: tensor<5120xbf16> loc("p512.15507"), %arg1351: tensor<128xbf16> loc("p513.15546"), %arg1352: tensor<32x1x128x128xbf16> loc("p514.15579"), %arg1353: tensor<128x5120xbf16> loc("p515.15587"), %arg1354: tensor<32x1x128x128xbf16> loc("p516.15609"), %arg1355: tensor<128x5120xbf16> loc("p517.15622"), %arg1356: tensor<5120x3200xbf16> loc("p518.15631"), %arg1357: tensor<3200x5120xbf16> loc("p519.15636"), %arg1358: tensor<5120x1024xbf16> loc("p520.15645"), %arg1359: tensor<1024x5120xbf16> loc("p521.15686"), %arg1360: tensor<128xbf16> loc("p522.15724"), %arg1361: tensor<5120xbf16> loc("p523.15847"), %arg1362: tensor<3200x5120xbf16> loc("p524.15856"), %arg1363: tensor<5120xbf16> loc("p525.15902"), %arg1364: tensor<128xbf16> loc("p526.15941"), %arg1365: tensor<32x1x128x128xbf16> loc("p527.15974"), %arg1366: tensor<128x5120xbf16> loc("p528.15982"), %arg1367: tensor<32x1x128x128xbf16> loc("p529.16004"), %arg1368: tensor<128x5120xbf16> loc("p530.16017"), %arg1369: tensor<5120x3200xbf16> loc("p531.16026"), %arg1370: tensor<3200x5120xbf16> loc("p532.16031"), %arg1371: tensor<5120x1024xbf16> loc("p533.16040"), %arg1372: tensor<1024x5120xbf16> loc("p534.16081"), %arg1373: tensor<128xbf16> loc("p535.16119"), %arg1374: tensor<5120xbf16> loc("p536.16242"), %arg1375: tensor<3200x5120xbf16> loc("p537.16251"), %arg1376: tensor<5120xbf16> loc("p538.16297"), %arg1377: tensor<128xbf16> loc("p539.16336"), %arg1378: tensor<32x1x128x128xbf16> loc("p540.16369"), %arg1379: tensor<128x5120xbf16> loc("p541.16377"), %arg1380: tensor<32x1x128x128xbf16> loc("p542.16399"), %arg1381: tensor<128x5120xbf16> loc("p543.16412"), %arg1382: tensor<5120x3200xbf16> loc("p544.16421"), %arg1383: tensor<3200x5120xbf16> loc("p545.16426"), %arg1384: tensor<5120x1024xbf16> loc("p546.16435"), %arg1385: tensor<1024x5120xbf16> loc("p547.16476"), %arg1386: tensor<128xbf16> loc("p548.16514"), %arg1387: tensor<5120xbf16> loc("p549.16637"), %arg1388: tensor<3200x5120xbf16> loc("p550.16646"), %arg1389: tensor<5120xbf16> loc("p551.16692"), %arg1390: tensor<128xbf16> loc("p552.16731"), %arg1391: tensor<32x1x128x128xbf16> loc("p553.16764"), %arg1392: tensor<128x5120xbf16> loc("p554.16772"), %arg1393: tensor<32x1x128x128xbf16> loc("p555.16794"), %arg1394: tensor<128x5120xbf16> loc("p556.16807"), %arg1395: tensor<5120x3200xbf16> loc("p557.16816"), %arg1396: tensor<3200x5120xbf16> loc("p558.16821"), %arg1397: tensor<5120x1024xbf16> loc("p559.16830"), %arg1398: tensor<1024x5120xbf16> loc("p560.16871"), %arg1399: tensor<128xbf16> loc("p561.16909"), %arg1400: tensor<5120xbf16> loc("p562.17032"), %arg1401: tensor<3200x5120xbf16> loc("p563.17041"), %arg1402: tensor<5120xbf16> loc("p564.17087"), %arg1403: tensor<128xbf16> loc("p565.17126"), %arg1404: tensor<32x1x128x128xbf16> loc("p566.17159"), %arg1405: tensor<128x5120xbf16> loc("p567.17167"), %arg1406: tensor<32x1x128x128xbf16> loc("p568.17189"), %arg1407: tensor<128x5120xbf16> loc("p569.17202"), %arg1408: tensor<5120x3200xbf16> loc("p570.17211"), %arg1409: tensor<3200x5120xbf16> loc("p571.17216"), %arg1410: tensor<5120x1024xbf16> loc("p572.17225"), %arg1411: tensor<1024x5120xbf16> loc("p573.17266"), %arg1412: tensor<128xbf16> loc("p574.17304"), %arg1413: tensor<5120xbf16> loc("p575.17427"), %arg1414: tensor<3200x5120xbf16> loc("p576.17436"), %arg1415: tensor<5120xbf16> loc("p577.17482"), %arg1416: tensor<128xbf16> loc("p578.17521"), %arg1417: tensor<32x1x128x128xbf16> loc("p579.17554"), %arg1418: tensor<128x5120xbf16> loc("p580.17562"), %arg1419: tensor<32x1x128x128xbf16> loc("p581.17584"), %arg1420: tensor<128x5120xbf16> loc("p582.17597"), %arg1421: tensor<5120x3200xbf16> loc("p583.17606"), %arg1422: tensor<3200x5120xbf16> loc("p584.17611"), %arg1423: tensor<5120x1024xbf16> loc("p585.17620"), %arg1424: tensor<1024x5120xbf16> loc("p586.17661"), %arg1425: tensor<128xbf16> loc("p587.17699"), %arg1426: tensor<5120xbf16> loc("p588.17822"), %arg1427: tensor<3200x5120xbf16> loc("p589.17831"), %arg1428: tensor<5120xbf16> loc("p590.17877"), %arg1429: tensor<128xbf16> loc("p591.17916"), %arg1430: tensor<32x1x128x128xbf16> loc("p592.17949"), %arg1431: tensor<128x5120xbf16> loc("p593.17957"), %arg1432: tensor<32x1x128x128xbf16> loc("p594.17979"), %arg1433: tensor<128x5120xbf16> loc("p595.17992"), %arg1434: tensor<5120x3200xbf16> loc("p596.18001"), %arg1435: tensor<3200x5120xbf16> loc("p597.18006"), %arg1436: tensor<5120x1024xbf16> loc("p598.18015"), %arg1437: tensor<1024x5120xbf16> loc("p599.18056"), %arg1438: tensor<128xbf16> loc("p600.18094"), %arg1439: tensor<5120xbf16> loc("p601.18217"), %arg1440: tensor<3200x5120xbf16> loc("p602.18226"), %arg1441: tensor<5120xbf16> loc("p603.18272"), %arg1442: tensor<128xbf16> loc("p604.18311"), %arg1443: tensor<32x1x128x128xbf16> loc("p605.18344"), %arg1444: tensor<128x5120xbf16> loc("p606.18352"), %arg1445: tensor<32x1x128x128xbf16> loc("p607.18374"), %arg1446: tensor<128x5120xbf16> loc("p608.18387"), %arg1447: tensor<5120x3200xbf16> loc("p609.18396"), %arg1448: tensor<3200x5120xbf16> loc("p610.18401"), %arg1449: tensor<5120x1024xbf16> loc("p611.18410"), %arg1450: tensor<1024x5120xbf16> loc("p612.18451"), %arg1451: tensor<128xbf16> loc("p613.18489"), %arg1452: tensor<5120xbf16> loc("p614.18612"), %arg1453: tensor<3200x5120xbf16> loc("p615.18621"), %arg1454: tensor<5120xbf16> loc("p616.18667"), %arg1455: tensor<128xbf16> loc("p617.18706"), %arg1456: tensor<32x1x128x128xbf16> loc("p618.18739"), %arg1457: tensor<128x5120xbf16> loc("p619.18747"), %arg1458: tensor<32x1x128x128xbf16> loc("p620.18769"), %arg1459: tensor<128x5120xbf16> loc("p621.18782"), %arg1460: tensor<5120x3200xbf16> loc("p622.18791"), %arg1461: tensor<3200x5120xbf16> loc("p623.18796"), %arg1462: tensor<5120x1024xbf16> loc("p624.18805"), %arg1463: tensor<1024x5120xbf16> loc("p625.18846"), %arg1464: tensor<128xbf16> loc("p626.18884"), %arg1465: tensor<5120xbf16> loc("p627.19007"), %arg1466: tensor<3200x5120xbf16> loc("p628.19016"), %arg1467: tensor<5120xbf16> loc("p629.19062"), %arg1468: tensor<128xbf16> loc("p630.19101"), %arg1469: tensor<32x1x128x128xbf16> loc("p631.19134"), %arg1470: tensor<128x5120xbf16> loc("p632.19142"), %arg1471: tensor<32x1x128x128xbf16> loc("p633.19164"), %arg1472: tensor<128x5120xbf16> loc("p634.19177"), %arg1473: tensor<5120x3200xbf16> loc("p635.19186"), %arg1474: tensor<3200x5120xbf16> loc("p636.19191"), %arg1475: tensor<5120x1024xbf16> loc("p637.19200"), %arg1476: tensor<1024x5120xbf16> loc("p638.19241"), %arg1477: tensor<128xbf16> loc("p639.19279"), %arg1478: tensor<5120xbf16> loc("p640.19402"), %arg1479: tensor<3200x5120xbf16> loc("p641.19411"), %arg1480: tensor<5120xbf16> loc("p642.19457"), %arg1481: tensor<128xbf16> loc("p643.19496"), %arg1482: tensor<32x1x128x128xbf16> loc("p644.19529"), %arg1483: tensor<128x5120xbf16> loc("p645.19537"), %arg1484: tensor<32x1x128x128xbf16> loc("p646.19559"), %arg1485: tensor<128x5120xbf16> loc("p647.19572"), %arg1486: tensor<5120x3200xbf16> loc("p648.19581"), %arg1487: tensor<3200x5120xbf16> loc("p649.19586"), %arg1488: tensor<5120x1024xbf16> loc("p650.19595"), %arg1489: tensor<1024x5120xbf16> loc("p651.19636"), %arg1490: tensor<128xbf16> loc("p652.19674"), %arg1491: tensor<5120xbf16> loc("p653.19797"), %arg1492: tensor<3200x5120xbf16> loc("p654.19806"), %arg1493: tensor<5120xbf16> loc("p655.19852"), %arg1494: tensor<128xbf16> loc("p656.19891"), %arg1495: tensor<32x1x128x128xbf16> loc("p657.19924"), %arg1496: tensor<128x5120xbf16> loc("p658.19932"), %arg1497: tensor<32x1x128x128xbf16> loc("p659.19954"), %arg1498: tensor<128x5120xbf16> loc("p660.19967"), %arg1499: tensor<5120x3200xbf16> loc("p661.19976"), %arg1500: tensor<3200x5120xbf16> loc("p662.19981"), %arg1501: tensor<5120x1024xbf16> loc("p663.19990"), %arg1502: tensor<1024x5120xbf16> loc("p664.20031"), %arg1503: tensor<128xbf16> loc("p665.20069"), %arg1504: tensor<5120xbf16> loc("p666.20192"), %arg1505: tensor<3200x5120xbf16> loc("p667.20201"), %arg1506: tensor<5120xbf16> loc("p668.20247"), %arg1507: tensor<128xbf16> loc("p669.20286"), %arg1508: tensor<32x1x128x128xbf16> loc("p670.20319"), %arg1509: tensor<128x5120xbf16> loc("p671.20327"), %arg1510: tensor<32x1x128x128xbf16> loc("p672.20349"), %arg1511: tensor<128x5120xbf16> loc("p673.20362"), %arg1512: tensor<5120x3200xbf16> loc("p674.20371"), %arg1513: tensor<3200x5120xbf16> loc("p675.20376"), %arg1514: tensor<5120x1024xbf16> loc("p676.20385"), %arg1515: tensor<1024x5120xbf16> loc("p677.20426"), %arg1516: tensor<128xbf16> loc("p678.20464"), %arg1517: tensor<5120xbf16> loc("p679.20587"), %arg1518: tensor<3200x5120xbf16> loc("p680.20596"), %arg1519: tensor<5120xbf16> loc("p681.20642"), %arg1520: tensor<128xbf16> loc("p682.20681"), %arg1521: tensor<32x1x128x128xbf16> loc("p683.20714"), %arg1522: tensor<128x5120xbf16> loc("p684.20722"), %arg1523: tensor<32x1x128x128xbf16> loc("p685.20744"), %arg1524: tensor<128x5120xbf16> loc("p686.20757"), %arg1525: tensor<5120x3200xbf16> loc("p687.20766"), %arg1526: tensor<3200x5120xbf16> loc("p688.20771"), %arg1527: tensor<5120x1024xbf16> loc("p689.20780"), %arg1528: tensor<1024x5120xbf16> loc("p690.20821"), %arg1529: tensor<128xbf16> loc("p691.20859"), %arg1530: tensor<5120xbf16> loc("p692.20982"), %arg1531: tensor<3200x5120xbf16> loc("p693.20991"), %arg1532: tensor<5120xbf16> loc("p694.21037"), %arg1533: tensor<128xbf16> loc("p695.21076"), %arg1534: tensor<32x1x128x128xbf16> loc("p696.21109"), %arg1535: tensor<128x5120xbf16> loc("p697.21117"), %arg1536: tensor<32x1x128x128xbf16> loc("p698.21139"), %arg1537: tensor<128x5120xbf16> loc("p699.21152"), %arg1538: tensor<5120x3200xbf16> loc("p700.21161"), %arg1539: tensor<3200x5120xbf16> loc("p701.21166"), %arg1540: tensor<5120x1024xbf16> loc("p702.21175"), %arg1541: tensor<1024x5120xbf16> loc("p703.21216"), %arg1542: tensor<128xbf16> loc("p704.21254"), %arg1543: tensor<5120xbf16> loc("p705.21377"), %arg1544: tensor<3200x5120xbf16> loc("p706.21386"), %arg1545: tensor<5120xbf16> loc("p707.21432"), %arg1546: tensor<128xbf16> loc("p708.21471"), %arg1547: tensor<32x1x128x128xbf16> loc("p709.21504"), %arg1548: tensor<128x5120xbf16> loc("p710.21512"), %arg1549: tensor<32x1x128x128xbf16> loc("p711.21534"), %arg1550: tensor<128x5120xbf16> loc("p712.21547"), %arg1551: tensor<5120x3200xbf16> loc("p713.21556"), %arg1552: tensor<3200x5120xbf16> loc("p714.21561"), %arg1553: tensor<5120x1024xbf16> loc("p715.21570"), %arg1554: tensor<1024x5120xbf16> loc("p716.21611"), %arg1555: tensor<128xbf16> loc("p717.21649"), %arg1556: tensor<5120xbf16> loc("p718.21772"), %arg1557: tensor<3200x5120xbf16> loc("p719.21781"), %arg1558: tensor<5120xbf16> loc("p720.21827"), %arg1559: tensor<128xbf16> loc("p721.21866"), %arg1560: tensor<32x1x128x128xbf16> loc("p722.21899"), %arg1561: tensor<128x5120xbf16> loc("p723.21907"), %arg1562: tensor<32x1x128x128xbf16> loc("p724.21929"), %arg1563: tensor<128x5120xbf16> loc("p725.21942"), %arg1564: tensor<5120x3200xbf16> loc("p726.21951"), %arg1565: tensor<3200x5120xbf16> loc("p727.21956"), %arg1566: tensor<5120x1024xbf16> loc("p728.21965"), %arg1567: tensor<1024x5120xbf16> loc("p729.22006"), %arg1568: tensor<128xbf16> loc("p730.22044"), %arg1569: tensor<5120xbf16> loc("p731.22167"), %arg1570: tensor<3200x5120xbf16> loc("p732.22176"), %arg1571: tensor<5120xbf16> loc("p733.22222"), %arg1572: tensor<128xbf16> loc("p734.22261"), %arg1573: tensor<32x1x128x128xbf16> loc("p735.22294"), %arg1574: tensor<128x5120xbf16> loc("p736.22302"), %arg1575: tensor<32x1x128x128xbf16> loc("p737.22324"), %arg1576: tensor<128x5120xbf16> loc("p738.22337"), %arg1577: tensor<5120x3200xbf16> loc("p739.22346"), %arg1578: tensor<3200x5120xbf16> loc("p740.22351"), %arg1579: tensor<5120x1024xbf16> loc("p741.22360"), %arg1580: tensor<1024x5120xbf16> loc("p742.22401"), %arg1581: tensor<128xbf16> loc("p743.22439"), %arg1582: tensor<5120xbf16> loc("p744.22562"), %arg1583: tensor<3200x5120xbf16> loc("p745.22571"), %arg1584: tensor<5120xbf16> loc("p746.22617"), %arg1585: tensor<128xbf16> loc("p747.22656"), %arg1586: tensor<32x1x128x128xbf16> loc("p748.22689"), %arg1587: tensor<128x5120xbf16> loc("p749.22697"), %arg1588: tensor<32x1x128x128xbf16> loc("p750.22719"), %arg1589: tensor<128x5120xbf16> loc("p751.22732"), %arg1590: tensor<5120x3200xbf16> loc("p752.22741"), %arg1591: tensor<3200x5120xbf16> loc("p753.22746"), %arg1592: tensor<5120x1024xbf16> loc("p754.22755"), %arg1593: tensor<1024x5120xbf16> loc("p755.22796"), %arg1594: tensor<128xbf16> loc("p756.22834"), %arg1595: tensor<5120xbf16> loc("p757.22957"), %arg1596: tensor<3200x5120xbf16> loc("p758.22966"), %arg1597: tensor<5120xbf16> loc("p759.23012"), %arg1598: tensor<128xbf16> loc("p760.23051"), %arg1599: tensor<32x1x128x128xbf16> loc("p761.23084"), %arg1600: tensor<128x5120xbf16> loc("p762.23092"), %arg1601: tensor<32x1x128x128xbf16> loc("p763.23114"), %arg1602: tensor<128x5120xbf16> loc("p764.23127"), %arg1603: tensor<5120x3200xbf16> loc("p765.23136"), %arg1604: tensor<3200x5120xbf16> loc("p766.23141"), %arg1605: tensor<5120x1024xbf16> loc("p767.23150"), %arg1606: tensor<1024x5120xbf16> loc("p768.23191"), %arg1607: tensor<128xbf16> loc("p769.23229"), %arg1608: tensor<5120xbf16> loc("p770.23352"), %arg1609: tensor<3200x5120xbf16> loc("p771.23361"), %arg1610: tensor<5120xbf16> loc("p772.23407"), %arg1611: tensor<128xbf16> loc("p773.23446"), %arg1612: tensor<32x1x128x128xbf16> loc("p774.23479"), %arg1613: tensor<128x5120xbf16> loc("p775.23487"), %arg1614: tensor<32x1x128x128xbf16> loc("p776.23509"), %arg1615: tensor<128x5120xbf16> loc("p777.23522"), %arg1616: tensor<5120x3200xbf16> loc("p778.23531"), %arg1617: tensor<3200x5120xbf16> loc("p779.23536"), %arg1618: tensor<5120x1024xbf16> loc("p780.23545"), %arg1619: tensor<1024x5120xbf16> loc("p781.23586"), %arg1620: tensor<128xbf16> loc("p782.23624"), %arg1621: tensor<5120xbf16> loc("p783.23747"), %arg1622: tensor<3200x5120xbf16> loc("p784.23756"), %arg1623: tensor<5120xbf16> loc("p785.23802"), %arg1624: tensor<128xbf16> loc("p786.23841"), %arg1625: tensor<32x1x128x128xbf16> loc("p787.23874"), %arg1626: tensor<128x5120xbf16> loc("p788.23882"), %arg1627: tensor<32x1x128x128xbf16> loc("p789.23904"), %arg1628: tensor<128x5120xbf16> loc("p790.23917"), %arg1629: tensor<5120x3200xbf16> loc("p791.23926"), %arg1630: tensor<3200x5120xbf16> loc("p792.23931"), %arg1631: tensor<5120x1024xbf16> loc("p793.23940"), %arg1632: tensor<1024x5120xbf16> loc("p794.23981"), %arg1633: tensor<128xbf16> loc("p795.24019"), %arg1634: tensor<5120xbf16> loc("p796.24142"), %arg1635: tensor<3200x5120xbf16> loc("p797.24151"), %arg1636: tensor<5120xbf16> loc("p798.24197"), %arg1637: tensor<128xbf16> loc("p799.24236"), %arg1638: tensor<32x1x128x128xbf16> loc("p800.24269"), %arg1639: tensor<128x5120xbf16> loc("p801.24277"), %arg1640: tensor<32x1x128x128xbf16> loc("p802.24299"), %arg1641: tensor<128x5120xbf16> loc("p803.24312"), %arg1642: tensor<5120x3200xbf16> loc("p804.24321"), %arg1643: tensor<3200x5120xbf16> loc("p805.24326"), %arg1644: tensor<5120x1024xbf16> loc("p806.24335"), %arg1645: tensor<1024x5120xbf16> loc("p807.24376"), %arg1646: tensor<128xbf16> loc("p808.24414"), %arg1647: tensor<5120xbf16> loc("p809.24537"), %arg1648: tensor<3200x5120xbf16> loc("p810.24546"), %arg1649: tensor<5120xbf16> loc("p811.24592"), %arg1650: tensor<128xbf16> loc("p812.24631"), %arg1651: tensor<32x1x128x128xbf16> loc("p813.24664"), %arg1652: tensor<128x5120xbf16> loc("p814.24672"), %arg1653: tensor<32x1x128x128xbf16> loc("p815.24694"), %arg1654: tensor<128x5120xbf16> loc("p816.24707"), %arg1655: tensor<5120x3200xbf16> loc("p817.24716"), %arg1656: tensor<3200x5120xbf16> loc("p818.24721"), %arg1657: tensor<5120x1024xbf16> loc("p819.24730"), %arg1658: tensor<1024x5120xbf16> loc("p820.24771"), %arg1659: tensor<128xbf16> loc("p821.24809"), %arg1660: tensor<5120xbf16> loc("p822.24932"), %arg1661: tensor<3200x5120xbf16> loc("p823.24941"), %arg1662: tensor<5120xbf16> loc("p824.24987"), %arg1663: tensor<128xbf16> loc("p825.25026"), %arg1664: tensor<32x1x128x128xbf16> loc("p826.25059"), %arg1665: tensor<128x5120xbf16> loc("p827.25067"), %arg1666: tensor<32x1x128x128xbf16> loc("p828.25089"), %arg1667: tensor<18992x5120xbf16> loc("p829.25097"), %arg1668: tensor<5120x3200xbf16> loc("p830.25106"), %arg1669: tensor<3200x5120xbf16> loc("p831.25111"), %arg1670: tensor<5120x1024xbf16> loc("p832.25120"), %arg1671: tensor<1024x5120xbf16> loc("p833.25161"), %arg1672: tensor<128xbf16> loc("p834.25199"), %arg1673: tensor<5120xbf16> loc("p835.25322"), %arg1674: tensor<3200x5120xbf16> loc("p836.25331"), %arg1675: tensor<5120xbf16> loc("p837.25377")) {
      %c = stablehlo.constant dense<true> : tensor<i1> loc(#loc)
      %c_0 = stablehlo.constant dense<"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tensor<128xi64> loc(#loc)
      %c_1 = stablehlo.constant dense<false> : tensor<i1> loc(#loc)
      %cst = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
      %c_2 = stablehlo.constant dense<0> : tensor<17xi64> loc(#loc)
      %c_3 = stablehlo.constant dense<128> : tensor<17xi64> loc(#loc)
      %cst_4 = stablehlo.constant dense<1.95312503E-4> : tensor<f32> loc(#loc)
      %cst_5 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
      %cst_6 = stablehlo.constant dense<7.812500e-03> : tensor<f32> loc(#loc)
      %cst_7 = stablehlo.constant dense<9.99999997E-7> : tensor<f32> loc(#loc)
      %cst_8 = stablehlo.constant dense<0.297301769> : tensor<f32> loc(#loc)
      %cst_9 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
      %cst_10 = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
      %cst_11 = stablehlo.constant dense<0xFFF0000000000000> : tensor<f64> loc(#loc)
      %cst_12 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %3 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x1x17x128xbf16> loc(#loc)
      %4 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<32x1x17x128xbf16> loc(#loc)
      %5 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %9 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %10 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %11 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %12 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %13 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %14 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %15 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %16 = stablehlo.reshape %arg838 : (tensor<17xi64>) -> tensor<1x1x17xi64> loc(#loc839)
      %17 = stablehlo.reshape %16 : (tensor<1x1x17xi64>) -> tensor<17xi64> loc(#loc840)
      %18 = stablehlo.compare  LT, %17, %c_2 : (tensor<17xi64>, tensor<17xi64>) -> tensor<17xi1> loc(#loc841)
      %19 = stablehlo.add %17, %c_3 : tensor<17xi64> loc(#loc842)
      %20 = stablehlo.select %18, %19, %17 : tensor<17xi1>, tensor<17xi64> loc(#loc843)
      %21 = stablehlo.reshape %20 : (tensor<17xi64>) -> tensor<17x1xi64> loc(#loc844)
      %22 = stablehlo.reshape %arg844 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc845)
      %23 = stablehlo.reshape %22 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc846)
      %24 = stablehlo.broadcast_in_dim %23, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc847)
      %25 = stablehlo.reshape %arg843 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc848)
      %26 = stablehlo.reshape %25 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc849)
      %27 = stablehlo.broadcast_in_dim %26, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc850)
      %28 = stablehlo.reshape %arg842 : (tensor<151936x5120xbf16>) -> tensor<1x151936x5120xbf16> loc(#loc851)
      %29 = stablehlo.reshape %28 : (tensor<1x151936x5120xbf16>) -> tensor<151936x5120xbf16> loc(#loc852)
      %30 = stablehlo.reshape %arg841 : (tensor<32x17xi64>) -> tensor<1x32x17xi64> loc(#loc853)
      %31 = stablehlo.reshape %30 : (tensor<1x32x17xi64>) -> tensor<544xi64> loc(#loc854)
      %32 = stablehlo.convert %31 : (tensor<544xi64>) -> tensor<544xui32> loc(#loc855)
      %33 = "stablehlo.gather"(%29, %32) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 5120>}> : (tensor<151936x5120xbf16>, tensor<544xui32>) -> tensor<544x5120xbf16> loc(#loc856)
      %34 = stablehlo.reshape %33 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc857)
      %35 = stablehlo.convert %34 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc858)
      %36 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %37 = stablehlo.power %35, %36 : tensor<32x17x5120xf32> loc(#loc859)
      %38 = stablehlo.reduce(%37 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc860)
      %39 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %40 = stablehlo.multiply %38, %39 : tensor<32x17xf32> loc(#loc861)
      %41 = stablehlo.reshape %40 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc862)
      %42 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %43 = stablehlo.add %41, %42 : tensor<32x17x1xf32> loc(#loc863)
      %44 = stablehlo.rsqrt %43 : tensor<32x17x1xf32> loc(#loc864)
      %45 = stablehlo.reshape %44 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc865)
      %46 = stablehlo.broadcast_in_dim %45, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc866)
      %47 = stablehlo.multiply %35, %46 : tensor<32x17x5120xf32> loc(#loc867)
      %48 = stablehlo.convert %47 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc868)
      %49 = stablehlo.multiply %27, %48 : tensor<32x17x5120xbf16> loc(#loc869)
      %50 = stablehlo.reshape %49 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc870)
      %51 = stablehlo.reshape %arg840 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc871)
      %52 = stablehlo.reshape %51 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc872)
      %53 = stablehlo.transpose %52, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc873)
      %54 = stablehlo.dot_general %50, %53, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc874)
      %55 = stablehlo.reshape %54 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc875)
      %56 = stablehlo.convert %55 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc876)
      %57 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %58 = stablehlo.power %56, %57 : tensor<32x17x1x128xf32> loc(#loc877)
      %59 = stablehlo.reduce(%58 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc878)
      %60 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %61 = stablehlo.multiply %59, %60 : tensor<32x17x1xf32> loc(#loc879)
      %62 = stablehlo.reshape %61 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc880)
      %63 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %64 = stablehlo.add %62, %63 : tensor<32x17x1x1xf32> loc(#loc881)
      %65 = stablehlo.rsqrt %64 : tensor<32x17x1x1xf32> loc(#loc882)
      %66 = stablehlo.reshape %65 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc883)
      %67 = stablehlo.broadcast_in_dim %66, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc884)
      %68 = stablehlo.multiply %56, %67 : tensor<32x17x1x128xf32> loc(#loc885)
      %69 = stablehlo.convert %68 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc886)
      %70 = stablehlo.multiply %24, %69 : tensor<32x17x1x128xbf16> loc(#loc887)
      %71 = stablehlo.transpose %70, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc888)
      %72 = stablehlo.reshape %arg839 : (tensor<64xbf16>) -> tensor<1x1x64xbf16> loc(#loc889)
      %73 = stablehlo.reshape %72 : (tensor<1x1x64xbf16>) -> tensor<1x64x1xbf16> loc(#loc890)
      %74 = stablehlo.convert %73 : (tensor<1x64x1xbf16>) -> tensor<1x64x1xf32> loc(#loc891)
      %75 = stablehlo.convert %16 : (tensor<1x1x17xi64>) -> tensor<1x1x17xf32> loc(#loc892)
      %76 = stablehlo.dot_general %74, %75, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x17xf32>) -> tensor<1x64x17xf32> loc(#loc893)
      %77 = stablehlo.transpose %76, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,17,64]{1,2,0}"} : (tensor<1x64x17xf32>) -> tensor<1x17x64xf32> loc(#loc894)
      %78 = stablehlo.concatenate %77, %77, dim = 2 : (tensor<1x17x64xf32>, tensor<1x17x64xf32>) -> tensor<1x17x128xf32> loc(#loc895)
      %79 = stablehlo.cosine %78 : tensor<1x17x128xf32> loc(#loc896)
      %80 = stablehlo.convert %79 : (tensor<1x17x128xf32>) -> tensor<1x17x128xbf16> loc(#loc897)
      %81 = stablehlo.reshape %80 : (tensor<1x17x128xbf16>) -> tensor<17x128xbf16> loc(#loc898)
      %82 = stablehlo.broadcast_in_dim %81, dims = [2, 3] : (tensor<17x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc899)
      %83 = stablehlo.multiply %71, %82 : tensor<32x1x17x128xbf16> loc(#loc900)
      %84 = stablehlo.slice %71 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc901)
      %85 = stablehlo.negate %84 : tensor<32x1x17x64xbf16> loc(#loc902)
      %86 = stablehlo.slice %71 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc903)
      %87 = stablehlo.concatenate %85, %86, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc904)
      %88 = stablehlo.sine %78 : tensor<1x17x128xf32> loc(#loc905)
      %89 = stablehlo.convert %88 : (tensor<1x17x128xf32>) -> tensor<1x17x128xbf16> loc(#loc906)
      %90 = stablehlo.reshape %89 : (tensor<1x17x128xbf16>) -> tensor<17x128xbf16> loc(#loc907)
      %91 = stablehlo.broadcast_in_dim %90, dims = [2, 3] : (tensor<17x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc908)
      %92 = stablehlo.multiply %87, %91 : tensor<32x1x17x128xbf16> loc(#loc909)
      %93 = stablehlo.add %83, %92 : tensor<32x1x17x128xbf16> loc(#loc910)
      %94 = "stablehlo.scatter"(%arg845, %21, %93) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.163"), %arg1677: tensor<bf16> loc("scatter.163")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc911)
      %95 = stablehlo.reshape %arg846 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc912)
      %96 = stablehlo.reshape %95 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc913)
      %97 = stablehlo.transpose %96, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc914)
      %98 = stablehlo.dot_general %50, %97, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc915)
      %99 = stablehlo.reshape %98 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc916)
      %100 = stablehlo.transpose %99, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc917)
      %101 = "stablehlo.scatter"(%arg847, %21, %100) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.193"), %arg1677: tensor<bf16> loc("scatter.193")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc918)
      %102 = stablehlo.reshape %arg857 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc919)
      %103 = stablehlo.reshape %102 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc920)
      %104 = stablehlo.broadcast_in_dim %103, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc921)
      %105 = stablehlo.reshape %arg856 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc922)
      %106 = stablehlo.reshape %105 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc923)
      %107 = stablehlo.broadcast_in_dim %106, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc924)
      %108 = stablehlo.reshape %arg853 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc925)
      %109 = stablehlo.reshape %108 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc926)
      %110 = stablehlo.broadcast_in_dim %109, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc927)
      %111 = stablehlo.reshape %arg852 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc928)
      %112 = stablehlo.reshape %111 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc929)
      %113 = stablehlo.transpose %112, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc930)
      %114 = stablehlo.dot_general %50, %113, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc931)
      %115 = stablehlo.reshape %114 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc932)
      %116 = stablehlo.convert %115 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc933)
      %117 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %118 = stablehlo.power %116, %117 : tensor<32x17x8x128xf32> loc(#loc934)
      %119 = stablehlo.reduce(%118 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc935)
      %120 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %121 = stablehlo.multiply %119, %120 : tensor<32x17x8xf32> loc(#loc936)
      %122 = stablehlo.reshape %121 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc937)
      %123 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %124 = stablehlo.add %122, %123 : tensor<32x17x8x1xf32> loc(#loc938)
      %125 = stablehlo.rsqrt %124 : tensor<32x17x8x1xf32> loc(#loc939)
      %126 = stablehlo.reshape %125 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc940)
      %127 = stablehlo.broadcast_in_dim %126, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc941)
      %128 = stablehlo.multiply %116, %127 : tensor<32x17x8x128xf32> loc(#loc942)
      %129 = stablehlo.convert %128 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc943)
      %130 = stablehlo.multiply %110, %129 : tensor<32x17x8x128xbf16> loc(#loc944)
      %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc945)
      %132 = stablehlo.broadcast_in_dim %81, dims = [2, 3] : (tensor<17x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc946)
      %133 = stablehlo.multiply %131, %132 : tensor<32x8x17x128xbf16> loc(#loc947)
      %134 = stablehlo.slice %131 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc948)
      %135 = stablehlo.negate %134 : tensor<32x8x17x64xbf16> loc(#loc949)
      %136 = stablehlo.slice %131 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc950)
      %137 = stablehlo.concatenate %135, %136, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc951)
      %138 = stablehlo.broadcast_in_dim %90, dims = [2, 3] : (tensor<17x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc952)
      %139 = stablehlo.multiply %137, %138 : tensor<32x8x17x128xbf16> loc(#loc953)
      %140 = stablehlo.add %133, %139 : tensor<32x8x17x128xbf16> loc(#loc954)
      %141 = stablehlo.convert %140 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc955)
      %142 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %143 = stablehlo.multiply %141, %142 : tensor<32x8x17x128xf32> loc(#loc956)
      %144 = stablehlo.broadcast_in_dim %94, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc957)
      %145 = stablehlo.reshape %144 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc958)
      %146 = stablehlo.convert %145 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc959)
      %147 = stablehlo.transpose %146, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc960)
      %148 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %149 = stablehlo.multiply %147, %148 : tensor<32x8x128x128xf32> loc(#loc961)
      %150 = stablehlo.dot_general %143, %149, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc962)
      %151 = stablehlo.broadcast_in_dim %c_0, dims = [1] : (tensor<128xi64>) -> tensor<17x128xi64> loc(#loc963)
      %152 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<17xi64>) -> tensor<17x128xi64> loc(#loc964)
      %153 = stablehlo.compare  LE, %151, %152 : (tensor<17x128xi64>, tensor<17x128xi64>) -> tensor<17x128xi1> loc(#loc965)
      %154 = stablehlo.reshape %153 : (tensor<17x128xi1>) -> tensor<1x17x128xi1> loc(#loc966)
      %155 = stablehlo.broadcast_in_dim %154, dims = [1, 2, 3] : (tensor<1x17x128xi1>) -> tensor<32x1x17x128xi1> loc(#loc967)
      %156 = stablehlo.select %155, %4, %3 : tensor<32x1x17x128xi1>, tensor<32x1x17x128xbf16> loc(#loc968)
      %157 = stablehlo.convert %156 : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x128xf32> loc(#loc969)
      %158 = stablehlo.reshape %157 : (tensor<32x1x17x128xf32>) -> tensor<32x17x128xf32> loc(#loc970)
      %159 = stablehlo.broadcast_in_dim %158, dims = [0, 2, 3] : (tensor<32x17x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc971)
      %160 = stablehlo.add %150, %159 : tensor<32x8x17x128xf32> loc(#loc972)
      %161 = stablehlo.convert %160 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc973)
      %162 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %163 = stablehlo.compare  EQ, %161, %162 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc974)
      %164 = stablehlo.not %163 : tensor<32x8x17x128xi1> loc(#loc975)
      %165 = stablehlo.reduce(%164 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.391"), %arg1677: tensor<i1> loc("reduce.391"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc977)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc978)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc976)
      %166 = stablehlo.reshape %165 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc979)
      %167 = stablehlo.not %166 : tensor<32x8x17x1xi1> loc(#loc980)
      %168 = stablehlo.reshape %167 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc981)
      %169 = stablehlo.broadcast_in_dim %168, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc982)
      %170 = stablehlo.reduce(%160 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc983)
      %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc984)
      %172 = stablehlo.subtract %160, %171 : tensor<32x8x17x128xf32> loc(#loc985)
      %173 = stablehlo.exponential %172 : tensor<32x8x17x128xf32> loc(#loc986)
      %174 = stablehlo.reduce(%173 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc987)
      %175 = stablehlo.broadcast_in_dim %174, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc988)
      %176 = stablehlo.divide %173, %175 : tensor<32x8x17x128xf32> loc(#loc989)
      %177 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %178 = stablehlo.select %169, %177, %176 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc990)
      %179 = stablehlo.broadcast_in_dim %101, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc991)
      %180 = stablehlo.reshape %179 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc992)
      %181 = stablehlo.convert %180 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc993)
      %182 = stablehlo.dot_general %178, %181, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc994)
      %183 = stablehlo.convert %182 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc995)
      %184 = stablehlo.transpose %183, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc996)
      %185 = stablehlo.reshape %184 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc997)
      %186 = stablehlo.reshape %arg851 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc998)
      %187 = stablehlo.reshape %186 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc999)
      %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc1000)
      %189 = stablehlo.dot_general %185, %188, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1001)
      %190 = "stablehlo.all_reduce"(%189) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.408"), %arg1677: tensor<bf16> loc("dot.408")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1001)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1001)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1001)
      %191 = stablehlo.reshape %190 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1002)
      %192 = stablehlo.add %34, %191 : tensor<32x17x5120xbf16> loc(#loc1003)
      %193 = stablehlo.reshape %arg854 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1004)
      %194 = stablehlo.reshape %193 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1005)
      %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1006)
      %196 = stablehlo.convert %192 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1007)
      %197 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %198 = stablehlo.power %196, %197 : tensor<32x17x5120xf32> loc(#loc1008)
      %199 = stablehlo.reduce(%198 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1009)
      %200 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %201 = stablehlo.multiply %199, %200 : tensor<32x17xf32> loc(#loc1010)
      %202 = stablehlo.reshape %201 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1011)
      %203 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %204 = stablehlo.add %202, %203 : tensor<32x17x1xf32> loc(#loc1012)
      %205 = stablehlo.rsqrt %204 : tensor<32x17x1xf32> loc(#loc1013)
      %206 = stablehlo.reshape %205 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1014)
      %207 = stablehlo.broadcast_in_dim %206, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1015)
      %208 = stablehlo.multiply %196, %207 : tensor<32x17x5120xf32> loc(#loc1016)
      %209 = stablehlo.convert %208 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1017)
      %210 = stablehlo.multiply %195, %209 : tensor<32x17x5120xbf16> loc(#loc1018)
      %211 = stablehlo.reshape %210 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1019)
      %212 = stablehlo.reshape %arg855 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1020)
      %213 = stablehlo.reshape %212 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1021)
      %214 = stablehlo.transpose %213, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1022)
      %215 = stablehlo.dot_general %211, %214, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1023)
      %216 = stablehlo.reshape %215 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1024)
      %217 = stablehlo.logistic %216 : tensor<32x17x3200xbf16> loc(#loc1025)
      %218 = stablehlo.multiply %216, %217 : tensor<32x17x3200xbf16> loc(#loc1026)
      %219 = stablehlo.reshape %arg850 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1027)
      %220 = stablehlo.reshape %219 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1028)
      %221 = stablehlo.transpose %220, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1029)
      %222 = stablehlo.dot_general %211, %221, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1030)
      %223 = stablehlo.reshape %222 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1031)
      %224 = stablehlo.multiply %218, %223 : tensor<32x17x3200xbf16> loc(#loc1032)
      %225 = stablehlo.reshape %224 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1033)
      %226 = stablehlo.reshape %arg849 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc1034)
      %227 = stablehlo.reshape %226 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc1035)
      %228 = stablehlo.transpose %227, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc1036)
      %229 = stablehlo.dot_general %225, %228, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1037)
      %230 = "stablehlo.all_reduce"(%229) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.463"), %arg1677: tensor<bf16> loc("dot.463")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1037)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1037)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1037)
      %231 = stablehlo.reshape %230 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1038)
      %232 = stablehlo.add %192, %231 : tensor<32x17x5120xbf16> loc(#loc1039)
      %233 = stablehlo.convert %232 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1040)
      %234 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %235 = stablehlo.power %233, %234 : tensor<32x17x5120xf32> loc(#loc1041)
      %236 = stablehlo.reduce(%235 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1042)
      %237 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %238 = stablehlo.multiply %236, %237 : tensor<32x17xf32> loc(#loc1043)
      %239 = stablehlo.reshape %238 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1044)
      %240 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %241 = stablehlo.add %239, %240 : tensor<32x17x1xf32> loc(#loc1045)
      %242 = stablehlo.rsqrt %241 : tensor<32x17x1xf32> loc(#loc1046)
      %243 = stablehlo.reshape %242 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1047)
      %244 = stablehlo.broadcast_in_dim %243, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1048)
      %245 = stablehlo.multiply %233, %244 : tensor<32x17x5120xf32> loc(#loc1049)
      %246 = stablehlo.convert %245 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1050)
      %247 = stablehlo.multiply %107, %246 : tensor<32x17x5120xbf16> loc(#loc1051)
      %248 = stablehlo.reshape %247 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1052)
      %249 = stablehlo.reshape %arg848 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1053)
      %250 = stablehlo.reshape %249 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1054)
      %251 = stablehlo.transpose %250, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1055)
      %252 = stablehlo.dot_general %248, %251, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1056)
      %253 = stablehlo.reshape %252 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1057)
      %254 = stablehlo.convert %253 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc1058)
      %255 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %256 = stablehlo.power %254, %255 : tensor<32x17x1x128xf32> loc(#loc1059)
      %257 = stablehlo.reduce(%256 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc1060)
      %258 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %259 = stablehlo.multiply %257, %258 : tensor<32x17x1xf32> loc(#loc1061)
      %260 = stablehlo.reshape %259 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc1062)
      %261 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %262 = stablehlo.add %260, %261 : tensor<32x17x1x1xf32> loc(#loc1063)
      %263 = stablehlo.rsqrt %262 : tensor<32x17x1x1xf32> loc(#loc1064)
      %264 = stablehlo.reshape %263 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc1065)
      %265 = stablehlo.broadcast_in_dim %264, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc1066)
      %266 = stablehlo.multiply %254, %265 : tensor<32x17x1x128xf32> loc(#loc1067)
      %267 = stablehlo.convert %266 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc1068)
      %268 = stablehlo.multiply %104, %267 : tensor<32x17x1x128xbf16> loc(#loc1069)
      %269 = stablehlo.transpose %268, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1070)
      %270 = stablehlo.multiply %269, %82 : tensor<32x1x17x128xbf16> loc(#loc1071)
      %271 = stablehlo.slice %269 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1072)
      %272 = stablehlo.negate %271 : tensor<32x1x17x64xbf16> loc(#loc1073)
      %273 = stablehlo.slice %269 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1074)
      %274 = stablehlo.concatenate %272, %273, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1075)
      %275 = stablehlo.multiply %274, %91 : tensor<32x1x17x128xbf16> loc(#loc1076)
      %276 = stablehlo.add %270, %275 : tensor<32x1x17x128xbf16> loc(#loc1077)
      %277 = "stablehlo.scatter"(%arg858, %21, %276) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.575"), %arg1677: tensor<bf16> loc("scatter.575")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1078)
      %278 = stablehlo.reshape %arg859 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1079)
      %279 = stablehlo.reshape %278 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1080)
      %280 = stablehlo.transpose %279, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1081)
      %281 = stablehlo.dot_general %248, %280, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1082)
      %282 = stablehlo.reshape %281 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1083)
      %283 = stablehlo.transpose %282, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1084)
      %284 = "stablehlo.scatter"(%arg860, %21, %283) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.605"), %arg1677: tensor<bf16> loc("scatter.605")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1085)
      %285 = stablehlo.reshape %arg870 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1086)
      %286 = stablehlo.reshape %285 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1087)
      %287 = stablehlo.broadcast_in_dim %286, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1088)
      %288 = stablehlo.reshape %arg869 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1089)
      %289 = stablehlo.reshape %288 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1090)
      %290 = stablehlo.broadcast_in_dim %289, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1091)
      %291 = stablehlo.reshape %arg866 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1092)
      %292 = stablehlo.reshape %291 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1093)
      %293 = stablehlo.broadcast_in_dim %292, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1094)
      %294 = stablehlo.reshape %arg865 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc1095)
      %295 = stablehlo.reshape %294 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc1096)
      %296 = stablehlo.transpose %295, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc1097)
      %297 = stablehlo.dot_general %248, %296, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc1098)
      %298 = stablehlo.reshape %297 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1099)
      %299 = stablehlo.convert %298 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc1100)
      %300 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %301 = stablehlo.power %299, %300 : tensor<32x17x8x128xf32> loc(#loc1101)
      %302 = stablehlo.reduce(%301 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc1102)
      %303 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %304 = stablehlo.multiply %302, %303 : tensor<32x17x8xf32> loc(#loc1103)
      %305 = stablehlo.reshape %304 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc1104)
      %306 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %307 = stablehlo.add %305, %306 : tensor<32x17x8x1xf32> loc(#loc1105)
      %308 = stablehlo.rsqrt %307 : tensor<32x17x8x1xf32> loc(#loc1106)
      %309 = stablehlo.reshape %308 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc1107)
      %310 = stablehlo.broadcast_in_dim %309, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc1108)
      %311 = stablehlo.multiply %299, %310 : tensor<32x17x8x128xf32> loc(#loc1109)
      %312 = stablehlo.convert %311 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc1110)
      %313 = stablehlo.multiply %293, %312 : tensor<32x17x8x128xbf16> loc(#loc1111)
      %314 = stablehlo.transpose %313, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1112)
      %315 = stablehlo.multiply %314, %132 : tensor<32x8x17x128xbf16> loc(#loc1113)
      %316 = stablehlo.slice %314 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1114)
      %317 = stablehlo.negate %316 : tensor<32x8x17x64xbf16> loc(#loc1115)
      %318 = stablehlo.slice %314 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1116)
      %319 = stablehlo.concatenate %317, %318, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1117)
      %320 = stablehlo.multiply %319, %138 : tensor<32x8x17x128xbf16> loc(#loc1118)
      %321 = stablehlo.add %315, %320 : tensor<32x8x17x128xbf16> loc(#loc1119)
      %322 = stablehlo.convert %321 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc1120)
      %323 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %324 = stablehlo.multiply %322, %323 : tensor<32x8x17x128xf32> loc(#loc1121)
      %325 = stablehlo.broadcast_in_dim %277, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1122)
      %326 = stablehlo.reshape %325 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1123)
      %327 = stablehlo.convert %326 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1124)
      %328 = stablehlo.transpose %327, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc1125)
      %329 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %330 = stablehlo.multiply %328, %329 : tensor<32x8x128x128xf32> loc(#loc1126)
      %331 = stablehlo.dot_general %324, %330, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1127)
      %332 = stablehlo.add %331, %159 : tensor<32x8x17x128xf32> loc(#loc1128)
      %333 = stablehlo.convert %332 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc1129)
      %334 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %335 = stablehlo.compare  EQ, %333, %334 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc1130)
      %336 = stablehlo.not %335 : tensor<32x8x17x128xi1> loc(#loc1131)
      %337 = stablehlo.reduce(%336 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.786"), %arg1677: tensor<i1> loc("reduce.786"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc1133)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc1134)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc1132)
      %338 = stablehlo.reshape %337 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc1135)
      %339 = stablehlo.not %338 : tensor<32x8x17x1xi1> loc(#loc1136)
      %340 = stablehlo.reshape %339 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc1137)
      %341 = stablehlo.broadcast_in_dim %340, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc1138)
      %342 = stablehlo.reduce(%332 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1139)
      %343 = stablehlo.broadcast_in_dim %342, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1140)
      %344 = stablehlo.subtract %332, %343 : tensor<32x8x17x128xf32> loc(#loc1141)
      %345 = stablehlo.exponential %344 : tensor<32x8x17x128xf32> loc(#loc1142)
      %346 = stablehlo.reduce(%345 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1143)
      %347 = stablehlo.broadcast_in_dim %346, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1144)
      %348 = stablehlo.divide %345, %347 : tensor<32x8x17x128xf32> loc(#loc1145)
      %349 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %350 = stablehlo.select %341, %349, %348 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc1146)
      %351 = stablehlo.broadcast_in_dim %284, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1147)
      %352 = stablehlo.reshape %351 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1148)
      %353 = stablehlo.convert %352 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1149)
      %354 = stablehlo.dot_general %350, %353, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1150)
      %355 = stablehlo.convert %354 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc1151)
      %356 = stablehlo.transpose %355, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1152)
      %357 = stablehlo.reshape %356 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc1153)
      %358 = stablehlo.reshape %arg864 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc1154)
      %359 = stablehlo.reshape %358 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc1155)
      %360 = stablehlo.transpose %359, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc1156)
      %361 = stablehlo.dot_general %357, %360, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1157)
      %362 = "stablehlo.all_reduce"(%361) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.803"), %arg1677: tensor<bf16> loc("dot.803")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1157)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1157)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1157)
      %363 = stablehlo.reshape %362 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1158)
      %364 = stablehlo.add %232, %363 : tensor<32x17x5120xbf16> loc(#loc1159)
      %365 = stablehlo.reshape %arg867 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1160)
      %366 = stablehlo.reshape %365 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1161)
      %367 = stablehlo.broadcast_in_dim %366, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1162)
      %368 = stablehlo.convert %364 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1163)
      %369 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %370 = stablehlo.power %368, %369 : tensor<32x17x5120xf32> loc(#loc1164)
      %371 = stablehlo.reduce(%370 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1165)
      %372 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %373 = stablehlo.multiply %371, %372 : tensor<32x17xf32> loc(#loc1166)
      %374 = stablehlo.reshape %373 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1167)
      %375 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %376 = stablehlo.add %374, %375 : tensor<32x17x1xf32> loc(#loc1168)
      %377 = stablehlo.rsqrt %376 : tensor<32x17x1xf32> loc(#loc1169)
      %378 = stablehlo.reshape %377 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1170)
      %379 = stablehlo.broadcast_in_dim %378, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1171)
      %380 = stablehlo.multiply %368, %379 : tensor<32x17x5120xf32> loc(#loc1172)
      %381 = stablehlo.convert %380 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1173)
      %382 = stablehlo.multiply %367, %381 : tensor<32x17x5120xbf16> loc(#loc1174)
      %383 = stablehlo.reshape %382 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1175)
      %384 = stablehlo.reshape %arg868 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1176)
      %385 = stablehlo.reshape %384 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1177)
      %386 = stablehlo.transpose %385, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1178)
      %387 = stablehlo.dot_general %383, %386, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1179)
      %388 = stablehlo.reshape %387 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1180)
      %389 = stablehlo.logistic %388 : tensor<32x17x3200xbf16> loc(#loc1181)
      %390 = stablehlo.multiply %388, %389 : tensor<32x17x3200xbf16> loc(#loc1182)
      %391 = stablehlo.reshape %arg863 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1183)
      %392 = stablehlo.reshape %391 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1184)
      %393 = stablehlo.transpose %392, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1185)
      %394 = stablehlo.dot_general %383, %393, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1186)
      %395 = stablehlo.reshape %394 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1187)
      %396 = stablehlo.multiply %390, %395 : tensor<32x17x3200xbf16> loc(#loc1188)
      %397 = stablehlo.reshape %396 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1189)
      %398 = stablehlo.reshape %arg862 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc1190)
      %399 = stablehlo.reshape %398 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc1191)
      %400 = stablehlo.transpose %399, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc1192)
      %401 = stablehlo.dot_general %397, %400, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1193)
      %402 = "stablehlo.all_reduce"(%401) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.858"), %arg1677: tensor<bf16> loc("dot.858")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1193)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1193)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1193)
      %403 = stablehlo.reshape %402 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1194)
      %404 = stablehlo.add %364, %403 : tensor<32x17x5120xbf16> loc(#loc1195)
      %405 = stablehlo.convert %404 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1196)
      %406 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %407 = stablehlo.power %405, %406 : tensor<32x17x5120xf32> loc(#loc1197)
      %408 = stablehlo.reduce(%407 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1198)
      %409 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %410 = stablehlo.multiply %408, %409 : tensor<32x17xf32> loc(#loc1199)
      %411 = stablehlo.reshape %410 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1200)
      %412 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %413 = stablehlo.add %411, %412 : tensor<32x17x1xf32> loc(#loc1201)
      %414 = stablehlo.rsqrt %413 : tensor<32x17x1xf32> loc(#loc1202)
      %415 = stablehlo.reshape %414 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1203)
      %416 = stablehlo.broadcast_in_dim %415, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1204)
      %417 = stablehlo.multiply %405, %416 : tensor<32x17x5120xf32> loc(#loc1205)
      %418 = stablehlo.convert %417 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1206)
      %419 = stablehlo.multiply %290, %418 : tensor<32x17x5120xbf16> loc(#loc1207)
      %420 = stablehlo.reshape %419 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1208)
      %421 = stablehlo.reshape %arg861 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1209)
      %422 = stablehlo.reshape %421 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1210)
      %423 = stablehlo.transpose %422, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1211)
      %424 = stablehlo.dot_general %420, %423, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1212)
      %425 = stablehlo.reshape %424 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1213)
      %426 = stablehlo.convert %425 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc1214)
      %427 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %428 = stablehlo.power %426, %427 : tensor<32x17x1x128xf32> loc(#loc1215)
      %429 = stablehlo.reduce(%428 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc1216)
      %430 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %431 = stablehlo.multiply %429, %430 : tensor<32x17x1xf32> loc(#loc1217)
      %432 = stablehlo.reshape %431 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc1218)
      %433 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %434 = stablehlo.add %432, %433 : tensor<32x17x1x1xf32> loc(#loc1219)
      %435 = stablehlo.rsqrt %434 : tensor<32x17x1x1xf32> loc(#loc1220)
      %436 = stablehlo.reshape %435 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc1221)
      %437 = stablehlo.broadcast_in_dim %436, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc1222)
      %438 = stablehlo.multiply %426, %437 : tensor<32x17x1x128xf32> loc(#loc1223)
      %439 = stablehlo.convert %438 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc1224)
      %440 = stablehlo.multiply %287, %439 : tensor<32x17x1x128xbf16> loc(#loc1225)
      %441 = stablehlo.transpose %440, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1226)
      %442 = stablehlo.multiply %441, %82 : tensor<32x1x17x128xbf16> loc(#loc1227)
      %443 = stablehlo.slice %441 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1228)
      %444 = stablehlo.negate %443 : tensor<32x1x17x64xbf16> loc(#loc1229)
      %445 = stablehlo.slice %441 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1230)
      %446 = stablehlo.concatenate %444, %445, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1231)
      %447 = stablehlo.multiply %446, %91 : tensor<32x1x17x128xbf16> loc(#loc1232)
      %448 = stablehlo.add %442, %447 : tensor<32x1x17x128xbf16> loc(#loc1233)
      %449 = "stablehlo.scatter"(%arg871, %21, %448) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.970"), %arg1677: tensor<bf16> loc("scatter.970")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1234)
      %450 = stablehlo.reshape %arg872 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1235)
      %451 = stablehlo.reshape %450 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1236)
      %452 = stablehlo.transpose %451, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1237)
      %453 = stablehlo.dot_general %420, %452, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1238)
      %454 = stablehlo.reshape %453 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1239)
      %455 = stablehlo.transpose %454, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1240)
      %456 = "stablehlo.scatter"(%arg873, %21, %455) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.1000"), %arg1677: tensor<bf16> loc("scatter.1000")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1241)
      %457 = stablehlo.reshape %arg883 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1242)
      %458 = stablehlo.reshape %457 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1243)
      %459 = stablehlo.broadcast_in_dim %458, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1244)
      %460 = stablehlo.reshape %arg882 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1245)
      %461 = stablehlo.reshape %460 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1246)
      %462 = stablehlo.broadcast_in_dim %461, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1247)
      %463 = stablehlo.reshape %arg879 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1248)
      %464 = stablehlo.reshape %463 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1249)
      %465 = stablehlo.broadcast_in_dim %464, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1250)
      %466 = stablehlo.reshape %arg878 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc1251)
      %467 = stablehlo.reshape %466 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc1252)
      %468 = stablehlo.transpose %467, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc1253)
      %469 = stablehlo.dot_general %420, %468, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc1254)
      %470 = stablehlo.reshape %469 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1255)
      %471 = stablehlo.convert %470 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc1256)
      %472 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %473 = stablehlo.power %471, %472 : tensor<32x17x8x128xf32> loc(#loc1257)
      %474 = stablehlo.reduce(%473 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc1258)
      %475 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %476 = stablehlo.multiply %474, %475 : tensor<32x17x8xf32> loc(#loc1259)
      %477 = stablehlo.reshape %476 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc1260)
      %478 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %479 = stablehlo.add %477, %478 : tensor<32x17x8x1xf32> loc(#loc1261)
      %480 = stablehlo.rsqrt %479 : tensor<32x17x8x1xf32> loc(#loc1262)
      %481 = stablehlo.reshape %480 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc1263)
      %482 = stablehlo.broadcast_in_dim %481, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc1264)
      %483 = stablehlo.multiply %471, %482 : tensor<32x17x8x128xf32> loc(#loc1265)
      %484 = stablehlo.convert %483 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc1266)
      %485 = stablehlo.multiply %465, %484 : tensor<32x17x8x128xbf16> loc(#loc1267)
      %486 = stablehlo.transpose %485, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1268)
      %487 = stablehlo.multiply %486, %132 : tensor<32x8x17x128xbf16> loc(#loc1269)
      %488 = stablehlo.slice %486 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1270)
      %489 = stablehlo.negate %488 : tensor<32x8x17x64xbf16> loc(#loc1271)
      %490 = stablehlo.slice %486 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1272)
      %491 = stablehlo.concatenate %489, %490, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1273)
      %492 = stablehlo.multiply %491, %138 : tensor<32x8x17x128xbf16> loc(#loc1274)
      %493 = stablehlo.add %487, %492 : tensor<32x8x17x128xbf16> loc(#loc1275)
      %494 = stablehlo.convert %493 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc1276)
      %495 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %496 = stablehlo.multiply %494, %495 : tensor<32x8x17x128xf32> loc(#loc1277)
      %497 = stablehlo.broadcast_in_dim %449, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1278)
      %498 = stablehlo.reshape %497 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1279)
      %499 = stablehlo.convert %498 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1280)
      %500 = stablehlo.transpose %499, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc1281)
      %501 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %502 = stablehlo.multiply %500, %501 : tensor<32x8x128x128xf32> loc(#loc1282)
      %503 = stablehlo.dot_general %496, %502, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1283)
      %504 = stablehlo.add %503, %159 : tensor<32x8x17x128xf32> loc(#loc1284)
      %505 = stablehlo.convert %504 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc1285)
      %506 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %507 = stablehlo.compare  EQ, %505, %506 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc1286)
      %508 = stablehlo.not %507 : tensor<32x8x17x128xi1> loc(#loc1287)
      %509 = stablehlo.reduce(%508 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.1181"), %arg1677: tensor<i1> loc("reduce.1181"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc1289)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc1290)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc1288)
      %510 = stablehlo.reshape %509 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc1291)
      %511 = stablehlo.not %510 : tensor<32x8x17x1xi1> loc(#loc1292)
      %512 = stablehlo.reshape %511 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc1293)
      %513 = stablehlo.broadcast_in_dim %512, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc1294)
      %514 = stablehlo.reduce(%504 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1295)
      %515 = stablehlo.broadcast_in_dim %514, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1296)
      %516 = stablehlo.subtract %504, %515 : tensor<32x8x17x128xf32> loc(#loc1297)
      %517 = stablehlo.exponential %516 : tensor<32x8x17x128xf32> loc(#loc1298)
      %518 = stablehlo.reduce(%517 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1299)
      %519 = stablehlo.broadcast_in_dim %518, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1300)
      %520 = stablehlo.divide %517, %519 : tensor<32x8x17x128xf32> loc(#loc1301)
      %521 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %522 = stablehlo.select %513, %521, %520 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc1302)
      %523 = stablehlo.broadcast_in_dim %456, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1303)
      %524 = stablehlo.reshape %523 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1304)
      %525 = stablehlo.convert %524 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1305)
      %526 = stablehlo.dot_general %522, %525, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1306)
      %527 = stablehlo.convert %526 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc1307)
      %528 = stablehlo.transpose %527, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1308)
      %529 = stablehlo.reshape %528 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc1309)
      %530 = stablehlo.reshape %arg877 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc1310)
      %531 = stablehlo.reshape %530 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc1311)
      %532 = stablehlo.transpose %531, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc1312)
      %533 = stablehlo.dot_general %529, %532, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1313)
      %534 = "stablehlo.all_reduce"(%533) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.1198"), %arg1677: tensor<bf16> loc("dot.1198")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1313)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1313)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1313)
      %535 = stablehlo.reshape %534 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1314)
      %536 = stablehlo.add %404, %535 : tensor<32x17x5120xbf16> loc(#loc1315)
      %537 = stablehlo.reshape %arg880 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1316)
      %538 = stablehlo.reshape %537 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1317)
      %539 = stablehlo.broadcast_in_dim %538, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1318)
      %540 = stablehlo.convert %536 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1319)
      %541 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %542 = stablehlo.power %540, %541 : tensor<32x17x5120xf32> loc(#loc1320)
      %543 = stablehlo.reduce(%542 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1321)
      %544 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %545 = stablehlo.multiply %543, %544 : tensor<32x17xf32> loc(#loc1322)
      %546 = stablehlo.reshape %545 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1323)
      %547 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %548 = stablehlo.add %546, %547 : tensor<32x17x1xf32> loc(#loc1324)
      %549 = stablehlo.rsqrt %548 : tensor<32x17x1xf32> loc(#loc1325)
      %550 = stablehlo.reshape %549 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1326)
      %551 = stablehlo.broadcast_in_dim %550, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1327)
      %552 = stablehlo.multiply %540, %551 : tensor<32x17x5120xf32> loc(#loc1328)
      %553 = stablehlo.convert %552 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1329)
      %554 = stablehlo.multiply %539, %553 : tensor<32x17x5120xbf16> loc(#loc1330)
      %555 = stablehlo.reshape %554 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1331)
      %556 = stablehlo.reshape %arg881 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1332)
      %557 = stablehlo.reshape %556 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1333)
      %558 = stablehlo.transpose %557, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1334)
      %559 = stablehlo.dot_general %555, %558, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1335)
      %560 = stablehlo.reshape %559 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1336)
      %561 = stablehlo.logistic %560 : tensor<32x17x3200xbf16> loc(#loc1337)
      %562 = stablehlo.multiply %560, %561 : tensor<32x17x3200xbf16> loc(#loc1338)
      %563 = stablehlo.reshape %arg876 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1339)
      %564 = stablehlo.reshape %563 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1340)
      %565 = stablehlo.transpose %564, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1341)
      %566 = stablehlo.dot_general %555, %565, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1342)
      %567 = stablehlo.reshape %566 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1343)
      %568 = stablehlo.multiply %562, %567 : tensor<32x17x3200xbf16> loc(#loc1344)
      %569 = stablehlo.reshape %568 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1345)
      %570 = stablehlo.reshape %arg875 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc1346)
      %571 = stablehlo.reshape %570 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc1347)
      %572 = stablehlo.transpose %571, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc1348)
      %573 = stablehlo.dot_general %569, %572, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1349)
      %574 = "stablehlo.all_reduce"(%573) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.1253"), %arg1677: tensor<bf16> loc("dot.1253")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1349)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1349)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1349)
      %575 = stablehlo.reshape %574 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1350)
      %576 = stablehlo.add %536, %575 : tensor<32x17x5120xbf16> loc(#loc1351)
      %577 = stablehlo.convert %576 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1352)
      %578 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %579 = stablehlo.power %577, %578 : tensor<32x17x5120xf32> loc(#loc1353)
      %580 = stablehlo.reduce(%579 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1354)
      %581 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %582 = stablehlo.multiply %580, %581 : tensor<32x17xf32> loc(#loc1355)
      %583 = stablehlo.reshape %582 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1356)
      %584 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %585 = stablehlo.add %583, %584 : tensor<32x17x1xf32> loc(#loc1357)
      %586 = stablehlo.rsqrt %585 : tensor<32x17x1xf32> loc(#loc1358)
      %587 = stablehlo.reshape %586 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1359)
      %588 = stablehlo.broadcast_in_dim %587, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1360)
      %589 = stablehlo.multiply %577, %588 : tensor<32x17x5120xf32> loc(#loc1361)
      %590 = stablehlo.convert %589 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1362)
      %591 = stablehlo.multiply %462, %590 : tensor<32x17x5120xbf16> loc(#loc1363)
      %592 = stablehlo.reshape %591 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1364)
      %593 = stablehlo.reshape %arg874 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1365)
      %594 = stablehlo.reshape %593 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1366)
      %595 = stablehlo.transpose %594, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1367)
      %596 = stablehlo.dot_general %592, %595, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1368)
      %597 = stablehlo.reshape %596 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1369)
      %598 = stablehlo.convert %597 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc1370)
      %599 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %600 = stablehlo.power %598, %599 : tensor<32x17x1x128xf32> loc(#loc1371)
      %601 = stablehlo.reduce(%600 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc1372)
      %602 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %603 = stablehlo.multiply %601, %602 : tensor<32x17x1xf32> loc(#loc1373)
      %604 = stablehlo.reshape %603 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc1374)
      %605 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %606 = stablehlo.add %604, %605 : tensor<32x17x1x1xf32> loc(#loc1375)
      %607 = stablehlo.rsqrt %606 : tensor<32x17x1x1xf32> loc(#loc1376)
      %608 = stablehlo.reshape %607 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc1377)
      %609 = stablehlo.broadcast_in_dim %608, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc1378)
      %610 = stablehlo.multiply %598, %609 : tensor<32x17x1x128xf32> loc(#loc1379)
      %611 = stablehlo.convert %610 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc1380)
      %612 = stablehlo.multiply %459, %611 : tensor<32x17x1x128xbf16> loc(#loc1381)
      %613 = stablehlo.transpose %612, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1382)
      %614 = stablehlo.multiply %613, %82 : tensor<32x1x17x128xbf16> loc(#loc1383)
      %615 = stablehlo.slice %613 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1384)
      %616 = stablehlo.negate %615 : tensor<32x1x17x64xbf16> loc(#loc1385)
      %617 = stablehlo.slice %613 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1386)
      %618 = stablehlo.concatenate %616, %617, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1387)
      %619 = stablehlo.multiply %618, %91 : tensor<32x1x17x128xbf16> loc(#loc1388)
      %620 = stablehlo.add %614, %619 : tensor<32x1x17x128xbf16> loc(#loc1389)
      %621 = "stablehlo.scatter"(%arg884, %21, %620) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.1365"), %arg1677: tensor<bf16> loc("scatter.1365")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1390)
      %622 = stablehlo.reshape %arg885 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1391)
      %623 = stablehlo.reshape %622 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1392)
      %624 = stablehlo.transpose %623, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1393)
      %625 = stablehlo.dot_general %592, %624, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1394)
      %626 = stablehlo.reshape %625 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1395)
      %627 = stablehlo.transpose %626, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1396)
      %628 = "stablehlo.scatter"(%arg886, %21, %627) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.1395"), %arg1677: tensor<bf16> loc("scatter.1395")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1397)
      %629 = stablehlo.reshape %arg896 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1398)
      %630 = stablehlo.reshape %629 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1399)
      %631 = stablehlo.broadcast_in_dim %630, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1400)
      %632 = stablehlo.reshape %arg895 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1401)
      %633 = stablehlo.reshape %632 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1402)
      %634 = stablehlo.broadcast_in_dim %633, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1403)
      %635 = stablehlo.reshape %arg892 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1404)
      %636 = stablehlo.reshape %635 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1405)
      %637 = stablehlo.broadcast_in_dim %636, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1406)
      %638 = stablehlo.reshape %arg891 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc1407)
      %639 = stablehlo.reshape %638 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc1408)
      %640 = stablehlo.transpose %639, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc1409)
      %641 = stablehlo.dot_general %592, %640, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc1410)
      %642 = stablehlo.reshape %641 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1411)
      %643 = stablehlo.convert %642 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc1412)
      %644 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %645 = stablehlo.power %643, %644 : tensor<32x17x8x128xf32> loc(#loc1413)
      %646 = stablehlo.reduce(%645 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc1414)
      %647 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %648 = stablehlo.multiply %646, %647 : tensor<32x17x8xf32> loc(#loc1415)
      %649 = stablehlo.reshape %648 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc1416)
      %650 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %651 = stablehlo.add %649, %650 : tensor<32x17x8x1xf32> loc(#loc1417)
      %652 = stablehlo.rsqrt %651 : tensor<32x17x8x1xf32> loc(#loc1418)
      %653 = stablehlo.reshape %652 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc1419)
      %654 = stablehlo.broadcast_in_dim %653, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc1420)
      %655 = stablehlo.multiply %643, %654 : tensor<32x17x8x128xf32> loc(#loc1421)
      %656 = stablehlo.convert %655 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc1422)
      %657 = stablehlo.multiply %637, %656 : tensor<32x17x8x128xbf16> loc(#loc1423)
      %658 = stablehlo.transpose %657, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1424)
      %659 = stablehlo.multiply %658, %132 : tensor<32x8x17x128xbf16> loc(#loc1425)
      %660 = stablehlo.slice %658 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1426)
      %661 = stablehlo.negate %660 : tensor<32x8x17x64xbf16> loc(#loc1427)
      %662 = stablehlo.slice %658 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1428)
      %663 = stablehlo.concatenate %661, %662, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1429)
      %664 = stablehlo.multiply %663, %138 : tensor<32x8x17x128xbf16> loc(#loc1430)
      %665 = stablehlo.add %659, %664 : tensor<32x8x17x128xbf16> loc(#loc1431)
      %666 = stablehlo.convert %665 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc1432)
      %667 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %668 = stablehlo.multiply %666, %667 : tensor<32x8x17x128xf32> loc(#loc1433)
      %669 = stablehlo.broadcast_in_dim %621, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1434)
      %670 = stablehlo.reshape %669 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1435)
      %671 = stablehlo.convert %670 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1436)
      %672 = stablehlo.transpose %671, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc1437)
      %673 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %674 = stablehlo.multiply %672, %673 : tensor<32x8x128x128xf32> loc(#loc1438)
      %675 = stablehlo.dot_general %668, %674, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1439)
      %676 = stablehlo.add %675, %159 : tensor<32x8x17x128xf32> loc(#loc1440)
      %677 = stablehlo.convert %676 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc1441)
      %678 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %679 = stablehlo.compare  EQ, %677, %678 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc1442)
      %680 = stablehlo.not %679 : tensor<32x8x17x128xi1> loc(#loc1443)
      %681 = stablehlo.reduce(%680 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.1576"), %arg1677: tensor<i1> loc("reduce.1576"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc1445)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc1446)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc1444)
      %682 = stablehlo.reshape %681 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc1447)
      %683 = stablehlo.not %682 : tensor<32x8x17x1xi1> loc(#loc1448)
      %684 = stablehlo.reshape %683 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc1449)
      %685 = stablehlo.broadcast_in_dim %684, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc1450)
      %686 = stablehlo.reduce(%676 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1451)
      %687 = stablehlo.broadcast_in_dim %686, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1452)
      %688 = stablehlo.subtract %676, %687 : tensor<32x8x17x128xf32> loc(#loc1453)
      %689 = stablehlo.exponential %688 : tensor<32x8x17x128xf32> loc(#loc1454)
      %690 = stablehlo.reduce(%689 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1455)
      %691 = stablehlo.broadcast_in_dim %690, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1456)
      %692 = stablehlo.divide %689, %691 : tensor<32x8x17x128xf32> loc(#loc1457)
      %693 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %694 = stablehlo.select %685, %693, %692 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc1458)
      %695 = stablehlo.broadcast_in_dim %628, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1459)
      %696 = stablehlo.reshape %695 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1460)
      %697 = stablehlo.convert %696 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1461)
      %698 = stablehlo.dot_general %694, %697, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1462)
      %699 = stablehlo.convert %698 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc1463)
      %700 = stablehlo.transpose %699, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1464)
      %701 = stablehlo.reshape %700 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc1465)
      %702 = stablehlo.reshape %arg890 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc1466)
      %703 = stablehlo.reshape %702 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc1467)
      %704 = stablehlo.transpose %703, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc1468)
      %705 = stablehlo.dot_general %701, %704, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1469)
      %706 = "stablehlo.all_reduce"(%705) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.1593"), %arg1677: tensor<bf16> loc("dot.1593")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1469)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1469)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1469)
      %707 = stablehlo.reshape %706 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1470)
      %708 = stablehlo.add %576, %707 : tensor<32x17x5120xbf16> loc(#loc1471)
      %709 = stablehlo.reshape %arg893 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1472)
      %710 = stablehlo.reshape %709 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1473)
      %711 = stablehlo.broadcast_in_dim %710, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1474)
      %712 = stablehlo.convert %708 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1475)
      %713 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %714 = stablehlo.power %712, %713 : tensor<32x17x5120xf32> loc(#loc1476)
      %715 = stablehlo.reduce(%714 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1477)
      %716 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %717 = stablehlo.multiply %715, %716 : tensor<32x17xf32> loc(#loc1478)
      %718 = stablehlo.reshape %717 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1479)
      %719 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %720 = stablehlo.add %718, %719 : tensor<32x17x1xf32> loc(#loc1480)
      %721 = stablehlo.rsqrt %720 : tensor<32x17x1xf32> loc(#loc1481)
      %722 = stablehlo.reshape %721 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1482)
      %723 = stablehlo.broadcast_in_dim %722, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1483)
      %724 = stablehlo.multiply %712, %723 : tensor<32x17x5120xf32> loc(#loc1484)
      %725 = stablehlo.convert %724 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1485)
      %726 = stablehlo.multiply %711, %725 : tensor<32x17x5120xbf16> loc(#loc1486)
      %727 = stablehlo.reshape %726 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1487)
      %728 = stablehlo.reshape %arg894 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1488)
      %729 = stablehlo.reshape %728 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1489)
      %730 = stablehlo.transpose %729, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1490)
      %731 = stablehlo.dot_general %727, %730, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1491)
      %732 = stablehlo.reshape %731 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1492)
      %733 = stablehlo.logistic %732 : tensor<32x17x3200xbf16> loc(#loc1493)
      %734 = stablehlo.multiply %732, %733 : tensor<32x17x3200xbf16> loc(#loc1494)
      %735 = stablehlo.reshape %arg889 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1495)
      %736 = stablehlo.reshape %735 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1496)
      %737 = stablehlo.transpose %736, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1497)
      %738 = stablehlo.dot_general %727, %737, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1498)
      %739 = stablehlo.reshape %738 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1499)
      %740 = stablehlo.multiply %734, %739 : tensor<32x17x3200xbf16> loc(#loc1500)
      %741 = stablehlo.reshape %740 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1501)
      %742 = stablehlo.reshape %arg888 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc1502)
      %743 = stablehlo.reshape %742 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc1503)
      %744 = stablehlo.transpose %743, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc1504)
      %745 = stablehlo.dot_general %741, %744, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1505)
      %746 = "stablehlo.all_reduce"(%745) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.1648"), %arg1677: tensor<bf16> loc("dot.1648")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1505)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1505)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1505)
      %747 = stablehlo.reshape %746 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1506)
      %748 = stablehlo.add %708, %747 : tensor<32x17x5120xbf16> loc(#loc1507)
      %749 = stablehlo.convert %748 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1508)
      %750 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %751 = stablehlo.power %749, %750 : tensor<32x17x5120xf32> loc(#loc1509)
      %752 = stablehlo.reduce(%751 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1510)
      %753 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %754 = stablehlo.multiply %752, %753 : tensor<32x17xf32> loc(#loc1511)
      %755 = stablehlo.reshape %754 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1512)
      %756 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %757 = stablehlo.add %755, %756 : tensor<32x17x1xf32> loc(#loc1513)
      %758 = stablehlo.rsqrt %757 : tensor<32x17x1xf32> loc(#loc1514)
      %759 = stablehlo.reshape %758 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1515)
      %760 = stablehlo.broadcast_in_dim %759, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1516)
      %761 = stablehlo.multiply %749, %760 : tensor<32x17x5120xf32> loc(#loc1517)
      %762 = stablehlo.convert %761 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1518)
      %763 = stablehlo.multiply %634, %762 : tensor<32x17x5120xbf16> loc(#loc1519)
      %764 = stablehlo.reshape %763 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1520)
      %765 = stablehlo.reshape %arg887 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1521)
      %766 = stablehlo.reshape %765 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1522)
      %767 = stablehlo.transpose %766, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1523)
      %768 = stablehlo.dot_general %764, %767, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1524)
      %769 = stablehlo.reshape %768 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1525)
      %770 = stablehlo.convert %769 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc1526)
      %771 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %772 = stablehlo.power %770, %771 : tensor<32x17x1x128xf32> loc(#loc1527)
      %773 = stablehlo.reduce(%772 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc1528)
      %774 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %775 = stablehlo.multiply %773, %774 : tensor<32x17x1xf32> loc(#loc1529)
      %776 = stablehlo.reshape %775 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc1530)
      %777 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %778 = stablehlo.add %776, %777 : tensor<32x17x1x1xf32> loc(#loc1531)
      %779 = stablehlo.rsqrt %778 : tensor<32x17x1x1xf32> loc(#loc1532)
      %780 = stablehlo.reshape %779 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc1533)
      %781 = stablehlo.broadcast_in_dim %780, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc1534)
      %782 = stablehlo.multiply %770, %781 : tensor<32x17x1x128xf32> loc(#loc1535)
      %783 = stablehlo.convert %782 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc1536)
      %784 = stablehlo.multiply %631, %783 : tensor<32x17x1x128xbf16> loc(#loc1537)
      %785 = stablehlo.transpose %784, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1538)
      %786 = stablehlo.multiply %785, %82 : tensor<32x1x17x128xbf16> loc(#loc1539)
      %787 = stablehlo.slice %785 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1540)
      %788 = stablehlo.negate %787 : tensor<32x1x17x64xbf16> loc(#loc1541)
      %789 = stablehlo.slice %785 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1542)
      %790 = stablehlo.concatenate %788, %789, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1543)
      %791 = stablehlo.multiply %790, %91 : tensor<32x1x17x128xbf16> loc(#loc1544)
      %792 = stablehlo.add %786, %791 : tensor<32x1x17x128xbf16> loc(#loc1545)
      %793 = "stablehlo.scatter"(%arg897, %21, %792) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.1760"), %arg1677: tensor<bf16> loc("scatter.1760")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1546)
      %794 = stablehlo.reshape %arg898 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1547)
      %795 = stablehlo.reshape %794 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1548)
      %796 = stablehlo.transpose %795, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1549)
      %797 = stablehlo.dot_general %764, %796, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1550)
      %798 = stablehlo.reshape %797 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1551)
      %799 = stablehlo.transpose %798, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1552)
      %800 = "stablehlo.scatter"(%arg899, %21, %799) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.1790"), %arg1677: tensor<bf16> loc("scatter.1790")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1553)
      %801 = stablehlo.reshape %arg909 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1554)
      %802 = stablehlo.reshape %801 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1555)
      %803 = stablehlo.broadcast_in_dim %802, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1556)
      %804 = stablehlo.reshape %arg908 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1557)
      %805 = stablehlo.reshape %804 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1558)
      %806 = stablehlo.broadcast_in_dim %805, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1559)
      %807 = stablehlo.reshape %arg905 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1560)
      %808 = stablehlo.reshape %807 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1561)
      %809 = stablehlo.broadcast_in_dim %808, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1562)
      %810 = stablehlo.reshape %arg904 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc1563)
      %811 = stablehlo.reshape %810 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc1564)
      %812 = stablehlo.transpose %811, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc1565)
      %813 = stablehlo.dot_general %764, %812, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc1566)
      %814 = stablehlo.reshape %813 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1567)
      %815 = stablehlo.convert %814 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc1568)
      %816 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %817 = stablehlo.power %815, %816 : tensor<32x17x8x128xf32> loc(#loc1569)
      %818 = stablehlo.reduce(%817 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc1570)
      %819 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %820 = stablehlo.multiply %818, %819 : tensor<32x17x8xf32> loc(#loc1571)
      %821 = stablehlo.reshape %820 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc1572)
      %822 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %823 = stablehlo.add %821, %822 : tensor<32x17x8x1xf32> loc(#loc1573)
      %824 = stablehlo.rsqrt %823 : tensor<32x17x8x1xf32> loc(#loc1574)
      %825 = stablehlo.reshape %824 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc1575)
      %826 = stablehlo.broadcast_in_dim %825, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc1576)
      %827 = stablehlo.multiply %815, %826 : tensor<32x17x8x128xf32> loc(#loc1577)
      %828 = stablehlo.convert %827 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc1578)
      %829 = stablehlo.multiply %809, %828 : tensor<32x17x8x128xbf16> loc(#loc1579)
      %830 = stablehlo.transpose %829, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1580)
      %831 = stablehlo.multiply %830, %132 : tensor<32x8x17x128xbf16> loc(#loc1581)
      %832 = stablehlo.slice %830 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1582)
      %833 = stablehlo.negate %832 : tensor<32x8x17x64xbf16> loc(#loc1583)
      %834 = stablehlo.slice %830 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1584)
      %835 = stablehlo.concatenate %833, %834, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1585)
      %836 = stablehlo.multiply %835, %138 : tensor<32x8x17x128xbf16> loc(#loc1586)
      %837 = stablehlo.add %831, %836 : tensor<32x8x17x128xbf16> loc(#loc1587)
      %838 = stablehlo.convert %837 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc1588)
      %839 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %840 = stablehlo.multiply %838, %839 : tensor<32x8x17x128xf32> loc(#loc1589)
      %841 = stablehlo.broadcast_in_dim %793, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1590)
      %842 = stablehlo.reshape %841 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1591)
      %843 = stablehlo.convert %842 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1592)
      %844 = stablehlo.transpose %843, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc1593)
      %845 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %846 = stablehlo.multiply %844, %845 : tensor<32x8x128x128xf32> loc(#loc1594)
      %847 = stablehlo.dot_general %840, %846, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1595)
      %848 = stablehlo.add %847, %159 : tensor<32x8x17x128xf32> loc(#loc1596)
      %849 = stablehlo.convert %848 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc1597)
      %850 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %851 = stablehlo.compare  EQ, %849, %850 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc1598)
      %852 = stablehlo.not %851 : tensor<32x8x17x128xi1> loc(#loc1599)
      %853 = stablehlo.reduce(%852 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.1971"), %arg1677: tensor<i1> loc("reduce.1971"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc1601)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc1602)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc1600)
      %854 = stablehlo.reshape %853 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc1603)
      %855 = stablehlo.not %854 : tensor<32x8x17x1xi1> loc(#loc1604)
      %856 = stablehlo.reshape %855 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc1605)
      %857 = stablehlo.broadcast_in_dim %856, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc1606)
      %858 = stablehlo.reduce(%848 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1607)
      %859 = stablehlo.broadcast_in_dim %858, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1608)
      %860 = stablehlo.subtract %848, %859 : tensor<32x8x17x128xf32> loc(#loc1609)
      %861 = stablehlo.exponential %860 : tensor<32x8x17x128xf32> loc(#loc1610)
      %862 = stablehlo.reduce(%861 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1611)
      %863 = stablehlo.broadcast_in_dim %862, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1612)
      %864 = stablehlo.divide %861, %863 : tensor<32x8x17x128xf32> loc(#loc1613)
      %865 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %866 = stablehlo.select %857, %865, %864 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc1614)
      %867 = stablehlo.broadcast_in_dim %800, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1615)
      %868 = stablehlo.reshape %867 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1616)
      %869 = stablehlo.convert %868 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1617)
      %870 = stablehlo.dot_general %866, %869, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1618)
      %871 = stablehlo.convert %870 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc1619)
      %872 = stablehlo.transpose %871, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1620)
      %873 = stablehlo.reshape %872 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc1621)
      %874 = stablehlo.reshape %arg903 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc1622)
      %875 = stablehlo.reshape %874 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc1623)
      %876 = stablehlo.transpose %875, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc1624)
      %877 = stablehlo.dot_general %873, %876, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1625)
      %878 = "stablehlo.all_reduce"(%877) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.1988"), %arg1677: tensor<bf16> loc("dot.1988")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1625)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1625)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1625)
      %879 = stablehlo.reshape %878 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1626)
      %880 = stablehlo.add %748, %879 : tensor<32x17x5120xbf16> loc(#loc1627)
      %881 = stablehlo.reshape %arg906 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1628)
      %882 = stablehlo.reshape %881 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1629)
      %883 = stablehlo.broadcast_in_dim %882, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1630)
      %884 = stablehlo.convert %880 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1631)
      %885 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %886 = stablehlo.power %884, %885 : tensor<32x17x5120xf32> loc(#loc1632)
      %887 = stablehlo.reduce(%886 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1633)
      %888 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %889 = stablehlo.multiply %887, %888 : tensor<32x17xf32> loc(#loc1634)
      %890 = stablehlo.reshape %889 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1635)
      %891 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %892 = stablehlo.add %890, %891 : tensor<32x17x1xf32> loc(#loc1636)
      %893 = stablehlo.rsqrt %892 : tensor<32x17x1xf32> loc(#loc1637)
      %894 = stablehlo.reshape %893 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1638)
      %895 = stablehlo.broadcast_in_dim %894, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1639)
      %896 = stablehlo.multiply %884, %895 : tensor<32x17x5120xf32> loc(#loc1640)
      %897 = stablehlo.convert %896 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1641)
      %898 = stablehlo.multiply %883, %897 : tensor<32x17x5120xbf16> loc(#loc1642)
      %899 = stablehlo.reshape %898 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1643)
      %900 = stablehlo.reshape %arg907 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1644)
      %901 = stablehlo.reshape %900 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1645)
      %902 = stablehlo.transpose %901, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1646)
      %903 = stablehlo.dot_general %899, %902, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1647)
      %904 = stablehlo.reshape %903 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1648)
      %905 = stablehlo.logistic %904 : tensor<32x17x3200xbf16> loc(#loc1649)
      %906 = stablehlo.multiply %904, %905 : tensor<32x17x3200xbf16> loc(#loc1650)
      %907 = stablehlo.reshape %arg902 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1651)
      %908 = stablehlo.reshape %907 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1652)
      %909 = stablehlo.transpose %908, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1653)
      %910 = stablehlo.dot_general %899, %909, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1654)
      %911 = stablehlo.reshape %910 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1655)
      %912 = stablehlo.multiply %906, %911 : tensor<32x17x3200xbf16> loc(#loc1656)
      %913 = stablehlo.reshape %912 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1657)
      %914 = stablehlo.reshape %arg901 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc1658)
      %915 = stablehlo.reshape %914 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc1659)
      %916 = stablehlo.transpose %915, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc1660)
      %917 = stablehlo.dot_general %913, %916, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1661)
      %918 = "stablehlo.all_reduce"(%917) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.2043"), %arg1677: tensor<bf16> loc("dot.2043")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1661)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1661)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1661)
      %919 = stablehlo.reshape %918 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1662)
      %920 = stablehlo.add %880, %919 : tensor<32x17x5120xbf16> loc(#loc1663)
      %921 = stablehlo.convert %920 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1664)
      %922 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %923 = stablehlo.power %921, %922 : tensor<32x17x5120xf32> loc(#loc1665)
      %924 = stablehlo.reduce(%923 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1666)
      %925 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %926 = stablehlo.multiply %924, %925 : tensor<32x17xf32> loc(#loc1667)
      %927 = stablehlo.reshape %926 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1668)
      %928 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %929 = stablehlo.add %927, %928 : tensor<32x17x1xf32> loc(#loc1669)
      %930 = stablehlo.rsqrt %929 : tensor<32x17x1xf32> loc(#loc1670)
      %931 = stablehlo.reshape %930 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1671)
      %932 = stablehlo.broadcast_in_dim %931, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1672)
      %933 = stablehlo.multiply %921, %932 : tensor<32x17x5120xf32> loc(#loc1673)
      %934 = stablehlo.convert %933 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1674)
      %935 = stablehlo.multiply %806, %934 : tensor<32x17x5120xbf16> loc(#loc1675)
      %936 = stablehlo.reshape %935 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1676)
      %937 = stablehlo.reshape %arg900 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1677)
      %938 = stablehlo.reshape %937 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1678)
      %939 = stablehlo.transpose %938, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1679)
      %940 = stablehlo.dot_general %936, %939, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1680)
      %941 = stablehlo.reshape %940 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1681)
      %942 = stablehlo.convert %941 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc1682)
      %943 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %944 = stablehlo.power %942, %943 : tensor<32x17x1x128xf32> loc(#loc1683)
      %945 = stablehlo.reduce(%944 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc1684)
      %946 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %947 = stablehlo.multiply %945, %946 : tensor<32x17x1xf32> loc(#loc1685)
      %948 = stablehlo.reshape %947 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc1686)
      %949 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %950 = stablehlo.add %948, %949 : tensor<32x17x1x1xf32> loc(#loc1687)
      %951 = stablehlo.rsqrt %950 : tensor<32x17x1x1xf32> loc(#loc1688)
      %952 = stablehlo.reshape %951 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc1689)
      %953 = stablehlo.broadcast_in_dim %952, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc1690)
      %954 = stablehlo.multiply %942, %953 : tensor<32x17x1x128xf32> loc(#loc1691)
      %955 = stablehlo.convert %954 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc1692)
      %956 = stablehlo.multiply %803, %955 : tensor<32x17x1x128xbf16> loc(#loc1693)
      %957 = stablehlo.transpose %956, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1694)
      %958 = stablehlo.multiply %957, %82 : tensor<32x1x17x128xbf16> loc(#loc1695)
      %959 = stablehlo.slice %957 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1696)
      %960 = stablehlo.negate %959 : tensor<32x1x17x64xbf16> loc(#loc1697)
      %961 = stablehlo.slice %957 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1698)
      %962 = stablehlo.concatenate %960, %961, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1699)
      %963 = stablehlo.multiply %962, %91 : tensor<32x1x17x128xbf16> loc(#loc1700)
      %964 = stablehlo.add %958, %963 : tensor<32x1x17x128xbf16> loc(#loc1701)
      %965 = "stablehlo.scatter"(%arg910, %21, %964) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.2155"), %arg1677: tensor<bf16> loc("scatter.2155")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1702)
      %966 = stablehlo.reshape %arg911 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1703)
      %967 = stablehlo.reshape %966 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1704)
      %968 = stablehlo.transpose %967, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1705)
      %969 = stablehlo.dot_general %936, %968, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1706)
      %970 = stablehlo.reshape %969 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1707)
      %971 = stablehlo.transpose %970, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1708)
      %972 = "stablehlo.scatter"(%arg912, %21, %971) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.2185"), %arg1677: tensor<bf16> loc("scatter.2185")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1709)
      %973 = stablehlo.reshape %arg922 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1710)
      %974 = stablehlo.reshape %973 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1711)
      %975 = stablehlo.broadcast_in_dim %974, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1712)
      %976 = stablehlo.reshape %arg921 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1713)
      %977 = stablehlo.reshape %976 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1714)
      %978 = stablehlo.broadcast_in_dim %977, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1715)
      %979 = stablehlo.reshape %arg918 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1716)
      %980 = stablehlo.reshape %979 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1717)
      %981 = stablehlo.broadcast_in_dim %980, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1718)
      %982 = stablehlo.reshape %arg917 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc1719)
      %983 = stablehlo.reshape %982 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc1720)
      %984 = stablehlo.transpose %983, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc1721)
      %985 = stablehlo.dot_general %936, %984, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc1722)
      %986 = stablehlo.reshape %985 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1723)
      %987 = stablehlo.convert %986 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc1724)
      %988 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %989 = stablehlo.power %987, %988 : tensor<32x17x8x128xf32> loc(#loc1725)
      %990 = stablehlo.reduce(%989 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc1726)
      %991 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %992 = stablehlo.multiply %990, %991 : tensor<32x17x8xf32> loc(#loc1727)
      %993 = stablehlo.reshape %992 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc1728)
      %994 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %995 = stablehlo.add %993, %994 : tensor<32x17x8x1xf32> loc(#loc1729)
      %996 = stablehlo.rsqrt %995 : tensor<32x17x8x1xf32> loc(#loc1730)
      %997 = stablehlo.reshape %996 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc1731)
      %998 = stablehlo.broadcast_in_dim %997, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc1732)
      %999 = stablehlo.multiply %987, %998 : tensor<32x17x8x128xf32> loc(#loc1733)
      %1000 = stablehlo.convert %999 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc1734)
      %1001 = stablehlo.multiply %981, %1000 : tensor<32x17x8x128xbf16> loc(#loc1735)
      %1002 = stablehlo.transpose %1001, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1736)
      %1003 = stablehlo.multiply %1002, %132 : tensor<32x8x17x128xbf16> loc(#loc1737)
      %1004 = stablehlo.slice %1002 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1738)
      %1005 = stablehlo.negate %1004 : tensor<32x8x17x64xbf16> loc(#loc1739)
      %1006 = stablehlo.slice %1002 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1740)
      %1007 = stablehlo.concatenate %1005, %1006, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1741)
      %1008 = stablehlo.multiply %1007, %138 : tensor<32x8x17x128xbf16> loc(#loc1742)
      %1009 = stablehlo.add %1003, %1008 : tensor<32x8x17x128xbf16> loc(#loc1743)
      %1010 = stablehlo.convert %1009 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc1744)
      %1011 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1012 = stablehlo.multiply %1010, %1011 : tensor<32x8x17x128xf32> loc(#loc1745)
      %1013 = stablehlo.broadcast_in_dim %965, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1746)
      %1014 = stablehlo.reshape %1013 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1747)
      %1015 = stablehlo.convert %1014 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1748)
      %1016 = stablehlo.transpose %1015, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc1749)
      %1017 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %1018 = stablehlo.multiply %1016, %1017 : tensor<32x8x128x128xf32> loc(#loc1750)
      %1019 = stablehlo.dot_general %1012, %1018, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1751)
      %1020 = stablehlo.add %1019, %159 : tensor<32x8x17x128xf32> loc(#loc1752)
      %1021 = stablehlo.convert %1020 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc1753)
      %1022 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %1023 = stablehlo.compare  EQ, %1021, %1022 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc1754)
      %1024 = stablehlo.not %1023 : tensor<32x8x17x128xi1> loc(#loc1755)
      %1025 = stablehlo.reduce(%1024 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.2366"), %arg1677: tensor<i1> loc("reduce.2366"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc1757)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc1758)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc1756)
      %1026 = stablehlo.reshape %1025 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc1759)
      %1027 = stablehlo.not %1026 : tensor<32x8x17x1xi1> loc(#loc1760)
      %1028 = stablehlo.reshape %1027 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc1761)
      %1029 = stablehlo.broadcast_in_dim %1028, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc1762)
      %1030 = stablehlo.reduce(%1020 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1763)
      %1031 = stablehlo.broadcast_in_dim %1030, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1764)
      %1032 = stablehlo.subtract %1020, %1031 : tensor<32x8x17x128xf32> loc(#loc1765)
      %1033 = stablehlo.exponential %1032 : tensor<32x8x17x128xf32> loc(#loc1766)
      %1034 = stablehlo.reduce(%1033 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1767)
      %1035 = stablehlo.broadcast_in_dim %1034, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1768)
      %1036 = stablehlo.divide %1033, %1035 : tensor<32x8x17x128xf32> loc(#loc1769)
      %1037 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1038 = stablehlo.select %1029, %1037, %1036 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc1770)
      %1039 = stablehlo.broadcast_in_dim %972, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1771)
      %1040 = stablehlo.reshape %1039 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1772)
      %1041 = stablehlo.convert %1040 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1773)
      %1042 = stablehlo.dot_general %1038, %1041, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1774)
      %1043 = stablehlo.convert %1042 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc1775)
      %1044 = stablehlo.transpose %1043, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1776)
      %1045 = stablehlo.reshape %1044 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc1777)
      %1046 = stablehlo.reshape %arg916 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc1778)
      %1047 = stablehlo.reshape %1046 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc1779)
      %1048 = stablehlo.transpose %1047, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc1780)
      %1049 = stablehlo.dot_general %1045, %1048, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1781)
      %1050 = "stablehlo.all_reduce"(%1049) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.2383"), %arg1677: tensor<bf16> loc("dot.2383")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1781)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1781)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1781)
      %1051 = stablehlo.reshape %1050 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1782)
      %1052 = stablehlo.add %920, %1051 : tensor<32x17x5120xbf16> loc(#loc1783)
      %1053 = stablehlo.reshape %arg919 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1784)
      %1054 = stablehlo.reshape %1053 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1785)
      %1055 = stablehlo.broadcast_in_dim %1054, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1786)
      %1056 = stablehlo.convert %1052 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1787)
      %1057 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1058 = stablehlo.power %1056, %1057 : tensor<32x17x5120xf32> loc(#loc1788)
      %1059 = stablehlo.reduce(%1058 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1789)
      %1060 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1061 = stablehlo.multiply %1059, %1060 : tensor<32x17xf32> loc(#loc1790)
      %1062 = stablehlo.reshape %1061 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1791)
      %1063 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1064 = stablehlo.add %1062, %1063 : tensor<32x17x1xf32> loc(#loc1792)
      %1065 = stablehlo.rsqrt %1064 : tensor<32x17x1xf32> loc(#loc1793)
      %1066 = stablehlo.reshape %1065 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1794)
      %1067 = stablehlo.broadcast_in_dim %1066, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1795)
      %1068 = stablehlo.multiply %1056, %1067 : tensor<32x17x5120xf32> loc(#loc1796)
      %1069 = stablehlo.convert %1068 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1797)
      %1070 = stablehlo.multiply %1055, %1069 : tensor<32x17x5120xbf16> loc(#loc1798)
      %1071 = stablehlo.reshape %1070 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1799)
      %1072 = stablehlo.reshape %arg920 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1800)
      %1073 = stablehlo.reshape %1072 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1801)
      %1074 = stablehlo.transpose %1073, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1802)
      %1075 = stablehlo.dot_general %1071, %1074, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1803)
      %1076 = stablehlo.reshape %1075 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1804)
      %1077 = stablehlo.logistic %1076 : tensor<32x17x3200xbf16> loc(#loc1805)
      %1078 = stablehlo.multiply %1076, %1077 : tensor<32x17x3200xbf16> loc(#loc1806)
      %1079 = stablehlo.reshape %arg915 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1807)
      %1080 = stablehlo.reshape %1079 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1808)
      %1081 = stablehlo.transpose %1080, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1809)
      %1082 = stablehlo.dot_general %1071, %1081, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1810)
      %1083 = stablehlo.reshape %1082 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1811)
      %1084 = stablehlo.multiply %1078, %1083 : tensor<32x17x3200xbf16> loc(#loc1812)
      %1085 = stablehlo.reshape %1084 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1813)
      %1086 = stablehlo.reshape %arg914 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc1814)
      %1087 = stablehlo.reshape %1086 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc1815)
      %1088 = stablehlo.transpose %1087, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc1816)
      %1089 = stablehlo.dot_general %1085, %1088, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1817)
      %1090 = "stablehlo.all_reduce"(%1089) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.2438"), %arg1677: tensor<bf16> loc("dot.2438")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1817)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1817)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1817)
      %1091 = stablehlo.reshape %1090 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1818)
      %1092 = stablehlo.add %1052, %1091 : tensor<32x17x5120xbf16> loc(#loc1819)
      %1093 = stablehlo.convert %1092 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1820)
      %1094 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1095 = stablehlo.power %1093, %1094 : tensor<32x17x5120xf32> loc(#loc1821)
      %1096 = stablehlo.reduce(%1095 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1822)
      %1097 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1098 = stablehlo.multiply %1096, %1097 : tensor<32x17xf32> loc(#loc1823)
      %1099 = stablehlo.reshape %1098 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1824)
      %1100 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1101 = stablehlo.add %1099, %1100 : tensor<32x17x1xf32> loc(#loc1825)
      %1102 = stablehlo.rsqrt %1101 : tensor<32x17x1xf32> loc(#loc1826)
      %1103 = stablehlo.reshape %1102 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1827)
      %1104 = stablehlo.broadcast_in_dim %1103, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1828)
      %1105 = stablehlo.multiply %1093, %1104 : tensor<32x17x5120xf32> loc(#loc1829)
      %1106 = stablehlo.convert %1105 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1830)
      %1107 = stablehlo.multiply %978, %1106 : tensor<32x17x5120xbf16> loc(#loc1831)
      %1108 = stablehlo.reshape %1107 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1832)
      %1109 = stablehlo.reshape %arg913 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1833)
      %1110 = stablehlo.reshape %1109 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1834)
      %1111 = stablehlo.transpose %1110, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1835)
      %1112 = stablehlo.dot_general %1108, %1111, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1836)
      %1113 = stablehlo.reshape %1112 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1837)
      %1114 = stablehlo.convert %1113 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc1838)
      %1115 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %1116 = stablehlo.power %1114, %1115 : tensor<32x17x1x128xf32> loc(#loc1839)
      %1117 = stablehlo.reduce(%1116 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc1840)
      %1118 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1119 = stablehlo.multiply %1117, %1118 : tensor<32x17x1xf32> loc(#loc1841)
      %1120 = stablehlo.reshape %1119 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc1842)
      %1121 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %1122 = stablehlo.add %1120, %1121 : tensor<32x17x1x1xf32> loc(#loc1843)
      %1123 = stablehlo.rsqrt %1122 : tensor<32x17x1x1xf32> loc(#loc1844)
      %1124 = stablehlo.reshape %1123 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc1845)
      %1125 = stablehlo.broadcast_in_dim %1124, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc1846)
      %1126 = stablehlo.multiply %1114, %1125 : tensor<32x17x1x128xf32> loc(#loc1847)
      %1127 = stablehlo.convert %1126 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc1848)
      %1128 = stablehlo.multiply %975, %1127 : tensor<32x17x1x128xbf16> loc(#loc1849)
      %1129 = stablehlo.transpose %1128, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1850)
      %1130 = stablehlo.multiply %1129, %82 : tensor<32x1x17x128xbf16> loc(#loc1851)
      %1131 = stablehlo.slice %1129 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1852)
      %1132 = stablehlo.negate %1131 : tensor<32x1x17x64xbf16> loc(#loc1853)
      %1133 = stablehlo.slice %1129 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc1854)
      %1134 = stablehlo.concatenate %1132, %1133, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1855)
      %1135 = stablehlo.multiply %1134, %91 : tensor<32x1x17x128xbf16> loc(#loc1856)
      %1136 = stablehlo.add %1130, %1135 : tensor<32x1x17x128xbf16> loc(#loc1857)
      %1137 = "stablehlo.scatter"(%arg923, %21, %1136) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.2550"), %arg1677: tensor<bf16> loc("scatter.2550")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1858)
      %1138 = stablehlo.reshape %arg924 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1859)
      %1139 = stablehlo.reshape %1138 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1860)
      %1140 = stablehlo.transpose %1139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1861)
      %1141 = stablehlo.dot_general %1108, %1140, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1862)
      %1142 = stablehlo.reshape %1141 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1863)
      %1143 = stablehlo.transpose %1142, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc1864)
      %1144 = "stablehlo.scatter"(%arg925, %21, %1143) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.2580"), %arg1677: tensor<bf16> loc("scatter.2580")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc1865)
      %1145 = stablehlo.reshape %arg935 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1866)
      %1146 = stablehlo.reshape %1145 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1867)
      %1147 = stablehlo.broadcast_in_dim %1146, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1868)
      %1148 = stablehlo.reshape %arg934 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1869)
      %1149 = stablehlo.reshape %1148 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1870)
      %1150 = stablehlo.broadcast_in_dim %1149, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1871)
      %1151 = stablehlo.reshape %arg931 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc1872)
      %1152 = stablehlo.reshape %1151 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc1873)
      %1153 = stablehlo.broadcast_in_dim %1152, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1874)
      %1154 = stablehlo.reshape %arg930 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc1875)
      %1155 = stablehlo.reshape %1154 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc1876)
      %1156 = stablehlo.transpose %1155, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc1877)
      %1157 = stablehlo.dot_general %1108, %1156, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc1878)
      %1158 = stablehlo.reshape %1157 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1879)
      %1159 = stablehlo.convert %1158 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc1880)
      %1160 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %1161 = stablehlo.power %1159, %1160 : tensor<32x17x8x128xf32> loc(#loc1881)
      %1162 = stablehlo.reduce(%1161 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc1882)
      %1163 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %1164 = stablehlo.multiply %1162, %1163 : tensor<32x17x8xf32> loc(#loc1883)
      %1165 = stablehlo.reshape %1164 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc1884)
      %1166 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %1167 = stablehlo.add %1165, %1166 : tensor<32x17x8x1xf32> loc(#loc1885)
      %1168 = stablehlo.rsqrt %1167 : tensor<32x17x8x1xf32> loc(#loc1886)
      %1169 = stablehlo.reshape %1168 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc1887)
      %1170 = stablehlo.broadcast_in_dim %1169, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc1888)
      %1171 = stablehlo.multiply %1159, %1170 : tensor<32x17x8x128xf32> loc(#loc1889)
      %1172 = stablehlo.convert %1171 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc1890)
      %1173 = stablehlo.multiply %1153, %1172 : tensor<32x17x8x128xbf16> loc(#loc1891)
      %1174 = stablehlo.transpose %1173, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1892)
      %1175 = stablehlo.multiply %1174, %132 : tensor<32x8x17x128xbf16> loc(#loc1893)
      %1176 = stablehlo.slice %1174 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1894)
      %1177 = stablehlo.negate %1176 : tensor<32x8x17x64xbf16> loc(#loc1895)
      %1178 = stablehlo.slice %1174 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc1896)
      %1179 = stablehlo.concatenate %1177, %1178, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc1897)
      %1180 = stablehlo.multiply %1179, %138 : tensor<32x8x17x128xbf16> loc(#loc1898)
      %1181 = stablehlo.add %1175, %1180 : tensor<32x8x17x128xbf16> loc(#loc1899)
      %1182 = stablehlo.convert %1181 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc1900)
      %1183 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1184 = stablehlo.multiply %1182, %1183 : tensor<32x8x17x128xf32> loc(#loc1901)
      %1185 = stablehlo.broadcast_in_dim %1137, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1902)
      %1186 = stablehlo.reshape %1185 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1903)
      %1187 = stablehlo.convert %1186 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1904)
      %1188 = stablehlo.transpose %1187, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc1905)
      %1189 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %1190 = stablehlo.multiply %1188, %1189 : tensor<32x8x128x128xf32> loc(#loc1906)
      %1191 = stablehlo.dot_general %1184, %1190, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1907)
      %1192 = stablehlo.add %1191, %159 : tensor<32x8x17x128xf32> loc(#loc1908)
      %1193 = stablehlo.convert %1192 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc1909)
      %1194 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %1195 = stablehlo.compare  EQ, %1193, %1194 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc1910)
      %1196 = stablehlo.not %1195 : tensor<32x8x17x128xi1> loc(#loc1911)
      %1197 = stablehlo.reduce(%1196 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.2761"), %arg1677: tensor<i1> loc("reduce.2761"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc1913)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc1914)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc1912)
      %1198 = stablehlo.reshape %1197 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc1915)
      %1199 = stablehlo.not %1198 : tensor<32x8x17x1xi1> loc(#loc1916)
      %1200 = stablehlo.reshape %1199 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc1917)
      %1201 = stablehlo.broadcast_in_dim %1200, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc1918)
      %1202 = stablehlo.reduce(%1192 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1919)
      %1203 = stablehlo.broadcast_in_dim %1202, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1920)
      %1204 = stablehlo.subtract %1192, %1203 : tensor<32x8x17x128xf32> loc(#loc1921)
      %1205 = stablehlo.exponential %1204 : tensor<32x8x17x128xf32> loc(#loc1922)
      %1206 = stablehlo.reduce(%1205 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc1923)
      %1207 = stablehlo.broadcast_in_dim %1206, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc1924)
      %1208 = stablehlo.divide %1205, %1207 : tensor<32x8x17x128xf32> loc(#loc1925)
      %1209 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1210 = stablehlo.select %1201, %1209, %1208 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc1926)
      %1211 = stablehlo.broadcast_in_dim %1144, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc1927)
      %1212 = stablehlo.reshape %1211 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc1928)
      %1213 = stablehlo.convert %1212 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc1929)
      %1214 = stablehlo.dot_general %1210, %1213, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc1930)
      %1215 = stablehlo.convert %1214 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc1931)
      %1216 = stablehlo.transpose %1215, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc1932)
      %1217 = stablehlo.reshape %1216 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc1933)
      %1218 = stablehlo.reshape %arg929 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc1934)
      %1219 = stablehlo.reshape %1218 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc1935)
      %1220 = stablehlo.transpose %1219, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc1936)
      %1221 = stablehlo.dot_general %1217, %1220, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1937)
      %1222 = "stablehlo.all_reduce"(%1221) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.2778"), %arg1677: tensor<bf16> loc("dot.2778")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1937)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1937)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1937)
      %1223 = stablehlo.reshape %1222 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1938)
      %1224 = stablehlo.add %1092, %1223 : tensor<32x17x5120xbf16> loc(#loc1939)
      %1225 = stablehlo.reshape %arg932 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc1940)
      %1226 = stablehlo.reshape %1225 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc1941)
      %1227 = stablehlo.broadcast_in_dim %1226, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1942)
      %1228 = stablehlo.convert %1224 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1943)
      %1229 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1230 = stablehlo.power %1228, %1229 : tensor<32x17x5120xf32> loc(#loc1944)
      %1231 = stablehlo.reduce(%1230 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1945)
      %1232 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1233 = stablehlo.multiply %1231, %1232 : tensor<32x17xf32> loc(#loc1946)
      %1234 = stablehlo.reshape %1233 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1947)
      %1235 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1236 = stablehlo.add %1234, %1235 : tensor<32x17x1xf32> loc(#loc1948)
      %1237 = stablehlo.rsqrt %1236 : tensor<32x17x1xf32> loc(#loc1949)
      %1238 = stablehlo.reshape %1237 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1950)
      %1239 = stablehlo.broadcast_in_dim %1238, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1951)
      %1240 = stablehlo.multiply %1228, %1239 : tensor<32x17x5120xf32> loc(#loc1952)
      %1241 = stablehlo.convert %1240 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1953)
      %1242 = stablehlo.multiply %1227, %1241 : tensor<32x17x5120xbf16> loc(#loc1954)
      %1243 = stablehlo.reshape %1242 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1955)
      %1244 = stablehlo.reshape %arg933 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1956)
      %1245 = stablehlo.reshape %1244 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1957)
      %1246 = stablehlo.transpose %1245, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1958)
      %1247 = stablehlo.dot_general %1243, %1246, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1959)
      %1248 = stablehlo.reshape %1247 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1960)
      %1249 = stablehlo.logistic %1248 : tensor<32x17x3200xbf16> loc(#loc1961)
      %1250 = stablehlo.multiply %1248, %1249 : tensor<32x17x3200xbf16> loc(#loc1962)
      %1251 = stablehlo.reshape %arg928 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc1963)
      %1252 = stablehlo.reshape %1251 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc1964)
      %1253 = stablehlo.transpose %1252, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc1965)
      %1254 = stablehlo.dot_general %1243, %1253, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1966)
      %1255 = stablehlo.reshape %1254 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc1967)
      %1256 = stablehlo.multiply %1250, %1255 : tensor<32x17x3200xbf16> loc(#loc1968)
      %1257 = stablehlo.reshape %1256 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc1969)
      %1258 = stablehlo.reshape %arg927 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc1970)
      %1259 = stablehlo.reshape %1258 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc1971)
      %1260 = stablehlo.transpose %1259, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc1972)
      %1261 = stablehlo.dot_general %1257, %1260, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1973)
      %1262 = "stablehlo.all_reduce"(%1261) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.2833"), %arg1677: tensor<bf16> loc("dot.2833")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc1973)
        stablehlo.return %11074 : tensor<bf16> loc(#loc1973)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1973)
      %1263 = stablehlo.reshape %1262 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc1974)
      %1264 = stablehlo.add %1224, %1263 : tensor<32x17x5120xbf16> loc(#loc1975)
      %1265 = stablehlo.convert %1264 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc1976)
      %1266 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1267 = stablehlo.power %1265, %1266 : tensor<32x17x5120xf32> loc(#loc1977)
      %1268 = stablehlo.reduce(%1267 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc1978)
      %1269 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1270 = stablehlo.multiply %1268, %1269 : tensor<32x17xf32> loc(#loc1979)
      %1271 = stablehlo.reshape %1270 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc1980)
      %1272 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1273 = stablehlo.add %1271, %1272 : tensor<32x17x1xf32> loc(#loc1981)
      %1274 = stablehlo.rsqrt %1273 : tensor<32x17x1xf32> loc(#loc1982)
      %1275 = stablehlo.reshape %1274 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc1983)
      %1276 = stablehlo.broadcast_in_dim %1275, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc1984)
      %1277 = stablehlo.multiply %1265, %1276 : tensor<32x17x5120xf32> loc(#loc1985)
      %1278 = stablehlo.convert %1277 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc1986)
      %1279 = stablehlo.multiply %1150, %1278 : tensor<32x17x5120xbf16> loc(#loc1987)
      %1280 = stablehlo.reshape %1279 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc1988)
      %1281 = stablehlo.reshape %arg926 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc1989)
      %1282 = stablehlo.reshape %1281 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc1990)
      %1283 = stablehlo.transpose %1282, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc1991)
      %1284 = stablehlo.dot_general %1280, %1283, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc1992)
      %1285 = stablehlo.reshape %1284 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc1993)
      %1286 = stablehlo.convert %1285 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc1994)
      %1287 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %1288 = stablehlo.power %1286, %1287 : tensor<32x17x1x128xf32> loc(#loc1995)
      %1289 = stablehlo.reduce(%1288 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc1996)
      %1290 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1291 = stablehlo.multiply %1289, %1290 : tensor<32x17x1xf32> loc(#loc1997)
      %1292 = stablehlo.reshape %1291 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc1998)
      %1293 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %1294 = stablehlo.add %1292, %1293 : tensor<32x17x1x1xf32> loc(#loc1999)
      %1295 = stablehlo.rsqrt %1294 : tensor<32x17x1x1xf32> loc(#loc2000)
      %1296 = stablehlo.reshape %1295 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc2001)
      %1297 = stablehlo.broadcast_in_dim %1296, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc2002)
      %1298 = stablehlo.multiply %1286, %1297 : tensor<32x17x1x128xf32> loc(#loc2003)
      %1299 = stablehlo.convert %1298 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc2004)
      %1300 = stablehlo.multiply %1147, %1299 : tensor<32x17x1x128xbf16> loc(#loc2005)
      %1301 = stablehlo.transpose %1300, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2006)
      %1302 = stablehlo.multiply %1301, %82 : tensor<32x1x17x128xbf16> loc(#loc2007)
      %1303 = stablehlo.slice %1301 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2008)
      %1304 = stablehlo.negate %1303 : tensor<32x1x17x64xbf16> loc(#loc2009)
      %1305 = stablehlo.slice %1301 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2010)
      %1306 = stablehlo.concatenate %1304, %1305, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2011)
      %1307 = stablehlo.multiply %1306, %91 : tensor<32x1x17x128xbf16> loc(#loc2012)
      %1308 = stablehlo.add %1302, %1307 : tensor<32x1x17x128xbf16> loc(#loc2013)
      %1309 = "stablehlo.scatter"(%arg936, %21, %1308) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.2945"), %arg1677: tensor<bf16> loc("scatter.2945")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2014)
      %1310 = stablehlo.reshape %arg937 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2015)
      %1311 = stablehlo.reshape %1310 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2016)
      %1312 = stablehlo.transpose %1311, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2017)
      %1313 = stablehlo.dot_general %1280, %1312, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2018)
      %1314 = stablehlo.reshape %1313 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2019)
      %1315 = stablehlo.transpose %1314, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2020)
      %1316 = "stablehlo.scatter"(%arg938, %21, %1315) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.2975"), %arg1677: tensor<bf16> loc("scatter.2975")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2021)
      %1317 = stablehlo.reshape %arg948 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2022)
      %1318 = stablehlo.reshape %1317 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2023)
      %1319 = stablehlo.broadcast_in_dim %1318, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2024)
      %1320 = stablehlo.reshape %arg947 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2025)
      %1321 = stablehlo.reshape %1320 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2026)
      %1322 = stablehlo.broadcast_in_dim %1321, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2027)
      %1323 = stablehlo.reshape %arg944 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2028)
      %1324 = stablehlo.reshape %1323 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2029)
      %1325 = stablehlo.broadcast_in_dim %1324, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2030)
      %1326 = stablehlo.reshape %arg943 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc2031)
      %1327 = stablehlo.reshape %1326 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc2032)
      %1328 = stablehlo.transpose %1327, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc2033)
      %1329 = stablehlo.dot_general %1280, %1328, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc2034)
      %1330 = stablehlo.reshape %1329 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2035)
      %1331 = stablehlo.convert %1330 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc2036)
      %1332 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %1333 = stablehlo.power %1331, %1332 : tensor<32x17x8x128xf32> loc(#loc2037)
      %1334 = stablehlo.reduce(%1333 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc2038)
      %1335 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %1336 = stablehlo.multiply %1334, %1335 : tensor<32x17x8xf32> loc(#loc2039)
      %1337 = stablehlo.reshape %1336 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc2040)
      %1338 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %1339 = stablehlo.add %1337, %1338 : tensor<32x17x8x1xf32> loc(#loc2041)
      %1340 = stablehlo.rsqrt %1339 : tensor<32x17x8x1xf32> loc(#loc2042)
      %1341 = stablehlo.reshape %1340 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc2043)
      %1342 = stablehlo.broadcast_in_dim %1341, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc2044)
      %1343 = stablehlo.multiply %1331, %1342 : tensor<32x17x8x128xf32> loc(#loc2045)
      %1344 = stablehlo.convert %1343 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc2046)
      %1345 = stablehlo.multiply %1325, %1344 : tensor<32x17x8x128xbf16> loc(#loc2047)
      %1346 = stablehlo.transpose %1345, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2048)
      %1347 = stablehlo.multiply %1346, %132 : tensor<32x8x17x128xbf16> loc(#loc2049)
      %1348 = stablehlo.slice %1346 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2050)
      %1349 = stablehlo.negate %1348 : tensor<32x8x17x64xbf16> loc(#loc2051)
      %1350 = stablehlo.slice %1346 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2052)
      %1351 = stablehlo.concatenate %1349, %1350, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2053)
      %1352 = stablehlo.multiply %1351, %138 : tensor<32x8x17x128xbf16> loc(#loc2054)
      %1353 = stablehlo.add %1347, %1352 : tensor<32x8x17x128xbf16> loc(#loc2055)
      %1354 = stablehlo.convert %1353 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc2056)
      %1355 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1356 = stablehlo.multiply %1354, %1355 : tensor<32x8x17x128xf32> loc(#loc2057)
      %1357 = stablehlo.broadcast_in_dim %1309, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2058)
      %1358 = stablehlo.reshape %1357 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2059)
      %1359 = stablehlo.convert %1358 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2060)
      %1360 = stablehlo.transpose %1359, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc2061)
      %1361 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %1362 = stablehlo.multiply %1360, %1361 : tensor<32x8x128x128xf32> loc(#loc2062)
      %1363 = stablehlo.dot_general %1356, %1362, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2063)
      %1364 = stablehlo.add %1363, %159 : tensor<32x8x17x128xf32> loc(#loc2064)
      %1365 = stablehlo.convert %1364 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc2065)
      %1366 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %1367 = stablehlo.compare  EQ, %1365, %1366 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc2066)
      %1368 = stablehlo.not %1367 : tensor<32x8x17x128xi1> loc(#loc2067)
      %1369 = stablehlo.reduce(%1368 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.3156"), %arg1677: tensor<i1> loc("reduce.3156"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc2069)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc2070)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc2068)
      %1370 = stablehlo.reshape %1369 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc2071)
      %1371 = stablehlo.not %1370 : tensor<32x8x17x1xi1> loc(#loc2072)
      %1372 = stablehlo.reshape %1371 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc2073)
      %1373 = stablehlo.broadcast_in_dim %1372, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc2074)
      %1374 = stablehlo.reduce(%1364 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2075)
      %1375 = stablehlo.broadcast_in_dim %1374, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2076)
      %1376 = stablehlo.subtract %1364, %1375 : tensor<32x8x17x128xf32> loc(#loc2077)
      %1377 = stablehlo.exponential %1376 : tensor<32x8x17x128xf32> loc(#loc2078)
      %1378 = stablehlo.reduce(%1377 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2079)
      %1379 = stablehlo.broadcast_in_dim %1378, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2080)
      %1380 = stablehlo.divide %1377, %1379 : tensor<32x8x17x128xf32> loc(#loc2081)
      %1381 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1382 = stablehlo.select %1373, %1381, %1380 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc2082)
      %1383 = stablehlo.broadcast_in_dim %1316, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2083)
      %1384 = stablehlo.reshape %1383 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2084)
      %1385 = stablehlo.convert %1384 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2085)
      %1386 = stablehlo.dot_general %1382, %1385, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2086)
      %1387 = stablehlo.convert %1386 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc2087)
      %1388 = stablehlo.transpose %1387, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2088)
      %1389 = stablehlo.reshape %1388 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc2089)
      %1390 = stablehlo.reshape %arg942 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc2090)
      %1391 = stablehlo.reshape %1390 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc2091)
      %1392 = stablehlo.transpose %1391, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc2092)
      %1393 = stablehlo.dot_general %1389, %1392, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2093)
      %1394 = "stablehlo.all_reduce"(%1393) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.3173"), %arg1677: tensor<bf16> loc("dot.3173")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2093)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2093)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2093)
      %1395 = stablehlo.reshape %1394 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2094)
      %1396 = stablehlo.add %1264, %1395 : tensor<32x17x5120xbf16> loc(#loc2095)
      %1397 = stablehlo.reshape %arg945 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2096)
      %1398 = stablehlo.reshape %1397 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2097)
      %1399 = stablehlo.broadcast_in_dim %1398, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2098)
      %1400 = stablehlo.convert %1396 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2099)
      %1401 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1402 = stablehlo.power %1400, %1401 : tensor<32x17x5120xf32> loc(#loc2100)
      %1403 = stablehlo.reduce(%1402 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2101)
      %1404 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1405 = stablehlo.multiply %1403, %1404 : tensor<32x17xf32> loc(#loc2102)
      %1406 = stablehlo.reshape %1405 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2103)
      %1407 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1408 = stablehlo.add %1406, %1407 : tensor<32x17x1xf32> loc(#loc2104)
      %1409 = stablehlo.rsqrt %1408 : tensor<32x17x1xf32> loc(#loc2105)
      %1410 = stablehlo.reshape %1409 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2106)
      %1411 = stablehlo.broadcast_in_dim %1410, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2107)
      %1412 = stablehlo.multiply %1400, %1411 : tensor<32x17x5120xf32> loc(#loc2108)
      %1413 = stablehlo.convert %1412 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2109)
      %1414 = stablehlo.multiply %1399, %1413 : tensor<32x17x5120xbf16> loc(#loc2110)
      %1415 = stablehlo.reshape %1414 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2111)
      %1416 = stablehlo.reshape %arg946 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2112)
      %1417 = stablehlo.reshape %1416 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2113)
      %1418 = stablehlo.transpose %1417, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2114)
      %1419 = stablehlo.dot_general %1415, %1418, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2115)
      %1420 = stablehlo.reshape %1419 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2116)
      %1421 = stablehlo.logistic %1420 : tensor<32x17x3200xbf16> loc(#loc2117)
      %1422 = stablehlo.multiply %1420, %1421 : tensor<32x17x3200xbf16> loc(#loc2118)
      %1423 = stablehlo.reshape %arg941 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2119)
      %1424 = stablehlo.reshape %1423 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2120)
      %1425 = stablehlo.transpose %1424, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2121)
      %1426 = stablehlo.dot_general %1415, %1425, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2122)
      %1427 = stablehlo.reshape %1426 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2123)
      %1428 = stablehlo.multiply %1422, %1427 : tensor<32x17x3200xbf16> loc(#loc2124)
      %1429 = stablehlo.reshape %1428 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2125)
      %1430 = stablehlo.reshape %arg940 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc2126)
      %1431 = stablehlo.reshape %1430 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc2127)
      %1432 = stablehlo.transpose %1431, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc2128)
      %1433 = stablehlo.dot_general %1429, %1432, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2129)
      %1434 = "stablehlo.all_reduce"(%1433) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.3228"), %arg1677: tensor<bf16> loc("dot.3228")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2129)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2129)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2129)
      %1435 = stablehlo.reshape %1434 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2130)
      %1436 = stablehlo.add %1396, %1435 : tensor<32x17x5120xbf16> loc(#loc2131)
      %1437 = stablehlo.convert %1436 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2132)
      %1438 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1439 = stablehlo.power %1437, %1438 : tensor<32x17x5120xf32> loc(#loc2133)
      %1440 = stablehlo.reduce(%1439 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2134)
      %1441 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1442 = stablehlo.multiply %1440, %1441 : tensor<32x17xf32> loc(#loc2135)
      %1443 = stablehlo.reshape %1442 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2136)
      %1444 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1445 = stablehlo.add %1443, %1444 : tensor<32x17x1xf32> loc(#loc2137)
      %1446 = stablehlo.rsqrt %1445 : tensor<32x17x1xf32> loc(#loc2138)
      %1447 = stablehlo.reshape %1446 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2139)
      %1448 = stablehlo.broadcast_in_dim %1447, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2140)
      %1449 = stablehlo.multiply %1437, %1448 : tensor<32x17x5120xf32> loc(#loc2141)
      %1450 = stablehlo.convert %1449 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2142)
      %1451 = stablehlo.multiply %1322, %1450 : tensor<32x17x5120xbf16> loc(#loc2143)
      %1452 = stablehlo.reshape %1451 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2144)
      %1453 = stablehlo.reshape %arg939 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2145)
      %1454 = stablehlo.reshape %1453 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2146)
      %1455 = stablehlo.transpose %1454, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2147)
      %1456 = stablehlo.dot_general %1452, %1455, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2148)
      %1457 = stablehlo.reshape %1456 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2149)
      %1458 = stablehlo.convert %1457 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc2150)
      %1459 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %1460 = stablehlo.power %1458, %1459 : tensor<32x17x1x128xf32> loc(#loc2151)
      %1461 = stablehlo.reduce(%1460 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc2152)
      %1462 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1463 = stablehlo.multiply %1461, %1462 : tensor<32x17x1xf32> loc(#loc2153)
      %1464 = stablehlo.reshape %1463 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc2154)
      %1465 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %1466 = stablehlo.add %1464, %1465 : tensor<32x17x1x1xf32> loc(#loc2155)
      %1467 = stablehlo.rsqrt %1466 : tensor<32x17x1x1xf32> loc(#loc2156)
      %1468 = stablehlo.reshape %1467 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc2157)
      %1469 = stablehlo.broadcast_in_dim %1468, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc2158)
      %1470 = stablehlo.multiply %1458, %1469 : tensor<32x17x1x128xf32> loc(#loc2159)
      %1471 = stablehlo.convert %1470 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc2160)
      %1472 = stablehlo.multiply %1319, %1471 : tensor<32x17x1x128xbf16> loc(#loc2161)
      %1473 = stablehlo.transpose %1472, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2162)
      %1474 = stablehlo.multiply %1473, %82 : tensor<32x1x17x128xbf16> loc(#loc2163)
      %1475 = stablehlo.slice %1473 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2164)
      %1476 = stablehlo.negate %1475 : tensor<32x1x17x64xbf16> loc(#loc2165)
      %1477 = stablehlo.slice %1473 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2166)
      %1478 = stablehlo.concatenate %1476, %1477, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2167)
      %1479 = stablehlo.multiply %1478, %91 : tensor<32x1x17x128xbf16> loc(#loc2168)
      %1480 = stablehlo.add %1474, %1479 : tensor<32x1x17x128xbf16> loc(#loc2169)
      %1481 = "stablehlo.scatter"(%arg949, %21, %1480) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.3340"), %arg1677: tensor<bf16> loc("scatter.3340")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2170)
      %1482 = stablehlo.reshape %arg950 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2171)
      %1483 = stablehlo.reshape %1482 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2172)
      %1484 = stablehlo.transpose %1483, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2173)
      %1485 = stablehlo.dot_general %1452, %1484, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2174)
      %1486 = stablehlo.reshape %1485 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2175)
      %1487 = stablehlo.transpose %1486, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2176)
      %1488 = "stablehlo.scatter"(%arg951, %21, %1487) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.3370"), %arg1677: tensor<bf16> loc("scatter.3370")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2177)
      %1489 = stablehlo.reshape %arg961 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2178)
      %1490 = stablehlo.reshape %1489 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2179)
      %1491 = stablehlo.broadcast_in_dim %1490, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2180)
      %1492 = stablehlo.reshape %arg960 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2181)
      %1493 = stablehlo.reshape %1492 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2182)
      %1494 = stablehlo.broadcast_in_dim %1493, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2183)
      %1495 = stablehlo.reshape %arg957 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2184)
      %1496 = stablehlo.reshape %1495 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2185)
      %1497 = stablehlo.broadcast_in_dim %1496, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2186)
      %1498 = stablehlo.reshape %arg956 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc2187)
      %1499 = stablehlo.reshape %1498 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc2188)
      %1500 = stablehlo.transpose %1499, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc2189)
      %1501 = stablehlo.dot_general %1452, %1500, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc2190)
      %1502 = stablehlo.reshape %1501 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2191)
      %1503 = stablehlo.convert %1502 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc2192)
      %1504 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %1505 = stablehlo.power %1503, %1504 : tensor<32x17x8x128xf32> loc(#loc2193)
      %1506 = stablehlo.reduce(%1505 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc2194)
      %1507 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %1508 = stablehlo.multiply %1506, %1507 : tensor<32x17x8xf32> loc(#loc2195)
      %1509 = stablehlo.reshape %1508 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc2196)
      %1510 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %1511 = stablehlo.add %1509, %1510 : tensor<32x17x8x1xf32> loc(#loc2197)
      %1512 = stablehlo.rsqrt %1511 : tensor<32x17x8x1xf32> loc(#loc2198)
      %1513 = stablehlo.reshape %1512 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc2199)
      %1514 = stablehlo.broadcast_in_dim %1513, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc2200)
      %1515 = stablehlo.multiply %1503, %1514 : tensor<32x17x8x128xf32> loc(#loc2201)
      %1516 = stablehlo.convert %1515 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc2202)
      %1517 = stablehlo.multiply %1497, %1516 : tensor<32x17x8x128xbf16> loc(#loc2203)
      %1518 = stablehlo.transpose %1517, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2204)
      %1519 = stablehlo.multiply %1518, %132 : tensor<32x8x17x128xbf16> loc(#loc2205)
      %1520 = stablehlo.slice %1518 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2206)
      %1521 = stablehlo.negate %1520 : tensor<32x8x17x64xbf16> loc(#loc2207)
      %1522 = stablehlo.slice %1518 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2208)
      %1523 = stablehlo.concatenate %1521, %1522, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2209)
      %1524 = stablehlo.multiply %1523, %138 : tensor<32x8x17x128xbf16> loc(#loc2210)
      %1525 = stablehlo.add %1519, %1524 : tensor<32x8x17x128xbf16> loc(#loc2211)
      %1526 = stablehlo.convert %1525 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc2212)
      %1527 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1528 = stablehlo.multiply %1526, %1527 : tensor<32x8x17x128xf32> loc(#loc2213)
      %1529 = stablehlo.broadcast_in_dim %1481, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2214)
      %1530 = stablehlo.reshape %1529 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2215)
      %1531 = stablehlo.convert %1530 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2216)
      %1532 = stablehlo.transpose %1531, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc2217)
      %1533 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %1534 = stablehlo.multiply %1532, %1533 : tensor<32x8x128x128xf32> loc(#loc2218)
      %1535 = stablehlo.dot_general %1528, %1534, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2219)
      %1536 = stablehlo.add %1535, %159 : tensor<32x8x17x128xf32> loc(#loc2220)
      %1537 = stablehlo.convert %1536 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc2221)
      %1538 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %1539 = stablehlo.compare  EQ, %1537, %1538 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc2222)
      %1540 = stablehlo.not %1539 : tensor<32x8x17x128xi1> loc(#loc2223)
      %1541 = stablehlo.reduce(%1540 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.3551"), %arg1677: tensor<i1> loc("reduce.3551"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc2225)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc2226)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc2224)
      %1542 = stablehlo.reshape %1541 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc2227)
      %1543 = stablehlo.not %1542 : tensor<32x8x17x1xi1> loc(#loc2228)
      %1544 = stablehlo.reshape %1543 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc2229)
      %1545 = stablehlo.broadcast_in_dim %1544, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc2230)
      %1546 = stablehlo.reduce(%1536 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2231)
      %1547 = stablehlo.broadcast_in_dim %1546, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2232)
      %1548 = stablehlo.subtract %1536, %1547 : tensor<32x8x17x128xf32> loc(#loc2233)
      %1549 = stablehlo.exponential %1548 : tensor<32x8x17x128xf32> loc(#loc2234)
      %1550 = stablehlo.reduce(%1549 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2235)
      %1551 = stablehlo.broadcast_in_dim %1550, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2236)
      %1552 = stablehlo.divide %1549, %1551 : tensor<32x8x17x128xf32> loc(#loc2237)
      %1553 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1554 = stablehlo.select %1545, %1553, %1552 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc2238)
      %1555 = stablehlo.broadcast_in_dim %1488, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2239)
      %1556 = stablehlo.reshape %1555 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2240)
      %1557 = stablehlo.convert %1556 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2241)
      %1558 = stablehlo.dot_general %1554, %1557, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2242)
      %1559 = stablehlo.convert %1558 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc2243)
      %1560 = stablehlo.transpose %1559, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2244)
      %1561 = stablehlo.reshape %1560 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc2245)
      %1562 = stablehlo.reshape %arg955 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc2246)
      %1563 = stablehlo.reshape %1562 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc2247)
      %1564 = stablehlo.transpose %1563, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc2248)
      %1565 = stablehlo.dot_general %1561, %1564, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2249)
      %1566 = "stablehlo.all_reduce"(%1565) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.3568"), %arg1677: tensor<bf16> loc("dot.3568")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2249)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2249)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2249)
      %1567 = stablehlo.reshape %1566 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2250)
      %1568 = stablehlo.add %1436, %1567 : tensor<32x17x5120xbf16> loc(#loc2251)
      %1569 = stablehlo.reshape %arg958 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2252)
      %1570 = stablehlo.reshape %1569 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2253)
      %1571 = stablehlo.broadcast_in_dim %1570, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2254)
      %1572 = stablehlo.convert %1568 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2255)
      %1573 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1574 = stablehlo.power %1572, %1573 : tensor<32x17x5120xf32> loc(#loc2256)
      %1575 = stablehlo.reduce(%1574 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2257)
      %1576 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1577 = stablehlo.multiply %1575, %1576 : tensor<32x17xf32> loc(#loc2258)
      %1578 = stablehlo.reshape %1577 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2259)
      %1579 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1580 = stablehlo.add %1578, %1579 : tensor<32x17x1xf32> loc(#loc2260)
      %1581 = stablehlo.rsqrt %1580 : tensor<32x17x1xf32> loc(#loc2261)
      %1582 = stablehlo.reshape %1581 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2262)
      %1583 = stablehlo.broadcast_in_dim %1582, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2263)
      %1584 = stablehlo.multiply %1572, %1583 : tensor<32x17x5120xf32> loc(#loc2264)
      %1585 = stablehlo.convert %1584 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2265)
      %1586 = stablehlo.multiply %1571, %1585 : tensor<32x17x5120xbf16> loc(#loc2266)
      %1587 = stablehlo.reshape %1586 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2267)
      %1588 = stablehlo.reshape %arg959 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2268)
      %1589 = stablehlo.reshape %1588 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2269)
      %1590 = stablehlo.transpose %1589, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2270)
      %1591 = stablehlo.dot_general %1587, %1590, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2271)
      %1592 = stablehlo.reshape %1591 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2272)
      %1593 = stablehlo.logistic %1592 : tensor<32x17x3200xbf16> loc(#loc2273)
      %1594 = stablehlo.multiply %1592, %1593 : tensor<32x17x3200xbf16> loc(#loc2274)
      %1595 = stablehlo.reshape %arg954 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2275)
      %1596 = stablehlo.reshape %1595 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2276)
      %1597 = stablehlo.transpose %1596, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2277)
      %1598 = stablehlo.dot_general %1587, %1597, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2278)
      %1599 = stablehlo.reshape %1598 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2279)
      %1600 = stablehlo.multiply %1594, %1599 : tensor<32x17x3200xbf16> loc(#loc2280)
      %1601 = stablehlo.reshape %1600 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2281)
      %1602 = stablehlo.reshape %arg953 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc2282)
      %1603 = stablehlo.reshape %1602 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc2283)
      %1604 = stablehlo.transpose %1603, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc2284)
      %1605 = stablehlo.dot_general %1601, %1604, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2285)
      %1606 = "stablehlo.all_reduce"(%1605) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.3623"), %arg1677: tensor<bf16> loc("dot.3623")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2285)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2285)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2285)
      %1607 = stablehlo.reshape %1606 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2286)
      %1608 = stablehlo.add %1568, %1607 : tensor<32x17x5120xbf16> loc(#loc2287)
      %1609 = stablehlo.convert %1608 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2288)
      %1610 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1611 = stablehlo.power %1609, %1610 : tensor<32x17x5120xf32> loc(#loc2289)
      %1612 = stablehlo.reduce(%1611 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2290)
      %1613 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1614 = stablehlo.multiply %1612, %1613 : tensor<32x17xf32> loc(#loc2291)
      %1615 = stablehlo.reshape %1614 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2292)
      %1616 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1617 = stablehlo.add %1615, %1616 : tensor<32x17x1xf32> loc(#loc2293)
      %1618 = stablehlo.rsqrt %1617 : tensor<32x17x1xf32> loc(#loc2294)
      %1619 = stablehlo.reshape %1618 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2295)
      %1620 = stablehlo.broadcast_in_dim %1619, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2296)
      %1621 = stablehlo.multiply %1609, %1620 : tensor<32x17x5120xf32> loc(#loc2297)
      %1622 = stablehlo.convert %1621 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2298)
      %1623 = stablehlo.multiply %1494, %1622 : tensor<32x17x5120xbf16> loc(#loc2299)
      %1624 = stablehlo.reshape %1623 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2300)
      %1625 = stablehlo.reshape %arg952 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2301)
      %1626 = stablehlo.reshape %1625 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2302)
      %1627 = stablehlo.transpose %1626, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2303)
      %1628 = stablehlo.dot_general %1624, %1627, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2304)
      %1629 = stablehlo.reshape %1628 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2305)
      %1630 = stablehlo.convert %1629 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc2306)
      %1631 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %1632 = stablehlo.power %1630, %1631 : tensor<32x17x1x128xf32> loc(#loc2307)
      %1633 = stablehlo.reduce(%1632 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc2308)
      %1634 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1635 = stablehlo.multiply %1633, %1634 : tensor<32x17x1xf32> loc(#loc2309)
      %1636 = stablehlo.reshape %1635 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc2310)
      %1637 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %1638 = stablehlo.add %1636, %1637 : tensor<32x17x1x1xf32> loc(#loc2311)
      %1639 = stablehlo.rsqrt %1638 : tensor<32x17x1x1xf32> loc(#loc2312)
      %1640 = stablehlo.reshape %1639 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc2313)
      %1641 = stablehlo.broadcast_in_dim %1640, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc2314)
      %1642 = stablehlo.multiply %1630, %1641 : tensor<32x17x1x128xf32> loc(#loc2315)
      %1643 = stablehlo.convert %1642 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc2316)
      %1644 = stablehlo.multiply %1491, %1643 : tensor<32x17x1x128xbf16> loc(#loc2317)
      %1645 = stablehlo.transpose %1644, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2318)
      %1646 = stablehlo.multiply %1645, %82 : tensor<32x1x17x128xbf16> loc(#loc2319)
      %1647 = stablehlo.slice %1645 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2320)
      %1648 = stablehlo.negate %1647 : tensor<32x1x17x64xbf16> loc(#loc2321)
      %1649 = stablehlo.slice %1645 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2322)
      %1650 = stablehlo.concatenate %1648, %1649, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2323)
      %1651 = stablehlo.multiply %1650, %91 : tensor<32x1x17x128xbf16> loc(#loc2324)
      %1652 = stablehlo.add %1646, %1651 : tensor<32x1x17x128xbf16> loc(#loc2325)
      %1653 = "stablehlo.scatter"(%arg962, %21, %1652) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.3735"), %arg1677: tensor<bf16> loc("scatter.3735")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2326)
      %1654 = stablehlo.reshape %arg963 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2327)
      %1655 = stablehlo.reshape %1654 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2328)
      %1656 = stablehlo.transpose %1655, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2329)
      %1657 = stablehlo.dot_general %1624, %1656, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2330)
      %1658 = stablehlo.reshape %1657 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2331)
      %1659 = stablehlo.transpose %1658, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2332)
      %1660 = "stablehlo.scatter"(%arg964, %21, %1659) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.3765"), %arg1677: tensor<bf16> loc("scatter.3765")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2333)
      %1661 = stablehlo.reshape %arg974 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2334)
      %1662 = stablehlo.reshape %1661 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2335)
      %1663 = stablehlo.broadcast_in_dim %1662, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2336)
      %1664 = stablehlo.reshape %arg973 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2337)
      %1665 = stablehlo.reshape %1664 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2338)
      %1666 = stablehlo.broadcast_in_dim %1665, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2339)
      %1667 = stablehlo.reshape %arg970 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2340)
      %1668 = stablehlo.reshape %1667 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2341)
      %1669 = stablehlo.broadcast_in_dim %1668, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2342)
      %1670 = stablehlo.reshape %arg969 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc2343)
      %1671 = stablehlo.reshape %1670 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc2344)
      %1672 = stablehlo.transpose %1671, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc2345)
      %1673 = stablehlo.dot_general %1624, %1672, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc2346)
      %1674 = stablehlo.reshape %1673 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2347)
      %1675 = stablehlo.convert %1674 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc2348)
      %1676 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %1677 = stablehlo.power %1675, %1676 : tensor<32x17x8x128xf32> loc(#loc2349)
      %1678 = stablehlo.reduce(%1677 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc2350)
      %1679 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %1680 = stablehlo.multiply %1678, %1679 : tensor<32x17x8xf32> loc(#loc2351)
      %1681 = stablehlo.reshape %1680 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc2352)
      %1682 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %1683 = stablehlo.add %1681, %1682 : tensor<32x17x8x1xf32> loc(#loc2353)
      %1684 = stablehlo.rsqrt %1683 : tensor<32x17x8x1xf32> loc(#loc2354)
      %1685 = stablehlo.reshape %1684 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc2355)
      %1686 = stablehlo.broadcast_in_dim %1685, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc2356)
      %1687 = stablehlo.multiply %1675, %1686 : tensor<32x17x8x128xf32> loc(#loc2357)
      %1688 = stablehlo.convert %1687 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc2358)
      %1689 = stablehlo.multiply %1669, %1688 : tensor<32x17x8x128xbf16> loc(#loc2359)
      %1690 = stablehlo.transpose %1689, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2360)
      %1691 = stablehlo.multiply %1690, %132 : tensor<32x8x17x128xbf16> loc(#loc2361)
      %1692 = stablehlo.slice %1690 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2362)
      %1693 = stablehlo.negate %1692 : tensor<32x8x17x64xbf16> loc(#loc2363)
      %1694 = stablehlo.slice %1690 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2364)
      %1695 = stablehlo.concatenate %1693, %1694, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2365)
      %1696 = stablehlo.multiply %1695, %138 : tensor<32x8x17x128xbf16> loc(#loc2366)
      %1697 = stablehlo.add %1691, %1696 : tensor<32x8x17x128xbf16> loc(#loc2367)
      %1698 = stablehlo.convert %1697 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc2368)
      %1699 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1700 = stablehlo.multiply %1698, %1699 : tensor<32x8x17x128xf32> loc(#loc2369)
      %1701 = stablehlo.broadcast_in_dim %1653, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2370)
      %1702 = stablehlo.reshape %1701 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2371)
      %1703 = stablehlo.convert %1702 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2372)
      %1704 = stablehlo.transpose %1703, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc2373)
      %1705 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %1706 = stablehlo.multiply %1704, %1705 : tensor<32x8x128x128xf32> loc(#loc2374)
      %1707 = stablehlo.dot_general %1700, %1706, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2375)
      %1708 = stablehlo.add %1707, %159 : tensor<32x8x17x128xf32> loc(#loc2376)
      %1709 = stablehlo.convert %1708 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc2377)
      %1710 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %1711 = stablehlo.compare  EQ, %1709, %1710 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc2378)
      %1712 = stablehlo.not %1711 : tensor<32x8x17x128xi1> loc(#loc2379)
      %1713 = stablehlo.reduce(%1712 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.3946"), %arg1677: tensor<i1> loc("reduce.3946"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc2381)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc2382)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc2380)
      %1714 = stablehlo.reshape %1713 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc2383)
      %1715 = stablehlo.not %1714 : tensor<32x8x17x1xi1> loc(#loc2384)
      %1716 = stablehlo.reshape %1715 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc2385)
      %1717 = stablehlo.broadcast_in_dim %1716, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc2386)
      %1718 = stablehlo.reduce(%1708 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2387)
      %1719 = stablehlo.broadcast_in_dim %1718, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2388)
      %1720 = stablehlo.subtract %1708, %1719 : tensor<32x8x17x128xf32> loc(#loc2389)
      %1721 = stablehlo.exponential %1720 : tensor<32x8x17x128xf32> loc(#loc2390)
      %1722 = stablehlo.reduce(%1721 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2391)
      %1723 = stablehlo.broadcast_in_dim %1722, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2392)
      %1724 = stablehlo.divide %1721, %1723 : tensor<32x8x17x128xf32> loc(#loc2393)
      %1725 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1726 = stablehlo.select %1717, %1725, %1724 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc2394)
      %1727 = stablehlo.broadcast_in_dim %1660, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2395)
      %1728 = stablehlo.reshape %1727 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2396)
      %1729 = stablehlo.convert %1728 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2397)
      %1730 = stablehlo.dot_general %1726, %1729, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2398)
      %1731 = stablehlo.convert %1730 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc2399)
      %1732 = stablehlo.transpose %1731, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2400)
      %1733 = stablehlo.reshape %1732 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc2401)
      %1734 = stablehlo.reshape %arg968 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc2402)
      %1735 = stablehlo.reshape %1734 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc2403)
      %1736 = stablehlo.transpose %1735, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc2404)
      %1737 = stablehlo.dot_general %1733, %1736, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2405)
      %1738 = "stablehlo.all_reduce"(%1737) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.3963"), %arg1677: tensor<bf16> loc("dot.3963")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2405)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2405)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2405)
      %1739 = stablehlo.reshape %1738 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2406)
      %1740 = stablehlo.add %1608, %1739 : tensor<32x17x5120xbf16> loc(#loc2407)
      %1741 = stablehlo.reshape %arg971 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2408)
      %1742 = stablehlo.reshape %1741 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2409)
      %1743 = stablehlo.broadcast_in_dim %1742, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2410)
      %1744 = stablehlo.convert %1740 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2411)
      %1745 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1746 = stablehlo.power %1744, %1745 : tensor<32x17x5120xf32> loc(#loc2412)
      %1747 = stablehlo.reduce(%1746 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2413)
      %1748 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1749 = stablehlo.multiply %1747, %1748 : tensor<32x17xf32> loc(#loc2414)
      %1750 = stablehlo.reshape %1749 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2415)
      %1751 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1752 = stablehlo.add %1750, %1751 : tensor<32x17x1xf32> loc(#loc2416)
      %1753 = stablehlo.rsqrt %1752 : tensor<32x17x1xf32> loc(#loc2417)
      %1754 = stablehlo.reshape %1753 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2418)
      %1755 = stablehlo.broadcast_in_dim %1754, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2419)
      %1756 = stablehlo.multiply %1744, %1755 : tensor<32x17x5120xf32> loc(#loc2420)
      %1757 = stablehlo.convert %1756 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2421)
      %1758 = stablehlo.multiply %1743, %1757 : tensor<32x17x5120xbf16> loc(#loc2422)
      %1759 = stablehlo.reshape %1758 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2423)
      %1760 = stablehlo.reshape %arg972 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2424)
      %1761 = stablehlo.reshape %1760 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2425)
      %1762 = stablehlo.transpose %1761, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2426)
      %1763 = stablehlo.dot_general %1759, %1762, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2427)
      %1764 = stablehlo.reshape %1763 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2428)
      %1765 = stablehlo.logistic %1764 : tensor<32x17x3200xbf16> loc(#loc2429)
      %1766 = stablehlo.multiply %1764, %1765 : tensor<32x17x3200xbf16> loc(#loc2430)
      %1767 = stablehlo.reshape %arg967 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2431)
      %1768 = stablehlo.reshape %1767 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2432)
      %1769 = stablehlo.transpose %1768, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2433)
      %1770 = stablehlo.dot_general %1759, %1769, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2434)
      %1771 = stablehlo.reshape %1770 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2435)
      %1772 = stablehlo.multiply %1766, %1771 : tensor<32x17x3200xbf16> loc(#loc2436)
      %1773 = stablehlo.reshape %1772 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2437)
      %1774 = stablehlo.reshape %arg966 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc2438)
      %1775 = stablehlo.reshape %1774 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc2439)
      %1776 = stablehlo.transpose %1775, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc2440)
      %1777 = stablehlo.dot_general %1773, %1776, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2441)
      %1778 = "stablehlo.all_reduce"(%1777) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.4018"), %arg1677: tensor<bf16> loc("dot.4018")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2441)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2441)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2441)
      %1779 = stablehlo.reshape %1778 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2442)
      %1780 = stablehlo.add %1740, %1779 : tensor<32x17x5120xbf16> loc(#loc2443)
      %1781 = stablehlo.convert %1780 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2444)
      %1782 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1783 = stablehlo.power %1781, %1782 : tensor<32x17x5120xf32> loc(#loc2445)
      %1784 = stablehlo.reduce(%1783 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2446)
      %1785 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1786 = stablehlo.multiply %1784, %1785 : tensor<32x17xf32> loc(#loc2447)
      %1787 = stablehlo.reshape %1786 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2448)
      %1788 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1789 = stablehlo.add %1787, %1788 : tensor<32x17x1xf32> loc(#loc2449)
      %1790 = stablehlo.rsqrt %1789 : tensor<32x17x1xf32> loc(#loc2450)
      %1791 = stablehlo.reshape %1790 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2451)
      %1792 = stablehlo.broadcast_in_dim %1791, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2452)
      %1793 = stablehlo.multiply %1781, %1792 : tensor<32x17x5120xf32> loc(#loc2453)
      %1794 = stablehlo.convert %1793 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2454)
      %1795 = stablehlo.multiply %1666, %1794 : tensor<32x17x5120xbf16> loc(#loc2455)
      %1796 = stablehlo.reshape %1795 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2456)
      %1797 = stablehlo.reshape %arg965 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2457)
      %1798 = stablehlo.reshape %1797 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2458)
      %1799 = stablehlo.transpose %1798, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2459)
      %1800 = stablehlo.dot_general %1796, %1799, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2460)
      %1801 = stablehlo.reshape %1800 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2461)
      %1802 = stablehlo.convert %1801 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc2462)
      %1803 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %1804 = stablehlo.power %1802, %1803 : tensor<32x17x1x128xf32> loc(#loc2463)
      %1805 = stablehlo.reduce(%1804 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc2464)
      %1806 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1807 = stablehlo.multiply %1805, %1806 : tensor<32x17x1xf32> loc(#loc2465)
      %1808 = stablehlo.reshape %1807 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc2466)
      %1809 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %1810 = stablehlo.add %1808, %1809 : tensor<32x17x1x1xf32> loc(#loc2467)
      %1811 = stablehlo.rsqrt %1810 : tensor<32x17x1x1xf32> loc(#loc2468)
      %1812 = stablehlo.reshape %1811 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc2469)
      %1813 = stablehlo.broadcast_in_dim %1812, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc2470)
      %1814 = stablehlo.multiply %1802, %1813 : tensor<32x17x1x128xf32> loc(#loc2471)
      %1815 = stablehlo.convert %1814 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc2472)
      %1816 = stablehlo.multiply %1663, %1815 : tensor<32x17x1x128xbf16> loc(#loc2473)
      %1817 = stablehlo.transpose %1816, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2474)
      %1818 = stablehlo.multiply %1817, %82 : tensor<32x1x17x128xbf16> loc(#loc2475)
      %1819 = stablehlo.slice %1817 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2476)
      %1820 = stablehlo.negate %1819 : tensor<32x1x17x64xbf16> loc(#loc2477)
      %1821 = stablehlo.slice %1817 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2478)
      %1822 = stablehlo.concatenate %1820, %1821, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2479)
      %1823 = stablehlo.multiply %1822, %91 : tensor<32x1x17x128xbf16> loc(#loc2480)
      %1824 = stablehlo.add %1818, %1823 : tensor<32x1x17x128xbf16> loc(#loc2481)
      %1825 = "stablehlo.scatter"(%arg975, %21, %1824) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.4130"), %arg1677: tensor<bf16> loc("scatter.4130")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2482)
      %1826 = stablehlo.reshape %arg976 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2483)
      %1827 = stablehlo.reshape %1826 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2484)
      %1828 = stablehlo.transpose %1827, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2485)
      %1829 = stablehlo.dot_general %1796, %1828, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2486)
      %1830 = stablehlo.reshape %1829 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2487)
      %1831 = stablehlo.transpose %1830, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2488)
      %1832 = "stablehlo.scatter"(%arg977, %21, %1831) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.4160"), %arg1677: tensor<bf16> loc("scatter.4160")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2489)
      %1833 = stablehlo.reshape %arg987 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2490)
      %1834 = stablehlo.reshape %1833 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2491)
      %1835 = stablehlo.broadcast_in_dim %1834, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2492)
      %1836 = stablehlo.reshape %arg986 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2493)
      %1837 = stablehlo.reshape %1836 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2494)
      %1838 = stablehlo.broadcast_in_dim %1837, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2495)
      %1839 = stablehlo.reshape %arg983 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2496)
      %1840 = stablehlo.reshape %1839 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2497)
      %1841 = stablehlo.broadcast_in_dim %1840, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2498)
      %1842 = stablehlo.reshape %arg982 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc2499)
      %1843 = stablehlo.reshape %1842 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc2500)
      %1844 = stablehlo.transpose %1843, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc2501)
      %1845 = stablehlo.dot_general %1796, %1844, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc2502)
      %1846 = stablehlo.reshape %1845 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2503)
      %1847 = stablehlo.convert %1846 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc2504)
      %1848 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %1849 = stablehlo.power %1847, %1848 : tensor<32x17x8x128xf32> loc(#loc2505)
      %1850 = stablehlo.reduce(%1849 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc2506)
      %1851 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %1852 = stablehlo.multiply %1850, %1851 : tensor<32x17x8xf32> loc(#loc2507)
      %1853 = stablehlo.reshape %1852 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc2508)
      %1854 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %1855 = stablehlo.add %1853, %1854 : tensor<32x17x8x1xf32> loc(#loc2509)
      %1856 = stablehlo.rsqrt %1855 : tensor<32x17x8x1xf32> loc(#loc2510)
      %1857 = stablehlo.reshape %1856 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc2511)
      %1858 = stablehlo.broadcast_in_dim %1857, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc2512)
      %1859 = stablehlo.multiply %1847, %1858 : tensor<32x17x8x128xf32> loc(#loc2513)
      %1860 = stablehlo.convert %1859 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc2514)
      %1861 = stablehlo.multiply %1841, %1860 : tensor<32x17x8x128xbf16> loc(#loc2515)
      %1862 = stablehlo.transpose %1861, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2516)
      %1863 = stablehlo.multiply %1862, %132 : tensor<32x8x17x128xbf16> loc(#loc2517)
      %1864 = stablehlo.slice %1862 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2518)
      %1865 = stablehlo.negate %1864 : tensor<32x8x17x64xbf16> loc(#loc2519)
      %1866 = stablehlo.slice %1862 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2520)
      %1867 = stablehlo.concatenate %1865, %1866, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2521)
      %1868 = stablehlo.multiply %1867, %138 : tensor<32x8x17x128xbf16> loc(#loc2522)
      %1869 = stablehlo.add %1863, %1868 : tensor<32x8x17x128xbf16> loc(#loc2523)
      %1870 = stablehlo.convert %1869 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc2524)
      %1871 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1872 = stablehlo.multiply %1870, %1871 : tensor<32x8x17x128xf32> loc(#loc2525)
      %1873 = stablehlo.broadcast_in_dim %1825, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2526)
      %1874 = stablehlo.reshape %1873 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2527)
      %1875 = stablehlo.convert %1874 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2528)
      %1876 = stablehlo.transpose %1875, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc2529)
      %1877 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %1878 = stablehlo.multiply %1876, %1877 : tensor<32x8x128x128xf32> loc(#loc2530)
      %1879 = stablehlo.dot_general %1872, %1878, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2531)
      %1880 = stablehlo.add %1879, %159 : tensor<32x8x17x128xf32> loc(#loc2532)
      %1881 = stablehlo.convert %1880 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc2533)
      %1882 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %1883 = stablehlo.compare  EQ, %1881, %1882 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc2534)
      %1884 = stablehlo.not %1883 : tensor<32x8x17x128xi1> loc(#loc2535)
      %1885 = stablehlo.reduce(%1884 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.4341"), %arg1677: tensor<i1> loc("reduce.4341"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc2537)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc2538)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc2536)
      %1886 = stablehlo.reshape %1885 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc2539)
      %1887 = stablehlo.not %1886 : tensor<32x8x17x1xi1> loc(#loc2540)
      %1888 = stablehlo.reshape %1887 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc2541)
      %1889 = stablehlo.broadcast_in_dim %1888, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc2542)
      %1890 = stablehlo.reduce(%1880 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2543)
      %1891 = stablehlo.broadcast_in_dim %1890, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2544)
      %1892 = stablehlo.subtract %1880, %1891 : tensor<32x8x17x128xf32> loc(#loc2545)
      %1893 = stablehlo.exponential %1892 : tensor<32x8x17x128xf32> loc(#loc2546)
      %1894 = stablehlo.reduce(%1893 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2547)
      %1895 = stablehlo.broadcast_in_dim %1894, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2548)
      %1896 = stablehlo.divide %1893, %1895 : tensor<32x8x17x128xf32> loc(#loc2549)
      %1897 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %1898 = stablehlo.select %1889, %1897, %1896 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc2550)
      %1899 = stablehlo.broadcast_in_dim %1832, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2551)
      %1900 = stablehlo.reshape %1899 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2552)
      %1901 = stablehlo.convert %1900 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2553)
      %1902 = stablehlo.dot_general %1898, %1901, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2554)
      %1903 = stablehlo.convert %1902 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc2555)
      %1904 = stablehlo.transpose %1903, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2556)
      %1905 = stablehlo.reshape %1904 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc2557)
      %1906 = stablehlo.reshape %arg981 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc2558)
      %1907 = stablehlo.reshape %1906 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc2559)
      %1908 = stablehlo.transpose %1907, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc2560)
      %1909 = stablehlo.dot_general %1905, %1908, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2561)
      %1910 = "stablehlo.all_reduce"(%1909) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.4358"), %arg1677: tensor<bf16> loc("dot.4358")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2561)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2561)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2561)
      %1911 = stablehlo.reshape %1910 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2562)
      %1912 = stablehlo.add %1780, %1911 : tensor<32x17x5120xbf16> loc(#loc2563)
      %1913 = stablehlo.reshape %arg984 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2564)
      %1914 = stablehlo.reshape %1913 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2565)
      %1915 = stablehlo.broadcast_in_dim %1914, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2566)
      %1916 = stablehlo.convert %1912 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2567)
      %1917 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1918 = stablehlo.power %1916, %1917 : tensor<32x17x5120xf32> loc(#loc2568)
      %1919 = stablehlo.reduce(%1918 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2569)
      %1920 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1921 = stablehlo.multiply %1919, %1920 : tensor<32x17xf32> loc(#loc2570)
      %1922 = stablehlo.reshape %1921 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2571)
      %1923 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1924 = stablehlo.add %1922, %1923 : tensor<32x17x1xf32> loc(#loc2572)
      %1925 = stablehlo.rsqrt %1924 : tensor<32x17x1xf32> loc(#loc2573)
      %1926 = stablehlo.reshape %1925 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2574)
      %1927 = stablehlo.broadcast_in_dim %1926, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2575)
      %1928 = stablehlo.multiply %1916, %1927 : tensor<32x17x5120xf32> loc(#loc2576)
      %1929 = stablehlo.convert %1928 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2577)
      %1930 = stablehlo.multiply %1915, %1929 : tensor<32x17x5120xbf16> loc(#loc2578)
      %1931 = stablehlo.reshape %1930 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2579)
      %1932 = stablehlo.reshape %arg985 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2580)
      %1933 = stablehlo.reshape %1932 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2581)
      %1934 = stablehlo.transpose %1933, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2582)
      %1935 = stablehlo.dot_general %1931, %1934, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2583)
      %1936 = stablehlo.reshape %1935 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2584)
      %1937 = stablehlo.logistic %1936 : tensor<32x17x3200xbf16> loc(#loc2585)
      %1938 = stablehlo.multiply %1936, %1937 : tensor<32x17x3200xbf16> loc(#loc2586)
      %1939 = stablehlo.reshape %arg980 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2587)
      %1940 = stablehlo.reshape %1939 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2588)
      %1941 = stablehlo.transpose %1940, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2589)
      %1942 = stablehlo.dot_general %1931, %1941, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2590)
      %1943 = stablehlo.reshape %1942 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2591)
      %1944 = stablehlo.multiply %1938, %1943 : tensor<32x17x3200xbf16> loc(#loc2592)
      %1945 = stablehlo.reshape %1944 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2593)
      %1946 = stablehlo.reshape %arg979 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc2594)
      %1947 = stablehlo.reshape %1946 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc2595)
      %1948 = stablehlo.transpose %1947, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc2596)
      %1949 = stablehlo.dot_general %1945, %1948, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2597)
      %1950 = "stablehlo.all_reduce"(%1949) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.4413"), %arg1677: tensor<bf16> loc("dot.4413")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2597)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2597)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2597)
      %1951 = stablehlo.reshape %1950 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2598)
      %1952 = stablehlo.add %1912, %1951 : tensor<32x17x5120xbf16> loc(#loc2599)
      %1953 = stablehlo.convert %1952 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2600)
      %1954 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %1955 = stablehlo.power %1953, %1954 : tensor<32x17x5120xf32> loc(#loc2601)
      %1956 = stablehlo.reduce(%1955 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2602)
      %1957 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %1958 = stablehlo.multiply %1956, %1957 : tensor<32x17xf32> loc(#loc2603)
      %1959 = stablehlo.reshape %1958 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2604)
      %1960 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1961 = stablehlo.add %1959, %1960 : tensor<32x17x1xf32> loc(#loc2605)
      %1962 = stablehlo.rsqrt %1961 : tensor<32x17x1xf32> loc(#loc2606)
      %1963 = stablehlo.reshape %1962 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2607)
      %1964 = stablehlo.broadcast_in_dim %1963, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2608)
      %1965 = stablehlo.multiply %1953, %1964 : tensor<32x17x5120xf32> loc(#loc2609)
      %1966 = stablehlo.convert %1965 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2610)
      %1967 = stablehlo.multiply %1838, %1966 : tensor<32x17x5120xbf16> loc(#loc2611)
      %1968 = stablehlo.reshape %1967 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2612)
      %1969 = stablehlo.reshape %arg978 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2613)
      %1970 = stablehlo.reshape %1969 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2614)
      %1971 = stablehlo.transpose %1970, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2615)
      %1972 = stablehlo.dot_general %1968, %1971, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2616)
      %1973 = stablehlo.reshape %1972 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2617)
      %1974 = stablehlo.convert %1973 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc2618)
      %1975 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %1976 = stablehlo.power %1974, %1975 : tensor<32x17x1x128xf32> loc(#loc2619)
      %1977 = stablehlo.reduce(%1976 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc2620)
      %1978 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %1979 = stablehlo.multiply %1977, %1978 : tensor<32x17x1xf32> loc(#loc2621)
      %1980 = stablehlo.reshape %1979 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc2622)
      %1981 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %1982 = stablehlo.add %1980, %1981 : tensor<32x17x1x1xf32> loc(#loc2623)
      %1983 = stablehlo.rsqrt %1982 : tensor<32x17x1x1xf32> loc(#loc2624)
      %1984 = stablehlo.reshape %1983 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc2625)
      %1985 = stablehlo.broadcast_in_dim %1984, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc2626)
      %1986 = stablehlo.multiply %1974, %1985 : tensor<32x17x1x128xf32> loc(#loc2627)
      %1987 = stablehlo.convert %1986 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc2628)
      %1988 = stablehlo.multiply %1835, %1987 : tensor<32x17x1x128xbf16> loc(#loc2629)
      %1989 = stablehlo.transpose %1988, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2630)
      %1990 = stablehlo.multiply %1989, %82 : tensor<32x1x17x128xbf16> loc(#loc2631)
      %1991 = stablehlo.slice %1989 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2632)
      %1992 = stablehlo.negate %1991 : tensor<32x1x17x64xbf16> loc(#loc2633)
      %1993 = stablehlo.slice %1989 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2634)
      %1994 = stablehlo.concatenate %1992, %1993, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2635)
      %1995 = stablehlo.multiply %1994, %91 : tensor<32x1x17x128xbf16> loc(#loc2636)
      %1996 = stablehlo.add %1990, %1995 : tensor<32x1x17x128xbf16> loc(#loc2637)
      %1997 = "stablehlo.scatter"(%arg988, %21, %1996) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.4525"), %arg1677: tensor<bf16> loc("scatter.4525")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2638)
      %1998 = stablehlo.reshape %arg989 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2639)
      %1999 = stablehlo.reshape %1998 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2640)
      %2000 = stablehlo.transpose %1999, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2641)
      %2001 = stablehlo.dot_general %1968, %2000, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2642)
      %2002 = stablehlo.reshape %2001 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2643)
      %2003 = stablehlo.transpose %2002, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2644)
      %2004 = "stablehlo.scatter"(%arg990, %21, %2003) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.4555"), %arg1677: tensor<bf16> loc("scatter.4555")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2645)
      %2005 = stablehlo.reshape %arg1000 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2646)
      %2006 = stablehlo.reshape %2005 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2647)
      %2007 = stablehlo.broadcast_in_dim %2006, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2648)
      %2008 = stablehlo.reshape %arg999 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2649)
      %2009 = stablehlo.reshape %2008 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2650)
      %2010 = stablehlo.broadcast_in_dim %2009, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2651)
      %2011 = stablehlo.reshape %arg996 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2652)
      %2012 = stablehlo.reshape %2011 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2653)
      %2013 = stablehlo.broadcast_in_dim %2012, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2654)
      %2014 = stablehlo.reshape %arg995 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc2655)
      %2015 = stablehlo.reshape %2014 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc2656)
      %2016 = stablehlo.transpose %2015, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc2657)
      %2017 = stablehlo.dot_general %1968, %2016, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc2658)
      %2018 = stablehlo.reshape %2017 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2659)
      %2019 = stablehlo.convert %2018 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc2660)
      %2020 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %2021 = stablehlo.power %2019, %2020 : tensor<32x17x8x128xf32> loc(#loc2661)
      %2022 = stablehlo.reduce(%2021 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc2662)
      %2023 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %2024 = stablehlo.multiply %2022, %2023 : tensor<32x17x8xf32> loc(#loc2663)
      %2025 = stablehlo.reshape %2024 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc2664)
      %2026 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %2027 = stablehlo.add %2025, %2026 : tensor<32x17x8x1xf32> loc(#loc2665)
      %2028 = stablehlo.rsqrt %2027 : tensor<32x17x8x1xf32> loc(#loc2666)
      %2029 = stablehlo.reshape %2028 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc2667)
      %2030 = stablehlo.broadcast_in_dim %2029, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc2668)
      %2031 = stablehlo.multiply %2019, %2030 : tensor<32x17x8x128xf32> loc(#loc2669)
      %2032 = stablehlo.convert %2031 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc2670)
      %2033 = stablehlo.multiply %2013, %2032 : tensor<32x17x8x128xbf16> loc(#loc2671)
      %2034 = stablehlo.transpose %2033, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2672)
      %2035 = stablehlo.multiply %2034, %132 : tensor<32x8x17x128xbf16> loc(#loc2673)
      %2036 = stablehlo.slice %2034 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2674)
      %2037 = stablehlo.negate %2036 : tensor<32x8x17x64xbf16> loc(#loc2675)
      %2038 = stablehlo.slice %2034 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2676)
      %2039 = stablehlo.concatenate %2037, %2038, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2677)
      %2040 = stablehlo.multiply %2039, %138 : tensor<32x8x17x128xbf16> loc(#loc2678)
      %2041 = stablehlo.add %2035, %2040 : tensor<32x8x17x128xbf16> loc(#loc2679)
      %2042 = stablehlo.convert %2041 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc2680)
      %2043 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2044 = stablehlo.multiply %2042, %2043 : tensor<32x8x17x128xf32> loc(#loc2681)
      %2045 = stablehlo.broadcast_in_dim %1997, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2682)
      %2046 = stablehlo.reshape %2045 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2683)
      %2047 = stablehlo.convert %2046 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2684)
      %2048 = stablehlo.transpose %2047, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc2685)
      %2049 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %2050 = stablehlo.multiply %2048, %2049 : tensor<32x8x128x128xf32> loc(#loc2686)
      %2051 = stablehlo.dot_general %2044, %2050, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2687)
      %2052 = stablehlo.add %2051, %159 : tensor<32x8x17x128xf32> loc(#loc2688)
      %2053 = stablehlo.convert %2052 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc2689)
      %2054 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %2055 = stablehlo.compare  EQ, %2053, %2054 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc2690)
      %2056 = stablehlo.not %2055 : tensor<32x8x17x128xi1> loc(#loc2691)
      %2057 = stablehlo.reduce(%2056 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.4736"), %arg1677: tensor<i1> loc("reduce.4736"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc2693)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc2694)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc2692)
      %2058 = stablehlo.reshape %2057 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc2695)
      %2059 = stablehlo.not %2058 : tensor<32x8x17x1xi1> loc(#loc2696)
      %2060 = stablehlo.reshape %2059 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc2697)
      %2061 = stablehlo.broadcast_in_dim %2060, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc2698)
      %2062 = stablehlo.reduce(%2052 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2699)
      %2063 = stablehlo.broadcast_in_dim %2062, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2700)
      %2064 = stablehlo.subtract %2052, %2063 : tensor<32x8x17x128xf32> loc(#loc2701)
      %2065 = stablehlo.exponential %2064 : tensor<32x8x17x128xf32> loc(#loc2702)
      %2066 = stablehlo.reduce(%2065 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2703)
      %2067 = stablehlo.broadcast_in_dim %2066, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2704)
      %2068 = stablehlo.divide %2065, %2067 : tensor<32x8x17x128xf32> loc(#loc2705)
      %2069 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2070 = stablehlo.select %2061, %2069, %2068 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc2706)
      %2071 = stablehlo.broadcast_in_dim %2004, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2707)
      %2072 = stablehlo.reshape %2071 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2708)
      %2073 = stablehlo.convert %2072 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2709)
      %2074 = stablehlo.dot_general %2070, %2073, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2710)
      %2075 = stablehlo.convert %2074 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc2711)
      %2076 = stablehlo.transpose %2075, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2712)
      %2077 = stablehlo.reshape %2076 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc2713)
      %2078 = stablehlo.reshape %arg994 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc2714)
      %2079 = stablehlo.reshape %2078 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc2715)
      %2080 = stablehlo.transpose %2079, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc2716)
      %2081 = stablehlo.dot_general %2077, %2080, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2717)
      %2082 = "stablehlo.all_reduce"(%2081) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.4753"), %arg1677: tensor<bf16> loc("dot.4753")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2717)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2717)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2717)
      %2083 = stablehlo.reshape %2082 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2718)
      %2084 = stablehlo.add %1952, %2083 : tensor<32x17x5120xbf16> loc(#loc2719)
      %2085 = stablehlo.reshape %arg997 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2720)
      %2086 = stablehlo.reshape %2085 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2721)
      %2087 = stablehlo.broadcast_in_dim %2086, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2722)
      %2088 = stablehlo.convert %2084 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2723)
      %2089 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2090 = stablehlo.power %2088, %2089 : tensor<32x17x5120xf32> loc(#loc2724)
      %2091 = stablehlo.reduce(%2090 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2725)
      %2092 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2093 = stablehlo.multiply %2091, %2092 : tensor<32x17xf32> loc(#loc2726)
      %2094 = stablehlo.reshape %2093 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2727)
      %2095 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2096 = stablehlo.add %2094, %2095 : tensor<32x17x1xf32> loc(#loc2728)
      %2097 = stablehlo.rsqrt %2096 : tensor<32x17x1xf32> loc(#loc2729)
      %2098 = stablehlo.reshape %2097 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2730)
      %2099 = stablehlo.broadcast_in_dim %2098, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2731)
      %2100 = stablehlo.multiply %2088, %2099 : tensor<32x17x5120xf32> loc(#loc2732)
      %2101 = stablehlo.convert %2100 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2733)
      %2102 = stablehlo.multiply %2087, %2101 : tensor<32x17x5120xbf16> loc(#loc2734)
      %2103 = stablehlo.reshape %2102 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2735)
      %2104 = stablehlo.reshape %arg998 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2736)
      %2105 = stablehlo.reshape %2104 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2737)
      %2106 = stablehlo.transpose %2105, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2738)
      %2107 = stablehlo.dot_general %2103, %2106, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2739)
      %2108 = stablehlo.reshape %2107 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2740)
      %2109 = stablehlo.logistic %2108 : tensor<32x17x3200xbf16> loc(#loc2741)
      %2110 = stablehlo.multiply %2108, %2109 : tensor<32x17x3200xbf16> loc(#loc2742)
      %2111 = stablehlo.reshape %arg993 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2743)
      %2112 = stablehlo.reshape %2111 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2744)
      %2113 = stablehlo.transpose %2112, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2745)
      %2114 = stablehlo.dot_general %2103, %2113, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2746)
      %2115 = stablehlo.reshape %2114 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2747)
      %2116 = stablehlo.multiply %2110, %2115 : tensor<32x17x3200xbf16> loc(#loc2748)
      %2117 = stablehlo.reshape %2116 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2749)
      %2118 = stablehlo.reshape %arg992 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc2750)
      %2119 = stablehlo.reshape %2118 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc2751)
      %2120 = stablehlo.transpose %2119, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc2752)
      %2121 = stablehlo.dot_general %2117, %2120, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2753)
      %2122 = "stablehlo.all_reduce"(%2121) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.4808"), %arg1677: tensor<bf16> loc("dot.4808")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2753)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2753)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2753)
      %2123 = stablehlo.reshape %2122 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2754)
      %2124 = stablehlo.add %2084, %2123 : tensor<32x17x5120xbf16> loc(#loc2755)
      %2125 = stablehlo.convert %2124 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2756)
      %2126 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2127 = stablehlo.power %2125, %2126 : tensor<32x17x5120xf32> loc(#loc2757)
      %2128 = stablehlo.reduce(%2127 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2758)
      %2129 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2130 = stablehlo.multiply %2128, %2129 : tensor<32x17xf32> loc(#loc2759)
      %2131 = stablehlo.reshape %2130 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2760)
      %2132 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2133 = stablehlo.add %2131, %2132 : tensor<32x17x1xf32> loc(#loc2761)
      %2134 = stablehlo.rsqrt %2133 : tensor<32x17x1xf32> loc(#loc2762)
      %2135 = stablehlo.reshape %2134 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2763)
      %2136 = stablehlo.broadcast_in_dim %2135, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2764)
      %2137 = stablehlo.multiply %2125, %2136 : tensor<32x17x5120xf32> loc(#loc2765)
      %2138 = stablehlo.convert %2137 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2766)
      %2139 = stablehlo.multiply %2010, %2138 : tensor<32x17x5120xbf16> loc(#loc2767)
      %2140 = stablehlo.reshape %2139 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2768)
      %2141 = stablehlo.reshape %arg991 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2769)
      %2142 = stablehlo.reshape %2141 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2770)
      %2143 = stablehlo.transpose %2142, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2771)
      %2144 = stablehlo.dot_general %2140, %2143, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2772)
      %2145 = stablehlo.reshape %2144 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2773)
      %2146 = stablehlo.convert %2145 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc2774)
      %2147 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %2148 = stablehlo.power %2146, %2147 : tensor<32x17x1x128xf32> loc(#loc2775)
      %2149 = stablehlo.reduce(%2148 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc2776)
      %2150 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2151 = stablehlo.multiply %2149, %2150 : tensor<32x17x1xf32> loc(#loc2777)
      %2152 = stablehlo.reshape %2151 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc2778)
      %2153 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %2154 = stablehlo.add %2152, %2153 : tensor<32x17x1x1xf32> loc(#loc2779)
      %2155 = stablehlo.rsqrt %2154 : tensor<32x17x1x1xf32> loc(#loc2780)
      %2156 = stablehlo.reshape %2155 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc2781)
      %2157 = stablehlo.broadcast_in_dim %2156, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc2782)
      %2158 = stablehlo.multiply %2146, %2157 : tensor<32x17x1x128xf32> loc(#loc2783)
      %2159 = stablehlo.convert %2158 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc2784)
      %2160 = stablehlo.multiply %2007, %2159 : tensor<32x17x1x128xbf16> loc(#loc2785)
      %2161 = stablehlo.transpose %2160, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2786)
      %2162 = stablehlo.multiply %2161, %82 : tensor<32x1x17x128xbf16> loc(#loc2787)
      %2163 = stablehlo.slice %2161 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2788)
      %2164 = stablehlo.negate %2163 : tensor<32x1x17x64xbf16> loc(#loc2789)
      %2165 = stablehlo.slice %2161 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2790)
      %2166 = stablehlo.concatenate %2164, %2165, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2791)
      %2167 = stablehlo.multiply %2166, %91 : tensor<32x1x17x128xbf16> loc(#loc2792)
      %2168 = stablehlo.add %2162, %2167 : tensor<32x1x17x128xbf16> loc(#loc2793)
      %2169 = "stablehlo.scatter"(%arg1001, %21, %2168) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.4920"), %arg1677: tensor<bf16> loc("scatter.4920")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2794)
      %2170 = stablehlo.reshape %arg1002 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2795)
      %2171 = stablehlo.reshape %2170 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2796)
      %2172 = stablehlo.transpose %2171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2797)
      %2173 = stablehlo.dot_general %2140, %2172, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2798)
      %2174 = stablehlo.reshape %2173 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2799)
      %2175 = stablehlo.transpose %2174, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2800)
      %2176 = "stablehlo.scatter"(%arg1003, %21, %2175) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.4950"), %arg1677: tensor<bf16> loc("scatter.4950")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2801)
      %2177 = stablehlo.reshape %arg1013 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2802)
      %2178 = stablehlo.reshape %2177 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2803)
      %2179 = stablehlo.broadcast_in_dim %2178, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2804)
      %2180 = stablehlo.reshape %arg1012 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2805)
      %2181 = stablehlo.reshape %2180 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2806)
      %2182 = stablehlo.broadcast_in_dim %2181, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2807)
      %2183 = stablehlo.reshape %arg1009 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2808)
      %2184 = stablehlo.reshape %2183 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2809)
      %2185 = stablehlo.broadcast_in_dim %2184, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2810)
      %2186 = stablehlo.reshape %arg1008 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc2811)
      %2187 = stablehlo.reshape %2186 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc2812)
      %2188 = stablehlo.transpose %2187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc2813)
      %2189 = stablehlo.dot_general %2140, %2188, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc2814)
      %2190 = stablehlo.reshape %2189 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2815)
      %2191 = stablehlo.convert %2190 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc2816)
      %2192 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %2193 = stablehlo.power %2191, %2192 : tensor<32x17x8x128xf32> loc(#loc2817)
      %2194 = stablehlo.reduce(%2193 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc2818)
      %2195 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %2196 = stablehlo.multiply %2194, %2195 : tensor<32x17x8xf32> loc(#loc2819)
      %2197 = stablehlo.reshape %2196 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc2820)
      %2198 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %2199 = stablehlo.add %2197, %2198 : tensor<32x17x8x1xf32> loc(#loc2821)
      %2200 = stablehlo.rsqrt %2199 : tensor<32x17x8x1xf32> loc(#loc2822)
      %2201 = stablehlo.reshape %2200 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc2823)
      %2202 = stablehlo.broadcast_in_dim %2201, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc2824)
      %2203 = stablehlo.multiply %2191, %2202 : tensor<32x17x8x128xf32> loc(#loc2825)
      %2204 = stablehlo.convert %2203 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc2826)
      %2205 = stablehlo.multiply %2185, %2204 : tensor<32x17x8x128xbf16> loc(#loc2827)
      %2206 = stablehlo.transpose %2205, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2828)
      %2207 = stablehlo.multiply %2206, %132 : tensor<32x8x17x128xbf16> loc(#loc2829)
      %2208 = stablehlo.slice %2206 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2830)
      %2209 = stablehlo.negate %2208 : tensor<32x8x17x64xbf16> loc(#loc2831)
      %2210 = stablehlo.slice %2206 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2832)
      %2211 = stablehlo.concatenate %2209, %2210, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2833)
      %2212 = stablehlo.multiply %2211, %138 : tensor<32x8x17x128xbf16> loc(#loc2834)
      %2213 = stablehlo.add %2207, %2212 : tensor<32x8x17x128xbf16> loc(#loc2835)
      %2214 = stablehlo.convert %2213 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc2836)
      %2215 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2216 = stablehlo.multiply %2214, %2215 : tensor<32x8x17x128xf32> loc(#loc2837)
      %2217 = stablehlo.broadcast_in_dim %2169, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2838)
      %2218 = stablehlo.reshape %2217 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2839)
      %2219 = stablehlo.convert %2218 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2840)
      %2220 = stablehlo.transpose %2219, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc2841)
      %2221 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %2222 = stablehlo.multiply %2220, %2221 : tensor<32x8x128x128xf32> loc(#loc2842)
      %2223 = stablehlo.dot_general %2216, %2222, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2843)
      %2224 = stablehlo.add %2223, %159 : tensor<32x8x17x128xf32> loc(#loc2844)
      %2225 = stablehlo.convert %2224 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc2845)
      %2226 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %2227 = stablehlo.compare  EQ, %2225, %2226 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc2846)
      %2228 = stablehlo.not %2227 : tensor<32x8x17x128xi1> loc(#loc2847)
      %2229 = stablehlo.reduce(%2228 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.5131"), %arg1677: tensor<i1> loc("reduce.5131"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc2849)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc2850)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc2848)
      %2230 = stablehlo.reshape %2229 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc2851)
      %2231 = stablehlo.not %2230 : tensor<32x8x17x1xi1> loc(#loc2852)
      %2232 = stablehlo.reshape %2231 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc2853)
      %2233 = stablehlo.broadcast_in_dim %2232, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc2854)
      %2234 = stablehlo.reduce(%2224 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2855)
      %2235 = stablehlo.broadcast_in_dim %2234, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2856)
      %2236 = stablehlo.subtract %2224, %2235 : tensor<32x8x17x128xf32> loc(#loc2857)
      %2237 = stablehlo.exponential %2236 : tensor<32x8x17x128xf32> loc(#loc2858)
      %2238 = stablehlo.reduce(%2237 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc2859)
      %2239 = stablehlo.broadcast_in_dim %2238, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc2860)
      %2240 = stablehlo.divide %2237, %2239 : tensor<32x8x17x128xf32> loc(#loc2861)
      %2241 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2242 = stablehlo.select %2233, %2241, %2240 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc2862)
      %2243 = stablehlo.broadcast_in_dim %2176, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2863)
      %2244 = stablehlo.reshape %2243 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2864)
      %2245 = stablehlo.convert %2244 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2865)
      %2246 = stablehlo.dot_general %2242, %2245, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2866)
      %2247 = stablehlo.convert %2246 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc2867)
      %2248 = stablehlo.transpose %2247, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2868)
      %2249 = stablehlo.reshape %2248 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc2869)
      %2250 = stablehlo.reshape %arg1007 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc2870)
      %2251 = stablehlo.reshape %2250 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc2871)
      %2252 = stablehlo.transpose %2251, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc2872)
      %2253 = stablehlo.dot_general %2249, %2252, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2873)
      %2254 = "stablehlo.all_reduce"(%2253) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.5148"), %arg1677: tensor<bf16> loc("dot.5148")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2873)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2873)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2873)
      %2255 = stablehlo.reshape %2254 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2874)
      %2256 = stablehlo.add %2124, %2255 : tensor<32x17x5120xbf16> loc(#loc2875)
      %2257 = stablehlo.reshape %arg1010 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2876)
      %2258 = stablehlo.reshape %2257 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2877)
      %2259 = stablehlo.broadcast_in_dim %2258, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2878)
      %2260 = stablehlo.convert %2256 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2879)
      %2261 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2262 = stablehlo.power %2260, %2261 : tensor<32x17x5120xf32> loc(#loc2880)
      %2263 = stablehlo.reduce(%2262 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2881)
      %2264 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2265 = stablehlo.multiply %2263, %2264 : tensor<32x17xf32> loc(#loc2882)
      %2266 = stablehlo.reshape %2265 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2883)
      %2267 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2268 = stablehlo.add %2266, %2267 : tensor<32x17x1xf32> loc(#loc2884)
      %2269 = stablehlo.rsqrt %2268 : tensor<32x17x1xf32> loc(#loc2885)
      %2270 = stablehlo.reshape %2269 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2886)
      %2271 = stablehlo.broadcast_in_dim %2270, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2887)
      %2272 = stablehlo.multiply %2260, %2271 : tensor<32x17x5120xf32> loc(#loc2888)
      %2273 = stablehlo.convert %2272 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2889)
      %2274 = stablehlo.multiply %2259, %2273 : tensor<32x17x5120xbf16> loc(#loc2890)
      %2275 = stablehlo.reshape %2274 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2891)
      %2276 = stablehlo.reshape %arg1011 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2892)
      %2277 = stablehlo.reshape %2276 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2893)
      %2278 = stablehlo.transpose %2277, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2894)
      %2279 = stablehlo.dot_general %2275, %2278, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2895)
      %2280 = stablehlo.reshape %2279 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2896)
      %2281 = stablehlo.logistic %2280 : tensor<32x17x3200xbf16> loc(#loc2897)
      %2282 = stablehlo.multiply %2280, %2281 : tensor<32x17x3200xbf16> loc(#loc2898)
      %2283 = stablehlo.reshape %arg1006 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc2899)
      %2284 = stablehlo.reshape %2283 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc2900)
      %2285 = stablehlo.transpose %2284, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc2901)
      %2286 = stablehlo.dot_general %2275, %2285, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2902)
      %2287 = stablehlo.reshape %2286 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc2903)
      %2288 = stablehlo.multiply %2282, %2287 : tensor<32x17x3200xbf16> loc(#loc2904)
      %2289 = stablehlo.reshape %2288 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc2905)
      %2290 = stablehlo.reshape %arg1005 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc2906)
      %2291 = stablehlo.reshape %2290 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc2907)
      %2292 = stablehlo.transpose %2291, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc2908)
      %2293 = stablehlo.dot_general %2289, %2292, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2909)
      %2294 = "stablehlo.all_reduce"(%2293) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.5203"), %arg1677: tensor<bf16> loc("dot.5203")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc2909)
        stablehlo.return %11074 : tensor<bf16> loc(#loc2909)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2909)
      %2295 = stablehlo.reshape %2294 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2910)
      %2296 = stablehlo.add %2256, %2295 : tensor<32x17x5120xbf16> loc(#loc2911)
      %2297 = stablehlo.convert %2296 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc2912)
      %2298 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2299 = stablehlo.power %2297, %2298 : tensor<32x17x5120xf32> loc(#loc2913)
      %2300 = stablehlo.reduce(%2299 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc2914)
      %2301 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2302 = stablehlo.multiply %2300, %2301 : tensor<32x17xf32> loc(#loc2915)
      %2303 = stablehlo.reshape %2302 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc2916)
      %2304 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2305 = stablehlo.add %2303, %2304 : tensor<32x17x1xf32> loc(#loc2917)
      %2306 = stablehlo.rsqrt %2305 : tensor<32x17x1xf32> loc(#loc2918)
      %2307 = stablehlo.reshape %2306 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc2919)
      %2308 = stablehlo.broadcast_in_dim %2307, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc2920)
      %2309 = stablehlo.multiply %2297, %2308 : tensor<32x17x5120xf32> loc(#loc2921)
      %2310 = stablehlo.convert %2309 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc2922)
      %2311 = stablehlo.multiply %2182, %2310 : tensor<32x17x5120xbf16> loc(#loc2923)
      %2312 = stablehlo.reshape %2311 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc2924)
      %2313 = stablehlo.reshape %arg1004 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2925)
      %2314 = stablehlo.reshape %2313 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2926)
      %2315 = stablehlo.transpose %2314, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2927)
      %2316 = stablehlo.dot_general %2312, %2315, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2928)
      %2317 = stablehlo.reshape %2316 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2929)
      %2318 = stablehlo.convert %2317 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc2930)
      %2319 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %2320 = stablehlo.power %2318, %2319 : tensor<32x17x1x128xf32> loc(#loc2931)
      %2321 = stablehlo.reduce(%2320 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc2932)
      %2322 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2323 = stablehlo.multiply %2321, %2322 : tensor<32x17x1xf32> loc(#loc2933)
      %2324 = stablehlo.reshape %2323 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc2934)
      %2325 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %2326 = stablehlo.add %2324, %2325 : tensor<32x17x1x1xf32> loc(#loc2935)
      %2327 = stablehlo.rsqrt %2326 : tensor<32x17x1x1xf32> loc(#loc2936)
      %2328 = stablehlo.reshape %2327 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc2937)
      %2329 = stablehlo.broadcast_in_dim %2328, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc2938)
      %2330 = stablehlo.multiply %2318, %2329 : tensor<32x17x1x128xf32> loc(#loc2939)
      %2331 = stablehlo.convert %2330 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc2940)
      %2332 = stablehlo.multiply %2179, %2331 : tensor<32x17x1x128xbf16> loc(#loc2941)
      %2333 = stablehlo.transpose %2332, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2942)
      %2334 = stablehlo.multiply %2333, %82 : tensor<32x1x17x128xbf16> loc(#loc2943)
      %2335 = stablehlo.slice %2333 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2944)
      %2336 = stablehlo.negate %2335 : tensor<32x1x17x64xbf16> loc(#loc2945)
      %2337 = stablehlo.slice %2333 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc2946)
      %2338 = stablehlo.concatenate %2336, %2337, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2947)
      %2339 = stablehlo.multiply %2338, %91 : tensor<32x1x17x128xbf16> loc(#loc2948)
      %2340 = stablehlo.add %2334, %2339 : tensor<32x1x17x128xbf16> loc(#loc2949)
      %2341 = "stablehlo.scatter"(%arg1014, %21, %2340) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.5315"), %arg1677: tensor<bf16> loc("scatter.5315")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2950)
      %2342 = stablehlo.reshape %arg1015 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc2951)
      %2343 = stablehlo.reshape %2342 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc2952)
      %2344 = stablehlo.transpose %2343, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc2953)
      %2345 = stablehlo.dot_general %2312, %2344, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc2954)
      %2346 = stablehlo.reshape %2345 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2955)
      %2347 = stablehlo.transpose %2346, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc2956)
      %2348 = "stablehlo.scatter"(%arg1016, %21, %2347) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.5345"), %arg1677: tensor<bf16> loc("scatter.5345")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc2957)
      %2349 = stablehlo.reshape %arg1026 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2958)
      %2350 = stablehlo.reshape %2349 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2959)
      %2351 = stablehlo.broadcast_in_dim %2350, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc2960)
      %2352 = stablehlo.reshape %arg1025 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc2961)
      %2353 = stablehlo.reshape %2352 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc2962)
      %2354 = stablehlo.broadcast_in_dim %2353, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc2963)
      %2355 = stablehlo.reshape %arg1022 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc2964)
      %2356 = stablehlo.reshape %2355 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc2965)
      %2357 = stablehlo.broadcast_in_dim %2356, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2966)
      %2358 = stablehlo.reshape %arg1021 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc2967)
      %2359 = stablehlo.reshape %2358 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc2968)
      %2360 = stablehlo.transpose %2359, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc2969)
      %2361 = stablehlo.dot_general %2312, %2360, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc2970)
      %2362 = stablehlo.reshape %2361 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc2971)
      %2363 = stablehlo.convert %2362 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc2972)
      %2364 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %2365 = stablehlo.power %2363, %2364 : tensor<32x17x8x128xf32> loc(#loc2973)
      %2366 = stablehlo.reduce(%2365 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc2974)
      %2367 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %2368 = stablehlo.multiply %2366, %2367 : tensor<32x17x8xf32> loc(#loc2975)
      %2369 = stablehlo.reshape %2368 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc2976)
      %2370 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %2371 = stablehlo.add %2369, %2370 : tensor<32x17x8x1xf32> loc(#loc2977)
      %2372 = stablehlo.rsqrt %2371 : tensor<32x17x8x1xf32> loc(#loc2978)
      %2373 = stablehlo.reshape %2372 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc2979)
      %2374 = stablehlo.broadcast_in_dim %2373, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc2980)
      %2375 = stablehlo.multiply %2363, %2374 : tensor<32x17x8x128xf32> loc(#loc2981)
      %2376 = stablehlo.convert %2375 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc2982)
      %2377 = stablehlo.multiply %2357, %2376 : tensor<32x17x8x128xbf16> loc(#loc2983)
      %2378 = stablehlo.transpose %2377, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2984)
      %2379 = stablehlo.multiply %2378, %132 : tensor<32x8x17x128xbf16> loc(#loc2985)
      %2380 = stablehlo.slice %2378 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2986)
      %2381 = stablehlo.negate %2380 : tensor<32x8x17x64xbf16> loc(#loc2987)
      %2382 = stablehlo.slice %2378 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc2988)
      %2383 = stablehlo.concatenate %2381, %2382, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc2989)
      %2384 = stablehlo.multiply %2383, %138 : tensor<32x8x17x128xbf16> loc(#loc2990)
      %2385 = stablehlo.add %2379, %2384 : tensor<32x8x17x128xbf16> loc(#loc2991)
      %2386 = stablehlo.convert %2385 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc2992)
      %2387 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2388 = stablehlo.multiply %2386, %2387 : tensor<32x8x17x128xf32> loc(#loc2993)
      %2389 = stablehlo.broadcast_in_dim %2341, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc2994)
      %2390 = stablehlo.reshape %2389 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc2995)
      %2391 = stablehlo.convert %2390 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc2996)
      %2392 = stablehlo.transpose %2391, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc2997)
      %2393 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %2394 = stablehlo.multiply %2392, %2393 : tensor<32x8x128x128xf32> loc(#loc2998)
      %2395 = stablehlo.dot_general %2388, %2394, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc2999)
      %2396 = stablehlo.add %2395, %159 : tensor<32x8x17x128xf32> loc(#loc3000)
      %2397 = stablehlo.convert %2396 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc3001)
      %2398 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %2399 = stablehlo.compare  EQ, %2397, %2398 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc3002)
      %2400 = stablehlo.not %2399 : tensor<32x8x17x128xi1> loc(#loc3003)
      %2401 = stablehlo.reduce(%2400 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.5526"), %arg1677: tensor<i1> loc("reduce.5526"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc3005)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc3006)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc3004)
      %2402 = stablehlo.reshape %2401 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc3007)
      %2403 = stablehlo.not %2402 : tensor<32x8x17x1xi1> loc(#loc3008)
      %2404 = stablehlo.reshape %2403 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc3009)
      %2405 = stablehlo.broadcast_in_dim %2404, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc3010)
      %2406 = stablehlo.reduce(%2396 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3011)
      %2407 = stablehlo.broadcast_in_dim %2406, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3012)
      %2408 = stablehlo.subtract %2396, %2407 : tensor<32x8x17x128xf32> loc(#loc3013)
      %2409 = stablehlo.exponential %2408 : tensor<32x8x17x128xf32> loc(#loc3014)
      %2410 = stablehlo.reduce(%2409 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3015)
      %2411 = stablehlo.broadcast_in_dim %2410, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3016)
      %2412 = stablehlo.divide %2409, %2411 : tensor<32x8x17x128xf32> loc(#loc3017)
      %2413 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2414 = stablehlo.select %2405, %2413, %2412 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc3018)
      %2415 = stablehlo.broadcast_in_dim %2348, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3019)
      %2416 = stablehlo.reshape %2415 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3020)
      %2417 = stablehlo.convert %2416 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3021)
      %2418 = stablehlo.dot_general %2414, %2417, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3022)
      %2419 = stablehlo.convert %2418 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc3023)
      %2420 = stablehlo.transpose %2419, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3024)
      %2421 = stablehlo.reshape %2420 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc3025)
      %2422 = stablehlo.reshape %arg1020 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc3026)
      %2423 = stablehlo.reshape %2422 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc3027)
      %2424 = stablehlo.transpose %2423, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc3028)
      %2425 = stablehlo.dot_general %2421, %2424, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3029)
      %2426 = "stablehlo.all_reduce"(%2425) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.5543"), %arg1677: tensor<bf16> loc("dot.5543")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3029)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3029)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3029)
      %2427 = stablehlo.reshape %2426 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3030)
      %2428 = stablehlo.add %2296, %2427 : tensor<32x17x5120xbf16> loc(#loc3031)
      %2429 = stablehlo.reshape %arg1023 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3032)
      %2430 = stablehlo.reshape %2429 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3033)
      %2431 = stablehlo.broadcast_in_dim %2430, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3034)
      %2432 = stablehlo.convert %2428 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3035)
      %2433 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2434 = stablehlo.power %2432, %2433 : tensor<32x17x5120xf32> loc(#loc3036)
      %2435 = stablehlo.reduce(%2434 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3037)
      %2436 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2437 = stablehlo.multiply %2435, %2436 : tensor<32x17xf32> loc(#loc3038)
      %2438 = stablehlo.reshape %2437 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3039)
      %2439 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2440 = stablehlo.add %2438, %2439 : tensor<32x17x1xf32> loc(#loc3040)
      %2441 = stablehlo.rsqrt %2440 : tensor<32x17x1xf32> loc(#loc3041)
      %2442 = stablehlo.reshape %2441 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3042)
      %2443 = stablehlo.broadcast_in_dim %2442, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3043)
      %2444 = stablehlo.multiply %2432, %2443 : tensor<32x17x5120xf32> loc(#loc3044)
      %2445 = stablehlo.convert %2444 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3045)
      %2446 = stablehlo.multiply %2431, %2445 : tensor<32x17x5120xbf16> loc(#loc3046)
      %2447 = stablehlo.reshape %2446 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3047)
      %2448 = stablehlo.reshape %arg1024 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3048)
      %2449 = stablehlo.reshape %2448 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3049)
      %2450 = stablehlo.transpose %2449, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3050)
      %2451 = stablehlo.dot_general %2447, %2450, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3051)
      %2452 = stablehlo.reshape %2451 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3052)
      %2453 = stablehlo.logistic %2452 : tensor<32x17x3200xbf16> loc(#loc3053)
      %2454 = stablehlo.multiply %2452, %2453 : tensor<32x17x3200xbf16> loc(#loc3054)
      %2455 = stablehlo.reshape %arg1019 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3055)
      %2456 = stablehlo.reshape %2455 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3056)
      %2457 = stablehlo.transpose %2456, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3057)
      %2458 = stablehlo.dot_general %2447, %2457, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3058)
      %2459 = stablehlo.reshape %2458 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3059)
      %2460 = stablehlo.multiply %2454, %2459 : tensor<32x17x3200xbf16> loc(#loc3060)
      %2461 = stablehlo.reshape %2460 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3061)
      %2462 = stablehlo.reshape %arg1018 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc3062)
      %2463 = stablehlo.reshape %2462 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc3063)
      %2464 = stablehlo.transpose %2463, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc3064)
      %2465 = stablehlo.dot_general %2461, %2464, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3065)
      %2466 = "stablehlo.all_reduce"(%2465) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.5598"), %arg1677: tensor<bf16> loc("dot.5598")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3065)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3065)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3065)
      %2467 = stablehlo.reshape %2466 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3066)
      %2468 = stablehlo.add %2428, %2467 : tensor<32x17x5120xbf16> loc(#loc3067)
      %2469 = stablehlo.convert %2468 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3068)
      %2470 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2471 = stablehlo.power %2469, %2470 : tensor<32x17x5120xf32> loc(#loc3069)
      %2472 = stablehlo.reduce(%2471 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3070)
      %2473 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2474 = stablehlo.multiply %2472, %2473 : tensor<32x17xf32> loc(#loc3071)
      %2475 = stablehlo.reshape %2474 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3072)
      %2476 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2477 = stablehlo.add %2475, %2476 : tensor<32x17x1xf32> loc(#loc3073)
      %2478 = stablehlo.rsqrt %2477 : tensor<32x17x1xf32> loc(#loc3074)
      %2479 = stablehlo.reshape %2478 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3075)
      %2480 = stablehlo.broadcast_in_dim %2479, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3076)
      %2481 = stablehlo.multiply %2469, %2480 : tensor<32x17x5120xf32> loc(#loc3077)
      %2482 = stablehlo.convert %2481 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3078)
      %2483 = stablehlo.multiply %2354, %2482 : tensor<32x17x5120xbf16> loc(#loc3079)
      %2484 = stablehlo.reshape %2483 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3080)
      %2485 = stablehlo.reshape %arg1017 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3081)
      %2486 = stablehlo.reshape %2485 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3082)
      %2487 = stablehlo.transpose %2486, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3083)
      %2488 = stablehlo.dot_general %2484, %2487, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3084)
      %2489 = stablehlo.reshape %2488 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3085)
      %2490 = stablehlo.convert %2489 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc3086)
      %2491 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %2492 = stablehlo.power %2490, %2491 : tensor<32x17x1x128xf32> loc(#loc3087)
      %2493 = stablehlo.reduce(%2492 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc3088)
      %2494 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2495 = stablehlo.multiply %2493, %2494 : tensor<32x17x1xf32> loc(#loc3089)
      %2496 = stablehlo.reshape %2495 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc3090)
      %2497 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %2498 = stablehlo.add %2496, %2497 : tensor<32x17x1x1xf32> loc(#loc3091)
      %2499 = stablehlo.rsqrt %2498 : tensor<32x17x1x1xf32> loc(#loc3092)
      %2500 = stablehlo.reshape %2499 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc3093)
      %2501 = stablehlo.broadcast_in_dim %2500, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc3094)
      %2502 = stablehlo.multiply %2490, %2501 : tensor<32x17x1x128xf32> loc(#loc3095)
      %2503 = stablehlo.convert %2502 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc3096)
      %2504 = stablehlo.multiply %2351, %2503 : tensor<32x17x1x128xbf16> loc(#loc3097)
      %2505 = stablehlo.transpose %2504, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3098)
      %2506 = stablehlo.multiply %2505, %82 : tensor<32x1x17x128xbf16> loc(#loc3099)
      %2507 = stablehlo.slice %2505 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3100)
      %2508 = stablehlo.negate %2507 : tensor<32x1x17x64xbf16> loc(#loc3101)
      %2509 = stablehlo.slice %2505 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3102)
      %2510 = stablehlo.concatenate %2508, %2509, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3103)
      %2511 = stablehlo.multiply %2510, %91 : tensor<32x1x17x128xbf16> loc(#loc3104)
      %2512 = stablehlo.add %2506, %2511 : tensor<32x1x17x128xbf16> loc(#loc3105)
      %2513 = "stablehlo.scatter"(%arg1027, %21, %2512) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.5710"), %arg1677: tensor<bf16> loc("scatter.5710")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3106)
      %2514 = stablehlo.reshape %arg1028 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3107)
      %2515 = stablehlo.reshape %2514 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3108)
      %2516 = stablehlo.transpose %2515, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3109)
      %2517 = stablehlo.dot_general %2484, %2516, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3110)
      %2518 = stablehlo.reshape %2517 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3111)
      %2519 = stablehlo.transpose %2518, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3112)
      %2520 = "stablehlo.scatter"(%arg1029, %21, %2519) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.5740"), %arg1677: tensor<bf16> loc("scatter.5740")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3113)
      %2521 = stablehlo.reshape %arg1039 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3114)
      %2522 = stablehlo.reshape %2521 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3115)
      %2523 = stablehlo.broadcast_in_dim %2522, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3116)
      %2524 = stablehlo.reshape %arg1038 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3117)
      %2525 = stablehlo.reshape %2524 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3118)
      %2526 = stablehlo.broadcast_in_dim %2525, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3119)
      %2527 = stablehlo.reshape %arg1035 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3120)
      %2528 = stablehlo.reshape %2527 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3121)
      %2529 = stablehlo.broadcast_in_dim %2528, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3122)
      %2530 = stablehlo.reshape %arg1034 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc3123)
      %2531 = stablehlo.reshape %2530 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc3124)
      %2532 = stablehlo.transpose %2531, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc3125)
      %2533 = stablehlo.dot_general %2484, %2532, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc3126)
      %2534 = stablehlo.reshape %2533 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3127)
      %2535 = stablehlo.convert %2534 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc3128)
      %2536 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %2537 = stablehlo.power %2535, %2536 : tensor<32x17x8x128xf32> loc(#loc3129)
      %2538 = stablehlo.reduce(%2537 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc3130)
      %2539 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %2540 = stablehlo.multiply %2538, %2539 : tensor<32x17x8xf32> loc(#loc3131)
      %2541 = stablehlo.reshape %2540 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc3132)
      %2542 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %2543 = stablehlo.add %2541, %2542 : tensor<32x17x8x1xf32> loc(#loc3133)
      %2544 = stablehlo.rsqrt %2543 : tensor<32x17x8x1xf32> loc(#loc3134)
      %2545 = stablehlo.reshape %2544 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc3135)
      %2546 = stablehlo.broadcast_in_dim %2545, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc3136)
      %2547 = stablehlo.multiply %2535, %2546 : tensor<32x17x8x128xf32> loc(#loc3137)
      %2548 = stablehlo.convert %2547 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc3138)
      %2549 = stablehlo.multiply %2529, %2548 : tensor<32x17x8x128xbf16> loc(#loc3139)
      %2550 = stablehlo.transpose %2549, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3140)
      %2551 = stablehlo.multiply %2550, %132 : tensor<32x8x17x128xbf16> loc(#loc3141)
      %2552 = stablehlo.slice %2550 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3142)
      %2553 = stablehlo.negate %2552 : tensor<32x8x17x64xbf16> loc(#loc3143)
      %2554 = stablehlo.slice %2550 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3144)
      %2555 = stablehlo.concatenate %2553, %2554, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3145)
      %2556 = stablehlo.multiply %2555, %138 : tensor<32x8x17x128xbf16> loc(#loc3146)
      %2557 = stablehlo.add %2551, %2556 : tensor<32x8x17x128xbf16> loc(#loc3147)
      %2558 = stablehlo.convert %2557 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc3148)
      %2559 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2560 = stablehlo.multiply %2558, %2559 : tensor<32x8x17x128xf32> loc(#loc3149)
      %2561 = stablehlo.broadcast_in_dim %2513, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3150)
      %2562 = stablehlo.reshape %2561 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3151)
      %2563 = stablehlo.convert %2562 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3152)
      %2564 = stablehlo.transpose %2563, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc3153)
      %2565 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %2566 = stablehlo.multiply %2564, %2565 : tensor<32x8x128x128xf32> loc(#loc3154)
      %2567 = stablehlo.dot_general %2560, %2566, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3155)
      %2568 = stablehlo.add %2567, %159 : tensor<32x8x17x128xf32> loc(#loc3156)
      %2569 = stablehlo.convert %2568 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc3157)
      %2570 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %2571 = stablehlo.compare  EQ, %2569, %2570 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc3158)
      %2572 = stablehlo.not %2571 : tensor<32x8x17x128xi1> loc(#loc3159)
      %2573 = stablehlo.reduce(%2572 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.5921"), %arg1677: tensor<i1> loc("reduce.5921"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc3161)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc3162)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc3160)
      %2574 = stablehlo.reshape %2573 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc3163)
      %2575 = stablehlo.not %2574 : tensor<32x8x17x1xi1> loc(#loc3164)
      %2576 = stablehlo.reshape %2575 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc3165)
      %2577 = stablehlo.broadcast_in_dim %2576, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc3166)
      %2578 = stablehlo.reduce(%2568 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3167)
      %2579 = stablehlo.broadcast_in_dim %2578, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3168)
      %2580 = stablehlo.subtract %2568, %2579 : tensor<32x8x17x128xf32> loc(#loc3169)
      %2581 = stablehlo.exponential %2580 : tensor<32x8x17x128xf32> loc(#loc3170)
      %2582 = stablehlo.reduce(%2581 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3171)
      %2583 = stablehlo.broadcast_in_dim %2582, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3172)
      %2584 = stablehlo.divide %2581, %2583 : tensor<32x8x17x128xf32> loc(#loc3173)
      %2585 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2586 = stablehlo.select %2577, %2585, %2584 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc3174)
      %2587 = stablehlo.broadcast_in_dim %2520, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3175)
      %2588 = stablehlo.reshape %2587 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3176)
      %2589 = stablehlo.convert %2588 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3177)
      %2590 = stablehlo.dot_general %2586, %2589, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3178)
      %2591 = stablehlo.convert %2590 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc3179)
      %2592 = stablehlo.transpose %2591, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3180)
      %2593 = stablehlo.reshape %2592 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc3181)
      %2594 = stablehlo.reshape %arg1033 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc3182)
      %2595 = stablehlo.reshape %2594 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc3183)
      %2596 = stablehlo.transpose %2595, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc3184)
      %2597 = stablehlo.dot_general %2593, %2596, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3185)
      %2598 = "stablehlo.all_reduce"(%2597) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.5938"), %arg1677: tensor<bf16> loc("dot.5938")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3185)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3185)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3185)
      %2599 = stablehlo.reshape %2598 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3186)
      %2600 = stablehlo.add %2468, %2599 : tensor<32x17x5120xbf16> loc(#loc3187)
      %2601 = stablehlo.reshape %arg1036 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3188)
      %2602 = stablehlo.reshape %2601 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3189)
      %2603 = stablehlo.broadcast_in_dim %2602, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3190)
      %2604 = stablehlo.convert %2600 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3191)
      %2605 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2606 = stablehlo.power %2604, %2605 : tensor<32x17x5120xf32> loc(#loc3192)
      %2607 = stablehlo.reduce(%2606 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3193)
      %2608 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2609 = stablehlo.multiply %2607, %2608 : tensor<32x17xf32> loc(#loc3194)
      %2610 = stablehlo.reshape %2609 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3195)
      %2611 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2612 = stablehlo.add %2610, %2611 : tensor<32x17x1xf32> loc(#loc3196)
      %2613 = stablehlo.rsqrt %2612 : tensor<32x17x1xf32> loc(#loc3197)
      %2614 = stablehlo.reshape %2613 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3198)
      %2615 = stablehlo.broadcast_in_dim %2614, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3199)
      %2616 = stablehlo.multiply %2604, %2615 : tensor<32x17x5120xf32> loc(#loc3200)
      %2617 = stablehlo.convert %2616 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3201)
      %2618 = stablehlo.multiply %2603, %2617 : tensor<32x17x5120xbf16> loc(#loc3202)
      %2619 = stablehlo.reshape %2618 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3203)
      %2620 = stablehlo.reshape %arg1037 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3204)
      %2621 = stablehlo.reshape %2620 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3205)
      %2622 = stablehlo.transpose %2621, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3206)
      %2623 = stablehlo.dot_general %2619, %2622, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3207)
      %2624 = stablehlo.reshape %2623 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3208)
      %2625 = stablehlo.logistic %2624 : tensor<32x17x3200xbf16> loc(#loc3209)
      %2626 = stablehlo.multiply %2624, %2625 : tensor<32x17x3200xbf16> loc(#loc3210)
      %2627 = stablehlo.reshape %arg1032 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3211)
      %2628 = stablehlo.reshape %2627 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3212)
      %2629 = stablehlo.transpose %2628, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3213)
      %2630 = stablehlo.dot_general %2619, %2629, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3214)
      %2631 = stablehlo.reshape %2630 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3215)
      %2632 = stablehlo.multiply %2626, %2631 : tensor<32x17x3200xbf16> loc(#loc3216)
      %2633 = stablehlo.reshape %2632 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3217)
      %2634 = stablehlo.reshape %arg1031 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc3218)
      %2635 = stablehlo.reshape %2634 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc3219)
      %2636 = stablehlo.transpose %2635, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc3220)
      %2637 = stablehlo.dot_general %2633, %2636, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3221)
      %2638 = "stablehlo.all_reduce"(%2637) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.5993"), %arg1677: tensor<bf16> loc("dot.5993")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3221)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3221)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3221)
      %2639 = stablehlo.reshape %2638 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3222)
      %2640 = stablehlo.add %2600, %2639 : tensor<32x17x5120xbf16> loc(#loc3223)
      %2641 = stablehlo.convert %2640 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3224)
      %2642 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2643 = stablehlo.power %2641, %2642 : tensor<32x17x5120xf32> loc(#loc3225)
      %2644 = stablehlo.reduce(%2643 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3226)
      %2645 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2646 = stablehlo.multiply %2644, %2645 : tensor<32x17xf32> loc(#loc3227)
      %2647 = stablehlo.reshape %2646 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3228)
      %2648 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2649 = stablehlo.add %2647, %2648 : tensor<32x17x1xf32> loc(#loc3229)
      %2650 = stablehlo.rsqrt %2649 : tensor<32x17x1xf32> loc(#loc3230)
      %2651 = stablehlo.reshape %2650 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3231)
      %2652 = stablehlo.broadcast_in_dim %2651, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3232)
      %2653 = stablehlo.multiply %2641, %2652 : tensor<32x17x5120xf32> loc(#loc3233)
      %2654 = stablehlo.convert %2653 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3234)
      %2655 = stablehlo.multiply %2526, %2654 : tensor<32x17x5120xbf16> loc(#loc3235)
      %2656 = stablehlo.reshape %2655 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3236)
      %2657 = stablehlo.reshape %arg1030 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3237)
      %2658 = stablehlo.reshape %2657 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3238)
      %2659 = stablehlo.transpose %2658, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3239)
      %2660 = stablehlo.dot_general %2656, %2659, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3240)
      %2661 = stablehlo.reshape %2660 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3241)
      %2662 = stablehlo.convert %2661 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc3242)
      %2663 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %2664 = stablehlo.power %2662, %2663 : tensor<32x17x1x128xf32> loc(#loc3243)
      %2665 = stablehlo.reduce(%2664 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc3244)
      %2666 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2667 = stablehlo.multiply %2665, %2666 : tensor<32x17x1xf32> loc(#loc3245)
      %2668 = stablehlo.reshape %2667 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc3246)
      %2669 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %2670 = stablehlo.add %2668, %2669 : tensor<32x17x1x1xf32> loc(#loc3247)
      %2671 = stablehlo.rsqrt %2670 : tensor<32x17x1x1xf32> loc(#loc3248)
      %2672 = stablehlo.reshape %2671 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc3249)
      %2673 = stablehlo.broadcast_in_dim %2672, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc3250)
      %2674 = stablehlo.multiply %2662, %2673 : tensor<32x17x1x128xf32> loc(#loc3251)
      %2675 = stablehlo.convert %2674 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc3252)
      %2676 = stablehlo.multiply %2523, %2675 : tensor<32x17x1x128xbf16> loc(#loc3253)
      %2677 = stablehlo.transpose %2676, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3254)
      %2678 = stablehlo.multiply %2677, %82 : tensor<32x1x17x128xbf16> loc(#loc3255)
      %2679 = stablehlo.slice %2677 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3256)
      %2680 = stablehlo.negate %2679 : tensor<32x1x17x64xbf16> loc(#loc3257)
      %2681 = stablehlo.slice %2677 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3258)
      %2682 = stablehlo.concatenate %2680, %2681, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3259)
      %2683 = stablehlo.multiply %2682, %91 : tensor<32x1x17x128xbf16> loc(#loc3260)
      %2684 = stablehlo.add %2678, %2683 : tensor<32x1x17x128xbf16> loc(#loc3261)
      %2685 = "stablehlo.scatter"(%arg1040, %21, %2684) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.6105"), %arg1677: tensor<bf16> loc("scatter.6105")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3262)
      %2686 = stablehlo.reshape %arg1041 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3263)
      %2687 = stablehlo.reshape %2686 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3264)
      %2688 = stablehlo.transpose %2687, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3265)
      %2689 = stablehlo.dot_general %2656, %2688, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3266)
      %2690 = stablehlo.reshape %2689 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3267)
      %2691 = stablehlo.transpose %2690, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3268)
      %2692 = "stablehlo.scatter"(%arg1042, %21, %2691) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.6135"), %arg1677: tensor<bf16> loc("scatter.6135")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3269)
      %2693 = stablehlo.reshape %arg1052 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3270)
      %2694 = stablehlo.reshape %2693 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3271)
      %2695 = stablehlo.broadcast_in_dim %2694, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3272)
      %2696 = stablehlo.reshape %arg1051 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3273)
      %2697 = stablehlo.reshape %2696 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3274)
      %2698 = stablehlo.broadcast_in_dim %2697, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3275)
      %2699 = stablehlo.reshape %arg1048 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3276)
      %2700 = stablehlo.reshape %2699 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3277)
      %2701 = stablehlo.broadcast_in_dim %2700, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3278)
      %2702 = stablehlo.reshape %arg1047 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc3279)
      %2703 = stablehlo.reshape %2702 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc3280)
      %2704 = stablehlo.transpose %2703, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc3281)
      %2705 = stablehlo.dot_general %2656, %2704, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc3282)
      %2706 = stablehlo.reshape %2705 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3283)
      %2707 = stablehlo.convert %2706 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc3284)
      %2708 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %2709 = stablehlo.power %2707, %2708 : tensor<32x17x8x128xf32> loc(#loc3285)
      %2710 = stablehlo.reduce(%2709 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc3286)
      %2711 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %2712 = stablehlo.multiply %2710, %2711 : tensor<32x17x8xf32> loc(#loc3287)
      %2713 = stablehlo.reshape %2712 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc3288)
      %2714 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %2715 = stablehlo.add %2713, %2714 : tensor<32x17x8x1xf32> loc(#loc3289)
      %2716 = stablehlo.rsqrt %2715 : tensor<32x17x8x1xf32> loc(#loc3290)
      %2717 = stablehlo.reshape %2716 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc3291)
      %2718 = stablehlo.broadcast_in_dim %2717, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc3292)
      %2719 = stablehlo.multiply %2707, %2718 : tensor<32x17x8x128xf32> loc(#loc3293)
      %2720 = stablehlo.convert %2719 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc3294)
      %2721 = stablehlo.multiply %2701, %2720 : tensor<32x17x8x128xbf16> loc(#loc3295)
      %2722 = stablehlo.transpose %2721, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3296)
      %2723 = stablehlo.multiply %2722, %132 : tensor<32x8x17x128xbf16> loc(#loc3297)
      %2724 = stablehlo.slice %2722 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3298)
      %2725 = stablehlo.negate %2724 : tensor<32x8x17x64xbf16> loc(#loc3299)
      %2726 = stablehlo.slice %2722 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3300)
      %2727 = stablehlo.concatenate %2725, %2726, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3301)
      %2728 = stablehlo.multiply %2727, %138 : tensor<32x8x17x128xbf16> loc(#loc3302)
      %2729 = stablehlo.add %2723, %2728 : tensor<32x8x17x128xbf16> loc(#loc3303)
      %2730 = stablehlo.convert %2729 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc3304)
      %2731 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2732 = stablehlo.multiply %2730, %2731 : tensor<32x8x17x128xf32> loc(#loc3305)
      %2733 = stablehlo.broadcast_in_dim %2685, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3306)
      %2734 = stablehlo.reshape %2733 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3307)
      %2735 = stablehlo.convert %2734 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3308)
      %2736 = stablehlo.transpose %2735, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc3309)
      %2737 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %2738 = stablehlo.multiply %2736, %2737 : tensor<32x8x128x128xf32> loc(#loc3310)
      %2739 = stablehlo.dot_general %2732, %2738, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3311)
      %2740 = stablehlo.add %2739, %159 : tensor<32x8x17x128xf32> loc(#loc3312)
      %2741 = stablehlo.convert %2740 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc3313)
      %2742 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %2743 = stablehlo.compare  EQ, %2741, %2742 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc3314)
      %2744 = stablehlo.not %2743 : tensor<32x8x17x128xi1> loc(#loc3315)
      %2745 = stablehlo.reduce(%2744 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.6316"), %arg1677: tensor<i1> loc("reduce.6316"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc3317)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc3318)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc3316)
      %2746 = stablehlo.reshape %2745 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc3319)
      %2747 = stablehlo.not %2746 : tensor<32x8x17x1xi1> loc(#loc3320)
      %2748 = stablehlo.reshape %2747 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc3321)
      %2749 = stablehlo.broadcast_in_dim %2748, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc3322)
      %2750 = stablehlo.reduce(%2740 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3323)
      %2751 = stablehlo.broadcast_in_dim %2750, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3324)
      %2752 = stablehlo.subtract %2740, %2751 : tensor<32x8x17x128xf32> loc(#loc3325)
      %2753 = stablehlo.exponential %2752 : tensor<32x8x17x128xf32> loc(#loc3326)
      %2754 = stablehlo.reduce(%2753 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3327)
      %2755 = stablehlo.broadcast_in_dim %2754, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3328)
      %2756 = stablehlo.divide %2753, %2755 : tensor<32x8x17x128xf32> loc(#loc3329)
      %2757 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2758 = stablehlo.select %2749, %2757, %2756 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc3330)
      %2759 = stablehlo.broadcast_in_dim %2692, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3331)
      %2760 = stablehlo.reshape %2759 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3332)
      %2761 = stablehlo.convert %2760 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3333)
      %2762 = stablehlo.dot_general %2758, %2761, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3334)
      %2763 = stablehlo.convert %2762 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc3335)
      %2764 = stablehlo.transpose %2763, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3336)
      %2765 = stablehlo.reshape %2764 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc3337)
      %2766 = stablehlo.reshape %arg1046 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc3338)
      %2767 = stablehlo.reshape %2766 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc3339)
      %2768 = stablehlo.transpose %2767, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc3340)
      %2769 = stablehlo.dot_general %2765, %2768, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3341)
      %2770 = "stablehlo.all_reduce"(%2769) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.6333"), %arg1677: tensor<bf16> loc("dot.6333")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3341)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3341)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3341)
      %2771 = stablehlo.reshape %2770 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3342)
      %2772 = stablehlo.add %2640, %2771 : tensor<32x17x5120xbf16> loc(#loc3343)
      %2773 = stablehlo.reshape %arg1049 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3344)
      %2774 = stablehlo.reshape %2773 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3345)
      %2775 = stablehlo.broadcast_in_dim %2774, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3346)
      %2776 = stablehlo.convert %2772 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3347)
      %2777 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2778 = stablehlo.power %2776, %2777 : tensor<32x17x5120xf32> loc(#loc3348)
      %2779 = stablehlo.reduce(%2778 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3349)
      %2780 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2781 = stablehlo.multiply %2779, %2780 : tensor<32x17xf32> loc(#loc3350)
      %2782 = stablehlo.reshape %2781 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3351)
      %2783 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2784 = stablehlo.add %2782, %2783 : tensor<32x17x1xf32> loc(#loc3352)
      %2785 = stablehlo.rsqrt %2784 : tensor<32x17x1xf32> loc(#loc3353)
      %2786 = stablehlo.reshape %2785 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3354)
      %2787 = stablehlo.broadcast_in_dim %2786, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3355)
      %2788 = stablehlo.multiply %2776, %2787 : tensor<32x17x5120xf32> loc(#loc3356)
      %2789 = stablehlo.convert %2788 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3357)
      %2790 = stablehlo.multiply %2775, %2789 : tensor<32x17x5120xbf16> loc(#loc3358)
      %2791 = stablehlo.reshape %2790 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3359)
      %2792 = stablehlo.reshape %arg1050 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3360)
      %2793 = stablehlo.reshape %2792 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3361)
      %2794 = stablehlo.transpose %2793, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3362)
      %2795 = stablehlo.dot_general %2791, %2794, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3363)
      %2796 = stablehlo.reshape %2795 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3364)
      %2797 = stablehlo.logistic %2796 : tensor<32x17x3200xbf16> loc(#loc3365)
      %2798 = stablehlo.multiply %2796, %2797 : tensor<32x17x3200xbf16> loc(#loc3366)
      %2799 = stablehlo.reshape %arg1045 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3367)
      %2800 = stablehlo.reshape %2799 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3368)
      %2801 = stablehlo.transpose %2800, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3369)
      %2802 = stablehlo.dot_general %2791, %2801, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3370)
      %2803 = stablehlo.reshape %2802 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3371)
      %2804 = stablehlo.multiply %2798, %2803 : tensor<32x17x3200xbf16> loc(#loc3372)
      %2805 = stablehlo.reshape %2804 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3373)
      %2806 = stablehlo.reshape %arg1044 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc3374)
      %2807 = stablehlo.reshape %2806 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc3375)
      %2808 = stablehlo.transpose %2807, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc3376)
      %2809 = stablehlo.dot_general %2805, %2808, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3377)
      %2810 = "stablehlo.all_reduce"(%2809) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.6388"), %arg1677: tensor<bf16> loc("dot.6388")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3377)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3377)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3377)
      %2811 = stablehlo.reshape %2810 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3378)
      %2812 = stablehlo.add %2772, %2811 : tensor<32x17x5120xbf16> loc(#loc3379)
      %2813 = stablehlo.convert %2812 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3380)
      %2814 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2815 = stablehlo.power %2813, %2814 : tensor<32x17x5120xf32> loc(#loc3381)
      %2816 = stablehlo.reduce(%2815 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3382)
      %2817 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2818 = stablehlo.multiply %2816, %2817 : tensor<32x17xf32> loc(#loc3383)
      %2819 = stablehlo.reshape %2818 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3384)
      %2820 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2821 = stablehlo.add %2819, %2820 : tensor<32x17x1xf32> loc(#loc3385)
      %2822 = stablehlo.rsqrt %2821 : tensor<32x17x1xf32> loc(#loc3386)
      %2823 = stablehlo.reshape %2822 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3387)
      %2824 = stablehlo.broadcast_in_dim %2823, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3388)
      %2825 = stablehlo.multiply %2813, %2824 : tensor<32x17x5120xf32> loc(#loc3389)
      %2826 = stablehlo.convert %2825 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3390)
      %2827 = stablehlo.multiply %2698, %2826 : tensor<32x17x5120xbf16> loc(#loc3391)
      %2828 = stablehlo.reshape %2827 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3392)
      %2829 = stablehlo.reshape %arg1043 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3393)
      %2830 = stablehlo.reshape %2829 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3394)
      %2831 = stablehlo.transpose %2830, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3395)
      %2832 = stablehlo.dot_general %2828, %2831, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3396)
      %2833 = stablehlo.reshape %2832 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3397)
      %2834 = stablehlo.convert %2833 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc3398)
      %2835 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %2836 = stablehlo.power %2834, %2835 : tensor<32x17x1x128xf32> loc(#loc3399)
      %2837 = stablehlo.reduce(%2836 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc3400)
      %2838 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2839 = stablehlo.multiply %2837, %2838 : tensor<32x17x1xf32> loc(#loc3401)
      %2840 = stablehlo.reshape %2839 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc3402)
      %2841 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %2842 = stablehlo.add %2840, %2841 : tensor<32x17x1x1xf32> loc(#loc3403)
      %2843 = stablehlo.rsqrt %2842 : tensor<32x17x1x1xf32> loc(#loc3404)
      %2844 = stablehlo.reshape %2843 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc3405)
      %2845 = stablehlo.broadcast_in_dim %2844, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc3406)
      %2846 = stablehlo.multiply %2834, %2845 : tensor<32x17x1x128xf32> loc(#loc3407)
      %2847 = stablehlo.convert %2846 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc3408)
      %2848 = stablehlo.multiply %2695, %2847 : tensor<32x17x1x128xbf16> loc(#loc3409)
      %2849 = stablehlo.transpose %2848, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3410)
      %2850 = stablehlo.multiply %2849, %82 : tensor<32x1x17x128xbf16> loc(#loc3411)
      %2851 = stablehlo.slice %2849 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3412)
      %2852 = stablehlo.negate %2851 : tensor<32x1x17x64xbf16> loc(#loc3413)
      %2853 = stablehlo.slice %2849 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3414)
      %2854 = stablehlo.concatenate %2852, %2853, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3415)
      %2855 = stablehlo.multiply %2854, %91 : tensor<32x1x17x128xbf16> loc(#loc3416)
      %2856 = stablehlo.add %2850, %2855 : tensor<32x1x17x128xbf16> loc(#loc3417)
      %2857 = "stablehlo.scatter"(%arg1053, %21, %2856) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.6500"), %arg1677: tensor<bf16> loc("scatter.6500")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3418)
      %2858 = stablehlo.reshape %arg1054 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3419)
      %2859 = stablehlo.reshape %2858 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3420)
      %2860 = stablehlo.transpose %2859, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3421)
      %2861 = stablehlo.dot_general %2828, %2860, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3422)
      %2862 = stablehlo.reshape %2861 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3423)
      %2863 = stablehlo.transpose %2862, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3424)
      %2864 = "stablehlo.scatter"(%arg1055, %21, %2863) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.6530"), %arg1677: tensor<bf16> loc("scatter.6530")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3425)
      %2865 = stablehlo.reshape %arg1065 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3426)
      %2866 = stablehlo.reshape %2865 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3427)
      %2867 = stablehlo.broadcast_in_dim %2866, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3428)
      %2868 = stablehlo.reshape %arg1064 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3429)
      %2869 = stablehlo.reshape %2868 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3430)
      %2870 = stablehlo.broadcast_in_dim %2869, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3431)
      %2871 = stablehlo.reshape %arg1061 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3432)
      %2872 = stablehlo.reshape %2871 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3433)
      %2873 = stablehlo.broadcast_in_dim %2872, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3434)
      %2874 = stablehlo.reshape %arg1060 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc3435)
      %2875 = stablehlo.reshape %2874 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc3436)
      %2876 = stablehlo.transpose %2875, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc3437)
      %2877 = stablehlo.dot_general %2828, %2876, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc3438)
      %2878 = stablehlo.reshape %2877 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3439)
      %2879 = stablehlo.convert %2878 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc3440)
      %2880 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %2881 = stablehlo.power %2879, %2880 : tensor<32x17x8x128xf32> loc(#loc3441)
      %2882 = stablehlo.reduce(%2881 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc3442)
      %2883 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %2884 = stablehlo.multiply %2882, %2883 : tensor<32x17x8xf32> loc(#loc3443)
      %2885 = stablehlo.reshape %2884 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc3444)
      %2886 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %2887 = stablehlo.add %2885, %2886 : tensor<32x17x8x1xf32> loc(#loc3445)
      %2888 = stablehlo.rsqrt %2887 : tensor<32x17x8x1xf32> loc(#loc3446)
      %2889 = stablehlo.reshape %2888 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc3447)
      %2890 = stablehlo.broadcast_in_dim %2889, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc3448)
      %2891 = stablehlo.multiply %2879, %2890 : tensor<32x17x8x128xf32> loc(#loc3449)
      %2892 = stablehlo.convert %2891 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc3450)
      %2893 = stablehlo.multiply %2873, %2892 : tensor<32x17x8x128xbf16> loc(#loc3451)
      %2894 = stablehlo.transpose %2893, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3452)
      %2895 = stablehlo.multiply %2894, %132 : tensor<32x8x17x128xbf16> loc(#loc3453)
      %2896 = stablehlo.slice %2894 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3454)
      %2897 = stablehlo.negate %2896 : tensor<32x8x17x64xbf16> loc(#loc3455)
      %2898 = stablehlo.slice %2894 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3456)
      %2899 = stablehlo.concatenate %2897, %2898, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3457)
      %2900 = stablehlo.multiply %2899, %138 : tensor<32x8x17x128xbf16> loc(#loc3458)
      %2901 = stablehlo.add %2895, %2900 : tensor<32x8x17x128xbf16> loc(#loc3459)
      %2902 = stablehlo.convert %2901 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc3460)
      %2903 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2904 = stablehlo.multiply %2902, %2903 : tensor<32x8x17x128xf32> loc(#loc3461)
      %2905 = stablehlo.broadcast_in_dim %2857, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3462)
      %2906 = stablehlo.reshape %2905 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3463)
      %2907 = stablehlo.convert %2906 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3464)
      %2908 = stablehlo.transpose %2907, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc3465)
      %2909 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %2910 = stablehlo.multiply %2908, %2909 : tensor<32x8x128x128xf32> loc(#loc3466)
      %2911 = stablehlo.dot_general %2904, %2910, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3467)
      %2912 = stablehlo.add %2911, %159 : tensor<32x8x17x128xf32> loc(#loc3468)
      %2913 = stablehlo.convert %2912 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc3469)
      %2914 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %2915 = stablehlo.compare  EQ, %2913, %2914 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc3470)
      %2916 = stablehlo.not %2915 : tensor<32x8x17x128xi1> loc(#loc3471)
      %2917 = stablehlo.reduce(%2916 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.6711"), %arg1677: tensor<i1> loc("reduce.6711"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc3473)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc3474)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc3472)
      %2918 = stablehlo.reshape %2917 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc3475)
      %2919 = stablehlo.not %2918 : tensor<32x8x17x1xi1> loc(#loc3476)
      %2920 = stablehlo.reshape %2919 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc3477)
      %2921 = stablehlo.broadcast_in_dim %2920, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc3478)
      %2922 = stablehlo.reduce(%2912 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3479)
      %2923 = stablehlo.broadcast_in_dim %2922, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3480)
      %2924 = stablehlo.subtract %2912, %2923 : tensor<32x8x17x128xf32> loc(#loc3481)
      %2925 = stablehlo.exponential %2924 : tensor<32x8x17x128xf32> loc(#loc3482)
      %2926 = stablehlo.reduce(%2925 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3483)
      %2927 = stablehlo.broadcast_in_dim %2926, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3484)
      %2928 = stablehlo.divide %2925, %2927 : tensor<32x8x17x128xf32> loc(#loc3485)
      %2929 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %2930 = stablehlo.select %2921, %2929, %2928 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc3486)
      %2931 = stablehlo.broadcast_in_dim %2864, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3487)
      %2932 = stablehlo.reshape %2931 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3488)
      %2933 = stablehlo.convert %2932 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3489)
      %2934 = stablehlo.dot_general %2930, %2933, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3490)
      %2935 = stablehlo.convert %2934 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc3491)
      %2936 = stablehlo.transpose %2935, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3492)
      %2937 = stablehlo.reshape %2936 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc3493)
      %2938 = stablehlo.reshape %arg1059 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc3494)
      %2939 = stablehlo.reshape %2938 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc3495)
      %2940 = stablehlo.transpose %2939, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc3496)
      %2941 = stablehlo.dot_general %2937, %2940, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3497)
      %2942 = "stablehlo.all_reduce"(%2941) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.6728"), %arg1677: tensor<bf16> loc("dot.6728")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3497)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3497)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3497)
      %2943 = stablehlo.reshape %2942 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3498)
      %2944 = stablehlo.add %2812, %2943 : tensor<32x17x5120xbf16> loc(#loc3499)
      %2945 = stablehlo.reshape %arg1062 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3500)
      %2946 = stablehlo.reshape %2945 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3501)
      %2947 = stablehlo.broadcast_in_dim %2946, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3502)
      %2948 = stablehlo.convert %2944 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3503)
      %2949 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2950 = stablehlo.power %2948, %2949 : tensor<32x17x5120xf32> loc(#loc3504)
      %2951 = stablehlo.reduce(%2950 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3505)
      %2952 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2953 = stablehlo.multiply %2951, %2952 : tensor<32x17xf32> loc(#loc3506)
      %2954 = stablehlo.reshape %2953 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3507)
      %2955 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2956 = stablehlo.add %2954, %2955 : tensor<32x17x1xf32> loc(#loc3508)
      %2957 = stablehlo.rsqrt %2956 : tensor<32x17x1xf32> loc(#loc3509)
      %2958 = stablehlo.reshape %2957 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3510)
      %2959 = stablehlo.broadcast_in_dim %2958, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3511)
      %2960 = stablehlo.multiply %2948, %2959 : tensor<32x17x5120xf32> loc(#loc3512)
      %2961 = stablehlo.convert %2960 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3513)
      %2962 = stablehlo.multiply %2947, %2961 : tensor<32x17x5120xbf16> loc(#loc3514)
      %2963 = stablehlo.reshape %2962 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3515)
      %2964 = stablehlo.reshape %arg1063 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3516)
      %2965 = stablehlo.reshape %2964 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3517)
      %2966 = stablehlo.transpose %2965, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3518)
      %2967 = stablehlo.dot_general %2963, %2966, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3519)
      %2968 = stablehlo.reshape %2967 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3520)
      %2969 = stablehlo.logistic %2968 : tensor<32x17x3200xbf16> loc(#loc3521)
      %2970 = stablehlo.multiply %2968, %2969 : tensor<32x17x3200xbf16> loc(#loc3522)
      %2971 = stablehlo.reshape %arg1058 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3523)
      %2972 = stablehlo.reshape %2971 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3524)
      %2973 = stablehlo.transpose %2972, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3525)
      %2974 = stablehlo.dot_general %2963, %2973, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3526)
      %2975 = stablehlo.reshape %2974 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3527)
      %2976 = stablehlo.multiply %2970, %2975 : tensor<32x17x3200xbf16> loc(#loc3528)
      %2977 = stablehlo.reshape %2976 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3529)
      %2978 = stablehlo.reshape %arg1057 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc3530)
      %2979 = stablehlo.reshape %2978 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc3531)
      %2980 = stablehlo.transpose %2979, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc3532)
      %2981 = stablehlo.dot_general %2977, %2980, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3533)
      %2982 = "stablehlo.all_reduce"(%2981) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.6783"), %arg1677: tensor<bf16> loc("dot.6783")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3533)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3533)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3533)
      %2983 = stablehlo.reshape %2982 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3534)
      %2984 = stablehlo.add %2944, %2983 : tensor<32x17x5120xbf16> loc(#loc3535)
      %2985 = stablehlo.convert %2984 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3536)
      %2986 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %2987 = stablehlo.power %2985, %2986 : tensor<32x17x5120xf32> loc(#loc3537)
      %2988 = stablehlo.reduce(%2987 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3538)
      %2989 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %2990 = stablehlo.multiply %2988, %2989 : tensor<32x17xf32> loc(#loc3539)
      %2991 = stablehlo.reshape %2990 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3540)
      %2992 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %2993 = stablehlo.add %2991, %2992 : tensor<32x17x1xf32> loc(#loc3541)
      %2994 = stablehlo.rsqrt %2993 : tensor<32x17x1xf32> loc(#loc3542)
      %2995 = stablehlo.reshape %2994 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3543)
      %2996 = stablehlo.broadcast_in_dim %2995, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3544)
      %2997 = stablehlo.multiply %2985, %2996 : tensor<32x17x5120xf32> loc(#loc3545)
      %2998 = stablehlo.convert %2997 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3546)
      %2999 = stablehlo.multiply %2870, %2998 : tensor<32x17x5120xbf16> loc(#loc3547)
      %3000 = stablehlo.reshape %2999 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3548)
      %3001 = stablehlo.reshape %arg1056 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3549)
      %3002 = stablehlo.reshape %3001 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3550)
      %3003 = stablehlo.transpose %3002, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3551)
      %3004 = stablehlo.dot_general %3000, %3003, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3552)
      %3005 = stablehlo.reshape %3004 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3553)
      %3006 = stablehlo.convert %3005 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc3554)
      %3007 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %3008 = stablehlo.power %3006, %3007 : tensor<32x17x1x128xf32> loc(#loc3555)
      %3009 = stablehlo.reduce(%3008 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc3556)
      %3010 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3011 = stablehlo.multiply %3009, %3010 : tensor<32x17x1xf32> loc(#loc3557)
      %3012 = stablehlo.reshape %3011 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc3558)
      %3013 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %3014 = stablehlo.add %3012, %3013 : tensor<32x17x1x1xf32> loc(#loc3559)
      %3015 = stablehlo.rsqrt %3014 : tensor<32x17x1x1xf32> loc(#loc3560)
      %3016 = stablehlo.reshape %3015 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc3561)
      %3017 = stablehlo.broadcast_in_dim %3016, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc3562)
      %3018 = stablehlo.multiply %3006, %3017 : tensor<32x17x1x128xf32> loc(#loc3563)
      %3019 = stablehlo.convert %3018 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc3564)
      %3020 = stablehlo.multiply %2867, %3019 : tensor<32x17x1x128xbf16> loc(#loc3565)
      %3021 = stablehlo.transpose %3020, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3566)
      %3022 = stablehlo.multiply %3021, %82 : tensor<32x1x17x128xbf16> loc(#loc3567)
      %3023 = stablehlo.slice %3021 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3568)
      %3024 = stablehlo.negate %3023 : tensor<32x1x17x64xbf16> loc(#loc3569)
      %3025 = stablehlo.slice %3021 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3570)
      %3026 = stablehlo.concatenate %3024, %3025, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3571)
      %3027 = stablehlo.multiply %3026, %91 : tensor<32x1x17x128xbf16> loc(#loc3572)
      %3028 = stablehlo.add %3022, %3027 : tensor<32x1x17x128xbf16> loc(#loc3573)
      %3029 = "stablehlo.scatter"(%arg1066, %21, %3028) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.6895"), %arg1677: tensor<bf16> loc("scatter.6895")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3574)
      %3030 = stablehlo.reshape %arg1067 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3575)
      %3031 = stablehlo.reshape %3030 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3576)
      %3032 = stablehlo.transpose %3031, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3577)
      %3033 = stablehlo.dot_general %3000, %3032, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3578)
      %3034 = stablehlo.reshape %3033 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3579)
      %3035 = stablehlo.transpose %3034, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3580)
      %3036 = "stablehlo.scatter"(%arg1068, %21, %3035) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.6925"), %arg1677: tensor<bf16> loc("scatter.6925")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3581)
      %3037 = stablehlo.reshape %arg1078 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3582)
      %3038 = stablehlo.reshape %3037 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3583)
      %3039 = stablehlo.broadcast_in_dim %3038, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3584)
      %3040 = stablehlo.reshape %arg1077 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3585)
      %3041 = stablehlo.reshape %3040 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3586)
      %3042 = stablehlo.broadcast_in_dim %3041, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3587)
      %3043 = stablehlo.reshape %arg1074 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3588)
      %3044 = stablehlo.reshape %3043 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3589)
      %3045 = stablehlo.broadcast_in_dim %3044, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3590)
      %3046 = stablehlo.reshape %arg1073 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc3591)
      %3047 = stablehlo.reshape %3046 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc3592)
      %3048 = stablehlo.transpose %3047, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc3593)
      %3049 = stablehlo.dot_general %3000, %3048, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc3594)
      %3050 = stablehlo.reshape %3049 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3595)
      %3051 = stablehlo.convert %3050 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc3596)
      %3052 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %3053 = stablehlo.power %3051, %3052 : tensor<32x17x8x128xf32> loc(#loc3597)
      %3054 = stablehlo.reduce(%3053 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc3598)
      %3055 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %3056 = stablehlo.multiply %3054, %3055 : tensor<32x17x8xf32> loc(#loc3599)
      %3057 = stablehlo.reshape %3056 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc3600)
      %3058 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %3059 = stablehlo.add %3057, %3058 : tensor<32x17x8x1xf32> loc(#loc3601)
      %3060 = stablehlo.rsqrt %3059 : tensor<32x17x8x1xf32> loc(#loc3602)
      %3061 = stablehlo.reshape %3060 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc3603)
      %3062 = stablehlo.broadcast_in_dim %3061, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc3604)
      %3063 = stablehlo.multiply %3051, %3062 : tensor<32x17x8x128xf32> loc(#loc3605)
      %3064 = stablehlo.convert %3063 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc3606)
      %3065 = stablehlo.multiply %3045, %3064 : tensor<32x17x8x128xbf16> loc(#loc3607)
      %3066 = stablehlo.transpose %3065, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3608)
      %3067 = stablehlo.multiply %3066, %132 : tensor<32x8x17x128xbf16> loc(#loc3609)
      %3068 = stablehlo.slice %3066 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3610)
      %3069 = stablehlo.negate %3068 : tensor<32x8x17x64xbf16> loc(#loc3611)
      %3070 = stablehlo.slice %3066 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3612)
      %3071 = stablehlo.concatenate %3069, %3070, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3613)
      %3072 = stablehlo.multiply %3071, %138 : tensor<32x8x17x128xbf16> loc(#loc3614)
      %3073 = stablehlo.add %3067, %3072 : tensor<32x8x17x128xbf16> loc(#loc3615)
      %3074 = stablehlo.convert %3073 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc3616)
      %3075 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3076 = stablehlo.multiply %3074, %3075 : tensor<32x8x17x128xf32> loc(#loc3617)
      %3077 = stablehlo.broadcast_in_dim %3029, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3618)
      %3078 = stablehlo.reshape %3077 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3619)
      %3079 = stablehlo.convert %3078 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3620)
      %3080 = stablehlo.transpose %3079, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc3621)
      %3081 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %3082 = stablehlo.multiply %3080, %3081 : tensor<32x8x128x128xf32> loc(#loc3622)
      %3083 = stablehlo.dot_general %3076, %3082, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3623)
      %3084 = stablehlo.add %3083, %159 : tensor<32x8x17x128xf32> loc(#loc3624)
      %3085 = stablehlo.convert %3084 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc3625)
      %3086 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %3087 = stablehlo.compare  EQ, %3085, %3086 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc3626)
      %3088 = stablehlo.not %3087 : tensor<32x8x17x128xi1> loc(#loc3627)
      %3089 = stablehlo.reduce(%3088 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.7106"), %arg1677: tensor<i1> loc("reduce.7106"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc3629)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc3630)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc3628)
      %3090 = stablehlo.reshape %3089 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc3631)
      %3091 = stablehlo.not %3090 : tensor<32x8x17x1xi1> loc(#loc3632)
      %3092 = stablehlo.reshape %3091 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc3633)
      %3093 = stablehlo.broadcast_in_dim %3092, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc3634)
      %3094 = stablehlo.reduce(%3084 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3635)
      %3095 = stablehlo.broadcast_in_dim %3094, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3636)
      %3096 = stablehlo.subtract %3084, %3095 : tensor<32x8x17x128xf32> loc(#loc3637)
      %3097 = stablehlo.exponential %3096 : tensor<32x8x17x128xf32> loc(#loc3638)
      %3098 = stablehlo.reduce(%3097 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3639)
      %3099 = stablehlo.broadcast_in_dim %3098, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3640)
      %3100 = stablehlo.divide %3097, %3099 : tensor<32x8x17x128xf32> loc(#loc3641)
      %3101 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3102 = stablehlo.select %3093, %3101, %3100 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc3642)
      %3103 = stablehlo.broadcast_in_dim %3036, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3643)
      %3104 = stablehlo.reshape %3103 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3644)
      %3105 = stablehlo.convert %3104 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3645)
      %3106 = stablehlo.dot_general %3102, %3105, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3646)
      %3107 = stablehlo.convert %3106 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc3647)
      %3108 = stablehlo.transpose %3107, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3648)
      %3109 = stablehlo.reshape %3108 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc3649)
      %3110 = stablehlo.reshape %arg1072 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc3650)
      %3111 = stablehlo.reshape %3110 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc3651)
      %3112 = stablehlo.transpose %3111, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc3652)
      %3113 = stablehlo.dot_general %3109, %3112, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3653)
      %3114 = "stablehlo.all_reduce"(%3113) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.7123"), %arg1677: tensor<bf16> loc("dot.7123")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3653)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3653)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3653)
      %3115 = stablehlo.reshape %3114 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3654)
      %3116 = stablehlo.add %2984, %3115 : tensor<32x17x5120xbf16> loc(#loc3655)
      %3117 = stablehlo.reshape %arg1075 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3656)
      %3118 = stablehlo.reshape %3117 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3657)
      %3119 = stablehlo.broadcast_in_dim %3118, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3658)
      %3120 = stablehlo.convert %3116 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3659)
      %3121 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3122 = stablehlo.power %3120, %3121 : tensor<32x17x5120xf32> loc(#loc3660)
      %3123 = stablehlo.reduce(%3122 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3661)
      %3124 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3125 = stablehlo.multiply %3123, %3124 : tensor<32x17xf32> loc(#loc3662)
      %3126 = stablehlo.reshape %3125 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3663)
      %3127 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3128 = stablehlo.add %3126, %3127 : tensor<32x17x1xf32> loc(#loc3664)
      %3129 = stablehlo.rsqrt %3128 : tensor<32x17x1xf32> loc(#loc3665)
      %3130 = stablehlo.reshape %3129 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3666)
      %3131 = stablehlo.broadcast_in_dim %3130, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3667)
      %3132 = stablehlo.multiply %3120, %3131 : tensor<32x17x5120xf32> loc(#loc3668)
      %3133 = stablehlo.convert %3132 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3669)
      %3134 = stablehlo.multiply %3119, %3133 : tensor<32x17x5120xbf16> loc(#loc3670)
      %3135 = stablehlo.reshape %3134 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3671)
      %3136 = stablehlo.reshape %arg1076 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3672)
      %3137 = stablehlo.reshape %3136 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3673)
      %3138 = stablehlo.transpose %3137, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3674)
      %3139 = stablehlo.dot_general %3135, %3138, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3675)
      %3140 = stablehlo.reshape %3139 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3676)
      %3141 = stablehlo.logistic %3140 : tensor<32x17x3200xbf16> loc(#loc3677)
      %3142 = stablehlo.multiply %3140, %3141 : tensor<32x17x3200xbf16> loc(#loc3678)
      %3143 = stablehlo.reshape %arg1071 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3679)
      %3144 = stablehlo.reshape %3143 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3680)
      %3145 = stablehlo.transpose %3144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3681)
      %3146 = stablehlo.dot_general %3135, %3145, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3682)
      %3147 = stablehlo.reshape %3146 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3683)
      %3148 = stablehlo.multiply %3142, %3147 : tensor<32x17x3200xbf16> loc(#loc3684)
      %3149 = stablehlo.reshape %3148 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3685)
      %3150 = stablehlo.reshape %arg1070 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc3686)
      %3151 = stablehlo.reshape %3150 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc3687)
      %3152 = stablehlo.transpose %3151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc3688)
      %3153 = stablehlo.dot_general %3149, %3152, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3689)
      %3154 = "stablehlo.all_reduce"(%3153) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.7178"), %arg1677: tensor<bf16> loc("dot.7178")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3689)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3689)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3689)
      %3155 = stablehlo.reshape %3154 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3690)
      %3156 = stablehlo.add %3116, %3155 : tensor<32x17x5120xbf16> loc(#loc3691)
      %3157 = stablehlo.convert %3156 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3692)
      %3158 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3159 = stablehlo.power %3157, %3158 : tensor<32x17x5120xf32> loc(#loc3693)
      %3160 = stablehlo.reduce(%3159 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3694)
      %3161 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3162 = stablehlo.multiply %3160, %3161 : tensor<32x17xf32> loc(#loc3695)
      %3163 = stablehlo.reshape %3162 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3696)
      %3164 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3165 = stablehlo.add %3163, %3164 : tensor<32x17x1xf32> loc(#loc3697)
      %3166 = stablehlo.rsqrt %3165 : tensor<32x17x1xf32> loc(#loc3698)
      %3167 = stablehlo.reshape %3166 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3699)
      %3168 = stablehlo.broadcast_in_dim %3167, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3700)
      %3169 = stablehlo.multiply %3157, %3168 : tensor<32x17x5120xf32> loc(#loc3701)
      %3170 = stablehlo.convert %3169 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3702)
      %3171 = stablehlo.multiply %3042, %3170 : tensor<32x17x5120xbf16> loc(#loc3703)
      %3172 = stablehlo.reshape %3171 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3704)
      %3173 = stablehlo.reshape %arg1069 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3705)
      %3174 = stablehlo.reshape %3173 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3706)
      %3175 = stablehlo.transpose %3174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3707)
      %3176 = stablehlo.dot_general %3172, %3175, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3708)
      %3177 = stablehlo.reshape %3176 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3709)
      %3178 = stablehlo.convert %3177 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc3710)
      %3179 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %3180 = stablehlo.power %3178, %3179 : tensor<32x17x1x128xf32> loc(#loc3711)
      %3181 = stablehlo.reduce(%3180 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc3712)
      %3182 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3183 = stablehlo.multiply %3181, %3182 : tensor<32x17x1xf32> loc(#loc3713)
      %3184 = stablehlo.reshape %3183 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc3714)
      %3185 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %3186 = stablehlo.add %3184, %3185 : tensor<32x17x1x1xf32> loc(#loc3715)
      %3187 = stablehlo.rsqrt %3186 : tensor<32x17x1x1xf32> loc(#loc3716)
      %3188 = stablehlo.reshape %3187 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc3717)
      %3189 = stablehlo.broadcast_in_dim %3188, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc3718)
      %3190 = stablehlo.multiply %3178, %3189 : tensor<32x17x1x128xf32> loc(#loc3719)
      %3191 = stablehlo.convert %3190 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc3720)
      %3192 = stablehlo.multiply %3039, %3191 : tensor<32x17x1x128xbf16> loc(#loc3721)
      %3193 = stablehlo.transpose %3192, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3722)
      %3194 = stablehlo.multiply %3193, %82 : tensor<32x1x17x128xbf16> loc(#loc3723)
      %3195 = stablehlo.slice %3193 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3724)
      %3196 = stablehlo.negate %3195 : tensor<32x1x17x64xbf16> loc(#loc3725)
      %3197 = stablehlo.slice %3193 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3726)
      %3198 = stablehlo.concatenate %3196, %3197, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3727)
      %3199 = stablehlo.multiply %3198, %91 : tensor<32x1x17x128xbf16> loc(#loc3728)
      %3200 = stablehlo.add %3194, %3199 : tensor<32x1x17x128xbf16> loc(#loc3729)
      %3201 = "stablehlo.scatter"(%arg1079, %21, %3200) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.7290"), %arg1677: tensor<bf16> loc("scatter.7290")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3730)
      %3202 = stablehlo.reshape %arg1080 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3731)
      %3203 = stablehlo.reshape %3202 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3732)
      %3204 = stablehlo.transpose %3203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3733)
      %3205 = stablehlo.dot_general %3172, %3204, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3734)
      %3206 = stablehlo.reshape %3205 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3735)
      %3207 = stablehlo.transpose %3206, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3736)
      %3208 = "stablehlo.scatter"(%arg1081, %21, %3207) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.7320"), %arg1677: tensor<bf16> loc("scatter.7320")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3737)
      %3209 = stablehlo.reshape %arg1091 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3738)
      %3210 = stablehlo.reshape %3209 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3739)
      %3211 = stablehlo.broadcast_in_dim %3210, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3740)
      %3212 = stablehlo.reshape %arg1090 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3741)
      %3213 = stablehlo.reshape %3212 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3742)
      %3214 = stablehlo.broadcast_in_dim %3213, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3743)
      %3215 = stablehlo.reshape %arg1087 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3744)
      %3216 = stablehlo.reshape %3215 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3745)
      %3217 = stablehlo.broadcast_in_dim %3216, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3746)
      %3218 = stablehlo.reshape %arg1086 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc3747)
      %3219 = stablehlo.reshape %3218 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc3748)
      %3220 = stablehlo.transpose %3219, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc3749)
      %3221 = stablehlo.dot_general %3172, %3220, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc3750)
      %3222 = stablehlo.reshape %3221 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3751)
      %3223 = stablehlo.convert %3222 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc3752)
      %3224 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %3225 = stablehlo.power %3223, %3224 : tensor<32x17x8x128xf32> loc(#loc3753)
      %3226 = stablehlo.reduce(%3225 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc3754)
      %3227 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %3228 = stablehlo.multiply %3226, %3227 : tensor<32x17x8xf32> loc(#loc3755)
      %3229 = stablehlo.reshape %3228 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc3756)
      %3230 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %3231 = stablehlo.add %3229, %3230 : tensor<32x17x8x1xf32> loc(#loc3757)
      %3232 = stablehlo.rsqrt %3231 : tensor<32x17x8x1xf32> loc(#loc3758)
      %3233 = stablehlo.reshape %3232 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc3759)
      %3234 = stablehlo.broadcast_in_dim %3233, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc3760)
      %3235 = stablehlo.multiply %3223, %3234 : tensor<32x17x8x128xf32> loc(#loc3761)
      %3236 = stablehlo.convert %3235 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc3762)
      %3237 = stablehlo.multiply %3217, %3236 : tensor<32x17x8x128xbf16> loc(#loc3763)
      %3238 = stablehlo.transpose %3237, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3764)
      %3239 = stablehlo.multiply %3238, %132 : tensor<32x8x17x128xbf16> loc(#loc3765)
      %3240 = stablehlo.slice %3238 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3766)
      %3241 = stablehlo.negate %3240 : tensor<32x8x17x64xbf16> loc(#loc3767)
      %3242 = stablehlo.slice %3238 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3768)
      %3243 = stablehlo.concatenate %3241, %3242, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3769)
      %3244 = stablehlo.multiply %3243, %138 : tensor<32x8x17x128xbf16> loc(#loc3770)
      %3245 = stablehlo.add %3239, %3244 : tensor<32x8x17x128xbf16> loc(#loc3771)
      %3246 = stablehlo.convert %3245 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc3772)
      %3247 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3248 = stablehlo.multiply %3246, %3247 : tensor<32x8x17x128xf32> loc(#loc3773)
      %3249 = stablehlo.broadcast_in_dim %3201, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3774)
      %3250 = stablehlo.reshape %3249 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3775)
      %3251 = stablehlo.convert %3250 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3776)
      %3252 = stablehlo.transpose %3251, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc3777)
      %3253 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %3254 = stablehlo.multiply %3252, %3253 : tensor<32x8x128x128xf32> loc(#loc3778)
      %3255 = stablehlo.dot_general %3248, %3254, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3779)
      %3256 = stablehlo.add %3255, %159 : tensor<32x8x17x128xf32> loc(#loc3780)
      %3257 = stablehlo.convert %3256 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc3781)
      %3258 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %3259 = stablehlo.compare  EQ, %3257, %3258 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc3782)
      %3260 = stablehlo.not %3259 : tensor<32x8x17x128xi1> loc(#loc3783)
      %3261 = stablehlo.reduce(%3260 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.7501"), %arg1677: tensor<i1> loc("reduce.7501"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc3785)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc3786)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc3784)
      %3262 = stablehlo.reshape %3261 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc3787)
      %3263 = stablehlo.not %3262 : tensor<32x8x17x1xi1> loc(#loc3788)
      %3264 = stablehlo.reshape %3263 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc3789)
      %3265 = stablehlo.broadcast_in_dim %3264, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc3790)
      %3266 = stablehlo.reduce(%3256 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3791)
      %3267 = stablehlo.broadcast_in_dim %3266, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3792)
      %3268 = stablehlo.subtract %3256, %3267 : tensor<32x8x17x128xf32> loc(#loc3793)
      %3269 = stablehlo.exponential %3268 : tensor<32x8x17x128xf32> loc(#loc3794)
      %3270 = stablehlo.reduce(%3269 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3795)
      %3271 = stablehlo.broadcast_in_dim %3270, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3796)
      %3272 = stablehlo.divide %3269, %3271 : tensor<32x8x17x128xf32> loc(#loc3797)
      %3273 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3274 = stablehlo.select %3265, %3273, %3272 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc3798)
      %3275 = stablehlo.broadcast_in_dim %3208, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3799)
      %3276 = stablehlo.reshape %3275 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3800)
      %3277 = stablehlo.convert %3276 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3801)
      %3278 = stablehlo.dot_general %3274, %3277, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3802)
      %3279 = stablehlo.convert %3278 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc3803)
      %3280 = stablehlo.transpose %3279, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3804)
      %3281 = stablehlo.reshape %3280 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc3805)
      %3282 = stablehlo.reshape %arg1085 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc3806)
      %3283 = stablehlo.reshape %3282 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc3807)
      %3284 = stablehlo.transpose %3283, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc3808)
      %3285 = stablehlo.dot_general %3281, %3284, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3809)
      %3286 = "stablehlo.all_reduce"(%3285) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.7518"), %arg1677: tensor<bf16> loc("dot.7518")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3809)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3809)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3809)
      %3287 = stablehlo.reshape %3286 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3810)
      %3288 = stablehlo.add %3156, %3287 : tensor<32x17x5120xbf16> loc(#loc3811)
      %3289 = stablehlo.reshape %arg1088 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3812)
      %3290 = stablehlo.reshape %3289 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3813)
      %3291 = stablehlo.broadcast_in_dim %3290, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3814)
      %3292 = stablehlo.convert %3288 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3815)
      %3293 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3294 = stablehlo.power %3292, %3293 : tensor<32x17x5120xf32> loc(#loc3816)
      %3295 = stablehlo.reduce(%3294 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3817)
      %3296 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3297 = stablehlo.multiply %3295, %3296 : tensor<32x17xf32> loc(#loc3818)
      %3298 = stablehlo.reshape %3297 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3819)
      %3299 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3300 = stablehlo.add %3298, %3299 : tensor<32x17x1xf32> loc(#loc3820)
      %3301 = stablehlo.rsqrt %3300 : tensor<32x17x1xf32> loc(#loc3821)
      %3302 = stablehlo.reshape %3301 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3822)
      %3303 = stablehlo.broadcast_in_dim %3302, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3823)
      %3304 = stablehlo.multiply %3292, %3303 : tensor<32x17x5120xf32> loc(#loc3824)
      %3305 = stablehlo.convert %3304 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3825)
      %3306 = stablehlo.multiply %3291, %3305 : tensor<32x17x5120xbf16> loc(#loc3826)
      %3307 = stablehlo.reshape %3306 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3827)
      %3308 = stablehlo.reshape %arg1089 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3828)
      %3309 = stablehlo.reshape %3308 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3829)
      %3310 = stablehlo.transpose %3309, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3830)
      %3311 = stablehlo.dot_general %3307, %3310, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3831)
      %3312 = stablehlo.reshape %3311 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3832)
      %3313 = stablehlo.logistic %3312 : tensor<32x17x3200xbf16> loc(#loc3833)
      %3314 = stablehlo.multiply %3312, %3313 : tensor<32x17x3200xbf16> loc(#loc3834)
      %3315 = stablehlo.reshape %arg1084 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3835)
      %3316 = stablehlo.reshape %3315 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3836)
      %3317 = stablehlo.transpose %3316, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3837)
      %3318 = stablehlo.dot_general %3307, %3317, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3838)
      %3319 = stablehlo.reshape %3318 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3839)
      %3320 = stablehlo.multiply %3314, %3319 : tensor<32x17x3200xbf16> loc(#loc3840)
      %3321 = stablehlo.reshape %3320 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3841)
      %3322 = stablehlo.reshape %arg1083 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc3842)
      %3323 = stablehlo.reshape %3322 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc3843)
      %3324 = stablehlo.transpose %3323, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc3844)
      %3325 = stablehlo.dot_general %3321, %3324, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3845)
      %3326 = "stablehlo.all_reduce"(%3325) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.7573"), %arg1677: tensor<bf16> loc("dot.7573")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3845)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3845)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3845)
      %3327 = stablehlo.reshape %3326 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3846)
      %3328 = stablehlo.add %3288, %3327 : tensor<32x17x5120xbf16> loc(#loc3847)
      %3329 = stablehlo.convert %3328 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3848)
      %3330 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3331 = stablehlo.power %3329, %3330 : tensor<32x17x5120xf32> loc(#loc3849)
      %3332 = stablehlo.reduce(%3331 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3850)
      %3333 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3334 = stablehlo.multiply %3332, %3333 : tensor<32x17xf32> loc(#loc3851)
      %3335 = stablehlo.reshape %3334 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3852)
      %3336 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3337 = stablehlo.add %3335, %3336 : tensor<32x17x1xf32> loc(#loc3853)
      %3338 = stablehlo.rsqrt %3337 : tensor<32x17x1xf32> loc(#loc3854)
      %3339 = stablehlo.reshape %3338 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3855)
      %3340 = stablehlo.broadcast_in_dim %3339, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3856)
      %3341 = stablehlo.multiply %3329, %3340 : tensor<32x17x5120xf32> loc(#loc3857)
      %3342 = stablehlo.convert %3341 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3858)
      %3343 = stablehlo.multiply %3214, %3342 : tensor<32x17x5120xbf16> loc(#loc3859)
      %3344 = stablehlo.reshape %3343 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3860)
      %3345 = stablehlo.reshape %arg1082 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3861)
      %3346 = stablehlo.reshape %3345 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3862)
      %3347 = stablehlo.transpose %3346, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3863)
      %3348 = stablehlo.dot_general %3344, %3347, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3864)
      %3349 = stablehlo.reshape %3348 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3865)
      %3350 = stablehlo.convert %3349 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc3866)
      %3351 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %3352 = stablehlo.power %3350, %3351 : tensor<32x17x1x128xf32> loc(#loc3867)
      %3353 = stablehlo.reduce(%3352 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc3868)
      %3354 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3355 = stablehlo.multiply %3353, %3354 : tensor<32x17x1xf32> loc(#loc3869)
      %3356 = stablehlo.reshape %3355 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc3870)
      %3357 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %3358 = stablehlo.add %3356, %3357 : tensor<32x17x1x1xf32> loc(#loc3871)
      %3359 = stablehlo.rsqrt %3358 : tensor<32x17x1x1xf32> loc(#loc3872)
      %3360 = stablehlo.reshape %3359 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc3873)
      %3361 = stablehlo.broadcast_in_dim %3360, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc3874)
      %3362 = stablehlo.multiply %3350, %3361 : tensor<32x17x1x128xf32> loc(#loc3875)
      %3363 = stablehlo.convert %3362 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc3876)
      %3364 = stablehlo.multiply %3211, %3363 : tensor<32x17x1x128xbf16> loc(#loc3877)
      %3365 = stablehlo.transpose %3364, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3878)
      %3366 = stablehlo.multiply %3365, %82 : tensor<32x1x17x128xbf16> loc(#loc3879)
      %3367 = stablehlo.slice %3365 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3880)
      %3368 = stablehlo.negate %3367 : tensor<32x1x17x64xbf16> loc(#loc3881)
      %3369 = stablehlo.slice %3365 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc3882)
      %3370 = stablehlo.concatenate %3368, %3369, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3883)
      %3371 = stablehlo.multiply %3370, %91 : tensor<32x1x17x128xbf16> loc(#loc3884)
      %3372 = stablehlo.add %3366, %3371 : tensor<32x1x17x128xbf16> loc(#loc3885)
      %3373 = "stablehlo.scatter"(%arg1092, %21, %3372) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.7685"), %arg1677: tensor<bf16> loc("scatter.7685")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3886)
      %3374 = stablehlo.reshape %arg1093 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc3887)
      %3375 = stablehlo.reshape %3374 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc3888)
      %3376 = stablehlo.transpose %3375, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc3889)
      %3377 = stablehlo.dot_general %3344, %3376, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc3890)
      %3378 = stablehlo.reshape %3377 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3891)
      %3379 = stablehlo.transpose %3378, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc3892)
      %3380 = "stablehlo.scatter"(%arg1094, %21, %3379) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.7715"), %arg1677: tensor<bf16> loc("scatter.7715")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc3893)
      %3381 = stablehlo.reshape %arg1104 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3894)
      %3382 = stablehlo.reshape %3381 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3895)
      %3383 = stablehlo.broadcast_in_dim %3382, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc3896)
      %3384 = stablehlo.reshape %arg1103 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3897)
      %3385 = stablehlo.reshape %3384 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3898)
      %3386 = stablehlo.broadcast_in_dim %3385, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3899)
      %3387 = stablehlo.reshape %arg1100 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc3900)
      %3388 = stablehlo.reshape %3387 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc3901)
      %3389 = stablehlo.broadcast_in_dim %3388, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3902)
      %3390 = stablehlo.reshape %arg1099 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc3903)
      %3391 = stablehlo.reshape %3390 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc3904)
      %3392 = stablehlo.transpose %3391, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc3905)
      %3393 = stablehlo.dot_general %3344, %3392, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc3906)
      %3394 = stablehlo.reshape %3393 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3907)
      %3395 = stablehlo.convert %3394 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc3908)
      %3396 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %3397 = stablehlo.power %3395, %3396 : tensor<32x17x8x128xf32> loc(#loc3909)
      %3398 = stablehlo.reduce(%3397 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc3910)
      %3399 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %3400 = stablehlo.multiply %3398, %3399 : tensor<32x17x8xf32> loc(#loc3911)
      %3401 = stablehlo.reshape %3400 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc3912)
      %3402 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %3403 = stablehlo.add %3401, %3402 : tensor<32x17x8x1xf32> loc(#loc3913)
      %3404 = stablehlo.rsqrt %3403 : tensor<32x17x8x1xf32> loc(#loc3914)
      %3405 = stablehlo.reshape %3404 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc3915)
      %3406 = stablehlo.broadcast_in_dim %3405, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc3916)
      %3407 = stablehlo.multiply %3395, %3406 : tensor<32x17x8x128xf32> loc(#loc3917)
      %3408 = stablehlo.convert %3407 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc3918)
      %3409 = stablehlo.multiply %3389, %3408 : tensor<32x17x8x128xbf16> loc(#loc3919)
      %3410 = stablehlo.transpose %3409, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3920)
      %3411 = stablehlo.multiply %3410, %132 : tensor<32x8x17x128xbf16> loc(#loc3921)
      %3412 = stablehlo.slice %3410 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3922)
      %3413 = stablehlo.negate %3412 : tensor<32x8x17x64xbf16> loc(#loc3923)
      %3414 = stablehlo.slice %3410 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc3924)
      %3415 = stablehlo.concatenate %3413, %3414, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc3925)
      %3416 = stablehlo.multiply %3415, %138 : tensor<32x8x17x128xbf16> loc(#loc3926)
      %3417 = stablehlo.add %3411, %3416 : tensor<32x8x17x128xbf16> loc(#loc3927)
      %3418 = stablehlo.convert %3417 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc3928)
      %3419 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3420 = stablehlo.multiply %3418, %3419 : tensor<32x8x17x128xf32> loc(#loc3929)
      %3421 = stablehlo.broadcast_in_dim %3373, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3930)
      %3422 = stablehlo.reshape %3421 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3931)
      %3423 = stablehlo.convert %3422 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3932)
      %3424 = stablehlo.transpose %3423, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc3933)
      %3425 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %3426 = stablehlo.multiply %3424, %3425 : tensor<32x8x128x128xf32> loc(#loc3934)
      %3427 = stablehlo.dot_general %3420, %3426, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3935)
      %3428 = stablehlo.add %3427, %159 : tensor<32x8x17x128xf32> loc(#loc3936)
      %3429 = stablehlo.convert %3428 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc3937)
      %3430 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %3431 = stablehlo.compare  EQ, %3429, %3430 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc3938)
      %3432 = stablehlo.not %3431 : tensor<32x8x17x128xi1> loc(#loc3939)
      %3433 = stablehlo.reduce(%3432 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.7896"), %arg1677: tensor<i1> loc("reduce.7896"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc3941)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc3942)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc3940)
      %3434 = stablehlo.reshape %3433 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc3943)
      %3435 = stablehlo.not %3434 : tensor<32x8x17x1xi1> loc(#loc3944)
      %3436 = stablehlo.reshape %3435 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc3945)
      %3437 = stablehlo.broadcast_in_dim %3436, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc3946)
      %3438 = stablehlo.reduce(%3428 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3947)
      %3439 = stablehlo.broadcast_in_dim %3438, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3948)
      %3440 = stablehlo.subtract %3428, %3439 : tensor<32x8x17x128xf32> loc(#loc3949)
      %3441 = stablehlo.exponential %3440 : tensor<32x8x17x128xf32> loc(#loc3950)
      %3442 = stablehlo.reduce(%3441 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc3951)
      %3443 = stablehlo.broadcast_in_dim %3442, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc3952)
      %3444 = stablehlo.divide %3441, %3443 : tensor<32x8x17x128xf32> loc(#loc3953)
      %3445 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3446 = stablehlo.select %3437, %3445, %3444 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc3954)
      %3447 = stablehlo.broadcast_in_dim %3380, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc3955)
      %3448 = stablehlo.reshape %3447 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc3956)
      %3449 = stablehlo.convert %3448 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc3957)
      %3450 = stablehlo.dot_general %3446, %3449, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc3958)
      %3451 = stablehlo.convert %3450 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc3959)
      %3452 = stablehlo.transpose %3451, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc3960)
      %3453 = stablehlo.reshape %3452 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc3961)
      %3454 = stablehlo.reshape %arg1098 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc3962)
      %3455 = stablehlo.reshape %3454 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc3963)
      %3456 = stablehlo.transpose %3455, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc3964)
      %3457 = stablehlo.dot_general %3453, %3456, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3965)
      %3458 = "stablehlo.all_reduce"(%3457) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.7913"), %arg1677: tensor<bf16> loc("dot.7913")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc3965)
        stablehlo.return %11074 : tensor<bf16> loc(#loc3965)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3965)
      %3459 = stablehlo.reshape %3458 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3966)
      %3460 = stablehlo.add %3328, %3459 : tensor<32x17x5120xbf16> loc(#loc3967)
      %3461 = stablehlo.reshape %arg1101 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc3968)
      %3462 = stablehlo.reshape %3461 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc3969)
      %3463 = stablehlo.broadcast_in_dim %3462, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc3970)
      %3464 = stablehlo.convert %3460 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc3971)
      %3465 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3466 = stablehlo.power %3464, %3465 : tensor<32x17x5120xf32> loc(#loc3972)
      %3467 = stablehlo.reduce(%3466 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc3973)
      %3468 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3469 = stablehlo.multiply %3467, %3468 : tensor<32x17xf32> loc(#loc3974)
      %3470 = stablehlo.reshape %3469 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc3975)
      %3471 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3472 = stablehlo.add %3470, %3471 : tensor<32x17x1xf32> loc(#loc3976)
      %3473 = stablehlo.rsqrt %3472 : tensor<32x17x1xf32> loc(#loc3977)
      %3474 = stablehlo.reshape %3473 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc3978)
      %3475 = stablehlo.broadcast_in_dim %3474, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc3979)
      %3476 = stablehlo.multiply %3464, %3475 : tensor<32x17x5120xf32> loc(#loc3980)
      %3477 = stablehlo.convert %3476 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc3981)
      %3478 = stablehlo.multiply %3463, %3477 : tensor<32x17x5120xbf16> loc(#loc3982)
      %3479 = stablehlo.reshape %3478 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc3983)
      %3480 = stablehlo.reshape %arg1102 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3984)
      %3481 = stablehlo.reshape %3480 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3985)
      %3482 = stablehlo.transpose %3481, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3986)
      %3483 = stablehlo.dot_general %3479, %3482, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3987)
      %3484 = stablehlo.reshape %3483 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3988)
      %3485 = stablehlo.logistic %3484 : tensor<32x17x3200xbf16> loc(#loc3989)
      %3486 = stablehlo.multiply %3484, %3485 : tensor<32x17x3200xbf16> loc(#loc3990)
      %3487 = stablehlo.reshape %arg1097 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc3991)
      %3488 = stablehlo.reshape %3487 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc3992)
      %3489 = stablehlo.transpose %3488, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc3993)
      %3490 = stablehlo.dot_general %3479, %3489, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3994)
      %3491 = stablehlo.reshape %3490 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc3995)
      %3492 = stablehlo.multiply %3486, %3491 : tensor<32x17x3200xbf16> loc(#loc3996)
      %3493 = stablehlo.reshape %3492 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc3997)
      %3494 = stablehlo.reshape %arg1096 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc3998)
      %3495 = stablehlo.reshape %3494 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc3999)
      %3496 = stablehlo.transpose %3495, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc4000)
      %3497 = stablehlo.dot_general %3493, %3496, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4001)
      %3498 = "stablehlo.all_reduce"(%3497) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.7968"), %arg1677: tensor<bf16> loc("dot.7968")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4001)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4001)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4001)
      %3499 = stablehlo.reshape %3498 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4002)
      %3500 = stablehlo.add %3460, %3499 : tensor<32x17x5120xbf16> loc(#loc4003)
      %3501 = stablehlo.convert %3500 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4004)
      %3502 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3503 = stablehlo.power %3501, %3502 : tensor<32x17x5120xf32> loc(#loc4005)
      %3504 = stablehlo.reduce(%3503 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4006)
      %3505 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3506 = stablehlo.multiply %3504, %3505 : tensor<32x17xf32> loc(#loc4007)
      %3507 = stablehlo.reshape %3506 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4008)
      %3508 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3509 = stablehlo.add %3507, %3508 : tensor<32x17x1xf32> loc(#loc4009)
      %3510 = stablehlo.rsqrt %3509 : tensor<32x17x1xf32> loc(#loc4010)
      %3511 = stablehlo.reshape %3510 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4011)
      %3512 = stablehlo.broadcast_in_dim %3511, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4012)
      %3513 = stablehlo.multiply %3501, %3512 : tensor<32x17x5120xf32> loc(#loc4013)
      %3514 = stablehlo.convert %3513 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4014)
      %3515 = stablehlo.multiply %3386, %3514 : tensor<32x17x5120xbf16> loc(#loc4015)
      %3516 = stablehlo.reshape %3515 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4016)
      %3517 = stablehlo.reshape %arg1095 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4017)
      %3518 = stablehlo.reshape %3517 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4018)
      %3519 = stablehlo.transpose %3518, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4019)
      %3520 = stablehlo.dot_general %3516, %3519, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4020)
      %3521 = stablehlo.reshape %3520 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4021)
      %3522 = stablehlo.convert %3521 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc4022)
      %3523 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %3524 = stablehlo.power %3522, %3523 : tensor<32x17x1x128xf32> loc(#loc4023)
      %3525 = stablehlo.reduce(%3524 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc4024)
      %3526 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3527 = stablehlo.multiply %3525, %3526 : tensor<32x17x1xf32> loc(#loc4025)
      %3528 = stablehlo.reshape %3527 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc4026)
      %3529 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %3530 = stablehlo.add %3528, %3529 : tensor<32x17x1x1xf32> loc(#loc4027)
      %3531 = stablehlo.rsqrt %3530 : tensor<32x17x1x1xf32> loc(#loc4028)
      %3532 = stablehlo.reshape %3531 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc4029)
      %3533 = stablehlo.broadcast_in_dim %3532, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc4030)
      %3534 = stablehlo.multiply %3522, %3533 : tensor<32x17x1x128xf32> loc(#loc4031)
      %3535 = stablehlo.convert %3534 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc4032)
      %3536 = stablehlo.multiply %3383, %3535 : tensor<32x17x1x128xbf16> loc(#loc4033)
      %3537 = stablehlo.transpose %3536, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4034)
      %3538 = stablehlo.multiply %3537, %82 : tensor<32x1x17x128xbf16> loc(#loc4035)
      %3539 = stablehlo.slice %3537 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4036)
      %3540 = stablehlo.negate %3539 : tensor<32x1x17x64xbf16> loc(#loc4037)
      %3541 = stablehlo.slice %3537 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4038)
      %3542 = stablehlo.concatenate %3540, %3541, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4039)
      %3543 = stablehlo.multiply %3542, %91 : tensor<32x1x17x128xbf16> loc(#loc4040)
      %3544 = stablehlo.add %3538, %3543 : tensor<32x1x17x128xbf16> loc(#loc4041)
      %3545 = "stablehlo.scatter"(%arg1105, %21, %3544) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.8080"), %arg1677: tensor<bf16> loc("scatter.8080")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4042)
      %3546 = stablehlo.reshape %arg1106 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4043)
      %3547 = stablehlo.reshape %3546 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4044)
      %3548 = stablehlo.transpose %3547, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4045)
      %3549 = stablehlo.dot_general %3516, %3548, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4046)
      %3550 = stablehlo.reshape %3549 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4047)
      %3551 = stablehlo.transpose %3550, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4048)
      %3552 = "stablehlo.scatter"(%arg1107, %21, %3551) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.8110"), %arg1677: tensor<bf16> loc("scatter.8110")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4049)
      %3553 = stablehlo.reshape %arg1117 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4050)
      %3554 = stablehlo.reshape %3553 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4051)
      %3555 = stablehlo.broadcast_in_dim %3554, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4052)
      %3556 = stablehlo.reshape %arg1116 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4053)
      %3557 = stablehlo.reshape %3556 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4054)
      %3558 = stablehlo.broadcast_in_dim %3557, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4055)
      %3559 = stablehlo.reshape %arg1113 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4056)
      %3560 = stablehlo.reshape %3559 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4057)
      %3561 = stablehlo.broadcast_in_dim %3560, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4058)
      %3562 = stablehlo.reshape %arg1112 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc4059)
      %3563 = stablehlo.reshape %3562 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc4060)
      %3564 = stablehlo.transpose %3563, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc4061)
      %3565 = stablehlo.dot_general %3516, %3564, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc4062)
      %3566 = stablehlo.reshape %3565 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4063)
      %3567 = stablehlo.convert %3566 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc4064)
      %3568 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %3569 = stablehlo.power %3567, %3568 : tensor<32x17x8x128xf32> loc(#loc4065)
      %3570 = stablehlo.reduce(%3569 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc4066)
      %3571 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %3572 = stablehlo.multiply %3570, %3571 : tensor<32x17x8xf32> loc(#loc4067)
      %3573 = stablehlo.reshape %3572 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc4068)
      %3574 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %3575 = stablehlo.add %3573, %3574 : tensor<32x17x8x1xf32> loc(#loc4069)
      %3576 = stablehlo.rsqrt %3575 : tensor<32x17x8x1xf32> loc(#loc4070)
      %3577 = stablehlo.reshape %3576 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc4071)
      %3578 = stablehlo.broadcast_in_dim %3577, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc4072)
      %3579 = stablehlo.multiply %3567, %3578 : tensor<32x17x8x128xf32> loc(#loc4073)
      %3580 = stablehlo.convert %3579 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc4074)
      %3581 = stablehlo.multiply %3561, %3580 : tensor<32x17x8x128xbf16> loc(#loc4075)
      %3582 = stablehlo.transpose %3581, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4076)
      %3583 = stablehlo.multiply %3582, %132 : tensor<32x8x17x128xbf16> loc(#loc4077)
      %3584 = stablehlo.slice %3582 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4078)
      %3585 = stablehlo.negate %3584 : tensor<32x8x17x64xbf16> loc(#loc4079)
      %3586 = stablehlo.slice %3582 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4080)
      %3587 = stablehlo.concatenate %3585, %3586, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4081)
      %3588 = stablehlo.multiply %3587, %138 : tensor<32x8x17x128xbf16> loc(#loc4082)
      %3589 = stablehlo.add %3583, %3588 : tensor<32x8x17x128xbf16> loc(#loc4083)
      %3590 = stablehlo.convert %3589 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc4084)
      %3591 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3592 = stablehlo.multiply %3590, %3591 : tensor<32x8x17x128xf32> loc(#loc4085)
      %3593 = stablehlo.broadcast_in_dim %3545, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4086)
      %3594 = stablehlo.reshape %3593 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4087)
      %3595 = stablehlo.convert %3594 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4088)
      %3596 = stablehlo.transpose %3595, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc4089)
      %3597 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %3598 = stablehlo.multiply %3596, %3597 : tensor<32x8x128x128xf32> loc(#loc4090)
      %3599 = stablehlo.dot_general %3592, %3598, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4091)
      %3600 = stablehlo.add %3599, %159 : tensor<32x8x17x128xf32> loc(#loc4092)
      %3601 = stablehlo.convert %3600 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc4093)
      %3602 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %3603 = stablehlo.compare  EQ, %3601, %3602 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc4094)
      %3604 = stablehlo.not %3603 : tensor<32x8x17x128xi1> loc(#loc4095)
      %3605 = stablehlo.reduce(%3604 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.8291"), %arg1677: tensor<i1> loc("reduce.8291"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc4097)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc4098)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc4096)
      %3606 = stablehlo.reshape %3605 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc4099)
      %3607 = stablehlo.not %3606 : tensor<32x8x17x1xi1> loc(#loc4100)
      %3608 = stablehlo.reshape %3607 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc4101)
      %3609 = stablehlo.broadcast_in_dim %3608, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc4102)
      %3610 = stablehlo.reduce(%3600 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4103)
      %3611 = stablehlo.broadcast_in_dim %3610, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4104)
      %3612 = stablehlo.subtract %3600, %3611 : tensor<32x8x17x128xf32> loc(#loc4105)
      %3613 = stablehlo.exponential %3612 : tensor<32x8x17x128xf32> loc(#loc4106)
      %3614 = stablehlo.reduce(%3613 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4107)
      %3615 = stablehlo.broadcast_in_dim %3614, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4108)
      %3616 = stablehlo.divide %3613, %3615 : tensor<32x8x17x128xf32> loc(#loc4109)
      %3617 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3618 = stablehlo.select %3609, %3617, %3616 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc4110)
      %3619 = stablehlo.broadcast_in_dim %3552, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4111)
      %3620 = stablehlo.reshape %3619 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4112)
      %3621 = stablehlo.convert %3620 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4113)
      %3622 = stablehlo.dot_general %3618, %3621, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4114)
      %3623 = stablehlo.convert %3622 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc4115)
      %3624 = stablehlo.transpose %3623, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4116)
      %3625 = stablehlo.reshape %3624 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc4117)
      %3626 = stablehlo.reshape %arg1111 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc4118)
      %3627 = stablehlo.reshape %3626 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc4119)
      %3628 = stablehlo.transpose %3627, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc4120)
      %3629 = stablehlo.dot_general %3625, %3628, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4121)
      %3630 = "stablehlo.all_reduce"(%3629) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.8308"), %arg1677: tensor<bf16> loc("dot.8308")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4121)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4121)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4121)
      %3631 = stablehlo.reshape %3630 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4122)
      %3632 = stablehlo.add %3500, %3631 : tensor<32x17x5120xbf16> loc(#loc4123)
      %3633 = stablehlo.reshape %arg1114 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4124)
      %3634 = stablehlo.reshape %3633 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4125)
      %3635 = stablehlo.broadcast_in_dim %3634, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4126)
      %3636 = stablehlo.convert %3632 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4127)
      %3637 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3638 = stablehlo.power %3636, %3637 : tensor<32x17x5120xf32> loc(#loc4128)
      %3639 = stablehlo.reduce(%3638 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4129)
      %3640 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3641 = stablehlo.multiply %3639, %3640 : tensor<32x17xf32> loc(#loc4130)
      %3642 = stablehlo.reshape %3641 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4131)
      %3643 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3644 = stablehlo.add %3642, %3643 : tensor<32x17x1xf32> loc(#loc4132)
      %3645 = stablehlo.rsqrt %3644 : tensor<32x17x1xf32> loc(#loc4133)
      %3646 = stablehlo.reshape %3645 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4134)
      %3647 = stablehlo.broadcast_in_dim %3646, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4135)
      %3648 = stablehlo.multiply %3636, %3647 : tensor<32x17x5120xf32> loc(#loc4136)
      %3649 = stablehlo.convert %3648 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4137)
      %3650 = stablehlo.multiply %3635, %3649 : tensor<32x17x5120xbf16> loc(#loc4138)
      %3651 = stablehlo.reshape %3650 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4139)
      %3652 = stablehlo.reshape %arg1115 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4140)
      %3653 = stablehlo.reshape %3652 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4141)
      %3654 = stablehlo.transpose %3653, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4142)
      %3655 = stablehlo.dot_general %3651, %3654, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4143)
      %3656 = stablehlo.reshape %3655 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4144)
      %3657 = stablehlo.logistic %3656 : tensor<32x17x3200xbf16> loc(#loc4145)
      %3658 = stablehlo.multiply %3656, %3657 : tensor<32x17x3200xbf16> loc(#loc4146)
      %3659 = stablehlo.reshape %arg1110 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4147)
      %3660 = stablehlo.reshape %3659 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4148)
      %3661 = stablehlo.transpose %3660, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4149)
      %3662 = stablehlo.dot_general %3651, %3661, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4150)
      %3663 = stablehlo.reshape %3662 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4151)
      %3664 = stablehlo.multiply %3658, %3663 : tensor<32x17x3200xbf16> loc(#loc4152)
      %3665 = stablehlo.reshape %3664 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4153)
      %3666 = stablehlo.reshape %arg1109 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc4154)
      %3667 = stablehlo.reshape %3666 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc4155)
      %3668 = stablehlo.transpose %3667, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc4156)
      %3669 = stablehlo.dot_general %3665, %3668, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4157)
      %3670 = "stablehlo.all_reduce"(%3669) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.8363"), %arg1677: tensor<bf16> loc("dot.8363")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4157)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4157)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4157)
      %3671 = stablehlo.reshape %3670 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4158)
      %3672 = stablehlo.add %3632, %3671 : tensor<32x17x5120xbf16> loc(#loc4159)
      %3673 = stablehlo.convert %3672 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4160)
      %3674 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3675 = stablehlo.power %3673, %3674 : tensor<32x17x5120xf32> loc(#loc4161)
      %3676 = stablehlo.reduce(%3675 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4162)
      %3677 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3678 = stablehlo.multiply %3676, %3677 : tensor<32x17xf32> loc(#loc4163)
      %3679 = stablehlo.reshape %3678 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4164)
      %3680 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3681 = stablehlo.add %3679, %3680 : tensor<32x17x1xf32> loc(#loc4165)
      %3682 = stablehlo.rsqrt %3681 : tensor<32x17x1xf32> loc(#loc4166)
      %3683 = stablehlo.reshape %3682 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4167)
      %3684 = stablehlo.broadcast_in_dim %3683, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4168)
      %3685 = stablehlo.multiply %3673, %3684 : tensor<32x17x5120xf32> loc(#loc4169)
      %3686 = stablehlo.convert %3685 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4170)
      %3687 = stablehlo.multiply %3558, %3686 : tensor<32x17x5120xbf16> loc(#loc4171)
      %3688 = stablehlo.reshape %3687 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4172)
      %3689 = stablehlo.reshape %arg1108 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4173)
      %3690 = stablehlo.reshape %3689 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4174)
      %3691 = stablehlo.transpose %3690, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4175)
      %3692 = stablehlo.dot_general %3688, %3691, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4176)
      %3693 = stablehlo.reshape %3692 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4177)
      %3694 = stablehlo.convert %3693 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc4178)
      %3695 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %3696 = stablehlo.power %3694, %3695 : tensor<32x17x1x128xf32> loc(#loc4179)
      %3697 = stablehlo.reduce(%3696 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc4180)
      %3698 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3699 = stablehlo.multiply %3697, %3698 : tensor<32x17x1xf32> loc(#loc4181)
      %3700 = stablehlo.reshape %3699 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc4182)
      %3701 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %3702 = stablehlo.add %3700, %3701 : tensor<32x17x1x1xf32> loc(#loc4183)
      %3703 = stablehlo.rsqrt %3702 : tensor<32x17x1x1xf32> loc(#loc4184)
      %3704 = stablehlo.reshape %3703 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc4185)
      %3705 = stablehlo.broadcast_in_dim %3704, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc4186)
      %3706 = stablehlo.multiply %3694, %3705 : tensor<32x17x1x128xf32> loc(#loc4187)
      %3707 = stablehlo.convert %3706 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc4188)
      %3708 = stablehlo.multiply %3555, %3707 : tensor<32x17x1x128xbf16> loc(#loc4189)
      %3709 = stablehlo.transpose %3708, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4190)
      %3710 = stablehlo.multiply %3709, %82 : tensor<32x1x17x128xbf16> loc(#loc4191)
      %3711 = stablehlo.slice %3709 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4192)
      %3712 = stablehlo.negate %3711 : tensor<32x1x17x64xbf16> loc(#loc4193)
      %3713 = stablehlo.slice %3709 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4194)
      %3714 = stablehlo.concatenate %3712, %3713, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4195)
      %3715 = stablehlo.multiply %3714, %91 : tensor<32x1x17x128xbf16> loc(#loc4196)
      %3716 = stablehlo.add %3710, %3715 : tensor<32x1x17x128xbf16> loc(#loc4197)
      %3717 = "stablehlo.scatter"(%arg1118, %21, %3716) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.8475"), %arg1677: tensor<bf16> loc("scatter.8475")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4198)
      %3718 = stablehlo.reshape %arg1119 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4199)
      %3719 = stablehlo.reshape %3718 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4200)
      %3720 = stablehlo.transpose %3719, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4201)
      %3721 = stablehlo.dot_general %3688, %3720, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4202)
      %3722 = stablehlo.reshape %3721 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4203)
      %3723 = stablehlo.transpose %3722, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4204)
      %3724 = "stablehlo.scatter"(%arg1120, %21, %3723) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.8505"), %arg1677: tensor<bf16> loc("scatter.8505")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4205)
      %3725 = stablehlo.reshape %arg1130 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4206)
      %3726 = stablehlo.reshape %3725 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4207)
      %3727 = stablehlo.broadcast_in_dim %3726, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4208)
      %3728 = stablehlo.reshape %arg1129 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4209)
      %3729 = stablehlo.reshape %3728 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4210)
      %3730 = stablehlo.broadcast_in_dim %3729, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4211)
      %3731 = stablehlo.reshape %arg1126 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4212)
      %3732 = stablehlo.reshape %3731 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4213)
      %3733 = stablehlo.broadcast_in_dim %3732, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4214)
      %3734 = stablehlo.reshape %arg1125 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc4215)
      %3735 = stablehlo.reshape %3734 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc4216)
      %3736 = stablehlo.transpose %3735, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc4217)
      %3737 = stablehlo.dot_general %3688, %3736, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc4218)
      %3738 = stablehlo.reshape %3737 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4219)
      %3739 = stablehlo.convert %3738 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc4220)
      %3740 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %3741 = stablehlo.power %3739, %3740 : tensor<32x17x8x128xf32> loc(#loc4221)
      %3742 = stablehlo.reduce(%3741 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc4222)
      %3743 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %3744 = stablehlo.multiply %3742, %3743 : tensor<32x17x8xf32> loc(#loc4223)
      %3745 = stablehlo.reshape %3744 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc4224)
      %3746 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %3747 = stablehlo.add %3745, %3746 : tensor<32x17x8x1xf32> loc(#loc4225)
      %3748 = stablehlo.rsqrt %3747 : tensor<32x17x8x1xf32> loc(#loc4226)
      %3749 = stablehlo.reshape %3748 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc4227)
      %3750 = stablehlo.broadcast_in_dim %3749, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc4228)
      %3751 = stablehlo.multiply %3739, %3750 : tensor<32x17x8x128xf32> loc(#loc4229)
      %3752 = stablehlo.convert %3751 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc4230)
      %3753 = stablehlo.multiply %3733, %3752 : tensor<32x17x8x128xbf16> loc(#loc4231)
      %3754 = stablehlo.transpose %3753, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4232)
      %3755 = stablehlo.multiply %3754, %132 : tensor<32x8x17x128xbf16> loc(#loc4233)
      %3756 = stablehlo.slice %3754 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4234)
      %3757 = stablehlo.negate %3756 : tensor<32x8x17x64xbf16> loc(#loc4235)
      %3758 = stablehlo.slice %3754 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4236)
      %3759 = stablehlo.concatenate %3757, %3758, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4237)
      %3760 = stablehlo.multiply %3759, %138 : tensor<32x8x17x128xbf16> loc(#loc4238)
      %3761 = stablehlo.add %3755, %3760 : tensor<32x8x17x128xbf16> loc(#loc4239)
      %3762 = stablehlo.convert %3761 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc4240)
      %3763 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3764 = stablehlo.multiply %3762, %3763 : tensor<32x8x17x128xf32> loc(#loc4241)
      %3765 = stablehlo.broadcast_in_dim %3717, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4242)
      %3766 = stablehlo.reshape %3765 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4243)
      %3767 = stablehlo.convert %3766 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4244)
      %3768 = stablehlo.transpose %3767, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc4245)
      %3769 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %3770 = stablehlo.multiply %3768, %3769 : tensor<32x8x128x128xf32> loc(#loc4246)
      %3771 = stablehlo.dot_general %3764, %3770, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4247)
      %3772 = stablehlo.add %3771, %159 : tensor<32x8x17x128xf32> loc(#loc4248)
      %3773 = stablehlo.convert %3772 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc4249)
      %3774 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %3775 = stablehlo.compare  EQ, %3773, %3774 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc4250)
      %3776 = stablehlo.not %3775 : tensor<32x8x17x128xi1> loc(#loc4251)
      %3777 = stablehlo.reduce(%3776 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.8686"), %arg1677: tensor<i1> loc("reduce.8686"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc4253)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc4254)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc4252)
      %3778 = stablehlo.reshape %3777 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc4255)
      %3779 = stablehlo.not %3778 : tensor<32x8x17x1xi1> loc(#loc4256)
      %3780 = stablehlo.reshape %3779 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc4257)
      %3781 = stablehlo.broadcast_in_dim %3780, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc4258)
      %3782 = stablehlo.reduce(%3772 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4259)
      %3783 = stablehlo.broadcast_in_dim %3782, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4260)
      %3784 = stablehlo.subtract %3772, %3783 : tensor<32x8x17x128xf32> loc(#loc4261)
      %3785 = stablehlo.exponential %3784 : tensor<32x8x17x128xf32> loc(#loc4262)
      %3786 = stablehlo.reduce(%3785 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4263)
      %3787 = stablehlo.broadcast_in_dim %3786, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4264)
      %3788 = stablehlo.divide %3785, %3787 : tensor<32x8x17x128xf32> loc(#loc4265)
      %3789 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3790 = stablehlo.select %3781, %3789, %3788 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc4266)
      %3791 = stablehlo.broadcast_in_dim %3724, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4267)
      %3792 = stablehlo.reshape %3791 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4268)
      %3793 = stablehlo.convert %3792 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4269)
      %3794 = stablehlo.dot_general %3790, %3793, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4270)
      %3795 = stablehlo.convert %3794 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc4271)
      %3796 = stablehlo.transpose %3795, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4272)
      %3797 = stablehlo.reshape %3796 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc4273)
      %3798 = stablehlo.reshape %arg1124 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc4274)
      %3799 = stablehlo.reshape %3798 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc4275)
      %3800 = stablehlo.transpose %3799, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc4276)
      %3801 = stablehlo.dot_general %3797, %3800, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4277)
      %3802 = "stablehlo.all_reduce"(%3801) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.8703"), %arg1677: tensor<bf16> loc("dot.8703")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4277)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4277)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4277)
      %3803 = stablehlo.reshape %3802 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4278)
      %3804 = stablehlo.add %3672, %3803 : tensor<32x17x5120xbf16> loc(#loc4279)
      %3805 = stablehlo.reshape %arg1127 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4280)
      %3806 = stablehlo.reshape %3805 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4281)
      %3807 = stablehlo.broadcast_in_dim %3806, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4282)
      %3808 = stablehlo.convert %3804 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4283)
      %3809 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3810 = stablehlo.power %3808, %3809 : tensor<32x17x5120xf32> loc(#loc4284)
      %3811 = stablehlo.reduce(%3810 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4285)
      %3812 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3813 = stablehlo.multiply %3811, %3812 : tensor<32x17xf32> loc(#loc4286)
      %3814 = stablehlo.reshape %3813 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4287)
      %3815 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3816 = stablehlo.add %3814, %3815 : tensor<32x17x1xf32> loc(#loc4288)
      %3817 = stablehlo.rsqrt %3816 : tensor<32x17x1xf32> loc(#loc4289)
      %3818 = stablehlo.reshape %3817 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4290)
      %3819 = stablehlo.broadcast_in_dim %3818, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4291)
      %3820 = stablehlo.multiply %3808, %3819 : tensor<32x17x5120xf32> loc(#loc4292)
      %3821 = stablehlo.convert %3820 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4293)
      %3822 = stablehlo.multiply %3807, %3821 : tensor<32x17x5120xbf16> loc(#loc4294)
      %3823 = stablehlo.reshape %3822 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4295)
      %3824 = stablehlo.reshape %arg1128 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4296)
      %3825 = stablehlo.reshape %3824 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4297)
      %3826 = stablehlo.transpose %3825, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4298)
      %3827 = stablehlo.dot_general %3823, %3826, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4299)
      %3828 = stablehlo.reshape %3827 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4300)
      %3829 = stablehlo.logistic %3828 : tensor<32x17x3200xbf16> loc(#loc4301)
      %3830 = stablehlo.multiply %3828, %3829 : tensor<32x17x3200xbf16> loc(#loc4302)
      %3831 = stablehlo.reshape %arg1123 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4303)
      %3832 = stablehlo.reshape %3831 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4304)
      %3833 = stablehlo.transpose %3832, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4305)
      %3834 = stablehlo.dot_general %3823, %3833, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4306)
      %3835 = stablehlo.reshape %3834 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4307)
      %3836 = stablehlo.multiply %3830, %3835 : tensor<32x17x3200xbf16> loc(#loc4308)
      %3837 = stablehlo.reshape %3836 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4309)
      %3838 = stablehlo.reshape %arg1122 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc4310)
      %3839 = stablehlo.reshape %3838 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc4311)
      %3840 = stablehlo.transpose %3839, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc4312)
      %3841 = stablehlo.dot_general %3837, %3840, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4313)
      %3842 = "stablehlo.all_reduce"(%3841) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.8758"), %arg1677: tensor<bf16> loc("dot.8758")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4313)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4313)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4313)
      %3843 = stablehlo.reshape %3842 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4314)
      %3844 = stablehlo.add %3804, %3843 : tensor<32x17x5120xbf16> loc(#loc4315)
      %3845 = stablehlo.convert %3844 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4316)
      %3846 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3847 = stablehlo.power %3845, %3846 : tensor<32x17x5120xf32> loc(#loc4317)
      %3848 = stablehlo.reduce(%3847 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4318)
      %3849 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3850 = stablehlo.multiply %3848, %3849 : tensor<32x17xf32> loc(#loc4319)
      %3851 = stablehlo.reshape %3850 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4320)
      %3852 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3853 = stablehlo.add %3851, %3852 : tensor<32x17x1xf32> loc(#loc4321)
      %3854 = stablehlo.rsqrt %3853 : tensor<32x17x1xf32> loc(#loc4322)
      %3855 = stablehlo.reshape %3854 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4323)
      %3856 = stablehlo.broadcast_in_dim %3855, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4324)
      %3857 = stablehlo.multiply %3845, %3856 : tensor<32x17x5120xf32> loc(#loc4325)
      %3858 = stablehlo.convert %3857 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4326)
      %3859 = stablehlo.multiply %3730, %3858 : tensor<32x17x5120xbf16> loc(#loc4327)
      %3860 = stablehlo.reshape %3859 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4328)
      %3861 = stablehlo.reshape %arg1121 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4329)
      %3862 = stablehlo.reshape %3861 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4330)
      %3863 = stablehlo.transpose %3862, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4331)
      %3864 = stablehlo.dot_general %3860, %3863, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4332)
      %3865 = stablehlo.reshape %3864 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4333)
      %3866 = stablehlo.convert %3865 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc4334)
      %3867 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %3868 = stablehlo.power %3866, %3867 : tensor<32x17x1x128xf32> loc(#loc4335)
      %3869 = stablehlo.reduce(%3868 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc4336)
      %3870 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3871 = stablehlo.multiply %3869, %3870 : tensor<32x17x1xf32> loc(#loc4337)
      %3872 = stablehlo.reshape %3871 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc4338)
      %3873 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %3874 = stablehlo.add %3872, %3873 : tensor<32x17x1x1xf32> loc(#loc4339)
      %3875 = stablehlo.rsqrt %3874 : tensor<32x17x1x1xf32> loc(#loc4340)
      %3876 = stablehlo.reshape %3875 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc4341)
      %3877 = stablehlo.broadcast_in_dim %3876, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc4342)
      %3878 = stablehlo.multiply %3866, %3877 : tensor<32x17x1x128xf32> loc(#loc4343)
      %3879 = stablehlo.convert %3878 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc4344)
      %3880 = stablehlo.multiply %3727, %3879 : tensor<32x17x1x128xbf16> loc(#loc4345)
      %3881 = stablehlo.transpose %3880, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4346)
      %3882 = stablehlo.multiply %3881, %82 : tensor<32x1x17x128xbf16> loc(#loc4347)
      %3883 = stablehlo.slice %3881 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4348)
      %3884 = stablehlo.negate %3883 : tensor<32x1x17x64xbf16> loc(#loc4349)
      %3885 = stablehlo.slice %3881 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4350)
      %3886 = stablehlo.concatenate %3884, %3885, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4351)
      %3887 = stablehlo.multiply %3886, %91 : tensor<32x1x17x128xbf16> loc(#loc4352)
      %3888 = stablehlo.add %3882, %3887 : tensor<32x1x17x128xbf16> loc(#loc4353)
      %3889 = "stablehlo.scatter"(%arg1131, %21, %3888) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.8870"), %arg1677: tensor<bf16> loc("scatter.8870")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4354)
      %3890 = stablehlo.reshape %arg1132 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4355)
      %3891 = stablehlo.reshape %3890 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4356)
      %3892 = stablehlo.transpose %3891, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4357)
      %3893 = stablehlo.dot_general %3860, %3892, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4358)
      %3894 = stablehlo.reshape %3893 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4359)
      %3895 = stablehlo.transpose %3894, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4360)
      %3896 = "stablehlo.scatter"(%arg1133, %21, %3895) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.8900"), %arg1677: tensor<bf16> loc("scatter.8900")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4361)
      %3897 = stablehlo.reshape %arg1143 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4362)
      %3898 = stablehlo.reshape %3897 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4363)
      %3899 = stablehlo.broadcast_in_dim %3898, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4364)
      %3900 = stablehlo.reshape %arg1142 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4365)
      %3901 = stablehlo.reshape %3900 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4366)
      %3902 = stablehlo.broadcast_in_dim %3901, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4367)
      %3903 = stablehlo.reshape %arg1139 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4368)
      %3904 = stablehlo.reshape %3903 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4369)
      %3905 = stablehlo.broadcast_in_dim %3904, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4370)
      %3906 = stablehlo.reshape %arg1138 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc4371)
      %3907 = stablehlo.reshape %3906 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc4372)
      %3908 = stablehlo.transpose %3907, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc4373)
      %3909 = stablehlo.dot_general %3860, %3908, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc4374)
      %3910 = stablehlo.reshape %3909 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4375)
      %3911 = stablehlo.convert %3910 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc4376)
      %3912 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %3913 = stablehlo.power %3911, %3912 : tensor<32x17x8x128xf32> loc(#loc4377)
      %3914 = stablehlo.reduce(%3913 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc4378)
      %3915 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %3916 = stablehlo.multiply %3914, %3915 : tensor<32x17x8xf32> loc(#loc4379)
      %3917 = stablehlo.reshape %3916 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc4380)
      %3918 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %3919 = stablehlo.add %3917, %3918 : tensor<32x17x8x1xf32> loc(#loc4381)
      %3920 = stablehlo.rsqrt %3919 : tensor<32x17x8x1xf32> loc(#loc4382)
      %3921 = stablehlo.reshape %3920 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc4383)
      %3922 = stablehlo.broadcast_in_dim %3921, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc4384)
      %3923 = stablehlo.multiply %3911, %3922 : tensor<32x17x8x128xf32> loc(#loc4385)
      %3924 = stablehlo.convert %3923 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc4386)
      %3925 = stablehlo.multiply %3905, %3924 : tensor<32x17x8x128xbf16> loc(#loc4387)
      %3926 = stablehlo.transpose %3925, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4388)
      %3927 = stablehlo.multiply %3926, %132 : tensor<32x8x17x128xbf16> loc(#loc4389)
      %3928 = stablehlo.slice %3926 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4390)
      %3929 = stablehlo.negate %3928 : tensor<32x8x17x64xbf16> loc(#loc4391)
      %3930 = stablehlo.slice %3926 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4392)
      %3931 = stablehlo.concatenate %3929, %3930, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4393)
      %3932 = stablehlo.multiply %3931, %138 : tensor<32x8x17x128xbf16> loc(#loc4394)
      %3933 = stablehlo.add %3927, %3932 : tensor<32x8x17x128xbf16> loc(#loc4395)
      %3934 = stablehlo.convert %3933 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc4396)
      %3935 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3936 = stablehlo.multiply %3934, %3935 : tensor<32x8x17x128xf32> loc(#loc4397)
      %3937 = stablehlo.broadcast_in_dim %3889, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4398)
      %3938 = stablehlo.reshape %3937 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4399)
      %3939 = stablehlo.convert %3938 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4400)
      %3940 = stablehlo.transpose %3939, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc4401)
      %3941 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %3942 = stablehlo.multiply %3940, %3941 : tensor<32x8x128x128xf32> loc(#loc4402)
      %3943 = stablehlo.dot_general %3936, %3942, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4403)
      %3944 = stablehlo.add %3943, %159 : tensor<32x8x17x128xf32> loc(#loc4404)
      %3945 = stablehlo.convert %3944 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc4405)
      %3946 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %3947 = stablehlo.compare  EQ, %3945, %3946 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc4406)
      %3948 = stablehlo.not %3947 : tensor<32x8x17x128xi1> loc(#loc4407)
      %3949 = stablehlo.reduce(%3948 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.9081"), %arg1677: tensor<i1> loc("reduce.9081"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc4409)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc4410)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc4408)
      %3950 = stablehlo.reshape %3949 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc4411)
      %3951 = stablehlo.not %3950 : tensor<32x8x17x1xi1> loc(#loc4412)
      %3952 = stablehlo.reshape %3951 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc4413)
      %3953 = stablehlo.broadcast_in_dim %3952, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc4414)
      %3954 = stablehlo.reduce(%3944 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4415)
      %3955 = stablehlo.broadcast_in_dim %3954, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4416)
      %3956 = stablehlo.subtract %3944, %3955 : tensor<32x8x17x128xf32> loc(#loc4417)
      %3957 = stablehlo.exponential %3956 : tensor<32x8x17x128xf32> loc(#loc4418)
      %3958 = stablehlo.reduce(%3957 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4419)
      %3959 = stablehlo.broadcast_in_dim %3958, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4420)
      %3960 = stablehlo.divide %3957, %3959 : tensor<32x8x17x128xf32> loc(#loc4421)
      %3961 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %3962 = stablehlo.select %3953, %3961, %3960 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc4422)
      %3963 = stablehlo.broadcast_in_dim %3896, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4423)
      %3964 = stablehlo.reshape %3963 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4424)
      %3965 = stablehlo.convert %3964 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4425)
      %3966 = stablehlo.dot_general %3962, %3965, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4426)
      %3967 = stablehlo.convert %3966 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc4427)
      %3968 = stablehlo.transpose %3967, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4428)
      %3969 = stablehlo.reshape %3968 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc4429)
      %3970 = stablehlo.reshape %arg1137 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc4430)
      %3971 = stablehlo.reshape %3970 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc4431)
      %3972 = stablehlo.transpose %3971, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc4432)
      %3973 = stablehlo.dot_general %3969, %3972, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4433)
      %3974 = "stablehlo.all_reduce"(%3973) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.9098"), %arg1677: tensor<bf16> loc("dot.9098")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4433)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4433)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4433)
      %3975 = stablehlo.reshape %3974 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4434)
      %3976 = stablehlo.add %3844, %3975 : tensor<32x17x5120xbf16> loc(#loc4435)
      %3977 = stablehlo.reshape %arg1140 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4436)
      %3978 = stablehlo.reshape %3977 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4437)
      %3979 = stablehlo.broadcast_in_dim %3978, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4438)
      %3980 = stablehlo.convert %3976 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4439)
      %3981 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %3982 = stablehlo.power %3980, %3981 : tensor<32x17x5120xf32> loc(#loc4440)
      %3983 = stablehlo.reduce(%3982 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4441)
      %3984 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %3985 = stablehlo.multiply %3983, %3984 : tensor<32x17xf32> loc(#loc4442)
      %3986 = stablehlo.reshape %3985 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4443)
      %3987 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %3988 = stablehlo.add %3986, %3987 : tensor<32x17x1xf32> loc(#loc4444)
      %3989 = stablehlo.rsqrt %3988 : tensor<32x17x1xf32> loc(#loc4445)
      %3990 = stablehlo.reshape %3989 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4446)
      %3991 = stablehlo.broadcast_in_dim %3990, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4447)
      %3992 = stablehlo.multiply %3980, %3991 : tensor<32x17x5120xf32> loc(#loc4448)
      %3993 = stablehlo.convert %3992 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4449)
      %3994 = stablehlo.multiply %3979, %3993 : tensor<32x17x5120xbf16> loc(#loc4450)
      %3995 = stablehlo.reshape %3994 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4451)
      %3996 = stablehlo.reshape %arg1141 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4452)
      %3997 = stablehlo.reshape %3996 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4453)
      %3998 = stablehlo.transpose %3997, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4454)
      %3999 = stablehlo.dot_general %3995, %3998, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4455)
      %4000 = stablehlo.reshape %3999 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4456)
      %4001 = stablehlo.logistic %4000 : tensor<32x17x3200xbf16> loc(#loc4457)
      %4002 = stablehlo.multiply %4000, %4001 : tensor<32x17x3200xbf16> loc(#loc4458)
      %4003 = stablehlo.reshape %arg1136 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4459)
      %4004 = stablehlo.reshape %4003 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4460)
      %4005 = stablehlo.transpose %4004, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4461)
      %4006 = stablehlo.dot_general %3995, %4005, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4462)
      %4007 = stablehlo.reshape %4006 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4463)
      %4008 = stablehlo.multiply %4002, %4007 : tensor<32x17x3200xbf16> loc(#loc4464)
      %4009 = stablehlo.reshape %4008 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4465)
      %4010 = stablehlo.reshape %arg1135 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc4466)
      %4011 = stablehlo.reshape %4010 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc4467)
      %4012 = stablehlo.transpose %4011, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc4468)
      %4013 = stablehlo.dot_general %4009, %4012, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4469)
      %4014 = "stablehlo.all_reduce"(%4013) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.9153"), %arg1677: tensor<bf16> loc("dot.9153")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4469)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4469)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4469)
      %4015 = stablehlo.reshape %4014 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4470)
      %4016 = stablehlo.add %3976, %4015 : tensor<32x17x5120xbf16> loc(#loc4471)
      %4017 = stablehlo.convert %4016 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4472)
      %4018 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4019 = stablehlo.power %4017, %4018 : tensor<32x17x5120xf32> loc(#loc4473)
      %4020 = stablehlo.reduce(%4019 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4474)
      %4021 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4022 = stablehlo.multiply %4020, %4021 : tensor<32x17xf32> loc(#loc4475)
      %4023 = stablehlo.reshape %4022 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4476)
      %4024 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4025 = stablehlo.add %4023, %4024 : tensor<32x17x1xf32> loc(#loc4477)
      %4026 = stablehlo.rsqrt %4025 : tensor<32x17x1xf32> loc(#loc4478)
      %4027 = stablehlo.reshape %4026 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4479)
      %4028 = stablehlo.broadcast_in_dim %4027, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4480)
      %4029 = stablehlo.multiply %4017, %4028 : tensor<32x17x5120xf32> loc(#loc4481)
      %4030 = stablehlo.convert %4029 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4482)
      %4031 = stablehlo.multiply %3902, %4030 : tensor<32x17x5120xbf16> loc(#loc4483)
      %4032 = stablehlo.reshape %4031 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4484)
      %4033 = stablehlo.reshape %arg1134 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4485)
      %4034 = stablehlo.reshape %4033 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4486)
      %4035 = stablehlo.transpose %4034, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4487)
      %4036 = stablehlo.dot_general %4032, %4035, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4488)
      %4037 = stablehlo.reshape %4036 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4489)
      %4038 = stablehlo.convert %4037 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc4490)
      %4039 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %4040 = stablehlo.power %4038, %4039 : tensor<32x17x1x128xf32> loc(#loc4491)
      %4041 = stablehlo.reduce(%4040 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc4492)
      %4042 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4043 = stablehlo.multiply %4041, %4042 : tensor<32x17x1xf32> loc(#loc4493)
      %4044 = stablehlo.reshape %4043 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc4494)
      %4045 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %4046 = stablehlo.add %4044, %4045 : tensor<32x17x1x1xf32> loc(#loc4495)
      %4047 = stablehlo.rsqrt %4046 : tensor<32x17x1x1xf32> loc(#loc4496)
      %4048 = stablehlo.reshape %4047 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc4497)
      %4049 = stablehlo.broadcast_in_dim %4048, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc4498)
      %4050 = stablehlo.multiply %4038, %4049 : tensor<32x17x1x128xf32> loc(#loc4499)
      %4051 = stablehlo.convert %4050 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc4500)
      %4052 = stablehlo.multiply %3899, %4051 : tensor<32x17x1x128xbf16> loc(#loc4501)
      %4053 = stablehlo.transpose %4052, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4502)
      %4054 = stablehlo.multiply %4053, %82 : tensor<32x1x17x128xbf16> loc(#loc4503)
      %4055 = stablehlo.slice %4053 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4504)
      %4056 = stablehlo.negate %4055 : tensor<32x1x17x64xbf16> loc(#loc4505)
      %4057 = stablehlo.slice %4053 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4506)
      %4058 = stablehlo.concatenate %4056, %4057, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4507)
      %4059 = stablehlo.multiply %4058, %91 : tensor<32x1x17x128xbf16> loc(#loc4508)
      %4060 = stablehlo.add %4054, %4059 : tensor<32x1x17x128xbf16> loc(#loc4509)
      %4061 = "stablehlo.scatter"(%arg1144, %21, %4060) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.9265"), %arg1677: tensor<bf16> loc("scatter.9265")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4510)
      %4062 = stablehlo.reshape %arg1145 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4511)
      %4063 = stablehlo.reshape %4062 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4512)
      %4064 = stablehlo.transpose %4063, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4513)
      %4065 = stablehlo.dot_general %4032, %4064, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4514)
      %4066 = stablehlo.reshape %4065 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4515)
      %4067 = stablehlo.transpose %4066, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4516)
      %4068 = "stablehlo.scatter"(%arg1146, %21, %4067) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.9295"), %arg1677: tensor<bf16> loc("scatter.9295")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4517)
      %4069 = stablehlo.reshape %arg1156 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4518)
      %4070 = stablehlo.reshape %4069 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4519)
      %4071 = stablehlo.broadcast_in_dim %4070, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4520)
      %4072 = stablehlo.reshape %arg1155 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4521)
      %4073 = stablehlo.reshape %4072 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4522)
      %4074 = stablehlo.broadcast_in_dim %4073, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4523)
      %4075 = stablehlo.reshape %arg1152 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4524)
      %4076 = stablehlo.reshape %4075 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4525)
      %4077 = stablehlo.broadcast_in_dim %4076, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4526)
      %4078 = stablehlo.reshape %arg1151 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc4527)
      %4079 = stablehlo.reshape %4078 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc4528)
      %4080 = stablehlo.transpose %4079, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc4529)
      %4081 = stablehlo.dot_general %4032, %4080, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc4530)
      %4082 = stablehlo.reshape %4081 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4531)
      %4083 = stablehlo.convert %4082 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc4532)
      %4084 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %4085 = stablehlo.power %4083, %4084 : tensor<32x17x8x128xf32> loc(#loc4533)
      %4086 = stablehlo.reduce(%4085 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc4534)
      %4087 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %4088 = stablehlo.multiply %4086, %4087 : tensor<32x17x8xf32> loc(#loc4535)
      %4089 = stablehlo.reshape %4088 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc4536)
      %4090 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %4091 = stablehlo.add %4089, %4090 : tensor<32x17x8x1xf32> loc(#loc4537)
      %4092 = stablehlo.rsqrt %4091 : tensor<32x17x8x1xf32> loc(#loc4538)
      %4093 = stablehlo.reshape %4092 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc4539)
      %4094 = stablehlo.broadcast_in_dim %4093, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc4540)
      %4095 = stablehlo.multiply %4083, %4094 : tensor<32x17x8x128xf32> loc(#loc4541)
      %4096 = stablehlo.convert %4095 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc4542)
      %4097 = stablehlo.multiply %4077, %4096 : tensor<32x17x8x128xbf16> loc(#loc4543)
      %4098 = stablehlo.transpose %4097, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4544)
      %4099 = stablehlo.multiply %4098, %132 : tensor<32x8x17x128xbf16> loc(#loc4545)
      %4100 = stablehlo.slice %4098 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4546)
      %4101 = stablehlo.negate %4100 : tensor<32x8x17x64xbf16> loc(#loc4547)
      %4102 = stablehlo.slice %4098 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4548)
      %4103 = stablehlo.concatenate %4101, %4102, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4549)
      %4104 = stablehlo.multiply %4103, %138 : tensor<32x8x17x128xbf16> loc(#loc4550)
      %4105 = stablehlo.add %4099, %4104 : tensor<32x8x17x128xbf16> loc(#loc4551)
      %4106 = stablehlo.convert %4105 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc4552)
      %4107 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4108 = stablehlo.multiply %4106, %4107 : tensor<32x8x17x128xf32> loc(#loc4553)
      %4109 = stablehlo.broadcast_in_dim %4061, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4554)
      %4110 = stablehlo.reshape %4109 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4555)
      %4111 = stablehlo.convert %4110 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4556)
      %4112 = stablehlo.transpose %4111, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc4557)
      %4113 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %4114 = stablehlo.multiply %4112, %4113 : tensor<32x8x128x128xf32> loc(#loc4558)
      %4115 = stablehlo.dot_general %4108, %4114, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4559)
      %4116 = stablehlo.add %4115, %159 : tensor<32x8x17x128xf32> loc(#loc4560)
      %4117 = stablehlo.convert %4116 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc4561)
      %4118 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %4119 = stablehlo.compare  EQ, %4117, %4118 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc4562)
      %4120 = stablehlo.not %4119 : tensor<32x8x17x128xi1> loc(#loc4563)
      %4121 = stablehlo.reduce(%4120 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.9476"), %arg1677: tensor<i1> loc("reduce.9476"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc4565)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc4566)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc4564)
      %4122 = stablehlo.reshape %4121 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc4567)
      %4123 = stablehlo.not %4122 : tensor<32x8x17x1xi1> loc(#loc4568)
      %4124 = stablehlo.reshape %4123 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc4569)
      %4125 = stablehlo.broadcast_in_dim %4124, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc4570)
      %4126 = stablehlo.reduce(%4116 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4571)
      %4127 = stablehlo.broadcast_in_dim %4126, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4572)
      %4128 = stablehlo.subtract %4116, %4127 : tensor<32x8x17x128xf32> loc(#loc4573)
      %4129 = stablehlo.exponential %4128 : tensor<32x8x17x128xf32> loc(#loc4574)
      %4130 = stablehlo.reduce(%4129 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4575)
      %4131 = stablehlo.broadcast_in_dim %4130, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4576)
      %4132 = stablehlo.divide %4129, %4131 : tensor<32x8x17x128xf32> loc(#loc4577)
      %4133 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4134 = stablehlo.select %4125, %4133, %4132 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc4578)
      %4135 = stablehlo.broadcast_in_dim %4068, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4579)
      %4136 = stablehlo.reshape %4135 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4580)
      %4137 = stablehlo.convert %4136 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4581)
      %4138 = stablehlo.dot_general %4134, %4137, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4582)
      %4139 = stablehlo.convert %4138 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc4583)
      %4140 = stablehlo.transpose %4139, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4584)
      %4141 = stablehlo.reshape %4140 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc4585)
      %4142 = stablehlo.reshape %arg1150 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc4586)
      %4143 = stablehlo.reshape %4142 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc4587)
      %4144 = stablehlo.transpose %4143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc4588)
      %4145 = stablehlo.dot_general %4141, %4144, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4589)
      %4146 = "stablehlo.all_reduce"(%4145) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.9493"), %arg1677: tensor<bf16> loc("dot.9493")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4589)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4589)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4589)
      %4147 = stablehlo.reshape %4146 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4590)
      %4148 = stablehlo.add %4016, %4147 : tensor<32x17x5120xbf16> loc(#loc4591)
      %4149 = stablehlo.reshape %arg1153 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4592)
      %4150 = stablehlo.reshape %4149 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4593)
      %4151 = stablehlo.broadcast_in_dim %4150, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4594)
      %4152 = stablehlo.convert %4148 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4595)
      %4153 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4154 = stablehlo.power %4152, %4153 : tensor<32x17x5120xf32> loc(#loc4596)
      %4155 = stablehlo.reduce(%4154 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4597)
      %4156 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4157 = stablehlo.multiply %4155, %4156 : tensor<32x17xf32> loc(#loc4598)
      %4158 = stablehlo.reshape %4157 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4599)
      %4159 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4160 = stablehlo.add %4158, %4159 : tensor<32x17x1xf32> loc(#loc4600)
      %4161 = stablehlo.rsqrt %4160 : tensor<32x17x1xf32> loc(#loc4601)
      %4162 = stablehlo.reshape %4161 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4602)
      %4163 = stablehlo.broadcast_in_dim %4162, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4603)
      %4164 = stablehlo.multiply %4152, %4163 : tensor<32x17x5120xf32> loc(#loc4604)
      %4165 = stablehlo.convert %4164 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4605)
      %4166 = stablehlo.multiply %4151, %4165 : tensor<32x17x5120xbf16> loc(#loc4606)
      %4167 = stablehlo.reshape %4166 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4607)
      %4168 = stablehlo.reshape %arg1154 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4608)
      %4169 = stablehlo.reshape %4168 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4609)
      %4170 = stablehlo.transpose %4169, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4610)
      %4171 = stablehlo.dot_general %4167, %4170, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4611)
      %4172 = stablehlo.reshape %4171 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4612)
      %4173 = stablehlo.logistic %4172 : tensor<32x17x3200xbf16> loc(#loc4613)
      %4174 = stablehlo.multiply %4172, %4173 : tensor<32x17x3200xbf16> loc(#loc4614)
      %4175 = stablehlo.reshape %arg1149 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4615)
      %4176 = stablehlo.reshape %4175 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4616)
      %4177 = stablehlo.transpose %4176, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4617)
      %4178 = stablehlo.dot_general %4167, %4177, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4618)
      %4179 = stablehlo.reshape %4178 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4619)
      %4180 = stablehlo.multiply %4174, %4179 : tensor<32x17x3200xbf16> loc(#loc4620)
      %4181 = stablehlo.reshape %4180 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4621)
      %4182 = stablehlo.reshape %arg1148 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc4622)
      %4183 = stablehlo.reshape %4182 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc4623)
      %4184 = stablehlo.transpose %4183, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc4624)
      %4185 = stablehlo.dot_general %4181, %4184, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4625)
      %4186 = "stablehlo.all_reduce"(%4185) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.9548"), %arg1677: tensor<bf16> loc("dot.9548")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4625)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4625)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4625)
      %4187 = stablehlo.reshape %4186 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4626)
      %4188 = stablehlo.add %4148, %4187 : tensor<32x17x5120xbf16> loc(#loc4627)
      %4189 = stablehlo.convert %4188 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4628)
      %4190 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4191 = stablehlo.power %4189, %4190 : tensor<32x17x5120xf32> loc(#loc4629)
      %4192 = stablehlo.reduce(%4191 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4630)
      %4193 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4194 = stablehlo.multiply %4192, %4193 : tensor<32x17xf32> loc(#loc4631)
      %4195 = stablehlo.reshape %4194 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4632)
      %4196 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4197 = stablehlo.add %4195, %4196 : tensor<32x17x1xf32> loc(#loc4633)
      %4198 = stablehlo.rsqrt %4197 : tensor<32x17x1xf32> loc(#loc4634)
      %4199 = stablehlo.reshape %4198 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4635)
      %4200 = stablehlo.broadcast_in_dim %4199, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4636)
      %4201 = stablehlo.multiply %4189, %4200 : tensor<32x17x5120xf32> loc(#loc4637)
      %4202 = stablehlo.convert %4201 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4638)
      %4203 = stablehlo.multiply %4074, %4202 : tensor<32x17x5120xbf16> loc(#loc4639)
      %4204 = stablehlo.reshape %4203 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4640)
      %4205 = stablehlo.reshape %arg1147 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4641)
      %4206 = stablehlo.reshape %4205 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4642)
      %4207 = stablehlo.transpose %4206, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4643)
      %4208 = stablehlo.dot_general %4204, %4207, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4644)
      %4209 = stablehlo.reshape %4208 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4645)
      %4210 = stablehlo.convert %4209 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc4646)
      %4211 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %4212 = stablehlo.power %4210, %4211 : tensor<32x17x1x128xf32> loc(#loc4647)
      %4213 = stablehlo.reduce(%4212 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc4648)
      %4214 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4215 = stablehlo.multiply %4213, %4214 : tensor<32x17x1xf32> loc(#loc4649)
      %4216 = stablehlo.reshape %4215 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc4650)
      %4217 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %4218 = stablehlo.add %4216, %4217 : tensor<32x17x1x1xf32> loc(#loc4651)
      %4219 = stablehlo.rsqrt %4218 : tensor<32x17x1x1xf32> loc(#loc4652)
      %4220 = stablehlo.reshape %4219 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc4653)
      %4221 = stablehlo.broadcast_in_dim %4220, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc4654)
      %4222 = stablehlo.multiply %4210, %4221 : tensor<32x17x1x128xf32> loc(#loc4655)
      %4223 = stablehlo.convert %4222 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc4656)
      %4224 = stablehlo.multiply %4071, %4223 : tensor<32x17x1x128xbf16> loc(#loc4657)
      %4225 = stablehlo.transpose %4224, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4658)
      %4226 = stablehlo.multiply %4225, %82 : tensor<32x1x17x128xbf16> loc(#loc4659)
      %4227 = stablehlo.slice %4225 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4660)
      %4228 = stablehlo.negate %4227 : tensor<32x1x17x64xbf16> loc(#loc4661)
      %4229 = stablehlo.slice %4225 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4662)
      %4230 = stablehlo.concatenate %4228, %4229, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4663)
      %4231 = stablehlo.multiply %4230, %91 : tensor<32x1x17x128xbf16> loc(#loc4664)
      %4232 = stablehlo.add %4226, %4231 : tensor<32x1x17x128xbf16> loc(#loc4665)
      %4233 = "stablehlo.scatter"(%arg1157, %21, %4232) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.9660"), %arg1677: tensor<bf16> loc("scatter.9660")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4666)
      %4234 = stablehlo.reshape %arg1158 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4667)
      %4235 = stablehlo.reshape %4234 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4668)
      %4236 = stablehlo.transpose %4235, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4669)
      %4237 = stablehlo.dot_general %4204, %4236, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4670)
      %4238 = stablehlo.reshape %4237 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4671)
      %4239 = stablehlo.transpose %4238, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4672)
      %4240 = "stablehlo.scatter"(%arg1159, %21, %4239) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.9690"), %arg1677: tensor<bf16> loc("scatter.9690")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4673)
      %4241 = stablehlo.reshape %arg1169 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4674)
      %4242 = stablehlo.reshape %4241 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4675)
      %4243 = stablehlo.broadcast_in_dim %4242, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4676)
      %4244 = stablehlo.reshape %arg1168 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4677)
      %4245 = stablehlo.reshape %4244 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4678)
      %4246 = stablehlo.broadcast_in_dim %4245, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4679)
      %4247 = stablehlo.reshape %arg1165 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4680)
      %4248 = stablehlo.reshape %4247 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4681)
      %4249 = stablehlo.broadcast_in_dim %4248, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4682)
      %4250 = stablehlo.reshape %arg1164 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc4683)
      %4251 = stablehlo.reshape %4250 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc4684)
      %4252 = stablehlo.transpose %4251, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc4685)
      %4253 = stablehlo.dot_general %4204, %4252, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc4686)
      %4254 = stablehlo.reshape %4253 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4687)
      %4255 = stablehlo.convert %4254 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc4688)
      %4256 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %4257 = stablehlo.power %4255, %4256 : tensor<32x17x8x128xf32> loc(#loc4689)
      %4258 = stablehlo.reduce(%4257 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc4690)
      %4259 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %4260 = stablehlo.multiply %4258, %4259 : tensor<32x17x8xf32> loc(#loc4691)
      %4261 = stablehlo.reshape %4260 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc4692)
      %4262 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %4263 = stablehlo.add %4261, %4262 : tensor<32x17x8x1xf32> loc(#loc4693)
      %4264 = stablehlo.rsqrt %4263 : tensor<32x17x8x1xf32> loc(#loc4694)
      %4265 = stablehlo.reshape %4264 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc4695)
      %4266 = stablehlo.broadcast_in_dim %4265, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc4696)
      %4267 = stablehlo.multiply %4255, %4266 : tensor<32x17x8x128xf32> loc(#loc4697)
      %4268 = stablehlo.convert %4267 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc4698)
      %4269 = stablehlo.multiply %4249, %4268 : tensor<32x17x8x128xbf16> loc(#loc4699)
      %4270 = stablehlo.transpose %4269, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4700)
      %4271 = stablehlo.multiply %4270, %132 : tensor<32x8x17x128xbf16> loc(#loc4701)
      %4272 = stablehlo.slice %4270 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4702)
      %4273 = stablehlo.negate %4272 : tensor<32x8x17x64xbf16> loc(#loc4703)
      %4274 = stablehlo.slice %4270 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4704)
      %4275 = stablehlo.concatenate %4273, %4274, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4705)
      %4276 = stablehlo.multiply %4275, %138 : tensor<32x8x17x128xbf16> loc(#loc4706)
      %4277 = stablehlo.add %4271, %4276 : tensor<32x8x17x128xbf16> loc(#loc4707)
      %4278 = stablehlo.convert %4277 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc4708)
      %4279 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4280 = stablehlo.multiply %4278, %4279 : tensor<32x8x17x128xf32> loc(#loc4709)
      %4281 = stablehlo.broadcast_in_dim %4233, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4710)
      %4282 = stablehlo.reshape %4281 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4711)
      %4283 = stablehlo.convert %4282 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4712)
      %4284 = stablehlo.transpose %4283, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc4713)
      %4285 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %4286 = stablehlo.multiply %4284, %4285 : tensor<32x8x128x128xf32> loc(#loc4714)
      %4287 = stablehlo.dot_general %4280, %4286, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4715)
      %4288 = stablehlo.add %4287, %159 : tensor<32x8x17x128xf32> loc(#loc4716)
      %4289 = stablehlo.convert %4288 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc4717)
      %4290 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %4291 = stablehlo.compare  EQ, %4289, %4290 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc4718)
      %4292 = stablehlo.not %4291 : tensor<32x8x17x128xi1> loc(#loc4719)
      %4293 = stablehlo.reduce(%4292 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.9871"), %arg1677: tensor<i1> loc("reduce.9871"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc4721)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc4722)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc4720)
      %4294 = stablehlo.reshape %4293 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc4723)
      %4295 = stablehlo.not %4294 : tensor<32x8x17x1xi1> loc(#loc4724)
      %4296 = stablehlo.reshape %4295 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc4725)
      %4297 = stablehlo.broadcast_in_dim %4296, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc4726)
      %4298 = stablehlo.reduce(%4288 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4727)
      %4299 = stablehlo.broadcast_in_dim %4298, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4728)
      %4300 = stablehlo.subtract %4288, %4299 : tensor<32x8x17x128xf32> loc(#loc4729)
      %4301 = stablehlo.exponential %4300 : tensor<32x8x17x128xf32> loc(#loc4730)
      %4302 = stablehlo.reduce(%4301 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4731)
      %4303 = stablehlo.broadcast_in_dim %4302, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4732)
      %4304 = stablehlo.divide %4301, %4303 : tensor<32x8x17x128xf32> loc(#loc4733)
      %4305 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4306 = stablehlo.select %4297, %4305, %4304 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc4734)
      %4307 = stablehlo.broadcast_in_dim %4240, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4735)
      %4308 = stablehlo.reshape %4307 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4736)
      %4309 = stablehlo.convert %4308 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4737)
      %4310 = stablehlo.dot_general %4306, %4309, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4738)
      %4311 = stablehlo.convert %4310 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc4739)
      %4312 = stablehlo.transpose %4311, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4740)
      %4313 = stablehlo.reshape %4312 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc4741)
      %4314 = stablehlo.reshape %arg1163 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc4742)
      %4315 = stablehlo.reshape %4314 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc4743)
      %4316 = stablehlo.transpose %4315, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc4744)
      %4317 = stablehlo.dot_general %4313, %4316, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4745)
      %4318 = "stablehlo.all_reduce"(%4317) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.9888"), %arg1677: tensor<bf16> loc("dot.9888")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4745)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4745)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4745)
      %4319 = stablehlo.reshape %4318 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4746)
      %4320 = stablehlo.add %4188, %4319 : tensor<32x17x5120xbf16> loc(#loc4747)
      %4321 = stablehlo.reshape %arg1166 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4748)
      %4322 = stablehlo.reshape %4321 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4749)
      %4323 = stablehlo.broadcast_in_dim %4322, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4750)
      %4324 = stablehlo.convert %4320 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4751)
      %4325 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4326 = stablehlo.power %4324, %4325 : tensor<32x17x5120xf32> loc(#loc4752)
      %4327 = stablehlo.reduce(%4326 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4753)
      %4328 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4329 = stablehlo.multiply %4327, %4328 : tensor<32x17xf32> loc(#loc4754)
      %4330 = stablehlo.reshape %4329 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4755)
      %4331 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4332 = stablehlo.add %4330, %4331 : tensor<32x17x1xf32> loc(#loc4756)
      %4333 = stablehlo.rsqrt %4332 : tensor<32x17x1xf32> loc(#loc4757)
      %4334 = stablehlo.reshape %4333 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4758)
      %4335 = stablehlo.broadcast_in_dim %4334, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4759)
      %4336 = stablehlo.multiply %4324, %4335 : tensor<32x17x5120xf32> loc(#loc4760)
      %4337 = stablehlo.convert %4336 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4761)
      %4338 = stablehlo.multiply %4323, %4337 : tensor<32x17x5120xbf16> loc(#loc4762)
      %4339 = stablehlo.reshape %4338 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4763)
      %4340 = stablehlo.reshape %arg1167 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4764)
      %4341 = stablehlo.reshape %4340 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4765)
      %4342 = stablehlo.transpose %4341, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4766)
      %4343 = stablehlo.dot_general %4339, %4342, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4767)
      %4344 = stablehlo.reshape %4343 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4768)
      %4345 = stablehlo.logistic %4344 : tensor<32x17x3200xbf16> loc(#loc4769)
      %4346 = stablehlo.multiply %4344, %4345 : tensor<32x17x3200xbf16> loc(#loc4770)
      %4347 = stablehlo.reshape %arg1162 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4771)
      %4348 = stablehlo.reshape %4347 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4772)
      %4349 = stablehlo.transpose %4348, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4773)
      %4350 = stablehlo.dot_general %4339, %4349, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4774)
      %4351 = stablehlo.reshape %4350 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4775)
      %4352 = stablehlo.multiply %4346, %4351 : tensor<32x17x3200xbf16> loc(#loc4776)
      %4353 = stablehlo.reshape %4352 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4777)
      %4354 = stablehlo.reshape %arg1161 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc4778)
      %4355 = stablehlo.reshape %4354 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc4779)
      %4356 = stablehlo.transpose %4355, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc4780)
      %4357 = stablehlo.dot_general %4353, %4356, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4781)
      %4358 = "stablehlo.all_reduce"(%4357) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.9943"), %arg1677: tensor<bf16> loc("dot.9943")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4781)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4781)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4781)
      %4359 = stablehlo.reshape %4358 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4782)
      %4360 = stablehlo.add %4320, %4359 : tensor<32x17x5120xbf16> loc(#loc4783)
      %4361 = stablehlo.convert %4360 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4784)
      %4362 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4363 = stablehlo.power %4361, %4362 : tensor<32x17x5120xf32> loc(#loc4785)
      %4364 = stablehlo.reduce(%4363 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4786)
      %4365 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4366 = stablehlo.multiply %4364, %4365 : tensor<32x17xf32> loc(#loc4787)
      %4367 = stablehlo.reshape %4366 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4788)
      %4368 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4369 = stablehlo.add %4367, %4368 : tensor<32x17x1xf32> loc(#loc4789)
      %4370 = stablehlo.rsqrt %4369 : tensor<32x17x1xf32> loc(#loc4790)
      %4371 = stablehlo.reshape %4370 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4791)
      %4372 = stablehlo.broadcast_in_dim %4371, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4792)
      %4373 = stablehlo.multiply %4361, %4372 : tensor<32x17x5120xf32> loc(#loc4793)
      %4374 = stablehlo.convert %4373 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4794)
      %4375 = stablehlo.multiply %4246, %4374 : tensor<32x17x5120xbf16> loc(#loc4795)
      %4376 = stablehlo.reshape %4375 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4796)
      %4377 = stablehlo.reshape %arg1160 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4797)
      %4378 = stablehlo.reshape %4377 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4798)
      %4379 = stablehlo.transpose %4378, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4799)
      %4380 = stablehlo.dot_general %4376, %4379, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4800)
      %4381 = stablehlo.reshape %4380 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4801)
      %4382 = stablehlo.convert %4381 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc4802)
      %4383 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %4384 = stablehlo.power %4382, %4383 : tensor<32x17x1x128xf32> loc(#loc4803)
      %4385 = stablehlo.reduce(%4384 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc4804)
      %4386 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4387 = stablehlo.multiply %4385, %4386 : tensor<32x17x1xf32> loc(#loc4805)
      %4388 = stablehlo.reshape %4387 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc4806)
      %4389 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %4390 = stablehlo.add %4388, %4389 : tensor<32x17x1x1xf32> loc(#loc4807)
      %4391 = stablehlo.rsqrt %4390 : tensor<32x17x1x1xf32> loc(#loc4808)
      %4392 = stablehlo.reshape %4391 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc4809)
      %4393 = stablehlo.broadcast_in_dim %4392, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc4810)
      %4394 = stablehlo.multiply %4382, %4393 : tensor<32x17x1x128xf32> loc(#loc4811)
      %4395 = stablehlo.convert %4394 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc4812)
      %4396 = stablehlo.multiply %4243, %4395 : tensor<32x17x1x128xbf16> loc(#loc4813)
      %4397 = stablehlo.transpose %4396, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4814)
      %4398 = stablehlo.multiply %4397, %82 : tensor<32x1x17x128xbf16> loc(#loc4815)
      %4399 = stablehlo.slice %4397 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4816)
      %4400 = stablehlo.negate %4399 : tensor<32x1x17x64xbf16> loc(#loc4817)
      %4401 = stablehlo.slice %4397 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4818)
      %4402 = stablehlo.concatenate %4400, %4401, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4819)
      %4403 = stablehlo.multiply %4402, %91 : tensor<32x1x17x128xbf16> loc(#loc4820)
      %4404 = stablehlo.add %4398, %4403 : tensor<32x1x17x128xbf16> loc(#loc4821)
      %4405 = "stablehlo.scatter"(%arg1170, %21, %4404) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.10055"), %arg1677: tensor<bf16> loc("scatter.10055")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4822)
      %4406 = stablehlo.reshape %arg1171 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4823)
      %4407 = stablehlo.reshape %4406 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4824)
      %4408 = stablehlo.transpose %4407, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4825)
      %4409 = stablehlo.dot_general %4376, %4408, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4826)
      %4410 = stablehlo.reshape %4409 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4827)
      %4411 = stablehlo.transpose %4410, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4828)
      %4412 = "stablehlo.scatter"(%arg1172, %21, %4411) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.10085"), %arg1677: tensor<bf16> loc("scatter.10085")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4829)
      %4413 = stablehlo.reshape %arg1182 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4830)
      %4414 = stablehlo.reshape %4413 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4831)
      %4415 = stablehlo.broadcast_in_dim %4414, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4832)
      %4416 = stablehlo.reshape %arg1181 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4833)
      %4417 = stablehlo.reshape %4416 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4834)
      %4418 = stablehlo.broadcast_in_dim %4417, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4835)
      %4419 = stablehlo.reshape %arg1178 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4836)
      %4420 = stablehlo.reshape %4419 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4837)
      %4421 = stablehlo.broadcast_in_dim %4420, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4838)
      %4422 = stablehlo.reshape %arg1177 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc4839)
      %4423 = stablehlo.reshape %4422 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc4840)
      %4424 = stablehlo.transpose %4423, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc4841)
      %4425 = stablehlo.dot_general %4376, %4424, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc4842)
      %4426 = stablehlo.reshape %4425 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4843)
      %4427 = stablehlo.convert %4426 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc4844)
      %4428 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %4429 = stablehlo.power %4427, %4428 : tensor<32x17x8x128xf32> loc(#loc4845)
      %4430 = stablehlo.reduce(%4429 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc4846)
      %4431 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %4432 = stablehlo.multiply %4430, %4431 : tensor<32x17x8xf32> loc(#loc4847)
      %4433 = stablehlo.reshape %4432 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc4848)
      %4434 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %4435 = stablehlo.add %4433, %4434 : tensor<32x17x8x1xf32> loc(#loc4849)
      %4436 = stablehlo.rsqrt %4435 : tensor<32x17x8x1xf32> loc(#loc4850)
      %4437 = stablehlo.reshape %4436 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc4851)
      %4438 = stablehlo.broadcast_in_dim %4437, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc4852)
      %4439 = stablehlo.multiply %4427, %4438 : tensor<32x17x8x128xf32> loc(#loc4853)
      %4440 = stablehlo.convert %4439 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc4854)
      %4441 = stablehlo.multiply %4421, %4440 : tensor<32x17x8x128xbf16> loc(#loc4855)
      %4442 = stablehlo.transpose %4441, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4856)
      %4443 = stablehlo.multiply %4442, %132 : tensor<32x8x17x128xbf16> loc(#loc4857)
      %4444 = stablehlo.slice %4442 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4858)
      %4445 = stablehlo.negate %4444 : tensor<32x8x17x64xbf16> loc(#loc4859)
      %4446 = stablehlo.slice %4442 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc4860)
      %4447 = stablehlo.concatenate %4445, %4446, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc4861)
      %4448 = stablehlo.multiply %4447, %138 : tensor<32x8x17x128xbf16> loc(#loc4862)
      %4449 = stablehlo.add %4443, %4448 : tensor<32x8x17x128xbf16> loc(#loc4863)
      %4450 = stablehlo.convert %4449 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc4864)
      %4451 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4452 = stablehlo.multiply %4450, %4451 : tensor<32x8x17x128xf32> loc(#loc4865)
      %4453 = stablehlo.broadcast_in_dim %4405, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4866)
      %4454 = stablehlo.reshape %4453 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4867)
      %4455 = stablehlo.convert %4454 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4868)
      %4456 = stablehlo.transpose %4455, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc4869)
      %4457 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %4458 = stablehlo.multiply %4456, %4457 : tensor<32x8x128x128xf32> loc(#loc4870)
      %4459 = stablehlo.dot_general %4452, %4458, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4871)
      %4460 = stablehlo.add %4459, %159 : tensor<32x8x17x128xf32> loc(#loc4872)
      %4461 = stablehlo.convert %4460 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc4873)
      %4462 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %4463 = stablehlo.compare  EQ, %4461, %4462 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc4874)
      %4464 = stablehlo.not %4463 : tensor<32x8x17x128xi1> loc(#loc4875)
      %4465 = stablehlo.reduce(%4464 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.10266"), %arg1677: tensor<i1> loc("reduce.10266"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc4877)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc4878)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc4876)
      %4466 = stablehlo.reshape %4465 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc4879)
      %4467 = stablehlo.not %4466 : tensor<32x8x17x1xi1> loc(#loc4880)
      %4468 = stablehlo.reshape %4467 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc4881)
      %4469 = stablehlo.broadcast_in_dim %4468, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc4882)
      %4470 = stablehlo.reduce(%4460 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4883)
      %4471 = stablehlo.broadcast_in_dim %4470, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4884)
      %4472 = stablehlo.subtract %4460, %4471 : tensor<32x8x17x128xf32> loc(#loc4885)
      %4473 = stablehlo.exponential %4472 : tensor<32x8x17x128xf32> loc(#loc4886)
      %4474 = stablehlo.reduce(%4473 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc4887)
      %4475 = stablehlo.broadcast_in_dim %4474, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc4888)
      %4476 = stablehlo.divide %4473, %4475 : tensor<32x8x17x128xf32> loc(#loc4889)
      %4477 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4478 = stablehlo.select %4469, %4477, %4476 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc4890)
      %4479 = stablehlo.broadcast_in_dim %4412, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc4891)
      %4480 = stablehlo.reshape %4479 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc4892)
      %4481 = stablehlo.convert %4480 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc4893)
      %4482 = stablehlo.dot_general %4478, %4481, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc4894)
      %4483 = stablehlo.convert %4482 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc4895)
      %4484 = stablehlo.transpose %4483, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4896)
      %4485 = stablehlo.reshape %4484 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc4897)
      %4486 = stablehlo.reshape %arg1176 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc4898)
      %4487 = stablehlo.reshape %4486 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc4899)
      %4488 = stablehlo.transpose %4487, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc4900)
      %4489 = stablehlo.dot_general %4485, %4488, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4901)
      %4490 = "stablehlo.all_reduce"(%4489) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.10283"), %arg1677: tensor<bf16> loc("dot.10283")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4901)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4901)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4901)
      %4491 = stablehlo.reshape %4490 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4902)
      %4492 = stablehlo.add %4360, %4491 : tensor<32x17x5120xbf16> loc(#loc4903)
      %4493 = stablehlo.reshape %arg1179 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4904)
      %4494 = stablehlo.reshape %4493 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4905)
      %4495 = stablehlo.broadcast_in_dim %4494, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4906)
      %4496 = stablehlo.convert %4492 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4907)
      %4497 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4498 = stablehlo.power %4496, %4497 : tensor<32x17x5120xf32> loc(#loc4908)
      %4499 = stablehlo.reduce(%4498 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4909)
      %4500 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4501 = stablehlo.multiply %4499, %4500 : tensor<32x17xf32> loc(#loc4910)
      %4502 = stablehlo.reshape %4501 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4911)
      %4503 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4504 = stablehlo.add %4502, %4503 : tensor<32x17x1xf32> loc(#loc4912)
      %4505 = stablehlo.rsqrt %4504 : tensor<32x17x1xf32> loc(#loc4913)
      %4506 = stablehlo.reshape %4505 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4914)
      %4507 = stablehlo.broadcast_in_dim %4506, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4915)
      %4508 = stablehlo.multiply %4496, %4507 : tensor<32x17x5120xf32> loc(#loc4916)
      %4509 = stablehlo.convert %4508 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4917)
      %4510 = stablehlo.multiply %4495, %4509 : tensor<32x17x5120xbf16> loc(#loc4918)
      %4511 = stablehlo.reshape %4510 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4919)
      %4512 = stablehlo.reshape %arg1180 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4920)
      %4513 = stablehlo.reshape %4512 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4921)
      %4514 = stablehlo.transpose %4513, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4922)
      %4515 = stablehlo.dot_general %4511, %4514, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4923)
      %4516 = stablehlo.reshape %4515 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4924)
      %4517 = stablehlo.logistic %4516 : tensor<32x17x3200xbf16> loc(#loc4925)
      %4518 = stablehlo.multiply %4516, %4517 : tensor<32x17x3200xbf16> loc(#loc4926)
      %4519 = stablehlo.reshape %arg1175 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc4927)
      %4520 = stablehlo.reshape %4519 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc4928)
      %4521 = stablehlo.transpose %4520, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc4929)
      %4522 = stablehlo.dot_general %4511, %4521, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4930)
      %4523 = stablehlo.reshape %4522 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc4931)
      %4524 = stablehlo.multiply %4518, %4523 : tensor<32x17x3200xbf16> loc(#loc4932)
      %4525 = stablehlo.reshape %4524 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc4933)
      %4526 = stablehlo.reshape %arg1174 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc4934)
      %4527 = stablehlo.reshape %4526 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc4935)
      %4528 = stablehlo.transpose %4527, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc4936)
      %4529 = stablehlo.dot_general %4525, %4528, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4937)
      %4530 = "stablehlo.all_reduce"(%4529) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.10338"), %arg1677: tensor<bf16> loc("dot.10338")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc4937)
        stablehlo.return %11074 : tensor<bf16> loc(#loc4937)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4937)
      %4531 = stablehlo.reshape %4530 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4938)
      %4532 = stablehlo.add %4492, %4531 : tensor<32x17x5120xbf16> loc(#loc4939)
      %4533 = stablehlo.convert %4532 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc4940)
      %4534 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4535 = stablehlo.power %4533, %4534 : tensor<32x17x5120xf32> loc(#loc4941)
      %4536 = stablehlo.reduce(%4535 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc4942)
      %4537 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4538 = stablehlo.multiply %4536, %4537 : tensor<32x17xf32> loc(#loc4943)
      %4539 = stablehlo.reshape %4538 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc4944)
      %4540 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4541 = stablehlo.add %4539, %4540 : tensor<32x17x1xf32> loc(#loc4945)
      %4542 = stablehlo.rsqrt %4541 : tensor<32x17x1xf32> loc(#loc4946)
      %4543 = stablehlo.reshape %4542 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc4947)
      %4544 = stablehlo.broadcast_in_dim %4543, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc4948)
      %4545 = stablehlo.multiply %4533, %4544 : tensor<32x17x5120xf32> loc(#loc4949)
      %4546 = stablehlo.convert %4545 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc4950)
      %4547 = stablehlo.multiply %4418, %4546 : tensor<32x17x5120xbf16> loc(#loc4951)
      %4548 = stablehlo.reshape %4547 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc4952)
      %4549 = stablehlo.reshape %arg1173 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4953)
      %4550 = stablehlo.reshape %4549 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4954)
      %4551 = stablehlo.transpose %4550, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4955)
      %4552 = stablehlo.dot_general %4548, %4551, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4956)
      %4553 = stablehlo.reshape %4552 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4957)
      %4554 = stablehlo.convert %4553 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc4958)
      %4555 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %4556 = stablehlo.power %4554, %4555 : tensor<32x17x1x128xf32> loc(#loc4959)
      %4557 = stablehlo.reduce(%4556 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc4960)
      %4558 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4559 = stablehlo.multiply %4557, %4558 : tensor<32x17x1xf32> loc(#loc4961)
      %4560 = stablehlo.reshape %4559 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc4962)
      %4561 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %4562 = stablehlo.add %4560, %4561 : tensor<32x17x1x1xf32> loc(#loc4963)
      %4563 = stablehlo.rsqrt %4562 : tensor<32x17x1x1xf32> loc(#loc4964)
      %4564 = stablehlo.reshape %4563 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc4965)
      %4565 = stablehlo.broadcast_in_dim %4564, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc4966)
      %4566 = stablehlo.multiply %4554, %4565 : tensor<32x17x1x128xf32> loc(#loc4967)
      %4567 = stablehlo.convert %4566 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc4968)
      %4568 = stablehlo.multiply %4415, %4567 : tensor<32x17x1x128xbf16> loc(#loc4969)
      %4569 = stablehlo.transpose %4568, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4970)
      %4570 = stablehlo.multiply %4569, %82 : tensor<32x1x17x128xbf16> loc(#loc4971)
      %4571 = stablehlo.slice %4569 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4972)
      %4572 = stablehlo.negate %4571 : tensor<32x1x17x64xbf16> loc(#loc4973)
      %4573 = stablehlo.slice %4569 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc4974)
      %4574 = stablehlo.concatenate %4572, %4573, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4975)
      %4575 = stablehlo.multiply %4574, %91 : tensor<32x1x17x128xbf16> loc(#loc4976)
      %4576 = stablehlo.add %4570, %4575 : tensor<32x1x17x128xbf16> loc(#loc4977)
      %4577 = "stablehlo.scatter"(%arg1183, %21, %4576) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.10450"), %arg1677: tensor<bf16> loc("scatter.10450")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4978)
      %4578 = stablehlo.reshape %arg1184 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc4979)
      %4579 = stablehlo.reshape %4578 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc4980)
      %4580 = stablehlo.transpose %4579, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc4981)
      %4581 = stablehlo.dot_general %4548, %4580, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc4982)
      %4582 = stablehlo.reshape %4581 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4983)
      %4583 = stablehlo.transpose %4582, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc4984)
      %4584 = "stablehlo.scatter"(%arg1185, %21, %4583) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.10480"), %arg1677: tensor<bf16> loc("scatter.10480")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc4985)
      %4585 = stablehlo.reshape %arg1195 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4986)
      %4586 = stablehlo.reshape %4585 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4987)
      %4587 = stablehlo.broadcast_in_dim %4586, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc4988)
      %4588 = stablehlo.reshape %arg1194 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc4989)
      %4589 = stablehlo.reshape %4588 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc4990)
      %4590 = stablehlo.broadcast_in_dim %4589, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc4991)
      %4591 = stablehlo.reshape %arg1191 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc4992)
      %4592 = stablehlo.reshape %4591 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc4993)
      %4593 = stablehlo.broadcast_in_dim %4592, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4994)
      %4594 = stablehlo.reshape %arg1190 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc4995)
      %4595 = stablehlo.reshape %4594 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc4996)
      %4596 = stablehlo.transpose %4595, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc4997)
      %4597 = stablehlo.dot_general %4548, %4596, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc4998)
      %4598 = stablehlo.reshape %4597 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc4999)
      %4599 = stablehlo.convert %4598 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc5000)
      %4600 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %4601 = stablehlo.power %4599, %4600 : tensor<32x17x8x128xf32> loc(#loc5001)
      %4602 = stablehlo.reduce(%4601 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc5002)
      %4603 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %4604 = stablehlo.multiply %4602, %4603 : tensor<32x17x8xf32> loc(#loc5003)
      %4605 = stablehlo.reshape %4604 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc5004)
      %4606 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %4607 = stablehlo.add %4605, %4606 : tensor<32x17x8x1xf32> loc(#loc5005)
      %4608 = stablehlo.rsqrt %4607 : tensor<32x17x8x1xf32> loc(#loc5006)
      %4609 = stablehlo.reshape %4608 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc5007)
      %4610 = stablehlo.broadcast_in_dim %4609, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc5008)
      %4611 = stablehlo.multiply %4599, %4610 : tensor<32x17x8x128xf32> loc(#loc5009)
      %4612 = stablehlo.convert %4611 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc5010)
      %4613 = stablehlo.multiply %4593, %4612 : tensor<32x17x8x128xbf16> loc(#loc5011)
      %4614 = stablehlo.transpose %4613, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5012)
      %4615 = stablehlo.multiply %4614, %132 : tensor<32x8x17x128xbf16> loc(#loc5013)
      %4616 = stablehlo.slice %4614 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5014)
      %4617 = stablehlo.negate %4616 : tensor<32x8x17x64xbf16> loc(#loc5015)
      %4618 = stablehlo.slice %4614 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5016)
      %4619 = stablehlo.concatenate %4617, %4618, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5017)
      %4620 = stablehlo.multiply %4619, %138 : tensor<32x8x17x128xbf16> loc(#loc5018)
      %4621 = stablehlo.add %4615, %4620 : tensor<32x8x17x128xbf16> loc(#loc5019)
      %4622 = stablehlo.convert %4621 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc5020)
      %4623 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4624 = stablehlo.multiply %4622, %4623 : tensor<32x8x17x128xf32> loc(#loc5021)
      %4625 = stablehlo.broadcast_in_dim %4577, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5022)
      %4626 = stablehlo.reshape %4625 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5023)
      %4627 = stablehlo.convert %4626 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5024)
      %4628 = stablehlo.transpose %4627, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc5025)
      %4629 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %4630 = stablehlo.multiply %4628, %4629 : tensor<32x8x128x128xf32> loc(#loc5026)
      %4631 = stablehlo.dot_general %4624, %4630, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5027)
      %4632 = stablehlo.add %4631, %159 : tensor<32x8x17x128xf32> loc(#loc5028)
      %4633 = stablehlo.convert %4632 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc5029)
      %4634 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %4635 = stablehlo.compare  EQ, %4633, %4634 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc5030)
      %4636 = stablehlo.not %4635 : tensor<32x8x17x128xi1> loc(#loc5031)
      %4637 = stablehlo.reduce(%4636 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.10661"), %arg1677: tensor<i1> loc("reduce.10661"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc5033)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc5034)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc5032)
      %4638 = stablehlo.reshape %4637 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc5035)
      %4639 = stablehlo.not %4638 : tensor<32x8x17x1xi1> loc(#loc5036)
      %4640 = stablehlo.reshape %4639 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc5037)
      %4641 = stablehlo.broadcast_in_dim %4640, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc5038)
      %4642 = stablehlo.reduce(%4632 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5039)
      %4643 = stablehlo.broadcast_in_dim %4642, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5040)
      %4644 = stablehlo.subtract %4632, %4643 : tensor<32x8x17x128xf32> loc(#loc5041)
      %4645 = stablehlo.exponential %4644 : tensor<32x8x17x128xf32> loc(#loc5042)
      %4646 = stablehlo.reduce(%4645 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5043)
      %4647 = stablehlo.broadcast_in_dim %4646, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5044)
      %4648 = stablehlo.divide %4645, %4647 : tensor<32x8x17x128xf32> loc(#loc5045)
      %4649 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4650 = stablehlo.select %4641, %4649, %4648 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc5046)
      %4651 = stablehlo.broadcast_in_dim %4584, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5047)
      %4652 = stablehlo.reshape %4651 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5048)
      %4653 = stablehlo.convert %4652 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5049)
      %4654 = stablehlo.dot_general %4650, %4653, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5050)
      %4655 = stablehlo.convert %4654 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc5051)
      %4656 = stablehlo.transpose %4655, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5052)
      %4657 = stablehlo.reshape %4656 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc5053)
      %4658 = stablehlo.reshape %arg1189 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc5054)
      %4659 = stablehlo.reshape %4658 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc5055)
      %4660 = stablehlo.transpose %4659, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc5056)
      %4661 = stablehlo.dot_general %4657, %4660, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5057)
      %4662 = "stablehlo.all_reduce"(%4661) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.10678"), %arg1677: tensor<bf16> loc("dot.10678")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5057)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5057)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5057)
      %4663 = stablehlo.reshape %4662 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5058)
      %4664 = stablehlo.add %4532, %4663 : tensor<32x17x5120xbf16> loc(#loc5059)
      %4665 = stablehlo.reshape %arg1192 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5060)
      %4666 = stablehlo.reshape %4665 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5061)
      %4667 = stablehlo.broadcast_in_dim %4666, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5062)
      %4668 = stablehlo.convert %4664 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5063)
      %4669 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4670 = stablehlo.power %4668, %4669 : tensor<32x17x5120xf32> loc(#loc5064)
      %4671 = stablehlo.reduce(%4670 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5065)
      %4672 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4673 = stablehlo.multiply %4671, %4672 : tensor<32x17xf32> loc(#loc5066)
      %4674 = stablehlo.reshape %4673 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5067)
      %4675 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4676 = stablehlo.add %4674, %4675 : tensor<32x17x1xf32> loc(#loc5068)
      %4677 = stablehlo.rsqrt %4676 : tensor<32x17x1xf32> loc(#loc5069)
      %4678 = stablehlo.reshape %4677 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5070)
      %4679 = stablehlo.broadcast_in_dim %4678, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5071)
      %4680 = stablehlo.multiply %4668, %4679 : tensor<32x17x5120xf32> loc(#loc5072)
      %4681 = stablehlo.convert %4680 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5073)
      %4682 = stablehlo.multiply %4667, %4681 : tensor<32x17x5120xbf16> loc(#loc5074)
      %4683 = stablehlo.reshape %4682 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5075)
      %4684 = stablehlo.reshape %arg1193 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5076)
      %4685 = stablehlo.reshape %4684 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5077)
      %4686 = stablehlo.transpose %4685, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5078)
      %4687 = stablehlo.dot_general %4683, %4686, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5079)
      %4688 = stablehlo.reshape %4687 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5080)
      %4689 = stablehlo.logistic %4688 : tensor<32x17x3200xbf16> loc(#loc5081)
      %4690 = stablehlo.multiply %4688, %4689 : tensor<32x17x3200xbf16> loc(#loc5082)
      %4691 = stablehlo.reshape %arg1188 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5083)
      %4692 = stablehlo.reshape %4691 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5084)
      %4693 = stablehlo.transpose %4692, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5085)
      %4694 = stablehlo.dot_general %4683, %4693, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5086)
      %4695 = stablehlo.reshape %4694 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5087)
      %4696 = stablehlo.multiply %4690, %4695 : tensor<32x17x3200xbf16> loc(#loc5088)
      %4697 = stablehlo.reshape %4696 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5089)
      %4698 = stablehlo.reshape %arg1187 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc5090)
      %4699 = stablehlo.reshape %4698 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc5091)
      %4700 = stablehlo.transpose %4699, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc5092)
      %4701 = stablehlo.dot_general %4697, %4700, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5093)
      %4702 = "stablehlo.all_reduce"(%4701) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.10733"), %arg1677: tensor<bf16> loc("dot.10733")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5093)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5093)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5093)
      %4703 = stablehlo.reshape %4702 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5094)
      %4704 = stablehlo.add %4664, %4703 : tensor<32x17x5120xbf16> loc(#loc5095)
      %4705 = stablehlo.convert %4704 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5096)
      %4706 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4707 = stablehlo.power %4705, %4706 : tensor<32x17x5120xf32> loc(#loc5097)
      %4708 = stablehlo.reduce(%4707 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5098)
      %4709 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4710 = stablehlo.multiply %4708, %4709 : tensor<32x17xf32> loc(#loc5099)
      %4711 = stablehlo.reshape %4710 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5100)
      %4712 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4713 = stablehlo.add %4711, %4712 : tensor<32x17x1xf32> loc(#loc5101)
      %4714 = stablehlo.rsqrt %4713 : tensor<32x17x1xf32> loc(#loc5102)
      %4715 = stablehlo.reshape %4714 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5103)
      %4716 = stablehlo.broadcast_in_dim %4715, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5104)
      %4717 = stablehlo.multiply %4705, %4716 : tensor<32x17x5120xf32> loc(#loc5105)
      %4718 = stablehlo.convert %4717 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5106)
      %4719 = stablehlo.multiply %4590, %4718 : tensor<32x17x5120xbf16> loc(#loc5107)
      %4720 = stablehlo.reshape %4719 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5108)
      %4721 = stablehlo.reshape %arg1186 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5109)
      %4722 = stablehlo.reshape %4721 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5110)
      %4723 = stablehlo.transpose %4722, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5111)
      %4724 = stablehlo.dot_general %4720, %4723, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5112)
      %4725 = stablehlo.reshape %4724 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5113)
      %4726 = stablehlo.convert %4725 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc5114)
      %4727 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %4728 = stablehlo.power %4726, %4727 : tensor<32x17x1x128xf32> loc(#loc5115)
      %4729 = stablehlo.reduce(%4728 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc5116)
      %4730 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4731 = stablehlo.multiply %4729, %4730 : tensor<32x17x1xf32> loc(#loc5117)
      %4732 = stablehlo.reshape %4731 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc5118)
      %4733 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %4734 = stablehlo.add %4732, %4733 : tensor<32x17x1x1xf32> loc(#loc5119)
      %4735 = stablehlo.rsqrt %4734 : tensor<32x17x1x1xf32> loc(#loc5120)
      %4736 = stablehlo.reshape %4735 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc5121)
      %4737 = stablehlo.broadcast_in_dim %4736, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc5122)
      %4738 = stablehlo.multiply %4726, %4737 : tensor<32x17x1x128xf32> loc(#loc5123)
      %4739 = stablehlo.convert %4738 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc5124)
      %4740 = stablehlo.multiply %4587, %4739 : tensor<32x17x1x128xbf16> loc(#loc5125)
      %4741 = stablehlo.transpose %4740, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5126)
      %4742 = stablehlo.multiply %4741, %82 : tensor<32x1x17x128xbf16> loc(#loc5127)
      %4743 = stablehlo.slice %4741 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5128)
      %4744 = stablehlo.negate %4743 : tensor<32x1x17x64xbf16> loc(#loc5129)
      %4745 = stablehlo.slice %4741 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5130)
      %4746 = stablehlo.concatenate %4744, %4745, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5131)
      %4747 = stablehlo.multiply %4746, %91 : tensor<32x1x17x128xbf16> loc(#loc5132)
      %4748 = stablehlo.add %4742, %4747 : tensor<32x1x17x128xbf16> loc(#loc5133)
      %4749 = "stablehlo.scatter"(%arg1196, %21, %4748) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.10845"), %arg1677: tensor<bf16> loc("scatter.10845")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5134)
      %4750 = stablehlo.reshape %arg1197 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5135)
      %4751 = stablehlo.reshape %4750 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5136)
      %4752 = stablehlo.transpose %4751, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5137)
      %4753 = stablehlo.dot_general %4720, %4752, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5138)
      %4754 = stablehlo.reshape %4753 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5139)
      %4755 = stablehlo.transpose %4754, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5140)
      %4756 = "stablehlo.scatter"(%arg1198, %21, %4755) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.10875"), %arg1677: tensor<bf16> loc("scatter.10875")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5141)
      %4757 = stablehlo.reshape %arg1208 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5142)
      %4758 = stablehlo.reshape %4757 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5143)
      %4759 = stablehlo.broadcast_in_dim %4758, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5144)
      %4760 = stablehlo.reshape %arg1207 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5145)
      %4761 = stablehlo.reshape %4760 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5146)
      %4762 = stablehlo.broadcast_in_dim %4761, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5147)
      %4763 = stablehlo.reshape %arg1204 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5148)
      %4764 = stablehlo.reshape %4763 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5149)
      %4765 = stablehlo.broadcast_in_dim %4764, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5150)
      %4766 = stablehlo.reshape %arg1203 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc5151)
      %4767 = stablehlo.reshape %4766 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc5152)
      %4768 = stablehlo.transpose %4767, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc5153)
      %4769 = stablehlo.dot_general %4720, %4768, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc5154)
      %4770 = stablehlo.reshape %4769 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5155)
      %4771 = stablehlo.convert %4770 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc5156)
      %4772 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %4773 = stablehlo.power %4771, %4772 : tensor<32x17x8x128xf32> loc(#loc5157)
      %4774 = stablehlo.reduce(%4773 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc5158)
      %4775 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %4776 = stablehlo.multiply %4774, %4775 : tensor<32x17x8xf32> loc(#loc5159)
      %4777 = stablehlo.reshape %4776 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc5160)
      %4778 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %4779 = stablehlo.add %4777, %4778 : tensor<32x17x8x1xf32> loc(#loc5161)
      %4780 = stablehlo.rsqrt %4779 : tensor<32x17x8x1xf32> loc(#loc5162)
      %4781 = stablehlo.reshape %4780 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc5163)
      %4782 = stablehlo.broadcast_in_dim %4781, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc5164)
      %4783 = stablehlo.multiply %4771, %4782 : tensor<32x17x8x128xf32> loc(#loc5165)
      %4784 = stablehlo.convert %4783 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc5166)
      %4785 = stablehlo.multiply %4765, %4784 : tensor<32x17x8x128xbf16> loc(#loc5167)
      %4786 = stablehlo.transpose %4785, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5168)
      %4787 = stablehlo.multiply %4786, %132 : tensor<32x8x17x128xbf16> loc(#loc5169)
      %4788 = stablehlo.slice %4786 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5170)
      %4789 = stablehlo.negate %4788 : tensor<32x8x17x64xbf16> loc(#loc5171)
      %4790 = stablehlo.slice %4786 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5172)
      %4791 = stablehlo.concatenate %4789, %4790, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5173)
      %4792 = stablehlo.multiply %4791, %138 : tensor<32x8x17x128xbf16> loc(#loc5174)
      %4793 = stablehlo.add %4787, %4792 : tensor<32x8x17x128xbf16> loc(#loc5175)
      %4794 = stablehlo.convert %4793 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc5176)
      %4795 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4796 = stablehlo.multiply %4794, %4795 : tensor<32x8x17x128xf32> loc(#loc5177)
      %4797 = stablehlo.broadcast_in_dim %4749, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5178)
      %4798 = stablehlo.reshape %4797 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5179)
      %4799 = stablehlo.convert %4798 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5180)
      %4800 = stablehlo.transpose %4799, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc5181)
      %4801 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %4802 = stablehlo.multiply %4800, %4801 : tensor<32x8x128x128xf32> loc(#loc5182)
      %4803 = stablehlo.dot_general %4796, %4802, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5183)
      %4804 = stablehlo.add %4803, %159 : tensor<32x8x17x128xf32> loc(#loc5184)
      %4805 = stablehlo.convert %4804 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc5185)
      %4806 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %4807 = stablehlo.compare  EQ, %4805, %4806 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc5186)
      %4808 = stablehlo.not %4807 : tensor<32x8x17x128xi1> loc(#loc5187)
      %4809 = stablehlo.reduce(%4808 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.11056"), %arg1677: tensor<i1> loc("reduce.11056"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc5189)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc5190)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc5188)
      %4810 = stablehlo.reshape %4809 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc5191)
      %4811 = stablehlo.not %4810 : tensor<32x8x17x1xi1> loc(#loc5192)
      %4812 = stablehlo.reshape %4811 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc5193)
      %4813 = stablehlo.broadcast_in_dim %4812, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc5194)
      %4814 = stablehlo.reduce(%4804 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5195)
      %4815 = stablehlo.broadcast_in_dim %4814, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5196)
      %4816 = stablehlo.subtract %4804, %4815 : tensor<32x8x17x128xf32> loc(#loc5197)
      %4817 = stablehlo.exponential %4816 : tensor<32x8x17x128xf32> loc(#loc5198)
      %4818 = stablehlo.reduce(%4817 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5199)
      %4819 = stablehlo.broadcast_in_dim %4818, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5200)
      %4820 = stablehlo.divide %4817, %4819 : tensor<32x8x17x128xf32> loc(#loc5201)
      %4821 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4822 = stablehlo.select %4813, %4821, %4820 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc5202)
      %4823 = stablehlo.broadcast_in_dim %4756, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5203)
      %4824 = stablehlo.reshape %4823 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5204)
      %4825 = stablehlo.convert %4824 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5205)
      %4826 = stablehlo.dot_general %4822, %4825, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5206)
      %4827 = stablehlo.convert %4826 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc5207)
      %4828 = stablehlo.transpose %4827, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5208)
      %4829 = stablehlo.reshape %4828 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc5209)
      %4830 = stablehlo.reshape %arg1202 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc5210)
      %4831 = stablehlo.reshape %4830 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc5211)
      %4832 = stablehlo.transpose %4831, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc5212)
      %4833 = stablehlo.dot_general %4829, %4832, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5213)
      %4834 = "stablehlo.all_reduce"(%4833) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.11073"), %arg1677: tensor<bf16> loc("dot.11073")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5213)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5213)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5213)
      %4835 = stablehlo.reshape %4834 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5214)
      %4836 = stablehlo.add %4704, %4835 : tensor<32x17x5120xbf16> loc(#loc5215)
      %4837 = stablehlo.reshape %arg1205 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5216)
      %4838 = stablehlo.reshape %4837 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5217)
      %4839 = stablehlo.broadcast_in_dim %4838, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5218)
      %4840 = stablehlo.convert %4836 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5219)
      %4841 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4842 = stablehlo.power %4840, %4841 : tensor<32x17x5120xf32> loc(#loc5220)
      %4843 = stablehlo.reduce(%4842 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5221)
      %4844 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4845 = stablehlo.multiply %4843, %4844 : tensor<32x17xf32> loc(#loc5222)
      %4846 = stablehlo.reshape %4845 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5223)
      %4847 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4848 = stablehlo.add %4846, %4847 : tensor<32x17x1xf32> loc(#loc5224)
      %4849 = stablehlo.rsqrt %4848 : tensor<32x17x1xf32> loc(#loc5225)
      %4850 = stablehlo.reshape %4849 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5226)
      %4851 = stablehlo.broadcast_in_dim %4850, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5227)
      %4852 = stablehlo.multiply %4840, %4851 : tensor<32x17x5120xf32> loc(#loc5228)
      %4853 = stablehlo.convert %4852 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5229)
      %4854 = stablehlo.multiply %4839, %4853 : tensor<32x17x5120xbf16> loc(#loc5230)
      %4855 = stablehlo.reshape %4854 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5231)
      %4856 = stablehlo.reshape %arg1206 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5232)
      %4857 = stablehlo.reshape %4856 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5233)
      %4858 = stablehlo.transpose %4857, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5234)
      %4859 = stablehlo.dot_general %4855, %4858, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5235)
      %4860 = stablehlo.reshape %4859 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5236)
      %4861 = stablehlo.logistic %4860 : tensor<32x17x3200xbf16> loc(#loc5237)
      %4862 = stablehlo.multiply %4860, %4861 : tensor<32x17x3200xbf16> loc(#loc5238)
      %4863 = stablehlo.reshape %arg1201 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5239)
      %4864 = stablehlo.reshape %4863 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5240)
      %4865 = stablehlo.transpose %4864, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5241)
      %4866 = stablehlo.dot_general %4855, %4865, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5242)
      %4867 = stablehlo.reshape %4866 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5243)
      %4868 = stablehlo.multiply %4862, %4867 : tensor<32x17x3200xbf16> loc(#loc5244)
      %4869 = stablehlo.reshape %4868 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5245)
      %4870 = stablehlo.reshape %arg1200 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc5246)
      %4871 = stablehlo.reshape %4870 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc5247)
      %4872 = stablehlo.transpose %4871, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc5248)
      %4873 = stablehlo.dot_general %4869, %4872, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5249)
      %4874 = "stablehlo.all_reduce"(%4873) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.11128"), %arg1677: tensor<bf16> loc("dot.11128")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5249)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5249)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5249)
      %4875 = stablehlo.reshape %4874 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5250)
      %4876 = stablehlo.add %4836, %4875 : tensor<32x17x5120xbf16> loc(#loc5251)
      %4877 = stablehlo.convert %4876 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5252)
      %4878 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %4879 = stablehlo.power %4877, %4878 : tensor<32x17x5120xf32> loc(#loc5253)
      %4880 = stablehlo.reduce(%4879 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5254)
      %4881 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %4882 = stablehlo.multiply %4880, %4881 : tensor<32x17xf32> loc(#loc5255)
      %4883 = stablehlo.reshape %4882 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5256)
      %4884 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4885 = stablehlo.add %4883, %4884 : tensor<32x17x1xf32> loc(#loc5257)
      %4886 = stablehlo.rsqrt %4885 : tensor<32x17x1xf32> loc(#loc5258)
      %4887 = stablehlo.reshape %4886 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5259)
      %4888 = stablehlo.broadcast_in_dim %4887, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5260)
      %4889 = stablehlo.multiply %4877, %4888 : tensor<32x17x5120xf32> loc(#loc5261)
      %4890 = stablehlo.convert %4889 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5262)
      %4891 = stablehlo.multiply %4762, %4890 : tensor<32x17x5120xbf16> loc(#loc5263)
      %4892 = stablehlo.reshape %4891 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5264)
      %4893 = stablehlo.reshape %arg1199 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5265)
      %4894 = stablehlo.reshape %4893 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5266)
      %4895 = stablehlo.transpose %4894, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5267)
      %4896 = stablehlo.dot_general %4892, %4895, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5268)
      %4897 = stablehlo.reshape %4896 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5269)
      %4898 = stablehlo.convert %4897 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc5270)
      %4899 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %4900 = stablehlo.power %4898, %4899 : tensor<32x17x1x128xf32> loc(#loc5271)
      %4901 = stablehlo.reduce(%4900 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc5272)
      %4902 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %4903 = stablehlo.multiply %4901, %4902 : tensor<32x17x1xf32> loc(#loc5273)
      %4904 = stablehlo.reshape %4903 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc5274)
      %4905 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %4906 = stablehlo.add %4904, %4905 : tensor<32x17x1x1xf32> loc(#loc5275)
      %4907 = stablehlo.rsqrt %4906 : tensor<32x17x1x1xf32> loc(#loc5276)
      %4908 = stablehlo.reshape %4907 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc5277)
      %4909 = stablehlo.broadcast_in_dim %4908, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc5278)
      %4910 = stablehlo.multiply %4898, %4909 : tensor<32x17x1x128xf32> loc(#loc5279)
      %4911 = stablehlo.convert %4910 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc5280)
      %4912 = stablehlo.multiply %4759, %4911 : tensor<32x17x1x128xbf16> loc(#loc5281)
      %4913 = stablehlo.transpose %4912, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5282)
      %4914 = stablehlo.multiply %4913, %82 : tensor<32x1x17x128xbf16> loc(#loc5283)
      %4915 = stablehlo.slice %4913 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5284)
      %4916 = stablehlo.negate %4915 : tensor<32x1x17x64xbf16> loc(#loc5285)
      %4917 = stablehlo.slice %4913 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5286)
      %4918 = stablehlo.concatenate %4916, %4917, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5287)
      %4919 = stablehlo.multiply %4918, %91 : tensor<32x1x17x128xbf16> loc(#loc5288)
      %4920 = stablehlo.add %4914, %4919 : tensor<32x1x17x128xbf16> loc(#loc5289)
      %4921 = "stablehlo.scatter"(%arg1209, %21, %4920) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.11240"), %arg1677: tensor<bf16> loc("scatter.11240")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5290)
      %4922 = stablehlo.reshape %arg1210 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5291)
      %4923 = stablehlo.reshape %4922 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5292)
      %4924 = stablehlo.transpose %4923, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5293)
      %4925 = stablehlo.dot_general %4892, %4924, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5294)
      %4926 = stablehlo.reshape %4925 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5295)
      %4927 = stablehlo.transpose %4926, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5296)
      %4928 = "stablehlo.scatter"(%arg1211, %21, %4927) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.11270"), %arg1677: tensor<bf16> loc("scatter.11270")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5297)
      %4929 = stablehlo.reshape %arg1221 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5298)
      %4930 = stablehlo.reshape %4929 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5299)
      %4931 = stablehlo.broadcast_in_dim %4930, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5300)
      %4932 = stablehlo.reshape %arg1220 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5301)
      %4933 = stablehlo.reshape %4932 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5302)
      %4934 = stablehlo.broadcast_in_dim %4933, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5303)
      %4935 = stablehlo.reshape %arg1217 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5304)
      %4936 = stablehlo.reshape %4935 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5305)
      %4937 = stablehlo.broadcast_in_dim %4936, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5306)
      %4938 = stablehlo.reshape %arg1216 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc5307)
      %4939 = stablehlo.reshape %4938 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc5308)
      %4940 = stablehlo.transpose %4939, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc5309)
      %4941 = stablehlo.dot_general %4892, %4940, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc5310)
      %4942 = stablehlo.reshape %4941 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5311)
      %4943 = stablehlo.convert %4942 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc5312)
      %4944 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %4945 = stablehlo.power %4943, %4944 : tensor<32x17x8x128xf32> loc(#loc5313)
      %4946 = stablehlo.reduce(%4945 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc5314)
      %4947 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %4948 = stablehlo.multiply %4946, %4947 : tensor<32x17x8xf32> loc(#loc5315)
      %4949 = stablehlo.reshape %4948 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc5316)
      %4950 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %4951 = stablehlo.add %4949, %4950 : tensor<32x17x8x1xf32> loc(#loc5317)
      %4952 = stablehlo.rsqrt %4951 : tensor<32x17x8x1xf32> loc(#loc5318)
      %4953 = stablehlo.reshape %4952 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc5319)
      %4954 = stablehlo.broadcast_in_dim %4953, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc5320)
      %4955 = stablehlo.multiply %4943, %4954 : tensor<32x17x8x128xf32> loc(#loc5321)
      %4956 = stablehlo.convert %4955 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc5322)
      %4957 = stablehlo.multiply %4937, %4956 : tensor<32x17x8x128xbf16> loc(#loc5323)
      %4958 = stablehlo.transpose %4957, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5324)
      %4959 = stablehlo.multiply %4958, %132 : tensor<32x8x17x128xbf16> loc(#loc5325)
      %4960 = stablehlo.slice %4958 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5326)
      %4961 = stablehlo.negate %4960 : tensor<32x8x17x64xbf16> loc(#loc5327)
      %4962 = stablehlo.slice %4958 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5328)
      %4963 = stablehlo.concatenate %4961, %4962, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5329)
      %4964 = stablehlo.multiply %4963, %138 : tensor<32x8x17x128xbf16> loc(#loc5330)
      %4965 = stablehlo.add %4959, %4964 : tensor<32x8x17x128xbf16> loc(#loc5331)
      %4966 = stablehlo.convert %4965 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc5332)
      %4967 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4968 = stablehlo.multiply %4966, %4967 : tensor<32x8x17x128xf32> loc(#loc5333)
      %4969 = stablehlo.broadcast_in_dim %4921, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5334)
      %4970 = stablehlo.reshape %4969 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5335)
      %4971 = stablehlo.convert %4970 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5336)
      %4972 = stablehlo.transpose %4971, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc5337)
      %4973 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %4974 = stablehlo.multiply %4972, %4973 : tensor<32x8x128x128xf32> loc(#loc5338)
      %4975 = stablehlo.dot_general %4968, %4974, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5339)
      %4976 = stablehlo.add %4975, %159 : tensor<32x8x17x128xf32> loc(#loc5340)
      %4977 = stablehlo.convert %4976 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc5341)
      %4978 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %4979 = stablehlo.compare  EQ, %4977, %4978 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc5342)
      %4980 = stablehlo.not %4979 : tensor<32x8x17x128xi1> loc(#loc5343)
      %4981 = stablehlo.reduce(%4980 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.11451"), %arg1677: tensor<i1> loc("reduce.11451"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc5345)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc5346)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc5344)
      %4982 = stablehlo.reshape %4981 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc5347)
      %4983 = stablehlo.not %4982 : tensor<32x8x17x1xi1> loc(#loc5348)
      %4984 = stablehlo.reshape %4983 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc5349)
      %4985 = stablehlo.broadcast_in_dim %4984, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc5350)
      %4986 = stablehlo.reduce(%4976 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5351)
      %4987 = stablehlo.broadcast_in_dim %4986, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5352)
      %4988 = stablehlo.subtract %4976, %4987 : tensor<32x8x17x128xf32> loc(#loc5353)
      %4989 = stablehlo.exponential %4988 : tensor<32x8x17x128xf32> loc(#loc5354)
      %4990 = stablehlo.reduce(%4989 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5355)
      %4991 = stablehlo.broadcast_in_dim %4990, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5356)
      %4992 = stablehlo.divide %4989, %4991 : tensor<32x8x17x128xf32> loc(#loc5357)
      %4993 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %4994 = stablehlo.select %4985, %4993, %4992 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc5358)
      %4995 = stablehlo.broadcast_in_dim %4928, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5359)
      %4996 = stablehlo.reshape %4995 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5360)
      %4997 = stablehlo.convert %4996 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5361)
      %4998 = stablehlo.dot_general %4994, %4997, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5362)
      %4999 = stablehlo.convert %4998 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc5363)
      %5000 = stablehlo.transpose %4999, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5364)
      %5001 = stablehlo.reshape %5000 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc5365)
      %5002 = stablehlo.reshape %arg1215 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc5366)
      %5003 = stablehlo.reshape %5002 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc5367)
      %5004 = stablehlo.transpose %5003, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc5368)
      %5005 = stablehlo.dot_general %5001, %5004, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5369)
      %5006 = "stablehlo.all_reduce"(%5005) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.11468"), %arg1677: tensor<bf16> loc("dot.11468")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5369)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5369)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5369)
      %5007 = stablehlo.reshape %5006 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5370)
      %5008 = stablehlo.add %4876, %5007 : tensor<32x17x5120xbf16> loc(#loc5371)
      %5009 = stablehlo.reshape %arg1218 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5372)
      %5010 = stablehlo.reshape %5009 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5373)
      %5011 = stablehlo.broadcast_in_dim %5010, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5374)
      %5012 = stablehlo.convert %5008 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5375)
      %5013 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5014 = stablehlo.power %5012, %5013 : tensor<32x17x5120xf32> loc(#loc5376)
      %5015 = stablehlo.reduce(%5014 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5377)
      %5016 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5017 = stablehlo.multiply %5015, %5016 : tensor<32x17xf32> loc(#loc5378)
      %5018 = stablehlo.reshape %5017 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5379)
      %5019 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5020 = stablehlo.add %5018, %5019 : tensor<32x17x1xf32> loc(#loc5380)
      %5021 = stablehlo.rsqrt %5020 : tensor<32x17x1xf32> loc(#loc5381)
      %5022 = stablehlo.reshape %5021 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5382)
      %5023 = stablehlo.broadcast_in_dim %5022, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5383)
      %5024 = stablehlo.multiply %5012, %5023 : tensor<32x17x5120xf32> loc(#loc5384)
      %5025 = stablehlo.convert %5024 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5385)
      %5026 = stablehlo.multiply %5011, %5025 : tensor<32x17x5120xbf16> loc(#loc5386)
      %5027 = stablehlo.reshape %5026 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5387)
      %5028 = stablehlo.reshape %arg1219 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5388)
      %5029 = stablehlo.reshape %5028 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5389)
      %5030 = stablehlo.transpose %5029, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5390)
      %5031 = stablehlo.dot_general %5027, %5030, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5391)
      %5032 = stablehlo.reshape %5031 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5392)
      %5033 = stablehlo.logistic %5032 : tensor<32x17x3200xbf16> loc(#loc5393)
      %5034 = stablehlo.multiply %5032, %5033 : tensor<32x17x3200xbf16> loc(#loc5394)
      %5035 = stablehlo.reshape %arg1214 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5395)
      %5036 = stablehlo.reshape %5035 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5396)
      %5037 = stablehlo.transpose %5036, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5397)
      %5038 = stablehlo.dot_general %5027, %5037, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5398)
      %5039 = stablehlo.reshape %5038 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5399)
      %5040 = stablehlo.multiply %5034, %5039 : tensor<32x17x3200xbf16> loc(#loc5400)
      %5041 = stablehlo.reshape %5040 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5401)
      %5042 = stablehlo.reshape %arg1213 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc5402)
      %5043 = stablehlo.reshape %5042 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc5403)
      %5044 = stablehlo.transpose %5043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc5404)
      %5045 = stablehlo.dot_general %5041, %5044, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5405)
      %5046 = "stablehlo.all_reduce"(%5045) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.11523"), %arg1677: tensor<bf16> loc("dot.11523")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5405)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5405)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5405)
      %5047 = stablehlo.reshape %5046 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5406)
      %5048 = stablehlo.add %5008, %5047 : tensor<32x17x5120xbf16> loc(#loc5407)
      %5049 = stablehlo.convert %5048 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5408)
      %5050 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5051 = stablehlo.power %5049, %5050 : tensor<32x17x5120xf32> loc(#loc5409)
      %5052 = stablehlo.reduce(%5051 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5410)
      %5053 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5054 = stablehlo.multiply %5052, %5053 : tensor<32x17xf32> loc(#loc5411)
      %5055 = stablehlo.reshape %5054 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5412)
      %5056 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5057 = stablehlo.add %5055, %5056 : tensor<32x17x1xf32> loc(#loc5413)
      %5058 = stablehlo.rsqrt %5057 : tensor<32x17x1xf32> loc(#loc5414)
      %5059 = stablehlo.reshape %5058 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5415)
      %5060 = stablehlo.broadcast_in_dim %5059, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5416)
      %5061 = stablehlo.multiply %5049, %5060 : tensor<32x17x5120xf32> loc(#loc5417)
      %5062 = stablehlo.convert %5061 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5418)
      %5063 = stablehlo.multiply %4934, %5062 : tensor<32x17x5120xbf16> loc(#loc5419)
      %5064 = stablehlo.reshape %5063 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5420)
      %5065 = stablehlo.reshape %arg1212 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5421)
      %5066 = stablehlo.reshape %5065 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5422)
      %5067 = stablehlo.transpose %5066, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5423)
      %5068 = stablehlo.dot_general %5064, %5067, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5424)
      %5069 = stablehlo.reshape %5068 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5425)
      %5070 = stablehlo.convert %5069 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc5426)
      %5071 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %5072 = stablehlo.power %5070, %5071 : tensor<32x17x1x128xf32> loc(#loc5427)
      %5073 = stablehlo.reduce(%5072 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc5428)
      %5074 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5075 = stablehlo.multiply %5073, %5074 : tensor<32x17x1xf32> loc(#loc5429)
      %5076 = stablehlo.reshape %5075 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc5430)
      %5077 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %5078 = stablehlo.add %5076, %5077 : tensor<32x17x1x1xf32> loc(#loc5431)
      %5079 = stablehlo.rsqrt %5078 : tensor<32x17x1x1xf32> loc(#loc5432)
      %5080 = stablehlo.reshape %5079 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc5433)
      %5081 = stablehlo.broadcast_in_dim %5080, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc5434)
      %5082 = stablehlo.multiply %5070, %5081 : tensor<32x17x1x128xf32> loc(#loc5435)
      %5083 = stablehlo.convert %5082 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc5436)
      %5084 = stablehlo.multiply %4931, %5083 : tensor<32x17x1x128xbf16> loc(#loc5437)
      %5085 = stablehlo.transpose %5084, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5438)
      %5086 = stablehlo.multiply %5085, %82 : tensor<32x1x17x128xbf16> loc(#loc5439)
      %5087 = stablehlo.slice %5085 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5440)
      %5088 = stablehlo.negate %5087 : tensor<32x1x17x64xbf16> loc(#loc5441)
      %5089 = stablehlo.slice %5085 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5442)
      %5090 = stablehlo.concatenate %5088, %5089, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5443)
      %5091 = stablehlo.multiply %5090, %91 : tensor<32x1x17x128xbf16> loc(#loc5444)
      %5092 = stablehlo.add %5086, %5091 : tensor<32x1x17x128xbf16> loc(#loc5445)
      %5093 = "stablehlo.scatter"(%arg1222, %21, %5092) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.11635"), %arg1677: tensor<bf16> loc("scatter.11635")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5446)
      %5094 = stablehlo.reshape %arg1223 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5447)
      %5095 = stablehlo.reshape %5094 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5448)
      %5096 = stablehlo.transpose %5095, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5449)
      %5097 = stablehlo.dot_general %5064, %5096, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5450)
      %5098 = stablehlo.reshape %5097 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5451)
      %5099 = stablehlo.transpose %5098, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5452)
      %5100 = "stablehlo.scatter"(%arg1224, %21, %5099) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.11665"), %arg1677: tensor<bf16> loc("scatter.11665")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5453)
      %5101 = stablehlo.reshape %arg1234 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5454)
      %5102 = stablehlo.reshape %5101 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5455)
      %5103 = stablehlo.broadcast_in_dim %5102, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5456)
      %5104 = stablehlo.reshape %arg1233 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5457)
      %5105 = stablehlo.reshape %5104 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5458)
      %5106 = stablehlo.broadcast_in_dim %5105, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5459)
      %5107 = stablehlo.reshape %arg1230 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5460)
      %5108 = stablehlo.reshape %5107 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5461)
      %5109 = stablehlo.broadcast_in_dim %5108, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5462)
      %5110 = stablehlo.reshape %arg1229 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc5463)
      %5111 = stablehlo.reshape %5110 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc5464)
      %5112 = stablehlo.transpose %5111, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc5465)
      %5113 = stablehlo.dot_general %5064, %5112, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc5466)
      %5114 = stablehlo.reshape %5113 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5467)
      %5115 = stablehlo.convert %5114 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc5468)
      %5116 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %5117 = stablehlo.power %5115, %5116 : tensor<32x17x8x128xf32> loc(#loc5469)
      %5118 = stablehlo.reduce(%5117 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc5470)
      %5119 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %5120 = stablehlo.multiply %5118, %5119 : tensor<32x17x8xf32> loc(#loc5471)
      %5121 = stablehlo.reshape %5120 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc5472)
      %5122 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %5123 = stablehlo.add %5121, %5122 : tensor<32x17x8x1xf32> loc(#loc5473)
      %5124 = stablehlo.rsqrt %5123 : tensor<32x17x8x1xf32> loc(#loc5474)
      %5125 = stablehlo.reshape %5124 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc5475)
      %5126 = stablehlo.broadcast_in_dim %5125, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc5476)
      %5127 = stablehlo.multiply %5115, %5126 : tensor<32x17x8x128xf32> loc(#loc5477)
      %5128 = stablehlo.convert %5127 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc5478)
      %5129 = stablehlo.multiply %5109, %5128 : tensor<32x17x8x128xbf16> loc(#loc5479)
      %5130 = stablehlo.transpose %5129, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5480)
      %5131 = stablehlo.multiply %5130, %132 : tensor<32x8x17x128xbf16> loc(#loc5481)
      %5132 = stablehlo.slice %5130 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5482)
      %5133 = stablehlo.negate %5132 : tensor<32x8x17x64xbf16> loc(#loc5483)
      %5134 = stablehlo.slice %5130 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5484)
      %5135 = stablehlo.concatenate %5133, %5134, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5485)
      %5136 = stablehlo.multiply %5135, %138 : tensor<32x8x17x128xbf16> loc(#loc5486)
      %5137 = stablehlo.add %5131, %5136 : tensor<32x8x17x128xbf16> loc(#loc5487)
      %5138 = stablehlo.convert %5137 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc5488)
      %5139 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5140 = stablehlo.multiply %5138, %5139 : tensor<32x8x17x128xf32> loc(#loc5489)
      %5141 = stablehlo.broadcast_in_dim %5093, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5490)
      %5142 = stablehlo.reshape %5141 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5491)
      %5143 = stablehlo.convert %5142 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5492)
      %5144 = stablehlo.transpose %5143, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc5493)
      %5145 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %5146 = stablehlo.multiply %5144, %5145 : tensor<32x8x128x128xf32> loc(#loc5494)
      %5147 = stablehlo.dot_general %5140, %5146, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5495)
      %5148 = stablehlo.add %5147, %159 : tensor<32x8x17x128xf32> loc(#loc5496)
      %5149 = stablehlo.convert %5148 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc5497)
      %5150 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %5151 = stablehlo.compare  EQ, %5149, %5150 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc5498)
      %5152 = stablehlo.not %5151 : tensor<32x8x17x128xi1> loc(#loc5499)
      %5153 = stablehlo.reduce(%5152 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.11846"), %arg1677: tensor<i1> loc("reduce.11846"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc5501)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc5502)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc5500)
      %5154 = stablehlo.reshape %5153 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc5503)
      %5155 = stablehlo.not %5154 : tensor<32x8x17x1xi1> loc(#loc5504)
      %5156 = stablehlo.reshape %5155 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc5505)
      %5157 = stablehlo.broadcast_in_dim %5156, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc5506)
      %5158 = stablehlo.reduce(%5148 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5507)
      %5159 = stablehlo.broadcast_in_dim %5158, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5508)
      %5160 = stablehlo.subtract %5148, %5159 : tensor<32x8x17x128xf32> loc(#loc5509)
      %5161 = stablehlo.exponential %5160 : tensor<32x8x17x128xf32> loc(#loc5510)
      %5162 = stablehlo.reduce(%5161 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5511)
      %5163 = stablehlo.broadcast_in_dim %5162, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5512)
      %5164 = stablehlo.divide %5161, %5163 : tensor<32x8x17x128xf32> loc(#loc5513)
      %5165 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5166 = stablehlo.select %5157, %5165, %5164 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc5514)
      %5167 = stablehlo.broadcast_in_dim %5100, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5515)
      %5168 = stablehlo.reshape %5167 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5516)
      %5169 = stablehlo.convert %5168 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5517)
      %5170 = stablehlo.dot_general %5166, %5169, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5518)
      %5171 = stablehlo.convert %5170 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc5519)
      %5172 = stablehlo.transpose %5171, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5520)
      %5173 = stablehlo.reshape %5172 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc5521)
      %5174 = stablehlo.reshape %arg1228 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc5522)
      %5175 = stablehlo.reshape %5174 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc5523)
      %5176 = stablehlo.transpose %5175, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc5524)
      %5177 = stablehlo.dot_general %5173, %5176, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5525)
      %5178 = "stablehlo.all_reduce"(%5177) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.11863"), %arg1677: tensor<bf16> loc("dot.11863")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5525)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5525)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5525)
      %5179 = stablehlo.reshape %5178 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5526)
      %5180 = stablehlo.add %5048, %5179 : tensor<32x17x5120xbf16> loc(#loc5527)
      %5181 = stablehlo.reshape %arg1231 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5528)
      %5182 = stablehlo.reshape %5181 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5529)
      %5183 = stablehlo.broadcast_in_dim %5182, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5530)
      %5184 = stablehlo.convert %5180 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5531)
      %5185 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5186 = stablehlo.power %5184, %5185 : tensor<32x17x5120xf32> loc(#loc5532)
      %5187 = stablehlo.reduce(%5186 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5533)
      %5188 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5189 = stablehlo.multiply %5187, %5188 : tensor<32x17xf32> loc(#loc5534)
      %5190 = stablehlo.reshape %5189 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5535)
      %5191 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5192 = stablehlo.add %5190, %5191 : tensor<32x17x1xf32> loc(#loc5536)
      %5193 = stablehlo.rsqrt %5192 : tensor<32x17x1xf32> loc(#loc5537)
      %5194 = stablehlo.reshape %5193 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5538)
      %5195 = stablehlo.broadcast_in_dim %5194, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5539)
      %5196 = stablehlo.multiply %5184, %5195 : tensor<32x17x5120xf32> loc(#loc5540)
      %5197 = stablehlo.convert %5196 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5541)
      %5198 = stablehlo.multiply %5183, %5197 : tensor<32x17x5120xbf16> loc(#loc5542)
      %5199 = stablehlo.reshape %5198 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5543)
      %5200 = stablehlo.reshape %arg1232 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5544)
      %5201 = stablehlo.reshape %5200 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5545)
      %5202 = stablehlo.transpose %5201, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5546)
      %5203 = stablehlo.dot_general %5199, %5202, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5547)
      %5204 = stablehlo.reshape %5203 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5548)
      %5205 = stablehlo.logistic %5204 : tensor<32x17x3200xbf16> loc(#loc5549)
      %5206 = stablehlo.multiply %5204, %5205 : tensor<32x17x3200xbf16> loc(#loc5550)
      %5207 = stablehlo.reshape %arg1227 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5551)
      %5208 = stablehlo.reshape %5207 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5552)
      %5209 = stablehlo.transpose %5208, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5553)
      %5210 = stablehlo.dot_general %5199, %5209, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5554)
      %5211 = stablehlo.reshape %5210 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5555)
      %5212 = stablehlo.multiply %5206, %5211 : tensor<32x17x3200xbf16> loc(#loc5556)
      %5213 = stablehlo.reshape %5212 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5557)
      %5214 = stablehlo.reshape %arg1226 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc5558)
      %5215 = stablehlo.reshape %5214 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc5559)
      %5216 = stablehlo.transpose %5215, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc5560)
      %5217 = stablehlo.dot_general %5213, %5216, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5561)
      %5218 = "stablehlo.all_reduce"(%5217) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.11918"), %arg1677: tensor<bf16> loc("dot.11918")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5561)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5561)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5561)
      %5219 = stablehlo.reshape %5218 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5562)
      %5220 = stablehlo.add %5180, %5219 : tensor<32x17x5120xbf16> loc(#loc5563)
      %5221 = stablehlo.convert %5220 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5564)
      %5222 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5223 = stablehlo.power %5221, %5222 : tensor<32x17x5120xf32> loc(#loc5565)
      %5224 = stablehlo.reduce(%5223 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5566)
      %5225 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5226 = stablehlo.multiply %5224, %5225 : tensor<32x17xf32> loc(#loc5567)
      %5227 = stablehlo.reshape %5226 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5568)
      %5228 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5229 = stablehlo.add %5227, %5228 : tensor<32x17x1xf32> loc(#loc5569)
      %5230 = stablehlo.rsqrt %5229 : tensor<32x17x1xf32> loc(#loc5570)
      %5231 = stablehlo.reshape %5230 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5571)
      %5232 = stablehlo.broadcast_in_dim %5231, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5572)
      %5233 = stablehlo.multiply %5221, %5232 : tensor<32x17x5120xf32> loc(#loc5573)
      %5234 = stablehlo.convert %5233 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5574)
      %5235 = stablehlo.multiply %5106, %5234 : tensor<32x17x5120xbf16> loc(#loc5575)
      %5236 = stablehlo.reshape %5235 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5576)
      %5237 = stablehlo.reshape %arg1225 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5577)
      %5238 = stablehlo.reshape %5237 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5578)
      %5239 = stablehlo.transpose %5238, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5579)
      %5240 = stablehlo.dot_general %5236, %5239, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5580)
      %5241 = stablehlo.reshape %5240 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5581)
      %5242 = stablehlo.convert %5241 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc5582)
      %5243 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %5244 = stablehlo.power %5242, %5243 : tensor<32x17x1x128xf32> loc(#loc5583)
      %5245 = stablehlo.reduce(%5244 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc5584)
      %5246 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5247 = stablehlo.multiply %5245, %5246 : tensor<32x17x1xf32> loc(#loc5585)
      %5248 = stablehlo.reshape %5247 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc5586)
      %5249 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %5250 = stablehlo.add %5248, %5249 : tensor<32x17x1x1xf32> loc(#loc5587)
      %5251 = stablehlo.rsqrt %5250 : tensor<32x17x1x1xf32> loc(#loc5588)
      %5252 = stablehlo.reshape %5251 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc5589)
      %5253 = stablehlo.broadcast_in_dim %5252, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc5590)
      %5254 = stablehlo.multiply %5242, %5253 : tensor<32x17x1x128xf32> loc(#loc5591)
      %5255 = stablehlo.convert %5254 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc5592)
      %5256 = stablehlo.multiply %5103, %5255 : tensor<32x17x1x128xbf16> loc(#loc5593)
      %5257 = stablehlo.transpose %5256, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5594)
      %5258 = stablehlo.multiply %5257, %82 : tensor<32x1x17x128xbf16> loc(#loc5595)
      %5259 = stablehlo.slice %5257 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5596)
      %5260 = stablehlo.negate %5259 : tensor<32x1x17x64xbf16> loc(#loc5597)
      %5261 = stablehlo.slice %5257 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5598)
      %5262 = stablehlo.concatenate %5260, %5261, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5599)
      %5263 = stablehlo.multiply %5262, %91 : tensor<32x1x17x128xbf16> loc(#loc5600)
      %5264 = stablehlo.add %5258, %5263 : tensor<32x1x17x128xbf16> loc(#loc5601)
      %5265 = "stablehlo.scatter"(%arg1235, %21, %5264) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.12030"), %arg1677: tensor<bf16> loc("scatter.12030")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5602)
      %5266 = stablehlo.reshape %arg1236 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5603)
      %5267 = stablehlo.reshape %5266 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5604)
      %5268 = stablehlo.transpose %5267, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5605)
      %5269 = stablehlo.dot_general %5236, %5268, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5606)
      %5270 = stablehlo.reshape %5269 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5607)
      %5271 = stablehlo.transpose %5270, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5608)
      %5272 = "stablehlo.scatter"(%arg1237, %21, %5271) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.12060"), %arg1677: tensor<bf16> loc("scatter.12060")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5609)
      %5273 = stablehlo.reshape %arg1247 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5610)
      %5274 = stablehlo.reshape %5273 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5611)
      %5275 = stablehlo.broadcast_in_dim %5274, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5612)
      %5276 = stablehlo.reshape %arg1246 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5613)
      %5277 = stablehlo.reshape %5276 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5614)
      %5278 = stablehlo.broadcast_in_dim %5277, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5615)
      %5279 = stablehlo.reshape %arg1243 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5616)
      %5280 = stablehlo.reshape %5279 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5617)
      %5281 = stablehlo.broadcast_in_dim %5280, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5618)
      %5282 = stablehlo.reshape %arg1242 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc5619)
      %5283 = stablehlo.reshape %5282 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc5620)
      %5284 = stablehlo.transpose %5283, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc5621)
      %5285 = stablehlo.dot_general %5236, %5284, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc5622)
      %5286 = stablehlo.reshape %5285 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5623)
      %5287 = stablehlo.convert %5286 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc5624)
      %5288 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %5289 = stablehlo.power %5287, %5288 : tensor<32x17x8x128xf32> loc(#loc5625)
      %5290 = stablehlo.reduce(%5289 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc5626)
      %5291 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %5292 = stablehlo.multiply %5290, %5291 : tensor<32x17x8xf32> loc(#loc5627)
      %5293 = stablehlo.reshape %5292 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc5628)
      %5294 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %5295 = stablehlo.add %5293, %5294 : tensor<32x17x8x1xf32> loc(#loc5629)
      %5296 = stablehlo.rsqrt %5295 : tensor<32x17x8x1xf32> loc(#loc5630)
      %5297 = stablehlo.reshape %5296 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc5631)
      %5298 = stablehlo.broadcast_in_dim %5297, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc5632)
      %5299 = stablehlo.multiply %5287, %5298 : tensor<32x17x8x128xf32> loc(#loc5633)
      %5300 = stablehlo.convert %5299 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc5634)
      %5301 = stablehlo.multiply %5281, %5300 : tensor<32x17x8x128xbf16> loc(#loc5635)
      %5302 = stablehlo.transpose %5301, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5636)
      %5303 = stablehlo.multiply %5302, %132 : tensor<32x8x17x128xbf16> loc(#loc5637)
      %5304 = stablehlo.slice %5302 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5638)
      %5305 = stablehlo.negate %5304 : tensor<32x8x17x64xbf16> loc(#loc5639)
      %5306 = stablehlo.slice %5302 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5640)
      %5307 = stablehlo.concatenate %5305, %5306, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5641)
      %5308 = stablehlo.multiply %5307, %138 : tensor<32x8x17x128xbf16> loc(#loc5642)
      %5309 = stablehlo.add %5303, %5308 : tensor<32x8x17x128xbf16> loc(#loc5643)
      %5310 = stablehlo.convert %5309 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc5644)
      %5311 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5312 = stablehlo.multiply %5310, %5311 : tensor<32x8x17x128xf32> loc(#loc5645)
      %5313 = stablehlo.broadcast_in_dim %5265, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5646)
      %5314 = stablehlo.reshape %5313 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5647)
      %5315 = stablehlo.convert %5314 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5648)
      %5316 = stablehlo.transpose %5315, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc5649)
      %5317 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %5318 = stablehlo.multiply %5316, %5317 : tensor<32x8x128x128xf32> loc(#loc5650)
      %5319 = stablehlo.dot_general %5312, %5318, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5651)
      %5320 = stablehlo.add %5319, %159 : tensor<32x8x17x128xf32> loc(#loc5652)
      %5321 = stablehlo.convert %5320 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc5653)
      %5322 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %5323 = stablehlo.compare  EQ, %5321, %5322 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc5654)
      %5324 = stablehlo.not %5323 : tensor<32x8x17x128xi1> loc(#loc5655)
      %5325 = stablehlo.reduce(%5324 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.12241"), %arg1677: tensor<i1> loc("reduce.12241"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc5657)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc5658)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc5656)
      %5326 = stablehlo.reshape %5325 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc5659)
      %5327 = stablehlo.not %5326 : tensor<32x8x17x1xi1> loc(#loc5660)
      %5328 = stablehlo.reshape %5327 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc5661)
      %5329 = stablehlo.broadcast_in_dim %5328, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc5662)
      %5330 = stablehlo.reduce(%5320 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5663)
      %5331 = stablehlo.broadcast_in_dim %5330, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5664)
      %5332 = stablehlo.subtract %5320, %5331 : tensor<32x8x17x128xf32> loc(#loc5665)
      %5333 = stablehlo.exponential %5332 : tensor<32x8x17x128xf32> loc(#loc5666)
      %5334 = stablehlo.reduce(%5333 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5667)
      %5335 = stablehlo.broadcast_in_dim %5334, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5668)
      %5336 = stablehlo.divide %5333, %5335 : tensor<32x8x17x128xf32> loc(#loc5669)
      %5337 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5338 = stablehlo.select %5329, %5337, %5336 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc5670)
      %5339 = stablehlo.broadcast_in_dim %5272, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5671)
      %5340 = stablehlo.reshape %5339 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5672)
      %5341 = stablehlo.convert %5340 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5673)
      %5342 = stablehlo.dot_general %5338, %5341, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5674)
      %5343 = stablehlo.convert %5342 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc5675)
      %5344 = stablehlo.transpose %5343, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5676)
      %5345 = stablehlo.reshape %5344 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc5677)
      %5346 = stablehlo.reshape %arg1241 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc5678)
      %5347 = stablehlo.reshape %5346 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc5679)
      %5348 = stablehlo.transpose %5347, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc5680)
      %5349 = stablehlo.dot_general %5345, %5348, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5681)
      %5350 = "stablehlo.all_reduce"(%5349) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.12258"), %arg1677: tensor<bf16> loc("dot.12258")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5681)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5681)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5681)
      %5351 = stablehlo.reshape %5350 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5682)
      %5352 = stablehlo.add %5220, %5351 : tensor<32x17x5120xbf16> loc(#loc5683)
      %5353 = stablehlo.reshape %arg1244 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5684)
      %5354 = stablehlo.reshape %5353 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5685)
      %5355 = stablehlo.broadcast_in_dim %5354, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5686)
      %5356 = stablehlo.convert %5352 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5687)
      %5357 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5358 = stablehlo.power %5356, %5357 : tensor<32x17x5120xf32> loc(#loc5688)
      %5359 = stablehlo.reduce(%5358 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5689)
      %5360 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5361 = stablehlo.multiply %5359, %5360 : tensor<32x17xf32> loc(#loc5690)
      %5362 = stablehlo.reshape %5361 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5691)
      %5363 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5364 = stablehlo.add %5362, %5363 : tensor<32x17x1xf32> loc(#loc5692)
      %5365 = stablehlo.rsqrt %5364 : tensor<32x17x1xf32> loc(#loc5693)
      %5366 = stablehlo.reshape %5365 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5694)
      %5367 = stablehlo.broadcast_in_dim %5366, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5695)
      %5368 = stablehlo.multiply %5356, %5367 : tensor<32x17x5120xf32> loc(#loc5696)
      %5369 = stablehlo.convert %5368 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5697)
      %5370 = stablehlo.multiply %5355, %5369 : tensor<32x17x5120xbf16> loc(#loc5698)
      %5371 = stablehlo.reshape %5370 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5699)
      %5372 = stablehlo.reshape %arg1245 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5700)
      %5373 = stablehlo.reshape %5372 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5701)
      %5374 = stablehlo.transpose %5373, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5702)
      %5375 = stablehlo.dot_general %5371, %5374, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5703)
      %5376 = stablehlo.reshape %5375 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5704)
      %5377 = stablehlo.logistic %5376 : tensor<32x17x3200xbf16> loc(#loc5705)
      %5378 = stablehlo.multiply %5376, %5377 : tensor<32x17x3200xbf16> loc(#loc5706)
      %5379 = stablehlo.reshape %arg1240 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5707)
      %5380 = stablehlo.reshape %5379 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5708)
      %5381 = stablehlo.transpose %5380, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5709)
      %5382 = stablehlo.dot_general %5371, %5381, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5710)
      %5383 = stablehlo.reshape %5382 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5711)
      %5384 = stablehlo.multiply %5378, %5383 : tensor<32x17x3200xbf16> loc(#loc5712)
      %5385 = stablehlo.reshape %5384 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5713)
      %5386 = stablehlo.reshape %arg1239 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc5714)
      %5387 = stablehlo.reshape %5386 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc5715)
      %5388 = stablehlo.transpose %5387, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc5716)
      %5389 = stablehlo.dot_general %5385, %5388, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5717)
      %5390 = "stablehlo.all_reduce"(%5389) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.12313"), %arg1677: tensor<bf16> loc("dot.12313")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5717)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5717)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5717)
      %5391 = stablehlo.reshape %5390 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5718)
      %5392 = stablehlo.add %5352, %5391 : tensor<32x17x5120xbf16> loc(#loc5719)
      %5393 = stablehlo.convert %5392 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5720)
      %5394 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5395 = stablehlo.power %5393, %5394 : tensor<32x17x5120xf32> loc(#loc5721)
      %5396 = stablehlo.reduce(%5395 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5722)
      %5397 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5398 = stablehlo.multiply %5396, %5397 : tensor<32x17xf32> loc(#loc5723)
      %5399 = stablehlo.reshape %5398 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5724)
      %5400 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5401 = stablehlo.add %5399, %5400 : tensor<32x17x1xf32> loc(#loc5725)
      %5402 = stablehlo.rsqrt %5401 : tensor<32x17x1xf32> loc(#loc5726)
      %5403 = stablehlo.reshape %5402 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5727)
      %5404 = stablehlo.broadcast_in_dim %5403, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5728)
      %5405 = stablehlo.multiply %5393, %5404 : tensor<32x17x5120xf32> loc(#loc5729)
      %5406 = stablehlo.convert %5405 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5730)
      %5407 = stablehlo.multiply %5278, %5406 : tensor<32x17x5120xbf16> loc(#loc5731)
      %5408 = stablehlo.reshape %5407 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5732)
      %5409 = stablehlo.reshape %arg1238 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5733)
      %5410 = stablehlo.reshape %5409 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5734)
      %5411 = stablehlo.transpose %5410, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5735)
      %5412 = stablehlo.dot_general %5408, %5411, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5736)
      %5413 = stablehlo.reshape %5412 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5737)
      %5414 = stablehlo.convert %5413 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc5738)
      %5415 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %5416 = stablehlo.power %5414, %5415 : tensor<32x17x1x128xf32> loc(#loc5739)
      %5417 = stablehlo.reduce(%5416 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc5740)
      %5418 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5419 = stablehlo.multiply %5417, %5418 : tensor<32x17x1xf32> loc(#loc5741)
      %5420 = stablehlo.reshape %5419 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc5742)
      %5421 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %5422 = stablehlo.add %5420, %5421 : tensor<32x17x1x1xf32> loc(#loc5743)
      %5423 = stablehlo.rsqrt %5422 : tensor<32x17x1x1xf32> loc(#loc5744)
      %5424 = stablehlo.reshape %5423 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc5745)
      %5425 = stablehlo.broadcast_in_dim %5424, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc5746)
      %5426 = stablehlo.multiply %5414, %5425 : tensor<32x17x1x128xf32> loc(#loc5747)
      %5427 = stablehlo.convert %5426 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc5748)
      %5428 = stablehlo.multiply %5275, %5427 : tensor<32x17x1x128xbf16> loc(#loc5749)
      %5429 = stablehlo.transpose %5428, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5750)
      %5430 = stablehlo.multiply %5429, %82 : tensor<32x1x17x128xbf16> loc(#loc5751)
      %5431 = stablehlo.slice %5429 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5752)
      %5432 = stablehlo.negate %5431 : tensor<32x1x17x64xbf16> loc(#loc5753)
      %5433 = stablehlo.slice %5429 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5754)
      %5434 = stablehlo.concatenate %5432, %5433, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5755)
      %5435 = stablehlo.multiply %5434, %91 : tensor<32x1x17x128xbf16> loc(#loc5756)
      %5436 = stablehlo.add %5430, %5435 : tensor<32x1x17x128xbf16> loc(#loc5757)
      %5437 = "stablehlo.scatter"(%arg1248, %21, %5436) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.12425"), %arg1677: tensor<bf16> loc("scatter.12425")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5758)
      %5438 = stablehlo.reshape %arg1249 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5759)
      %5439 = stablehlo.reshape %5438 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5760)
      %5440 = stablehlo.transpose %5439, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5761)
      %5441 = stablehlo.dot_general %5408, %5440, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5762)
      %5442 = stablehlo.reshape %5441 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5763)
      %5443 = stablehlo.transpose %5442, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5764)
      %5444 = "stablehlo.scatter"(%arg1250, %21, %5443) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.12455"), %arg1677: tensor<bf16> loc("scatter.12455")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5765)
      %5445 = stablehlo.reshape %arg1260 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5766)
      %5446 = stablehlo.reshape %5445 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5767)
      %5447 = stablehlo.broadcast_in_dim %5446, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5768)
      %5448 = stablehlo.reshape %arg1259 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5769)
      %5449 = stablehlo.reshape %5448 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5770)
      %5450 = stablehlo.broadcast_in_dim %5449, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5771)
      %5451 = stablehlo.reshape %arg1256 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5772)
      %5452 = stablehlo.reshape %5451 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5773)
      %5453 = stablehlo.broadcast_in_dim %5452, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5774)
      %5454 = stablehlo.reshape %arg1255 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc5775)
      %5455 = stablehlo.reshape %5454 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc5776)
      %5456 = stablehlo.transpose %5455, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc5777)
      %5457 = stablehlo.dot_general %5408, %5456, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc5778)
      %5458 = stablehlo.reshape %5457 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5779)
      %5459 = stablehlo.convert %5458 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc5780)
      %5460 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %5461 = stablehlo.power %5459, %5460 : tensor<32x17x8x128xf32> loc(#loc5781)
      %5462 = stablehlo.reduce(%5461 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc5782)
      %5463 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %5464 = stablehlo.multiply %5462, %5463 : tensor<32x17x8xf32> loc(#loc5783)
      %5465 = stablehlo.reshape %5464 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc5784)
      %5466 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %5467 = stablehlo.add %5465, %5466 : tensor<32x17x8x1xf32> loc(#loc5785)
      %5468 = stablehlo.rsqrt %5467 : tensor<32x17x8x1xf32> loc(#loc5786)
      %5469 = stablehlo.reshape %5468 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc5787)
      %5470 = stablehlo.broadcast_in_dim %5469, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc5788)
      %5471 = stablehlo.multiply %5459, %5470 : tensor<32x17x8x128xf32> loc(#loc5789)
      %5472 = stablehlo.convert %5471 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc5790)
      %5473 = stablehlo.multiply %5453, %5472 : tensor<32x17x8x128xbf16> loc(#loc5791)
      %5474 = stablehlo.transpose %5473, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5792)
      %5475 = stablehlo.multiply %5474, %132 : tensor<32x8x17x128xbf16> loc(#loc5793)
      %5476 = stablehlo.slice %5474 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5794)
      %5477 = stablehlo.negate %5476 : tensor<32x8x17x64xbf16> loc(#loc5795)
      %5478 = stablehlo.slice %5474 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5796)
      %5479 = stablehlo.concatenate %5477, %5478, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5797)
      %5480 = stablehlo.multiply %5479, %138 : tensor<32x8x17x128xbf16> loc(#loc5798)
      %5481 = stablehlo.add %5475, %5480 : tensor<32x8x17x128xbf16> loc(#loc5799)
      %5482 = stablehlo.convert %5481 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc5800)
      %5483 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5484 = stablehlo.multiply %5482, %5483 : tensor<32x8x17x128xf32> loc(#loc5801)
      %5485 = stablehlo.broadcast_in_dim %5437, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5802)
      %5486 = stablehlo.reshape %5485 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5803)
      %5487 = stablehlo.convert %5486 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5804)
      %5488 = stablehlo.transpose %5487, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc5805)
      %5489 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %5490 = stablehlo.multiply %5488, %5489 : tensor<32x8x128x128xf32> loc(#loc5806)
      %5491 = stablehlo.dot_general %5484, %5490, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5807)
      %5492 = stablehlo.add %5491, %159 : tensor<32x8x17x128xf32> loc(#loc5808)
      %5493 = stablehlo.convert %5492 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc5809)
      %5494 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %5495 = stablehlo.compare  EQ, %5493, %5494 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc5810)
      %5496 = stablehlo.not %5495 : tensor<32x8x17x128xi1> loc(#loc5811)
      %5497 = stablehlo.reduce(%5496 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.12636"), %arg1677: tensor<i1> loc("reduce.12636"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc5813)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc5814)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc5812)
      %5498 = stablehlo.reshape %5497 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc5815)
      %5499 = stablehlo.not %5498 : tensor<32x8x17x1xi1> loc(#loc5816)
      %5500 = stablehlo.reshape %5499 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc5817)
      %5501 = stablehlo.broadcast_in_dim %5500, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc5818)
      %5502 = stablehlo.reduce(%5492 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5819)
      %5503 = stablehlo.broadcast_in_dim %5502, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5820)
      %5504 = stablehlo.subtract %5492, %5503 : tensor<32x8x17x128xf32> loc(#loc5821)
      %5505 = stablehlo.exponential %5504 : tensor<32x8x17x128xf32> loc(#loc5822)
      %5506 = stablehlo.reduce(%5505 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5823)
      %5507 = stablehlo.broadcast_in_dim %5506, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5824)
      %5508 = stablehlo.divide %5505, %5507 : tensor<32x8x17x128xf32> loc(#loc5825)
      %5509 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5510 = stablehlo.select %5501, %5509, %5508 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc5826)
      %5511 = stablehlo.broadcast_in_dim %5444, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5827)
      %5512 = stablehlo.reshape %5511 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5828)
      %5513 = stablehlo.convert %5512 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5829)
      %5514 = stablehlo.dot_general %5510, %5513, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5830)
      %5515 = stablehlo.convert %5514 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc5831)
      %5516 = stablehlo.transpose %5515, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5832)
      %5517 = stablehlo.reshape %5516 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc5833)
      %5518 = stablehlo.reshape %arg1254 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc5834)
      %5519 = stablehlo.reshape %5518 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc5835)
      %5520 = stablehlo.transpose %5519, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc5836)
      %5521 = stablehlo.dot_general %5517, %5520, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5837)
      %5522 = "stablehlo.all_reduce"(%5521) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.12653"), %arg1677: tensor<bf16> loc("dot.12653")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5837)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5837)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5837)
      %5523 = stablehlo.reshape %5522 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5838)
      %5524 = stablehlo.add %5392, %5523 : tensor<32x17x5120xbf16> loc(#loc5839)
      %5525 = stablehlo.reshape %arg1257 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5840)
      %5526 = stablehlo.reshape %5525 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5841)
      %5527 = stablehlo.broadcast_in_dim %5526, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5842)
      %5528 = stablehlo.convert %5524 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5843)
      %5529 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5530 = stablehlo.power %5528, %5529 : tensor<32x17x5120xf32> loc(#loc5844)
      %5531 = stablehlo.reduce(%5530 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5845)
      %5532 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5533 = stablehlo.multiply %5531, %5532 : tensor<32x17xf32> loc(#loc5846)
      %5534 = stablehlo.reshape %5533 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5847)
      %5535 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5536 = stablehlo.add %5534, %5535 : tensor<32x17x1xf32> loc(#loc5848)
      %5537 = stablehlo.rsqrt %5536 : tensor<32x17x1xf32> loc(#loc5849)
      %5538 = stablehlo.reshape %5537 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5850)
      %5539 = stablehlo.broadcast_in_dim %5538, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5851)
      %5540 = stablehlo.multiply %5528, %5539 : tensor<32x17x5120xf32> loc(#loc5852)
      %5541 = stablehlo.convert %5540 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5853)
      %5542 = stablehlo.multiply %5527, %5541 : tensor<32x17x5120xbf16> loc(#loc5854)
      %5543 = stablehlo.reshape %5542 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5855)
      %5544 = stablehlo.reshape %arg1258 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5856)
      %5545 = stablehlo.reshape %5544 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5857)
      %5546 = stablehlo.transpose %5545, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5858)
      %5547 = stablehlo.dot_general %5543, %5546, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5859)
      %5548 = stablehlo.reshape %5547 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5860)
      %5549 = stablehlo.logistic %5548 : tensor<32x17x3200xbf16> loc(#loc5861)
      %5550 = stablehlo.multiply %5548, %5549 : tensor<32x17x3200xbf16> loc(#loc5862)
      %5551 = stablehlo.reshape %arg1253 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc5863)
      %5552 = stablehlo.reshape %5551 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc5864)
      %5553 = stablehlo.transpose %5552, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc5865)
      %5554 = stablehlo.dot_general %5543, %5553, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5866)
      %5555 = stablehlo.reshape %5554 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc5867)
      %5556 = stablehlo.multiply %5550, %5555 : tensor<32x17x3200xbf16> loc(#loc5868)
      %5557 = stablehlo.reshape %5556 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc5869)
      %5558 = stablehlo.reshape %arg1252 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc5870)
      %5559 = stablehlo.reshape %5558 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc5871)
      %5560 = stablehlo.transpose %5559, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc5872)
      %5561 = stablehlo.dot_general %5557, %5560, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5873)
      %5562 = "stablehlo.all_reduce"(%5561) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.12708"), %arg1677: tensor<bf16> loc("dot.12708")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5873)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5873)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5873)
      %5563 = stablehlo.reshape %5562 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5874)
      %5564 = stablehlo.add %5524, %5563 : tensor<32x17x5120xbf16> loc(#loc5875)
      %5565 = stablehlo.convert %5564 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5876)
      %5566 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5567 = stablehlo.power %5565, %5566 : tensor<32x17x5120xf32> loc(#loc5877)
      %5568 = stablehlo.reduce(%5567 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc5878)
      %5569 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5570 = stablehlo.multiply %5568, %5569 : tensor<32x17xf32> loc(#loc5879)
      %5571 = stablehlo.reshape %5570 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc5880)
      %5572 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5573 = stablehlo.add %5571, %5572 : tensor<32x17x1xf32> loc(#loc5881)
      %5574 = stablehlo.rsqrt %5573 : tensor<32x17x1xf32> loc(#loc5882)
      %5575 = stablehlo.reshape %5574 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc5883)
      %5576 = stablehlo.broadcast_in_dim %5575, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc5884)
      %5577 = stablehlo.multiply %5565, %5576 : tensor<32x17x5120xf32> loc(#loc5885)
      %5578 = stablehlo.convert %5577 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc5886)
      %5579 = stablehlo.multiply %5450, %5578 : tensor<32x17x5120xbf16> loc(#loc5887)
      %5580 = stablehlo.reshape %5579 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5888)
      %5581 = stablehlo.reshape %arg1251 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5889)
      %5582 = stablehlo.reshape %5581 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5890)
      %5583 = stablehlo.transpose %5582, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5891)
      %5584 = stablehlo.dot_general %5580, %5583, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5892)
      %5585 = stablehlo.reshape %5584 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5893)
      %5586 = stablehlo.convert %5585 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc5894)
      %5587 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %5588 = stablehlo.power %5586, %5587 : tensor<32x17x1x128xf32> loc(#loc5895)
      %5589 = stablehlo.reduce(%5588 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc5896)
      %5590 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5591 = stablehlo.multiply %5589, %5590 : tensor<32x17x1xf32> loc(#loc5897)
      %5592 = stablehlo.reshape %5591 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc5898)
      %5593 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %5594 = stablehlo.add %5592, %5593 : tensor<32x17x1x1xf32> loc(#loc5899)
      %5595 = stablehlo.rsqrt %5594 : tensor<32x17x1x1xf32> loc(#loc5900)
      %5596 = stablehlo.reshape %5595 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc5901)
      %5597 = stablehlo.broadcast_in_dim %5596, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc5902)
      %5598 = stablehlo.multiply %5586, %5597 : tensor<32x17x1x128xf32> loc(#loc5903)
      %5599 = stablehlo.convert %5598 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc5904)
      %5600 = stablehlo.multiply %5447, %5599 : tensor<32x17x1x128xbf16> loc(#loc5905)
      %5601 = stablehlo.transpose %5600, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5906)
      %5602 = stablehlo.multiply %5601, %82 : tensor<32x1x17x128xbf16> loc(#loc5907)
      %5603 = stablehlo.slice %5601 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5908)
      %5604 = stablehlo.negate %5603 : tensor<32x1x17x64xbf16> loc(#loc5909)
      %5605 = stablehlo.slice %5601 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc5910)
      %5606 = stablehlo.concatenate %5604, %5605, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5911)
      %5607 = stablehlo.multiply %5606, %91 : tensor<32x1x17x128xbf16> loc(#loc5912)
      %5608 = stablehlo.add %5602, %5607 : tensor<32x1x17x128xbf16> loc(#loc5913)
      %5609 = "stablehlo.scatter"(%arg1261, %21, %5608) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.12820"), %arg1677: tensor<bf16> loc("scatter.12820")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5914)
      %5610 = stablehlo.reshape %arg1262 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc5915)
      %5611 = stablehlo.reshape %5610 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc5916)
      %5612 = stablehlo.transpose %5611, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc5917)
      %5613 = stablehlo.dot_general %5580, %5612, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc5918)
      %5614 = stablehlo.reshape %5613 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5919)
      %5615 = stablehlo.transpose %5614, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc5920)
      %5616 = "stablehlo.scatter"(%arg1263, %21, %5615) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.12850"), %arg1677: tensor<bf16> loc("scatter.12850")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc5921)
      %5617 = stablehlo.reshape %arg1273 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5922)
      %5618 = stablehlo.reshape %5617 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5923)
      %5619 = stablehlo.broadcast_in_dim %5618, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc5924)
      %5620 = stablehlo.reshape %arg1272 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5925)
      %5621 = stablehlo.reshape %5620 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5926)
      %5622 = stablehlo.broadcast_in_dim %5621, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5927)
      %5623 = stablehlo.reshape %arg1269 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc5928)
      %5624 = stablehlo.reshape %5623 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc5929)
      %5625 = stablehlo.broadcast_in_dim %5624, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5930)
      %5626 = stablehlo.reshape %arg1268 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc5931)
      %5627 = stablehlo.reshape %5626 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc5932)
      %5628 = stablehlo.transpose %5627, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc5933)
      %5629 = stablehlo.dot_general %5580, %5628, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc5934)
      %5630 = stablehlo.reshape %5629 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5935)
      %5631 = stablehlo.convert %5630 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc5936)
      %5632 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %5633 = stablehlo.power %5631, %5632 : tensor<32x17x8x128xf32> loc(#loc5937)
      %5634 = stablehlo.reduce(%5633 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc5938)
      %5635 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %5636 = stablehlo.multiply %5634, %5635 : tensor<32x17x8xf32> loc(#loc5939)
      %5637 = stablehlo.reshape %5636 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc5940)
      %5638 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %5639 = stablehlo.add %5637, %5638 : tensor<32x17x8x1xf32> loc(#loc5941)
      %5640 = stablehlo.rsqrt %5639 : tensor<32x17x8x1xf32> loc(#loc5942)
      %5641 = stablehlo.reshape %5640 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc5943)
      %5642 = stablehlo.broadcast_in_dim %5641, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc5944)
      %5643 = stablehlo.multiply %5631, %5642 : tensor<32x17x8x128xf32> loc(#loc5945)
      %5644 = stablehlo.convert %5643 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc5946)
      %5645 = stablehlo.multiply %5625, %5644 : tensor<32x17x8x128xbf16> loc(#loc5947)
      %5646 = stablehlo.transpose %5645, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5948)
      %5647 = stablehlo.multiply %5646, %132 : tensor<32x8x17x128xbf16> loc(#loc5949)
      %5648 = stablehlo.slice %5646 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5950)
      %5649 = stablehlo.negate %5648 : tensor<32x8x17x64xbf16> loc(#loc5951)
      %5650 = stablehlo.slice %5646 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc5952)
      %5651 = stablehlo.concatenate %5649, %5650, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc5953)
      %5652 = stablehlo.multiply %5651, %138 : tensor<32x8x17x128xbf16> loc(#loc5954)
      %5653 = stablehlo.add %5647, %5652 : tensor<32x8x17x128xbf16> loc(#loc5955)
      %5654 = stablehlo.convert %5653 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc5956)
      %5655 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5656 = stablehlo.multiply %5654, %5655 : tensor<32x8x17x128xf32> loc(#loc5957)
      %5657 = stablehlo.broadcast_in_dim %5609, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5958)
      %5658 = stablehlo.reshape %5657 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5959)
      %5659 = stablehlo.convert %5658 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5960)
      %5660 = stablehlo.transpose %5659, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc5961)
      %5661 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %5662 = stablehlo.multiply %5660, %5661 : tensor<32x8x128x128xf32> loc(#loc5962)
      %5663 = stablehlo.dot_general %5656, %5662, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5963)
      %5664 = stablehlo.add %5663, %159 : tensor<32x8x17x128xf32> loc(#loc5964)
      %5665 = stablehlo.convert %5664 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc5965)
      %5666 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %5667 = stablehlo.compare  EQ, %5665, %5666 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc5966)
      %5668 = stablehlo.not %5667 : tensor<32x8x17x128xi1> loc(#loc5967)
      %5669 = stablehlo.reduce(%5668 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.13031"), %arg1677: tensor<i1> loc("reduce.13031"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc5969)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc5970)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc5968)
      %5670 = stablehlo.reshape %5669 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc5971)
      %5671 = stablehlo.not %5670 : tensor<32x8x17x1xi1> loc(#loc5972)
      %5672 = stablehlo.reshape %5671 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc5973)
      %5673 = stablehlo.broadcast_in_dim %5672, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc5974)
      %5674 = stablehlo.reduce(%5664 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5975)
      %5675 = stablehlo.broadcast_in_dim %5674, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5976)
      %5676 = stablehlo.subtract %5664, %5675 : tensor<32x8x17x128xf32> loc(#loc5977)
      %5677 = stablehlo.exponential %5676 : tensor<32x8x17x128xf32> loc(#loc5978)
      %5678 = stablehlo.reduce(%5677 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc5979)
      %5679 = stablehlo.broadcast_in_dim %5678, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc5980)
      %5680 = stablehlo.divide %5677, %5679 : tensor<32x8x17x128xf32> loc(#loc5981)
      %5681 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5682 = stablehlo.select %5673, %5681, %5680 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc5982)
      %5683 = stablehlo.broadcast_in_dim %5616, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc5983)
      %5684 = stablehlo.reshape %5683 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc5984)
      %5685 = stablehlo.convert %5684 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc5985)
      %5686 = stablehlo.dot_general %5682, %5685, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc5986)
      %5687 = stablehlo.convert %5686 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc5987)
      %5688 = stablehlo.transpose %5687, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc5988)
      %5689 = stablehlo.reshape %5688 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc5989)
      %5690 = stablehlo.reshape %arg1267 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc5990)
      %5691 = stablehlo.reshape %5690 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc5991)
      %5692 = stablehlo.transpose %5691, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc5992)
      %5693 = stablehlo.dot_general %5689, %5692, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5993)
      %5694 = "stablehlo.all_reduce"(%5693) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.13048"), %arg1677: tensor<bf16> loc("dot.13048")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc5993)
        stablehlo.return %11074 : tensor<bf16> loc(#loc5993)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc5993)
      %5695 = stablehlo.reshape %5694 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5994)
      %5696 = stablehlo.add %5564, %5695 : tensor<32x17x5120xbf16> loc(#loc5995)
      %5697 = stablehlo.reshape %arg1270 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc5996)
      %5698 = stablehlo.reshape %5697 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc5997)
      %5699 = stablehlo.broadcast_in_dim %5698, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc5998)
      %5700 = stablehlo.convert %5696 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc5999)
      %5701 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5702 = stablehlo.power %5700, %5701 : tensor<32x17x5120xf32> loc(#loc6000)
      %5703 = stablehlo.reduce(%5702 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6001)
      %5704 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5705 = stablehlo.multiply %5703, %5704 : tensor<32x17xf32> loc(#loc6002)
      %5706 = stablehlo.reshape %5705 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6003)
      %5707 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5708 = stablehlo.add %5706, %5707 : tensor<32x17x1xf32> loc(#loc6004)
      %5709 = stablehlo.rsqrt %5708 : tensor<32x17x1xf32> loc(#loc6005)
      %5710 = stablehlo.reshape %5709 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6006)
      %5711 = stablehlo.broadcast_in_dim %5710, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6007)
      %5712 = stablehlo.multiply %5700, %5711 : tensor<32x17x5120xf32> loc(#loc6008)
      %5713 = stablehlo.convert %5712 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6009)
      %5714 = stablehlo.multiply %5699, %5713 : tensor<32x17x5120xbf16> loc(#loc6010)
      %5715 = stablehlo.reshape %5714 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6011)
      %5716 = stablehlo.reshape %arg1271 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6012)
      %5717 = stablehlo.reshape %5716 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6013)
      %5718 = stablehlo.transpose %5717, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6014)
      %5719 = stablehlo.dot_general %5715, %5718, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6015)
      %5720 = stablehlo.reshape %5719 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6016)
      %5721 = stablehlo.logistic %5720 : tensor<32x17x3200xbf16> loc(#loc6017)
      %5722 = stablehlo.multiply %5720, %5721 : tensor<32x17x3200xbf16> loc(#loc6018)
      %5723 = stablehlo.reshape %arg1266 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6019)
      %5724 = stablehlo.reshape %5723 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6020)
      %5725 = stablehlo.transpose %5724, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6021)
      %5726 = stablehlo.dot_general %5715, %5725, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6022)
      %5727 = stablehlo.reshape %5726 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6023)
      %5728 = stablehlo.multiply %5722, %5727 : tensor<32x17x3200xbf16> loc(#loc6024)
      %5729 = stablehlo.reshape %5728 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6025)
      %5730 = stablehlo.reshape %arg1265 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc6026)
      %5731 = stablehlo.reshape %5730 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc6027)
      %5732 = stablehlo.transpose %5731, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc6028)
      %5733 = stablehlo.dot_general %5729, %5732, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6029)
      %5734 = "stablehlo.all_reduce"(%5733) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.13103"), %arg1677: tensor<bf16> loc("dot.13103")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6029)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6029)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6029)
      %5735 = stablehlo.reshape %5734 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6030)
      %5736 = stablehlo.add %5696, %5735 : tensor<32x17x5120xbf16> loc(#loc6031)
      %5737 = stablehlo.convert %5736 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6032)
      %5738 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5739 = stablehlo.power %5737, %5738 : tensor<32x17x5120xf32> loc(#loc6033)
      %5740 = stablehlo.reduce(%5739 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6034)
      %5741 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5742 = stablehlo.multiply %5740, %5741 : tensor<32x17xf32> loc(#loc6035)
      %5743 = stablehlo.reshape %5742 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6036)
      %5744 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5745 = stablehlo.add %5743, %5744 : tensor<32x17x1xf32> loc(#loc6037)
      %5746 = stablehlo.rsqrt %5745 : tensor<32x17x1xf32> loc(#loc6038)
      %5747 = stablehlo.reshape %5746 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6039)
      %5748 = stablehlo.broadcast_in_dim %5747, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6040)
      %5749 = stablehlo.multiply %5737, %5748 : tensor<32x17x5120xf32> loc(#loc6041)
      %5750 = stablehlo.convert %5749 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6042)
      %5751 = stablehlo.multiply %5622, %5750 : tensor<32x17x5120xbf16> loc(#loc6043)
      %5752 = stablehlo.reshape %5751 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6044)
      %5753 = stablehlo.reshape %arg1264 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6045)
      %5754 = stablehlo.reshape %5753 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6046)
      %5755 = stablehlo.transpose %5754, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6047)
      %5756 = stablehlo.dot_general %5752, %5755, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6048)
      %5757 = stablehlo.reshape %5756 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6049)
      %5758 = stablehlo.convert %5757 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc6050)
      %5759 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %5760 = stablehlo.power %5758, %5759 : tensor<32x17x1x128xf32> loc(#loc6051)
      %5761 = stablehlo.reduce(%5760 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc6052)
      %5762 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5763 = stablehlo.multiply %5761, %5762 : tensor<32x17x1xf32> loc(#loc6053)
      %5764 = stablehlo.reshape %5763 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc6054)
      %5765 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %5766 = stablehlo.add %5764, %5765 : tensor<32x17x1x1xf32> loc(#loc6055)
      %5767 = stablehlo.rsqrt %5766 : tensor<32x17x1x1xf32> loc(#loc6056)
      %5768 = stablehlo.reshape %5767 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc6057)
      %5769 = stablehlo.broadcast_in_dim %5768, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc6058)
      %5770 = stablehlo.multiply %5758, %5769 : tensor<32x17x1x128xf32> loc(#loc6059)
      %5771 = stablehlo.convert %5770 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc6060)
      %5772 = stablehlo.multiply %5619, %5771 : tensor<32x17x1x128xbf16> loc(#loc6061)
      %5773 = stablehlo.transpose %5772, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6062)
      %5774 = stablehlo.multiply %5773, %82 : tensor<32x1x17x128xbf16> loc(#loc6063)
      %5775 = stablehlo.slice %5773 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6064)
      %5776 = stablehlo.negate %5775 : tensor<32x1x17x64xbf16> loc(#loc6065)
      %5777 = stablehlo.slice %5773 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6066)
      %5778 = stablehlo.concatenate %5776, %5777, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6067)
      %5779 = stablehlo.multiply %5778, %91 : tensor<32x1x17x128xbf16> loc(#loc6068)
      %5780 = stablehlo.add %5774, %5779 : tensor<32x1x17x128xbf16> loc(#loc6069)
      %5781 = "stablehlo.scatter"(%arg1274, %21, %5780) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.13215"), %arg1677: tensor<bf16> loc("scatter.13215")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6070)
      %5782 = stablehlo.reshape %arg1275 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6071)
      %5783 = stablehlo.reshape %5782 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6072)
      %5784 = stablehlo.transpose %5783, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6073)
      %5785 = stablehlo.dot_general %5752, %5784, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6074)
      %5786 = stablehlo.reshape %5785 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6075)
      %5787 = stablehlo.transpose %5786, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6076)
      %5788 = "stablehlo.scatter"(%arg1276, %21, %5787) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.13245"), %arg1677: tensor<bf16> loc("scatter.13245")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6077)
      %5789 = stablehlo.reshape %arg1286 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6078)
      %5790 = stablehlo.reshape %5789 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6079)
      %5791 = stablehlo.broadcast_in_dim %5790, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6080)
      %5792 = stablehlo.reshape %arg1285 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6081)
      %5793 = stablehlo.reshape %5792 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6082)
      %5794 = stablehlo.broadcast_in_dim %5793, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6083)
      %5795 = stablehlo.reshape %arg1282 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6084)
      %5796 = stablehlo.reshape %5795 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6085)
      %5797 = stablehlo.broadcast_in_dim %5796, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6086)
      %5798 = stablehlo.reshape %arg1281 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc6087)
      %5799 = stablehlo.reshape %5798 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc6088)
      %5800 = stablehlo.transpose %5799, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc6089)
      %5801 = stablehlo.dot_general %5752, %5800, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc6090)
      %5802 = stablehlo.reshape %5801 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6091)
      %5803 = stablehlo.convert %5802 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc6092)
      %5804 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %5805 = stablehlo.power %5803, %5804 : tensor<32x17x8x128xf32> loc(#loc6093)
      %5806 = stablehlo.reduce(%5805 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc6094)
      %5807 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %5808 = stablehlo.multiply %5806, %5807 : tensor<32x17x8xf32> loc(#loc6095)
      %5809 = stablehlo.reshape %5808 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc6096)
      %5810 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %5811 = stablehlo.add %5809, %5810 : tensor<32x17x8x1xf32> loc(#loc6097)
      %5812 = stablehlo.rsqrt %5811 : tensor<32x17x8x1xf32> loc(#loc6098)
      %5813 = stablehlo.reshape %5812 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc6099)
      %5814 = stablehlo.broadcast_in_dim %5813, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc6100)
      %5815 = stablehlo.multiply %5803, %5814 : tensor<32x17x8x128xf32> loc(#loc6101)
      %5816 = stablehlo.convert %5815 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc6102)
      %5817 = stablehlo.multiply %5797, %5816 : tensor<32x17x8x128xbf16> loc(#loc6103)
      %5818 = stablehlo.transpose %5817, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6104)
      %5819 = stablehlo.multiply %5818, %132 : tensor<32x8x17x128xbf16> loc(#loc6105)
      %5820 = stablehlo.slice %5818 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6106)
      %5821 = stablehlo.negate %5820 : tensor<32x8x17x64xbf16> loc(#loc6107)
      %5822 = stablehlo.slice %5818 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6108)
      %5823 = stablehlo.concatenate %5821, %5822, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6109)
      %5824 = stablehlo.multiply %5823, %138 : tensor<32x8x17x128xbf16> loc(#loc6110)
      %5825 = stablehlo.add %5819, %5824 : tensor<32x8x17x128xbf16> loc(#loc6111)
      %5826 = stablehlo.convert %5825 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc6112)
      %5827 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5828 = stablehlo.multiply %5826, %5827 : tensor<32x8x17x128xf32> loc(#loc6113)
      %5829 = stablehlo.broadcast_in_dim %5781, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6114)
      %5830 = stablehlo.reshape %5829 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6115)
      %5831 = stablehlo.convert %5830 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6116)
      %5832 = stablehlo.transpose %5831, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc6117)
      %5833 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %5834 = stablehlo.multiply %5832, %5833 : tensor<32x8x128x128xf32> loc(#loc6118)
      %5835 = stablehlo.dot_general %5828, %5834, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6119)
      %5836 = stablehlo.add %5835, %159 : tensor<32x8x17x128xf32> loc(#loc6120)
      %5837 = stablehlo.convert %5836 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc6121)
      %5838 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %5839 = stablehlo.compare  EQ, %5837, %5838 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc6122)
      %5840 = stablehlo.not %5839 : tensor<32x8x17x128xi1> loc(#loc6123)
      %5841 = stablehlo.reduce(%5840 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.13426"), %arg1677: tensor<i1> loc("reduce.13426"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc6125)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc6126)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc6124)
      %5842 = stablehlo.reshape %5841 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc6127)
      %5843 = stablehlo.not %5842 : tensor<32x8x17x1xi1> loc(#loc6128)
      %5844 = stablehlo.reshape %5843 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc6129)
      %5845 = stablehlo.broadcast_in_dim %5844, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc6130)
      %5846 = stablehlo.reduce(%5836 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6131)
      %5847 = stablehlo.broadcast_in_dim %5846, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6132)
      %5848 = stablehlo.subtract %5836, %5847 : tensor<32x8x17x128xf32> loc(#loc6133)
      %5849 = stablehlo.exponential %5848 : tensor<32x8x17x128xf32> loc(#loc6134)
      %5850 = stablehlo.reduce(%5849 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6135)
      %5851 = stablehlo.broadcast_in_dim %5850, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6136)
      %5852 = stablehlo.divide %5849, %5851 : tensor<32x8x17x128xf32> loc(#loc6137)
      %5853 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %5854 = stablehlo.select %5845, %5853, %5852 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc6138)
      %5855 = stablehlo.broadcast_in_dim %5788, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6139)
      %5856 = stablehlo.reshape %5855 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6140)
      %5857 = stablehlo.convert %5856 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6141)
      %5858 = stablehlo.dot_general %5854, %5857, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6142)
      %5859 = stablehlo.convert %5858 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc6143)
      %5860 = stablehlo.transpose %5859, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6144)
      %5861 = stablehlo.reshape %5860 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc6145)
      %5862 = stablehlo.reshape %arg1280 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc6146)
      %5863 = stablehlo.reshape %5862 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc6147)
      %5864 = stablehlo.transpose %5863, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc6148)
      %5865 = stablehlo.dot_general %5861, %5864, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6149)
      %5866 = "stablehlo.all_reduce"(%5865) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.13443"), %arg1677: tensor<bf16> loc("dot.13443")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6149)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6149)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6149)
      %5867 = stablehlo.reshape %5866 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6150)
      %5868 = stablehlo.add %5736, %5867 : tensor<32x17x5120xbf16> loc(#loc6151)
      %5869 = stablehlo.reshape %arg1283 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6152)
      %5870 = stablehlo.reshape %5869 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6153)
      %5871 = stablehlo.broadcast_in_dim %5870, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6154)
      %5872 = stablehlo.convert %5868 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6155)
      %5873 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5874 = stablehlo.power %5872, %5873 : tensor<32x17x5120xf32> loc(#loc6156)
      %5875 = stablehlo.reduce(%5874 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6157)
      %5876 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5877 = stablehlo.multiply %5875, %5876 : tensor<32x17xf32> loc(#loc6158)
      %5878 = stablehlo.reshape %5877 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6159)
      %5879 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5880 = stablehlo.add %5878, %5879 : tensor<32x17x1xf32> loc(#loc6160)
      %5881 = stablehlo.rsqrt %5880 : tensor<32x17x1xf32> loc(#loc6161)
      %5882 = stablehlo.reshape %5881 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6162)
      %5883 = stablehlo.broadcast_in_dim %5882, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6163)
      %5884 = stablehlo.multiply %5872, %5883 : tensor<32x17x5120xf32> loc(#loc6164)
      %5885 = stablehlo.convert %5884 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6165)
      %5886 = stablehlo.multiply %5871, %5885 : tensor<32x17x5120xbf16> loc(#loc6166)
      %5887 = stablehlo.reshape %5886 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6167)
      %5888 = stablehlo.reshape %arg1284 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6168)
      %5889 = stablehlo.reshape %5888 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6169)
      %5890 = stablehlo.transpose %5889, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6170)
      %5891 = stablehlo.dot_general %5887, %5890, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6171)
      %5892 = stablehlo.reshape %5891 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6172)
      %5893 = stablehlo.logistic %5892 : tensor<32x17x3200xbf16> loc(#loc6173)
      %5894 = stablehlo.multiply %5892, %5893 : tensor<32x17x3200xbf16> loc(#loc6174)
      %5895 = stablehlo.reshape %arg1279 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6175)
      %5896 = stablehlo.reshape %5895 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6176)
      %5897 = stablehlo.transpose %5896, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6177)
      %5898 = stablehlo.dot_general %5887, %5897, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6178)
      %5899 = stablehlo.reshape %5898 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6179)
      %5900 = stablehlo.multiply %5894, %5899 : tensor<32x17x3200xbf16> loc(#loc6180)
      %5901 = stablehlo.reshape %5900 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6181)
      %5902 = stablehlo.reshape %arg1278 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc6182)
      %5903 = stablehlo.reshape %5902 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc6183)
      %5904 = stablehlo.transpose %5903, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc6184)
      %5905 = stablehlo.dot_general %5901, %5904, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6185)
      %5906 = "stablehlo.all_reduce"(%5905) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.13498"), %arg1677: tensor<bf16> loc("dot.13498")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6185)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6185)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6185)
      %5907 = stablehlo.reshape %5906 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6186)
      %5908 = stablehlo.add %5868, %5907 : tensor<32x17x5120xbf16> loc(#loc6187)
      %5909 = stablehlo.convert %5908 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6188)
      %5910 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %5911 = stablehlo.power %5909, %5910 : tensor<32x17x5120xf32> loc(#loc6189)
      %5912 = stablehlo.reduce(%5911 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6190)
      %5913 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %5914 = stablehlo.multiply %5912, %5913 : tensor<32x17xf32> loc(#loc6191)
      %5915 = stablehlo.reshape %5914 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6192)
      %5916 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5917 = stablehlo.add %5915, %5916 : tensor<32x17x1xf32> loc(#loc6193)
      %5918 = stablehlo.rsqrt %5917 : tensor<32x17x1xf32> loc(#loc6194)
      %5919 = stablehlo.reshape %5918 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6195)
      %5920 = stablehlo.broadcast_in_dim %5919, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6196)
      %5921 = stablehlo.multiply %5909, %5920 : tensor<32x17x5120xf32> loc(#loc6197)
      %5922 = stablehlo.convert %5921 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6198)
      %5923 = stablehlo.multiply %5794, %5922 : tensor<32x17x5120xbf16> loc(#loc6199)
      %5924 = stablehlo.reshape %5923 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6200)
      %5925 = stablehlo.reshape %arg1277 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6201)
      %5926 = stablehlo.reshape %5925 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6202)
      %5927 = stablehlo.transpose %5926, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6203)
      %5928 = stablehlo.dot_general %5924, %5927, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6204)
      %5929 = stablehlo.reshape %5928 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6205)
      %5930 = stablehlo.convert %5929 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc6206)
      %5931 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %5932 = stablehlo.power %5930, %5931 : tensor<32x17x1x128xf32> loc(#loc6207)
      %5933 = stablehlo.reduce(%5932 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc6208)
      %5934 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %5935 = stablehlo.multiply %5933, %5934 : tensor<32x17x1xf32> loc(#loc6209)
      %5936 = stablehlo.reshape %5935 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc6210)
      %5937 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %5938 = stablehlo.add %5936, %5937 : tensor<32x17x1x1xf32> loc(#loc6211)
      %5939 = stablehlo.rsqrt %5938 : tensor<32x17x1x1xf32> loc(#loc6212)
      %5940 = stablehlo.reshape %5939 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc6213)
      %5941 = stablehlo.broadcast_in_dim %5940, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc6214)
      %5942 = stablehlo.multiply %5930, %5941 : tensor<32x17x1x128xf32> loc(#loc6215)
      %5943 = stablehlo.convert %5942 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc6216)
      %5944 = stablehlo.multiply %5791, %5943 : tensor<32x17x1x128xbf16> loc(#loc6217)
      %5945 = stablehlo.transpose %5944, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6218)
      %5946 = stablehlo.multiply %5945, %82 : tensor<32x1x17x128xbf16> loc(#loc6219)
      %5947 = stablehlo.slice %5945 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6220)
      %5948 = stablehlo.negate %5947 : tensor<32x1x17x64xbf16> loc(#loc6221)
      %5949 = stablehlo.slice %5945 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6222)
      %5950 = stablehlo.concatenate %5948, %5949, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6223)
      %5951 = stablehlo.multiply %5950, %91 : tensor<32x1x17x128xbf16> loc(#loc6224)
      %5952 = stablehlo.add %5946, %5951 : tensor<32x1x17x128xbf16> loc(#loc6225)
      %5953 = "stablehlo.scatter"(%arg1287, %21, %5952) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.13610"), %arg1677: tensor<bf16> loc("scatter.13610")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6226)
      %5954 = stablehlo.reshape %arg1288 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6227)
      %5955 = stablehlo.reshape %5954 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6228)
      %5956 = stablehlo.transpose %5955, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6229)
      %5957 = stablehlo.dot_general %5924, %5956, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6230)
      %5958 = stablehlo.reshape %5957 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6231)
      %5959 = stablehlo.transpose %5958, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6232)
      %5960 = "stablehlo.scatter"(%arg1289, %21, %5959) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.13640"), %arg1677: tensor<bf16> loc("scatter.13640")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6233)
      %5961 = stablehlo.reshape %arg1299 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6234)
      %5962 = stablehlo.reshape %5961 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6235)
      %5963 = stablehlo.broadcast_in_dim %5962, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6236)
      %5964 = stablehlo.reshape %arg1298 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6237)
      %5965 = stablehlo.reshape %5964 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6238)
      %5966 = stablehlo.broadcast_in_dim %5965, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6239)
      %5967 = stablehlo.reshape %arg1295 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6240)
      %5968 = stablehlo.reshape %5967 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6241)
      %5969 = stablehlo.broadcast_in_dim %5968, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6242)
      %5970 = stablehlo.reshape %arg1294 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc6243)
      %5971 = stablehlo.reshape %5970 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc6244)
      %5972 = stablehlo.transpose %5971, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc6245)
      %5973 = stablehlo.dot_general %5924, %5972, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc6246)
      %5974 = stablehlo.reshape %5973 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6247)
      %5975 = stablehlo.convert %5974 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc6248)
      %5976 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %5977 = stablehlo.power %5975, %5976 : tensor<32x17x8x128xf32> loc(#loc6249)
      %5978 = stablehlo.reduce(%5977 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc6250)
      %5979 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %5980 = stablehlo.multiply %5978, %5979 : tensor<32x17x8xf32> loc(#loc6251)
      %5981 = stablehlo.reshape %5980 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc6252)
      %5982 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %5983 = stablehlo.add %5981, %5982 : tensor<32x17x8x1xf32> loc(#loc6253)
      %5984 = stablehlo.rsqrt %5983 : tensor<32x17x8x1xf32> loc(#loc6254)
      %5985 = stablehlo.reshape %5984 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc6255)
      %5986 = stablehlo.broadcast_in_dim %5985, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc6256)
      %5987 = stablehlo.multiply %5975, %5986 : tensor<32x17x8x128xf32> loc(#loc6257)
      %5988 = stablehlo.convert %5987 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc6258)
      %5989 = stablehlo.multiply %5969, %5988 : tensor<32x17x8x128xbf16> loc(#loc6259)
      %5990 = stablehlo.transpose %5989, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6260)
      %5991 = stablehlo.multiply %5990, %132 : tensor<32x8x17x128xbf16> loc(#loc6261)
      %5992 = stablehlo.slice %5990 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6262)
      %5993 = stablehlo.negate %5992 : tensor<32x8x17x64xbf16> loc(#loc6263)
      %5994 = stablehlo.slice %5990 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6264)
      %5995 = stablehlo.concatenate %5993, %5994, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6265)
      %5996 = stablehlo.multiply %5995, %138 : tensor<32x8x17x128xbf16> loc(#loc6266)
      %5997 = stablehlo.add %5991, %5996 : tensor<32x8x17x128xbf16> loc(#loc6267)
      %5998 = stablehlo.convert %5997 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc6268)
      %5999 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6000 = stablehlo.multiply %5998, %5999 : tensor<32x8x17x128xf32> loc(#loc6269)
      %6001 = stablehlo.broadcast_in_dim %5953, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6270)
      %6002 = stablehlo.reshape %6001 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6271)
      %6003 = stablehlo.convert %6002 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6272)
      %6004 = stablehlo.transpose %6003, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc6273)
      %6005 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %6006 = stablehlo.multiply %6004, %6005 : tensor<32x8x128x128xf32> loc(#loc6274)
      %6007 = stablehlo.dot_general %6000, %6006, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6275)
      %6008 = stablehlo.add %6007, %159 : tensor<32x8x17x128xf32> loc(#loc6276)
      %6009 = stablehlo.convert %6008 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc6277)
      %6010 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %6011 = stablehlo.compare  EQ, %6009, %6010 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc6278)
      %6012 = stablehlo.not %6011 : tensor<32x8x17x128xi1> loc(#loc6279)
      %6013 = stablehlo.reduce(%6012 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.13821"), %arg1677: tensor<i1> loc("reduce.13821"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc6281)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc6282)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc6280)
      %6014 = stablehlo.reshape %6013 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc6283)
      %6015 = stablehlo.not %6014 : tensor<32x8x17x1xi1> loc(#loc6284)
      %6016 = stablehlo.reshape %6015 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc6285)
      %6017 = stablehlo.broadcast_in_dim %6016, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc6286)
      %6018 = stablehlo.reduce(%6008 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6287)
      %6019 = stablehlo.broadcast_in_dim %6018, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6288)
      %6020 = stablehlo.subtract %6008, %6019 : tensor<32x8x17x128xf32> loc(#loc6289)
      %6021 = stablehlo.exponential %6020 : tensor<32x8x17x128xf32> loc(#loc6290)
      %6022 = stablehlo.reduce(%6021 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6291)
      %6023 = stablehlo.broadcast_in_dim %6022, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6292)
      %6024 = stablehlo.divide %6021, %6023 : tensor<32x8x17x128xf32> loc(#loc6293)
      %6025 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6026 = stablehlo.select %6017, %6025, %6024 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc6294)
      %6027 = stablehlo.broadcast_in_dim %5960, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6295)
      %6028 = stablehlo.reshape %6027 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6296)
      %6029 = stablehlo.convert %6028 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6297)
      %6030 = stablehlo.dot_general %6026, %6029, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6298)
      %6031 = stablehlo.convert %6030 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc6299)
      %6032 = stablehlo.transpose %6031, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6300)
      %6033 = stablehlo.reshape %6032 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc6301)
      %6034 = stablehlo.reshape %arg1293 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc6302)
      %6035 = stablehlo.reshape %6034 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc6303)
      %6036 = stablehlo.transpose %6035, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc6304)
      %6037 = stablehlo.dot_general %6033, %6036, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6305)
      %6038 = "stablehlo.all_reduce"(%6037) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.13838"), %arg1677: tensor<bf16> loc("dot.13838")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6305)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6305)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6305)
      %6039 = stablehlo.reshape %6038 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6306)
      %6040 = stablehlo.add %5908, %6039 : tensor<32x17x5120xbf16> loc(#loc6307)
      %6041 = stablehlo.reshape %arg1296 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6308)
      %6042 = stablehlo.reshape %6041 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6309)
      %6043 = stablehlo.broadcast_in_dim %6042, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6310)
      %6044 = stablehlo.convert %6040 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6311)
      %6045 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6046 = stablehlo.power %6044, %6045 : tensor<32x17x5120xf32> loc(#loc6312)
      %6047 = stablehlo.reduce(%6046 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6313)
      %6048 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6049 = stablehlo.multiply %6047, %6048 : tensor<32x17xf32> loc(#loc6314)
      %6050 = stablehlo.reshape %6049 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6315)
      %6051 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6052 = stablehlo.add %6050, %6051 : tensor<32x17x1xf32> loc(#loc6316)
      %6053 = stablehlo.rsqrt %6052 : tensor<32x17x1xf32> loc(#loc6317)
      %6054 = stablehlo.reshape %6053 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6318)
      %6055 = stablehlo.broadcast_in_dim %6054, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6319)
      %6056 = stablehlo.multiply %6044, %6055 : tensor<32x17x5120xf32> loc(#loc6320)
      %6057 = stablehlo.convert %6056 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6321)
      %6058 = stablehlo.multiply %6043, %6057 : tensor<32x17x5120xbf16> loc(#loc6322)
      %6059 = stablehlo.reshape %6058 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6323)
      %6060 = stablehlo.reshape %arg1297 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6324)
      %6061 = stablehlo.reshape %6060 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6325)
      %6062 = stablehlo.transpose %6061, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6326)
      %6063 = stablehlo.dot_general %6059, %6062, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6327)
      %6064 = stablehlo.reshape %6063 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6328)
      %6065 = stablehlo.logistic %6064 : tensor<32x17x3200xbf16> loc(#loc6329)
      %6066 = stablehlo.multiply %6064, %6065 : tensor<32x17x3200xbf16> loc(#loc6330)
      %6067 = stablehlo.reshape %arg1292 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6331)
      %6068 = stablehlo.reshape %6067 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6332)
      %6069 = stablehlo.transpose %6068, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6333)
      %6070 = stablehlo.dot_general %6059, %6069, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6334)
      %6071 = stablehlo.reshape %6070 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6335)
      %6072 = stablehlo.multiply %6066, %6071 : tensor<32x17x3200xbf16> loc(#loc6336)
      %6073 = stablehlo.reshape %6072 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6337)
      %6074 = stablehlo.reshape %arg1291 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc6338)
      %6075 = stablehlo.reshape %6074 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc6339)
      %6076 = stablehlo.transpose %6075, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc6340)
      %6077 = stablehlo.dot_general %6073, %6076, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6341)
      %6078 = "stablehlo.all_reduce"(%6077) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.13893"), %arg1677: tensor<bf16> loc("dot.13893")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6341)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6341)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6341)
      %6079 = stablehlo.reshape %6078 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6342)
      %6080 = stablehlo.add %6040, %6079 : tensor<32x17x5120xbf16> loc(#loc6343)
      %6081 = stablehlo.convert %6080 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6344)
      %6082 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6083 = stablehlo.power %6081, %6082 : tensor<32x17x5120xf32> loc(#loc6345)
      %6084 = stablehlo.reduce(%6083 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6346)
      %6085 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6086 = stablehlo.multiply %6084, %6085 : tensor<32x17xf32> loc(#loc6347)
      %6087 = stablehlo.reshape %6086 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6348)
      %6088 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6089 = stablehlo.add %6087, %6088 : tensor<32x17x1xf32> loc(#loc6349)
      %6090 = stablehlo.rsqrt %6089 : tensor<32x17x1xf32> loc(#loc6350)
      %6091 = stablehlo.reshape %6090 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6351)
      %6092 = stablehlo.broadcast_in_dim %6091, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6352)
      %6093 = stablehlo.multiply %6081, %6092 : tensor<32x17x5120xf32> loc(#loc6353)
      %6094 = stablehlo.convert %6093 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6354)
      %6095 = stablehlo.multiply %5966, %6094 : tensor<32x17x5120xbf16> loc(#loc6355)
      %6096 = stablehlo.reshape %6095 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6356)
      %6097 = stablehlo.reshape %arg1290 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6357)
      %6098 = stablehlo.reshape %6097 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6358)
      %6099 = stablehlo.transpose %6098, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6359)
      %6100 = stablehlo.dot_general %6096, %6099, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6360)
      %6101 = stablehlo.reshape %6100 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6361)
      %6102 = stablehlo.convert %6101 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc6362)
      %6103 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %6104 = stablehlo.power %6102, %6103 : tensor<32x17x1x128xf32> loc(#loc6363)
      %6105 = stablehlo.reduce(%6104 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc6364)
      %6106 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6107 = stablehlo.multiply %6105, %6106 : tensor<32x17x1xf32> loc(#loc6365)
      %6108 = stablehlo.reshape %6107 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc6366)
      %6109 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %6110 = stablehlo.add %6108, %6109 : tensor<32x17x1x1xf32> loc(#loc6367)
      %6111 = stablehlo.rsqrt %6110 : tensor<32x17x1x1xf32> loc(#loc6368)
      %6112 = stablehlo.reshape %6111 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc6369)
      %6113 = stablehlo.broadcast_in_dim %6112, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc6370)
      %6114 = stablehlo.multiply %6102, %6113 : tensor<32x17x1x128xf32> loc(#loc6371)
      %6115 = stablehlo.convert %6114 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc6372)
      %6116 = stablehlo.multiply %5963, %6115 : tensor<32x17x1x128xbf16> loc(#loc6373)
      %6117 = stablehlo.transpose %6116, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6374)
      %6118 = stablehlo.multiply %6117, %82 : tensor<32x1x17x128xbf16> loc(#loc6375)
      %6119 = stablehlo.slice %6117 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6376)
      %6120 = stablehlo.negate %6119 : tensor<32x1x17x64xbf16> loc(#loc6377)
      %6121 = stablehlo.slice %6117 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6378)
      %6122 = stablehlo.concatenate %6120, %6121, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6379)
      %6123 = stablehlo.multiply %6122, %91 : tensor<32x1x17x128xbf16> loc(#loc6380)
      %6124 = stablehlo.add %6118, %6123 : tensor<32x1x17x128xbf16> loc(#loc6381)
      %6125 = "stablehlo.scatter"(%arg1300, %21, %6124) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.14005"), %arg1677: tensor<bf16> loc("scatter.14005")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6382)
      %6126 = stablehlo.reshape %arg1301 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6383)
      %6127 = stablehlo.reshape %6126 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6384)
      %6128 = stablehlo.transpose %6127, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6385)
      %6129 = stablehlo.dot_general %6096, %6128, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6386)
      %6130 = stablehlo.reshape %6129 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6387)
      %6131 = stablehlo.transpose %6130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6388)
      %6132 = "stablehlo.scatter"(%arg1302, %21, %6131) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.14035"), %arg1677: tensor<bf16> loc("scatter.14035")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6389)
      %6133 = stablehlo.reshape %arg1312 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6390)
      %6134 = stablehlo.reshape %6133 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6391)
      %6135 = stablehlo.broadcast_in_dim %6134, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6392)
      %6136 = stablehlo.reshape %arg1311 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6393)
      %6137 = stablehlo.reshape %6136 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6394)
      %6138 = stablehlo.broadcast_in_dim %6137, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6395)
      %6139 = stablehlo.reshape %arg1308 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6396)
      %6140 = stablehlo.reshape %6139 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6397)
      %6141 = stablehlo.broadcast_in_dim %6140, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6398)
      %6142 = stablehlo.reshape %arg1307 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc6399)
      %6143 = stablehlo.reshape %6142 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc6400)
      %6144 = stablehlo.transpose %6143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc6401)
      %6145 = stablehlo.dot_general %6096, %6144, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc6402)
      %6146 = stablehlo.reshape %6145 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6403)
      %6147 = stablehlo.convert %6146 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc6404)
      %6148 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %6149 = stablehlo.power %6147, %6148 : tensor<32x17x8x128xf32> loc(#loc6405)
      %6150 = stablehlo.reduce(%6149 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc6406)
      %6151 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %6152 = stablehlo.multiply %6150, %6151 : tensor<32x17x8xf32> loc(#loc6407)
      %6153 = stablehlo.reshape %6152 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc6408)
      %6154 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %6155 = stablehlo.add %6153, %6154 : tensor<32x17x8x1xf32> loc(#loc6409)
      %6156 = stablehlo.rsqrt %6155 : tensor<32x17x8x1xf32> loc(#loc6410)
      %6157 = stablehlo.reshape %6156 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc6411)
      %6158 = stablehlo.broadcast_in_dim %6157, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc6412)
      %6159 = stablehlo.multiply %6147, %6158 : tensor<32x17x8x128xf32> loc(#loc6413)
      %6160 = stablehlo.convert %6159 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc6414)
      %6161 = stablehlo.multiply %6141, %6160 : tensor<32x17x8x128xbf16> loc(#loc6415)
      %6162 = stablehlo.transpose %6161, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6416)
      %6163 = stablehlo.multiply %6162, %132 : tensor<32x8x17x128xbf16> loc(#loc6417)
      %6164 = stablehlo.slice %6162 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6418)
      %6165 = stablehlo.negate %6164 : tensor<32x8x17x64xbf16> loc(#loc6419)
      %6166 = stablehlo.slice %6162 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6420)
      %6167 = stablehlo.concatenate %6165, %6166, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6421)
      %6168 = stablehlo.multiply %6167, %138 : tensor<32x8x17x128xbf16> loc(#loc6422)
      %6169 = stablehlo.add %6163, %6168 : tensor<32x8x17x128xbf16> loc(#loc6423)
      %6170 = stablehlo.convert %6169 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc6424)
      %6171 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6172 = stablehlo.multiply %6170, %6171 : tensor<32x8x17x128xf32> loc(#loc6425)
      %6173 = stablehlo.broadcast_in_dim %6125, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6426)
      %6174 = stablehlo.reshape %6173 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6427)
      %6175 = stablehlo.convert %6174 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6428)
      %6176 = stablehlo.transpose %6175, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc6429)
      %6177 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %6178 = stablehlo.multiply %6176, %6177 : tensor<32x8x128x128xf32> loc(#loc6430)
      %6179 = stablehlo.dot_general %6172, %6178, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6431)
      %6180 = stablehlo.add %6179, %159 : tensor<32x8x17x128xf32> loc(#loc6432)
      %6181 = stablehlo.convert %6180 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc6433)
      %6182 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %6183 = stablehlo.compare  EQ, %6181, %6182 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc6434)
      %6184 = stablehlo.not %6183 : tensor<32x8x17x128xi1> loc(#loc6435)
      %6185 = stablehlo.reduce(%6184 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.14216"), %arg1677: tensor<i1> loc("reduce.14216"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc6437)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc6438)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc6436)
      %6186 = stablehlo.reshape %6185 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc6439)
      %6187 = stablehlo.not %6186 : tensor<32x8x17x1xi1> loc(#loc6440)
      %6188 = stablehlo.reshape %6187 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc6441)
      %6189 = stablehlo.broadcast_in_dim %6188, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc6442)
      %6190 = stablehlo.reduce(%6180 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6443)
      %6191 = stablehlo.broadcast_in_dim %6190, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6444)
      %6192 = stablehlo.subtract %6180, %6191 : tensor<32x8x17x128xf32> loc(#loc6445)
      %6193 = stablehlo.exponential %6192 : tensor<32x8x17x128xf32> loc(#loc6446)
      %6194 = stablehlo.reduce(%6193 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6447)
      %6195 = stablehlo.broadcast_in_dim %6194, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6448)
      %6196 = stablehlo.divide %6193, %6195 : tensor<32x8x17x128xf32> loc(#loc6449)
      %6197 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6198 = stablehlo.select %6189, %6197, %6196 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc6450)
      %6199 = stablehlo.broadcast_in_dim %6132, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6451)
      %6200 = stablehlo.reshape %6199 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6452)
      %6201 = stablehlo.convert %6200 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6453)
      %6202 = stablehlo.dot_general %6198, %6201, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6454)
      %6203 = stablehlo.convert %6202 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc6455)
      %6204 = stablehlo.transpose %6203, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6456)
      %6205 = stablehlo.reshape %6204 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc6457)
      %6206 = stablehlo.reshape %arg1306 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc6458)
      %6207 = stablehlo.reshape %6206 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc6459)
      %6208 = stablehlo.transpose %6207, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc6460)
      %6209 = stablehlo.dot_general %6205, %6208, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6461)
      %6210 = "stablehlo.all_reduce"(%6209) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.14233"), %arg1677: tensor<bf16> loc("dot.14233")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6461)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6461)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6461)
      %6211 = stablehlo.reshape %6210 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6462)
      %6212 = stablehlo.add %6080, %6211 : tensor<32x17x5120xbf16> loc(#loc6463)
      %6213 = stablehlo.reshape %arg1309 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6464)
      %6214 = stablehlo.reshape %6213 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6465)
      %6215 = stablehlo.broadcast_in_dim %6214, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6466)
      %6216 = stablehlo.convert %6212 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6467)
      %6217 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6218 = stablehlo.power %6216, %6217 : tensor<32x17x5120xf32> loc(#loc6468)
      %6219 = stablehlo.reduce(%6218 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6469)
      %6220 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6221 = stablehlo.multiply %6219, %6220 : tensor<32x17xf32> loc(#loc6470)
      %6222 = stablehlo.reshape %6221 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6471)
      %6223 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6224 = stablehlo.add %6222, %6223 : tensor<32x17x1xf32> loc(#loc6472)
      %6225 = stablehlo.rsqrt %6224 : tensor<32x17x1xf32> loc(#loc6473)
      %6226 = stablehlo.reshape %6225 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6474)
      %6227 = stablehlo.broadcast_in_dim %6226, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6475)
      %6228 = stablehlo.multiply %6216, %6227 : tensor<32x17x5120xf32> loc(#loc6476)
      %6229 = stablehlo.convert %6228 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6477)
      %6230 = stablehlo.multiply %6215, %6229 : tensor<32x17x5120xbf16> loc(#loc6478)
      %6231 = stablehlo.reshape %6230 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6479)
      %6232 = stablehlo.reshape %arg1310 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6480)
      %6233 = stablehlo.reshape %6232 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6481)
      %6234 = stablehlo.transpose %6233, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6482)
      %6235 = stablehlo.dot_general %6231, %6234, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6483)
      %6236 = stablehlo.reshape %6235 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6484)
      %6237 = stablehlo.logistic %6236 : tensor<32x17x3200xbf16> loc(#loc6485)
      %6238 = stablehlo.multiply %6236, %6237 : tensor<32x17x3200xbf16> loc(#loc6486)
      %6239 = stablehlo.reshape %arg1305 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6487)
      %6240 = stablehlo.reshape %6239 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6488)
      %6241 = stablehlo.transpose %6240, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6489)
      %6242 = stablehlo.dot_general %6231, %6241, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6490)
      %6243 = stablehlo.reshape %6242 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6491)
      %6244 = stablehlo.multiply %6238, %6243 : tensor<32x17x3200xbf16> loc(#loc6492)
      %6245 = stablehlo.reshape %6244 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6493)
      %6246 = stablehlo.reshape %arg1304 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc6494)
      %6247 = stablehlo.reshape %6246 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc6495)
      %6248 = stablehlo.transpose %6247, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc6496)
      %6249 = stablehlo.dot_general %6245, %6248, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6497)
      %6250 = "stablehlo.all_reduce"(%6249) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.14288"), %arg1677: tensor<bf16> loc("dot.14288")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6497)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6497)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6497)
      %6251 = stablehlo.reshape %6250 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6498)
      %6252 = stablehlo.add %6212, %6251 : tensor<32x17x5120xbf16> loc(#loc6499)
      %6253 = stablehlo.convert %6252 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6500)
      %6254 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6255 = stablehlo.power %6253, %6254 : tensor<32x17x5120xf32> loc(#loc6501)
      %6256 = stablehlo.reduce(%6255 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6502)
      %6257 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6258 = stablehlo.multiply %6256, %6257 : tensor<32x17xf32> loc(#loc6503)
      %6259 = stablehlo.reshape %6258 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6504)
      %6260 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6261 = stablehlo.add %6259, %6260 : tensor<32x17x1xf32> loc(#loc6505)
      %6262 = stablehlo.rsqrt %6261 : tensor<32x17x1xf32> loc(#loc6506)
      %6263 = stablehlo.reshape %6262 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6507)
      %6264 = stablehlo.broadcast_in_dim %6263, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6508)
      %6265 = stablehlo.multiply %6253, %6264 : tensor<32x17x5120xf32> loc(#loc6509)
      %6266 = stablehlo.convert %6265 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6510)
      %6267 = stablehlo.multiply %6138, %6266 : tensor<32x17x5120xbf16> loc(#loc6511)
      %6268 = stablehlo.reshape %6267 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6512)
      %6269 = stablehlo.reshape %arg1303 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6513)
      %6270 = stablehlo.reshape %6269 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6514)
      %6271 = stablehlo.transpose %6270, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6515)
      %6272 = stablehlo.dot_general %6268, %6271, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6516)
      %6273 = stablehlo.reshape %6272 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6517)
      %6274 = stablehlo.convert %6273 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc6518)
      %6275 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %6276 = stablehlo.power %6274, %6275 : tensor<32x17x1x128xf32> loc(#loc6519)
      %6277 = stablehlo.reduce(%6276 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc6520)
      %6278 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6279 = stablehlo.multiply %6277, %6278 : tensor<32x17x1xf32> loc(#loc6521)
      %6280 = stablehlo.reshape %6279 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc6522)
      %6281 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %6282 = stablehlo.add %6280, %6281 : tensor<32x17x1x1xf32> loc(#loc6523)
      %6283 = stablehlo.rsqrt %6282 : tensor<32x17x1x1xf32> loc(#loc6524)
      %6284 = stablehlo.reshape %6283 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc6525)
      %6285 = stablehlo.broadcast_in_dim %6284, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc6526)
      %6286 = stablehlo.multiply %6274, %6285 : tensor<32x17x1x128xf32> loc(#loc6527)
      %6287 = stablehlo.convert %6286 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc6528)
      %6288 = stablehlo.multiply %6135, %6287 : tensor<32x17x1x128xbf16> loc(#loc6529)
      %6289 = stablehlo.transpose %6288, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6530)
      %6290 = stablehlo.multiply %6289, %82 : tensor<32x1x17x128xbf16> loc(#loc6531)
      %6291 = stablehlo.slice %6289 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6532)
      %6292 = stablehlo.negate %6291 : tensor<32x1x17x64xbf16> loc(#loc6533)
      %6293 = stablehlo.slice %6289 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6534)
      %6294 = stablehlo.concatenate %6292, %6293, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6535)
      %6295 = stablehlo.multiply %6294, %91 : tensor<32x1x17x128xbf16> loc(#loc6536)
      %6296 = stablehlo.add %6290, %6295 : tensor<32x1x17x128xbf16> loc(#loc6537)
      %6297 = "stablehlo.scatter"(%arg1313, %21, %6296) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.14400"), %arg1677: tensor<bf16> loc("scatter.14400")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6538)
      %6298 = stablehlo.reshape %arg1314 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6539)
      %6299 = stablehlo.reshape %6298 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6540)
      %6300 = stablehlo.transpose %6299, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6541)
      %6301 = stablehlo.dot_general %6268, %6300, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6542)
      %6302 = stablehlo.reshape %6301 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6543)
      %6303 = stablehlo.transpose %6302, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6544)
      %6304 = "stablehlo.scatter"(%arg1315, %21, %6303) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.14430"), %arg1677: tensor<bf16> loc("scatter.14430")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6545)
      %6305 = stablehlo.reshape %arg1325 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6546)
      %6306 = stablehlo.reshape %6305 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6547)
      %6307 = stablehlo.broadcast_in_dim %6306, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6548)
      %6308 = stablehlo.reshape %arg1324 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6549)
      %6309 = stablehlo.reshape %6308 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6550)
      %6310 = stablehlo.broadcast_in_dim %6309, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6551)
      %6311 = stablehlo.reshape %arg1321 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6552)
      %6312 = stablehlo.reshape %6311 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6553)
      %6313 = stablehlo.broadcast_in_dim %6312, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6554)
      %6314 = stablehlo.reshape %arg1320 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc6555)
      %6315 = stablehlo.reshape %6314 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc6556)
      %6316 = stablehlo.transpose %6315, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc6557)
      %6317 = stablehlo.dot_general %6268, %6316, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc6558)
      %6318 = stablehlo.reshape %6317 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6559)
      %6319 = stablehlo.convert %6318 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc6560)
      %6320 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %6321 = stablehlo.power %6319, %6320 : tensor<32x17x8x128xf32> loc(#loc6561)
      %6322 = stablehlo.reduce(%6321 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc6562)
      %6323 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %6324 = stablehlo.multiply %6322, %6323 : tensor<32x17x8xf32> loc(#loc6563)
      %6325 = stablehlo.reshape %6324 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc6564)
      %6326 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %6327 = stablehlo.add %6325, %6326 : tensor<32x17x8x1xf32> loc(#loc6565)
      %6328 = stablehlo.rsqrt %6327 : tensor<32x17x8x1xf32> loc(#loc6566)
      %6329 = stablehlo.reshape %6328 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc6567)
      %6330 = stablehlo.broadcast_in_dim %6329, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc6568)
      %6331 = stablehlo.multiply %6319, %6330 : tensor<32x17x8x128xf32> loc(#loc6569)
      %6332 = stablehlo.convert %6331 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc6570)
      %6333 = stablehlo.multiply %6313, %6332 : tensor<32x17x8x128xbf16> loc(#loc6571)
      %6334 = stablehlo.transpose %6333, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6572)
      %6335 = stablehlo.multiply %6334, %132 : tensor<32x8x17x128xbf16> loc(#loc6573)
      %6336 = stablehlo.slice %6334 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6574)
      %6337 = stablehlo.negate %6336 : tensor<32x8x17x64xbf16> loc(#loc6575)
      %6338 = stablehlo.slice %6334 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6576)
      %6339 = stablehlo.concatenate %6337, %6338, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6577)
      %6340 = stablehlo.multiply %6339, %138 : tensor<32x8x17x128xbf16> loc(#loc6578)
      %6341 = stablehlo.add %6335, %6340 : tensor<32x8x17x128xbf16> loc(#loc6579)
      %6342 = stablehlo.convert %6341 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc6580)
      %6343 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6344 = stablehlo.multiply %6342, %6343 : tensor<32x8x17x128xf32> loc(#loc6581)
      %6345 = stablehlo.broadcast_in_dim %6297, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6582)
      %6346 = stablehlo.reshape %6345 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6583)
      %6347 = stablehlo.convert %6346 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6584)
      %6348 = stablehlo.transpose %6347, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc6585)
      %6349 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %6350 = stablehlo.multiply %6348, %6349 : tensor<32x8x128x128xf32> loc(#loc6586)
      %6351 = stablehlo.dot_general %6344, %6350, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6587)
      %6352 = stablehlo.add %6351, %159 : tensor<32x8x17x128xf32> loc(#loc6588)
      %6353 = stablehlo.convert %6352 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc6589)
      %6354 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %6355 = stablehlo.compare  EQ, %6353, %6354 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc6590)
      %6356 = stablehlo.not %6355 : tensor<32x8x17x128xi1> loc(#loc6591)
      %6357 = stablehlo.reduce(%6356 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.14611"), %arg1677: tensor<i1> loc("reduce.14611"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc6593)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc6594)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc6592)
      %6358 = stablehlo.reshape %6357 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc6595)
      %6359 = stablehlo.not %6358 : tensor<32x8x17x1xi1> loc(#loc6596)
      %6360 = stablehlo.reshape %6359 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc6597)
      %6361 = stablehlo.broadcast_in_dim %6360, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc6598)
      %6362 = stablehlo.reduce(%6352 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6599)
      %6363 = stablehlo.broadcast_in_dim %6362, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6600)
      %6364 = stablehlo.subtract %6352, %6363 : tensor<32x8x17x128xf32> loc(#loc6601)
      %6365 = stablehlo.exponential %6364 : tensor<32x8x17x128xf32> loc(#loc6602)
      %6366 = stablehlo.reduce(%6365 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6603)
      %6367 = stablehlo.broadcast_in_dim %6366, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6604)
      %6368 = stablehlo.divide %6365, %6367 : tensor<32x8x17x128xf32> loc(#loc6605)
      %6369 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6370 = stablehlo.select %6361, %6369, %6368 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc6606)
      %6371 = stablehlo.broadcast_in_dim %6304, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6607)
      %6372 = stablehlo.reshape %6371 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6608)
      %6373 = stablehlo.convert %6372 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6609)
      %6374 = stablehlo.dot_general %6370, %6373, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6610)
      %6375 = stablehlo.convert %6374 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc6611)
      %6376 = stablehlo.transpose %6375, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6612)
      %6377 = stablehlo.reshape %6376 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc6613)
      %6378 = stablehlo.reshape %arg1319 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc6614)
      %6379 = stablehlo.reshape %6378 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc6615)
      %6380 = stablehlo.transpose %6379, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc6616)
      %6381 = stablehlo.dot_general %6377, %6380, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6617)
      %6382 = "stablehlo.all_reduce"(%6381) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.14628"), %arg1677: tensor<bf16> loc("dot.14628")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6617)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6617)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6617)
      %6383 = stablehlo.reshape %6382 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6618)
      %6384 = stablehlo.add %6252, %6383 : tensor<32x17x5120xbf16> loc(#loc6619)
      %6385 = stablehlo.reshape %arg1322 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6620)
      %6386 = stablehlo.reshape %6385 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6621)
      %6387 = stablehlo.broadcast_in_dim %6386, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6622)
      %6388 = stablehlo.convert %6384 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6623)
      %6389 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6390 = stablehlo.power %6388, %6389 : tensor<32x17x5120xf32> loc(#loc6624)
      %6391 = stablehlo.reduce(%6390 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6625)
      %6392 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6393 = stablehlo.multiply %6391, %6392 : tensor<32x17xf32> loc(#loc6626)
      %6394 = stablehlo.reshape %6393 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6627)
      %6395 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6396 = stablehlo.add %6394, %6395 : tensor<32x17x1xf32> loc(#loc6628)
      %6397 = stablehlo.rsqrt %6396 : tensor<32x17x1xf32> loc(#loc6629)
      %6398 = stablehlo.reshape %6397 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6630)
      %6399 = stablehlo.broadcast_in_dim %6398, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6631)
      %6400 = stablehlo.multiply %6388, %6399 : tensor<32x17x5120xf32> loc(#loc6632)
      %6401 = stablehlo.convert %6400 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6633)
      %6402 = stablehlo.multiply %6387, %6401 : tensor<32x17x5120xbf16> loc(#loc6634)
      %6403 = stablehlo.reshape %6402 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6635)
      %6404 = stablehlo.reshape %arg1323 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6636)
      %6405 = stablehlo.reshape %6404 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6637)
      %6406 = stablehlo.transpose %6405, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6638)
      %6407 = stablehlo.dot_general %6403, %6406, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6639)
      %6408 = stablehlo.reshape %6407 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6640)
      %6409 = stablehlo.logistic %6408 : tensor<32x17x3200xbf16> loc(#loc6641)
      %6410 = stablehlo.multiply %6408, %6409 : tensor<32x17x3200xbf16> loc(#loc6642)
      %6411 = stablehlo.reshape %arg1318 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6643)
      %6412 = stablehlo.reshape %6411 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6644)
      %6413 = stablehlo.transpose %6412, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6645)
      %6414 = stablehlo.dot_general %6403, %6413, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6646)
      %6415 = stablehlo.reshape %6414 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6647)
      %6416 = stablehlo.multiply %6410, %6415 : tensor<32x17x3200xbf16> loc(#loc6648)
      %6417 = stablehlo.reshape %6416 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6649)
      %6418 = stablehlo.reshape %arg1317 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc6650)
      %6419 = stablehlo.reshape %6418 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc6651)
      %6420 = stablehlo.transpose %6419, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc6652)
      %6421 = stablehlo.dot_general %6417, %6420, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6653)
      %6422 = "stablehlo.all_reduce"(%6421) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.14683"), %arg1677: tensor<bf16> loc("dot.14683")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6653)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6653)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6653)
      %6423 = stablehlo.reshape %6422 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6654)
      %6424 = stablehlo.add %6384, %6423 : tensor<32x17x5120xbf16> loc(#loc6655)
      %6425 = stablehlo.convert %6424 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6656)
      %6426 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6427 = stablehlo.power %6425, %6426 : tensor<32x17x5120xf32> loc(#loc6657)
      %6428 = stablehlo.reduce(%6427 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6658)
      %6429 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6430 = stablehlo.multiply %6428, %6429 : tensor<32x17xf32> loc(#loc6659)
      %6431 = stablehlo.reshape %6430 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6660)
      %6432 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6433 = stablehlo.add %6431, %6432 : tensor<32x17x1xf32> loc(#loc6661)
      %6434 = stablehlo.rsqrt %6433 : tensor<32x17x1xf32> loc(#loc6662)
      %6435 = stablehlo.reshape %6434 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6663)
      %6436 = stablehlo.broadcast_in_dim %6435, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6664)
      %6437 = stablehlo.multiply %6425, %6436 : tensor<32x17x5120xf32> loc(#loc6665)
      %6438 = stablehlo.convert %6437 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6666)
      %6439 = stablehlo.multiply %6310, %6438 : tensor<32x17x5120xbf16> loc(#loc6667)
      %6440 = stablehlo.reshape %6439 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6668)
      %6441 = stablehlo.reshape %arg1316 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6669)
      %6442 = stablehlo.reshape %6441 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6670)
      %6443 = stablehlo.transpose %6442, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6671)
      %6444 = stablehlo.dot_general %6440, %6443, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6672)
      %6445 = stablehlo.reshape %6444 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6673)
      %6446 = stablehlo.convert %6445 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc6674)
      %6447 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %6448 = stablehlo.power %6446, %6447 : tensor<32x17x1x128xf32> loc(#loc6675)
      %6449 = stablehlo.reduce(%6448 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc6676)
      %6450 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6451 = stablehlo.multiply %6449, %6450 : tensor<32x17x1xf32> loc(#loc6677)
      %6452 = stablehlo.reshape %6451 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc6678)
      %6453 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %6454 = stablehlo.add %6452, %6453 : tensor<32x17x1x1xf32> loc(#loc6679)
      %6455 = stablehlo.rsqrt %6454 : tensor<32x17x1x1xf32> loc(#loc6680)
      %6456 = stablehlo.reshape %6455 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc6681)
      %6457 = stablehlo.broadcast_in_dim %6456, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc6682)
      %6458 = stablehlo.multiply %6446, %6457 : tensor<32x17x1x128xf32> loc(#loc6683)
      %6459 = stablehlo.convert %6458 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc6684)
      %6460 = stablehlo.multiply %6307, %6459 : tensor<32x17x1x128xbf16> loc(#loc6685)
      %6461 = stablehlo.transpose %6460, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6686)
      %6462 = stablehlo.multiply %6461, %82 : tensor<32x1x17x128xbf16> loc(#loc6687)
      %6463 = stablehlo.slice %6461 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6688)
      %6464 = stablehlo.negate %6463 : tensor<32x1x17x64xbf16> loc(#loc6689)
      %6465 = stablehlo.slice %6461 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6690)
      %6466 = stablehlo.concatenate %6464, %6465, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6691)
      %6467 = stablehlo.multiply %6466, %91 : tensor<32x1x17x128xbf16> loc(#loc6692)
      %6468 = stablehlo.add %6462, %6467 : tensor<32x1x17x128xbf16> loc(#loc6693)
      %6469 = "stablehlo.scatter"(%arg1326, %21, %6468) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.14795"), %arg1677: tensor<bf16> loc("scatter.14795")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6694)
      %6470 = stablehlo.reshape %arg1327 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6695)
      %6471 = stablehlo.reshape %6470 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6696)
      %6472 = stablehlo.transpose %6471, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6697)
      %6473 = stablehlo.dot_general %6440, %6472, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6698)
      %6474 = stablehlo.reshape %6473 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6699)
      %6475 = stablehlo.transpose %6474, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6700)
      %6476 = "stablehlo.scatter"(%arg1328, %21, %6475) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.14825"), %arg1677: tensor<bf16> loc("scatter.14825")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6701)
      %6477 = stablehlo.reshape %arg1338 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6702)
      %6478 = stablehlo.reshape %6477 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6703)
      %6479 = stablehlo.broadcast_in_dim %6478, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6704)
      %6480 = stablehlo.reshape %arg1337 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6705)
      %6481 = stablehlo.reshape %6480 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6706)
      %6482 = stablehlo.broadcast_in_dim %6481, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6707)
      %6483 = stablehlo.reshape %arg1334 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6708)
      %6484 = stablehlo.reshape %6483 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6709)
      %6485 = stablehlo.broadcast_in_dim %6484, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6710)
      %6486 = stablehlo.reshape %arg1333 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc6711)
      %6487 = stablehlo.reshape %6486 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc6712)
      %6488 = stablehlo.transpose %6487, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc6713)
      %6489 = stablehlo.dot_general %6440, %6488, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc6714)
      %6490 = stablehlo.reshape %6489 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6715)
      %6491 = stablehlo.convert %6490 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc6716)
      %6492 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %6493 = stablehlo.power %6491, %6492 : tensor<32x17x8x128xf32> loc(#loc6717)
      %6494 = stablehlo.reduce(%6493 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc6718)
      %6495 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %6496 = stablehlo.multiply %6494, %6495 : tensor<32x17x8xf32> loc(#loc6719)
      %6497 = stablehlo.reshape %6496 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc6720)
      %6498 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %6499 = stablehlo.add %6497, %6498 : tensor<32x17x8x1xf32> loc(#loc6721)
      %6500 = stablehlo.rsqrt %6499 : tensor<32x17x8x1xf32> loc(#loc6722)
      %6501 = stablehlo.reshape %6500 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc6723)
      %6502 = stablehlo.broadcast_in_dim %6501, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc6724)
      %6503 = stablehlo.multiply %6491, %6502 : tensor<32x17x8x128xf32> loc(#loc6725)
      %6504 = stablehlo.convert %6503 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc6726)
      %6505 = stablehlo.multiply %6485, %6504 : tensor<32x17x8x128xbf16> loc(#loc6727)
      %6506 = stablehlo.transpose %6505, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6728)
      %6507 = stablehlo.multiply %6506, %132 : tensor<32x8x17x128xbf16> loc(#loc6729)
      %6508 = stablehlo.slice %6506 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6730)
      %6509 = stablehlo.negate %6508 : tensor<32x8x17x64xbf16> loc(#loc6731)
      %6510 = stablehlo.slice %6506 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6732)
      %6511 = stablehlo.concatenate %6509, %6510, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6733)
      %6512 = stablehlo.multiply %6511, %138 : tensor<32x8x17x128xbf16> loc(#loc6734)
      %6513 = stablehlo.add %6507, %6512 : tensor<32x8x17x128xbf16> loc(#loc6735)
      %6514 = stablehlo.convert %6513 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc6736)
      %6515 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6516 = stablehlo.multiply %6514, %6515 : tensor<32x8x17x128xf32> loc(#loc6737)
      %6517 = stablehlo.broadcast_in_dim %6469, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6738)
      %6518 = stablehlo.reshape %6517 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6739)
      %6519 = stablehlo.convert %6518 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6740)
      %6520 = stablehlo.transpose %6519, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc6741)
      %6521 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %6522 = stablehlo.multiply %6520, %6521 : tensor<32x8x128x128xf32> loc(#loc6742)
      %6523 = stablehlo.dot_general %6516, %6522, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6743)
      %6524 = stablehlo.add %6523, %159 : tensor<32x8x17x128xf32> loc(#loc6744)
      %6525 = stablehlo.convert %6524 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc6745)
      %6526 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %6527 = stablehlo.compare  EQ, %6525, %6526 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc6746)
      %6528 = stablehlo.not %6527 : tensor<32x8x17x128xi1> loc(#loc6747)
      %6529 = stablehlo.reduce(%6528 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.15006"), %arg1677: tensor<i1> loc("reduce.15006"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc6749)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc6750)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc6748)
      %6530 = stablehlo.reshape %6529 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc6751)
      %6531 = stablehlo.not %6530 : tensor<32x8x17x1xi1> loc(#loc6752)
      %6532 = stablehlo.reshape %6531 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc6753)
      %6533 = stablehlo.broadcast_in_dim %6532, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc6754)
      %6534 = stablehlo.reduce(%6524 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6755)
      %6535 = stablehlo.broadcast_in_dim %6534, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6756)
      %6536 = stablehlo.subtract %6524, %6535 : tensor<32x8x17x128xf32> loc(#loc6757)
      %6537 = stablehlo.exponential %6536 : tensor<32x8x17x128xf32> loc(#loc6758)
      %6538 = stablehlo.reduce(%6537 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6759)
      %6539 = stablehlo.broadcast_in_dim %6538, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6760)
      %6540 = stablehlo.divide %6537, %6539 : tensor<32x8x17x128xf32> loc(#loc6761)
      %6541 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6542 = stablehlo.select %6533, %6541, %6540 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc6762)
      %6543 = stablehlo.broadcast_in_dim %6476, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6763)
      %6544 = stablehlo.reshape %6543 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6764)
      %6545 = stablehlo.convert %6544 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6765)
      %6546 = stablehlo.dot_general %6542, %6545, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6766)
      %6547 = stablehlo.convert %6546 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc6767)
      %6548 = stablehlo.transpose %6547, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6768)
      %6549 = stablehlo.reshape %6548 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc6769)
      %6550 = stablehlo.reshape %arg1332 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc6770)
      %6551 = stablehlo.reshape %6550 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc6771)
      %6552 = stablehlo.transpose %6551, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc6772)
      %6553 = stablehlo.dot_general %6549, %6552, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6773)
      %6554 = "stablehlo.all_reduce"(%6553) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.15023"), %arg1677: tensor<bf16> loc("dot.15023")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6773)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6773)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6773)
      %6555 = stablehlo.reshape %6554 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6774)
      %6556 = stablehlo.add %6424, %6555 : tensor<32x17x5120xbf16> loc(#loc6775)
      %6557 = stablehlo.reshape %arg1335 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6776)
      %6558 = stablehlo.reshape %6557 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6777)
      %6559 = stablehlo.broadcast_in_dim %6558, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6778)
      %6560 = stablehlo.convert %6556 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6779)
      %6561 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6562 = stablehlo.power %6560, %6561 : tensor<32x17x5120xf32> loc(#loc6780)
      %6563 = stablehlo.reduce(%6562 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6781)
      %6564 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6565 = stablehlo.multiply %6563, %6564 : tensor<32x17xf32> loc(#loc6782)
      %6566 = stablehlo.reshape %6565 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6783)
      %6567 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6568 = stablehlo.add %6566, %6567 : tensor<32x17x1xf32> loc(#loc6784)
      %6569 = stablehlo.rsqrt %6568 : tensor<32x17x1xf32> loc(#loc6785)
      %6570 = stablehlo.reshape %6569 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6786)
      %6571 = stablehlo.broadcast_in_dim %6570, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6787)
      %6572 = stablehlo.multiply %6560, %6571 : tensor<32x17x5120xf32> loc(#loc6788)
      %6573 = stablehlo.convert %6572 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6789)
      %6574 = stablehlo.multiply %6559, %6573 : tensor<32x17x5120xbf16> loc(#loc6790)
      %6575 = stablehlo.reshape %6574 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6791)
      %6576 = stablehlo.reshape %arg1336 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6792)
      %6577 = stablehlo.reshape %6576 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6793)
      %6578 = stablehlo.transpose %6577, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6794)
      %6579 = stablehlo.dot_general %6575, %6578, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6795)
      %6580 = stablehlo.reshape %6579 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6796)
      %6581 = stablehlo.logistic %6580 : tensor<32x17x3200xbf16> loc(#loc6797)
      %6582 = stablehlo.multiply %6580, %6581 : tensor<32x17x3200xbf16> loc(#loc6798)
      %6583 = stablehlo.reshape %arg1331 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6799)
      %6584 = stablehlo.reshape %6583 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6800)
      %6585 = stablehlo.transpose %6584, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6801)
      %6586 = stablehlo.dot_general %6575, %6585, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6802)
      %6587 = stablehlo.reshape %6586 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6803)
      %6588 = stablehlo.multiply %6582, %6587 : tensor<32x17x3200xbf16> loc(#loc6804)
      %6589 = stablehlo.reshape %6588 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6805)
      %6590 = stablehlo.reshape %arg1330 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc6806)
      %6591 = stablehlo.reshape %6590 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc6807)
      %6592 = stablehlo.transpose %6591, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc6808)
      %6593 = stablehlo.dot_general %6589, %6592, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6809)
      %6594 = "stablehlo.all_reduce"(%6593) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.15078"), %arg1677: tensor<bf16> loc("dot.15078")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6809)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6809)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6809)
      %6595 = stablehlo.reshape %6594 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6810)
      %6596 = stablehlo.add %6556, %6595 : tensor<32x17x5120xbf16> loc(#loc6811)
      %6597 = stablehlo.convert %6596 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6812)
      %6598 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6599 = stablehlo.power %6597, %6598 : tensor<32x17x5120xf32> loc(#loc6813)
      %6600 = stablehlo.reduce(%6599 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6814)
      %6601 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6602 = stablehlo.multiply %6600, %6601 : tensor<32x17xf32> loc(#loc6815)
      %6603 = stablehlo.reshape %6602 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6816)
      %6604 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6605 = stablehlo.add %6603, %6604 : tensor<32x17x1xf32> loc(#loc6817)
      %6606 = stablehlo.rsqrt %6605 : tensor<32x17x1xf32> loc(#loc6818)
      %6607 = stablehlo.reshape %6606 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6819)
      %6608 = stablehlo.broadcast_in_dim %6607, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6820)
      %6609 = stablehlo.multiply %6597, %6608 : tensor<32x17x5120xf32> loc(#loc6821)
      %6610 = stablehlo.convert %6609 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6822)
      %6611 = stablehlo.multiply %6482, %6610 : tensor<32x17x5120xbf16> loc(#loc6823)
      %6612 = stablehlo.reshape %6611 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6824)
      %6613 = stablehlo.reshape %arg1329 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6825)
      %6614 = stablehlo.reshape %6613 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6826)
      %6615 = stablehlo.transpose %6614, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6827)
      %6616 = stablehlo.dot_general %6612, %6615, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6828)
      %6617 = stablehlo.reshape %6616 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6829)
      %6618 = stablehlo.convert %6617 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc6830)
      %6619 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %6620 = stablehlo.power %6618, %6619 : tensor<32x17x1x128xf32> loc(#loc6831)
      %6621 = stablehlo.reduce(%6620 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc6832)
      %6622 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6623 = stablehlo.multiply %6621, %6622 : tensor<32x17x1xf32> loc(#loc6833)
      %6624 = stablehlo.reshape %6623 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc6834)
      %6625 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %6626 = stablehlo.add %6624, %6625 : tensor<32x17x1x1xf32> loc(#loc6835)
      %6627 = stablehlo.rsqrt %6626 : tensor<32x17x1x1xf32> loc(#loc6836)
      %6628 = stablehlo.reshape %6627 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc6837)
      %6629 = stablehlo.broadcast_in_dim %6628, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc6838)
      %6630 = stablehlo.multiply %6618, %6629 : tensor<32x17x1x128xf32> loc(#loc6839)
      %6631 = stablehlo.convert %6630 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc6840)
      %6632 = stablehlo.multiply %6479, %6631 : tensor<32x17x1x128xbf16> loc(#loc6841)
      %6633 = stablehlo.transpose %6632, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6842)
      %6634 = stablehlo.multiply %6633, %82 : tensor<32x1x17x128xbf16> loc(#loc6843)
      %6635 = stablehlo.slice %6633 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6844)
      %6636 = stablehlo.negate %6635 : tensor<32x1x17x64xbf16> loc(#loc6845)
      %6637 = stablehlo.slice %6633 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc6846)
      %6638 = stablehlo.concatenate %6636, %6637, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6847)
      %6639 = stablehlo.multiply %6638, %91 : tensor<32x1x17x128xbf16> loc(#loc6848)
      %6640 = stablehlo.add %6634, %6639 : tensor<32x1x17x128xbf16> loc(#loc6849)
      %6641 = "stablehlo.scatter"(%arg1339, %21, %6640) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.15190"), %arg1677: tensor<bf16> loc("scatter.15190")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6850)
      %6642 = stablehlo.reshape %arg1340 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6851)
      %6643 = stablehlo.reshape %6642 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6852)
      %6644 = stablehlo.transpose %6643, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6853)
      %6645 = stablehlo.dot_general %6612, %6644, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6854)
      %6646 = stablehlo.reshape %6645 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6855)
      %6647 = stablehlo.transpose %6646, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6856)
      %6648 = "stablehlo.scatter"(%arg1341, %21, %6647) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.15220"), %arg1677: tensor<bf16> loc("scatter.15220")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc6857)
      %6649 = stablehlo.reshape %arg1351 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6858)
      %6650 = stablehlo.reshape %6649 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6859)
      %6651 = stablehlo.broadcast_in_dim %6650, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6860)
      %6652 = stablehlo.reshape %arg1350 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6861)
      %6653 = stablehlo.reshape %6652 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6862)
      %6654 = stablehlo.broadcast_in_dim %6653, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6863)
      %6655 = stablehlo.reshape %arg1347 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc6864)
      %6656 = stablehlo.reshape %6655 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc6865)
      %6657 = stablehlo.broadcast_in_dim %6656, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6866)
      %6658 = stablehlo.reshape %arg1346 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc6867)
      %6659 = stablehlo.reshape %6658 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc6868)
      %6660 = stablehlo.transpose %6659, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc6869)
      %6661 = stablehlo.dot_general %6612, %6660, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc6870)
      %6662 = stablehlo.reshape %6661 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6871)
      %6663 = stablehlo.convert %6662 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc6872)
      %6664 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %6665 = stablehlo.power %6663, %6664 : tensor<32x17x8x128xf32> loc(#loc6873)
      %6666 = stablehlo.reduce(%6665 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc6874)
      %6667 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %6668 = stablehlo.multiply %6666, %6667 : tensor<32x17x8xf32> loc(#loc6875)
      %6669 = stablehlo.reshape %6668 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc6876)
      %6670 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %6671 = stablehlo.add %6669, %6670 : tensor<32x17x8x1xf32> loc(#loc6877)
      %6672 = stablehlo.rsqrt %6671 : tensor<32x17x8x1xf32> loc(#loc6878)
      %6673 = stablehlo.reshape %6672 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc6879)
      %6674 = stablehlo.broadcast_in_dim %6673, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc6880)
      %6675 = stablehlo.multiply %6663, %6674 : tensor<32x17x8x128xf32> loc(#loc6881)
      %6676 = stablehlo.convert %6675 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc6882)
      %6677 = stablehlo.multiply %6657, %6676 : tensor<32x17x8x128xbf16> loc(#loc6883)
      %6678 = stablehlo.transpose %6677, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6884)
      %6679 = stablehlo.multiply %6678, %132 : tensor<32x8x17x128xbf16> loc(#loc6885)
      %6680 = stablehlo.slice %6678 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6886)
      %6681 = stablehlo.negate %6680 : tensor<32x8x17x64xbf16> loc(#loc6887)
      %6682 = stablehlo.slice %6678 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc6888)
      %6683 = stablehlo.concatenate %6681, %6682, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc6889)
      %6684 = stablehlo.multiply %6683, %138 : tensor<32x8x17x128xbf16> loc(#loc6890)
      %6685 = stablehlo.add %6679, %6684 : tensor<32x8x17x128xbf16> loc(#loc6891)
      %6686 = stablehlo.convert %6685 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc6892)
      %6687 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6688 = stablehlo.multiply %6686, %6687 : tensor<32x8x17x128xf32> loc(#loc6893)
      %6689 = stablehlo.broadcast_in_dim %6641, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6894)
      %6690 = stablehlo.reshape %6689 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6895)
      %6691 = stablehlo.convert %6690 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6896)
      %6692 = stablehlo.transpose %6691, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc6897)
      %6693 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %6694 = stablehlo.multiply %6692, %6693 : tensor<32x8x128x128xf32> loc(#loc6898)
      %6695 = stablehlo.dot_general %6688, %6694, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6899)
      %6696 = stablehlo.add %6695, %159 : tensor<32x8x17x128xf32> loc(#loc6900)
      %6697 = stablehlo.convert %6696 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc6901)
      %6698 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %6699 = stablehlo.compare  EQ, %6697, %6698 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc6902)
      %6700 = stablehlo.not %6699 : tensor<32x8x17x128xi1> loc(#loc6903)
      %6701 = stablehlo.reduce(%6700 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.15401"), %arg1677: tensor<i1> loc("reduce.15401"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc6905)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc6906)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc6904)
      %6702 = stablehlo.reshape %6701 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc6907)
      %6703 = stablehlo.not %6702 : tensor<32x8x17x1xi1> loc(#loc6908)
      %6704 = stablehlo.reshape %6703 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc6909)
      %6705 = stablehlo.broadcast_in_dim %6704, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc6910)
      %6706 = stablehlo.reduce(%6696 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6911)
      %6707 = stablehlo.broadcast_in_dim %6706, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6912)
      %6708 = stablehlo.subtract %6696, %6707 : tensor<32x8x17x128xf32> loc(#loc6913)
      %6709 = stablehlo.exponential %6708 : tensor<32x8x17x128xf32> loc(#loc6914)
      %6710 = stablehlo.reduce(%6709 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc6915)
      %6711 = stablehlo.broadcast_in_dim %6710, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc6916)
      %6712 = stablehlo.divide %6709, %6711 : tensor<32x8x17x128xf32> loc(#loc6917)
      %6713 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6714 = stablehlo.select %6705, %6713, %6712 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc6918)
      %6715 = stablehlo.broadcast_in_dim %6648, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc6919)
      %6716 = stablehlo.reshape %6715 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc6920)
      %6717 = stablehlo.convert %6716 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc6921)
      %6718 = stablehlo.dot_general %6714, %6717, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc6922)
      %6719 = stablehlo.convert %6718 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc6923)
      %6720 = stablehlo.transpose %6719, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc6924)
      %6721 = stablehlo.reshape %6720 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc6925)
      %6722 = stablehlo.reshape %arg1345 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc6926)
      %6723 = stablehlo.reshape %6722 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc6927)
      %6724 = stablehlo.transpose %6723, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc6928)
      %6725 = stablehlo.dot_general %6721, %6724, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6929)
      %6726 = "stablehlo.all_reduce"(%6725) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.15418"), %arg1677: tensor<bf16> loc("dot.15418")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6929)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6929)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6929)
      %6727 = stablehlo.reshape %6726 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6930)
      %6728 = stablehlo.add %6596, %6727 : tensor<32x17x5120xbf16> loc(#loc6931)
      %6729 = stablehlo.reshape %arg1348 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc6932)
      %6730 = stablehlo.reshape %6729 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc6933)
      %6731 = stablehlo.broadcast_in_dim %6730, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6934)
      %6732 = stablehlo.convert %6728 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6935)
      %6733 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6734 = stablehlo.power %6732, %6733 : tensor<32x17x5120xf32> loc(#loc6936)
      %6735 = stablehlo.reduce(%6734 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6937)
      %6736 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6737 = stablehlo.multiply %6735, %6736 : tensor<32x17xf32> loc(#loc6938)
      %6738 = stablehlo.reshape %6737 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6939)
      %6739 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6740 = stablehlo.add %6738, %6739 : tensor<32x17x1xf32> loc(#loc6940)
      %6741 = stablehlo.rsqrt %6740 : tensor<32x17x1xf32> loc(#loc6941)
      %6742 = stablehlo.reshape %6741 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6942)
      %6743 = stablehlo.broadcast_in_dim %6742, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6943)
      %6744 = stablehlo.multiply %6732, %6743 : tensor<32x17x5120xf32> loc(#loc6944)
      %6745 = stablehlo.convert %6744 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6945)
      %6746 = stablehlo.multiply %6731, %6745 : tensor<32x17x5120xbf16> loc(#loc6946)
      %6747 = stablehlo.reshape %6746 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6947)
      %6748 = stablehlo.reshape %arg1349 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6948)
      %6749 = stablehlo.reshape %6748 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6949)
      %6750 = stablehlo.transpose %6749, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6950)
      %6751 = stablehlo.dot_general %6747, %6750, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6951)
      %6752 = stablehlo.reshape %6751 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6952)
      %6753 = stablehlo.logistic %6752 : tensor<32x17x3200xbf16> loc(#loc6953)
      %6754 = stablehlo.multiply %6752, %6753 : tensor<32x17x3200xbf16> loc(#loc6954)
      %6755 = stablehlo.reshape %arg1344 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc6955)
      %6756 = stablehlo.reshape %6755 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc6956)
      %6757 = stablehlo.transpose %6756, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc6957)
      %6758 = stablehlo.dot_general %6747, %6757, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6958)
      %6759 = stablehlo.reshape %6758 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc6959)
      %6760 = stablehlo.multiply %6754, %6759 : tensor<32x17x3200xbf16> loc(#loc6960)
      %6761 = stablehlo.reshape %6760 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc6961)
      %6762 = stablehlo.reshape %arg1343 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc6962)
      %6763 = stablehlo.reshape %6762 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc6963)
      %6764 = stablehlo.transpose %6763, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc6964)
      %6765 = stablehlo.dot_general %6761, %6764, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6965)
      %6766 = "stablehlo.all_reduce"(%6765) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.15473"), %arg1677: tensor<bf16> loc("dot.15473")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc6965)
        stablehlo.return %11074 : tensor<bf16> loc(#loc6965)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6965)
      %6767 = stablehlo.reshape %6766 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc6966)
      %6768 = stablehlo.add %6728, %6767 : tensor<32x17x5120xbf16> loc(#loc6967)
      %6769 = stablehlo.convert %6768 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc6968)
      %6770 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6771 = stablehlo.power %6769, %6770 : tensor<32x17x5120xf32> loc(#loc6969)
      %6772 = stablehlo.reduce(%6771 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc6970)
      %6773 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6774 = stablehlo.multiply %6772, %6773 : tensor<32x17xf32> loc(#loc6971)
      %6775 = stablehlo.reshape %6774 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc6972)
      %6776 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6777 = stablehlo.add %6775, %6776 : tensor<32x17x1xf32> loc(#loc6973)
      %6778 = stablehlo.rsqrt %6777 : tensor<32x17x1xf32> loc(#loc6974)
      %6779 = stablehlo.reshape %6778 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc6975)
      %6780 = stablehlo.broadcast_in_dim %6779, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc6976)
      %6781 = stablehlo.multiply %6769, %6780 : tensor<32x17x5120xf32> loc(#loc6977)
      %6782 = stablehlo.convert %6781 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc6978)
      %6783 = stablehlo.multiply %6654, %6782 : tensor<32x17x5120xbf16> loc(#loc6979)
      %6784 = stablehlo.reshape %6783 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc6980)
      %6785 = stablehlo.reshape %arg1342 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc6981)
      %6786 = stablehlo.reshape %6785 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc6982)
      %6787 = stablehlo.transpose %6786, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc6983)
      %6788 = stablehlo.dot_general %6784, %6787, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc6984)
      %6789 = stablehlo.reshape %6788 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc6985)
      %6790 = stablehlo.convert %6789 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc6986)
      %6791 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %6792 = stablehlo.power %6790, %6791 : tensor<32x17x1x128xf32> loc(#loc6987)
      %6793 = stablehlo.reduce(%6792 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc6988)
      %6794 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6795 = stablehlo.multiply %6793, %6794 : tensor<32x17x1xf32> loc(#loc6989)
      %6796 = stablehlo.reshape %6795 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc6990)
      %6797 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %6798 = stablehlo.add %6796, %6797 : tensor<32x17x1x1xf32> loc(#loc6991)
      %6799 = stablehlo.rsqrt %6798 : tensor<32x17x1x1xf32> loc(#loc6992)
      %6800 = stablehlo.reshape %6799 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc6993)
      %6801 = stablehlo.broadcast_in_dim %6800, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc6994)
      %6802 = stablehlo.multiply %6790, %6801 : tensor<32x17x1x128xf32> loc(#loc6995)
      %6803 = stablehlo.convert %6802 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc6996)
      %6804 = stablehlo.multiply %6651, %6803 : tensor<32x17x1x128xbf16> loc(#loc6997)
      %6805 = stablehlo.transpose %6804, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc6998)
      %6806 = stablehlo.multiply %6805, %82 : tensor<32x1x17x128xbf16> loc(#loc6999)
      %6807 = stablehlo.slice %6805 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7000)
      %6808 = stablehlo.negate %6807 : tensor<32x1x17x64xbf16> loc(#loc7001)
      %6809 = stablehlo.slice %6805 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7002)
      %6810 = stablehlo.concatenate %6808, %6809, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7003)
      %6811 = stablehlo.multiply %6810, %91 : tensor<32x1x17x128xbf16> loc(#loc7004)
      %6812 = stablehlo.add %6806, %6811 : tensor<32x1x17x128xbf16> loc(#loc7005)
      %6813 = "stablehlo.scatter"(%arg1352, %21, %6812) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.15585"), %arg1677: tensor<bf16> loc("scatter.15585")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7006)
      %6814 = stablehlo.reshape %arg1353 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7007)
      %6815 = stablehlo.reshape %6814 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7008)
      %6816 = stablehlo.transpose %6815, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7009)
      %6817 = stablehlo.dot_general %6784, %6816, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7010)
      %6818 = stablehlo.reshape %6817 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7011)
      %6819 = stablehlo.transpose %6818, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7012)
      %6820 = "stablehlo.scatter"(%arg1354, %21, %6819) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.15615"), %arg1677: tensor<bf16> loc("scatter.15615")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7013)
      %6821 = stablehlo.reshape %arg1364 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7014)
      %6822 = stablehlo.reshape %6821 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7015)
      %6823 = stablehlo.broadcast_in_dim %6822, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7016)
      %6824 = stablehlo.reshape %arg1363 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7017)
      %6825 = stablehlo.reshape %6824 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7018)
      %6826 = stablehlo.broadcast_in_dim %6825, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7019)
      %6827 = stablehlo.reshape %arg1360 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7020)
      %6828 = stablehlo.reshape %6827 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7021)
      %6829 = stablehlo.broadcast_in_dim %6828, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7022)
      %6830 = stablehlo.reshape %arg1359 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc7023)
      %6831 = stablehlo.reshape %6830 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc7024)
      %6832 = stablehlo.transpose %6831, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc7025)
      %6833 = stablehlo.dot_general %6784, %6832, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc7026)
      %6834 = stablehlo.reshape %6833 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7027)
      %6835 = stablehlo.convert %6834 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc7028)
      %6836 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %6837 = stablehlo.power %6835, %6836 : tensor<32x17x8x128xf32> loc(#loc7029)
      %6838 = stablehlo.reduce(%6837 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc7030)
      %6839 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %6840 = stablehlo.multiply %6838, %6839 : tensor<32x17x8xf32> loc(#loc7031)
      %6841 = stablehlo.reshape %6840 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc7032)
      %6842 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %6843 = stablehlo.add %6841, %6842 : tensor<32x17x8x1xf32> loc(#loc7033)
      %6844 = stablehlo.rsqrt %6843 : tensor<32x17x8x1xf32> loc(#loc7034)
      %6845 = stablehlo.reshape %6844 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc7035)
      %6846 = stablehlo.broadcast_in_dim %6845, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc7036)
      %6847 = stablehlo.multiply %6835, %6846 : tensor<32x17x8x128xf32> loc(#loc7037)
      %6848 = stablehlo.convert %6847 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc7038)
      %6849 = stablehlo.multiply %6829, %6848 : tensor<32x17x8x128xbf16> loc(#loc7039)
      %6850 = stablehlo.transpose %6849, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7040)
      %6851 = stablehlo.multiply %6850, %132 : tensor<32x8x17x128xbf16> loc(#loc7041)
      %6852 = stablehlo.slice %6850 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7042)
      %6853 = stablehlo.negate %6852 : tensor<32x8x17x64xbf16> loc(#loc7043)
      %6854 = stablehlo.slice %6850 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7044)
      %6855 = stablehlo.concatenate %6853, %6854, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7045)
      %6856 = stablehlo.multiply %6855, %138 : tensor<32x8x17x128xbf16> loc(#loc7046)
      %6857 = stablehlo.add %6851, %6856 : tensor<32x8x17x128xbf16> loc(#loc7047)
      %6858 = stablehlo.convert %6857 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc7048)
      %6859 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6860 = stablehlo.multiply %6858, %6859 : tensor<32x8x17x128xf32> loc(#loc7049)
      %6861 = stablehlo.broadcast_in_dim %6813, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7050)
      %6862 = stablehlo.reshape %6861 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7051)
      %6863 = stablehlo.convert %6862 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7052)
      %6864 = stablehlo.transpose %6863, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc7053)
      %6865 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %6866 = stablehlo.multiply %6864, %6865 : tensor<32x8x128x128xf32> loc(#loc7054)
      %6867 = stablehlo.dot_general %6860, %6866, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7055)
      %6868 = stablehlo.add %6867, %159 : tensor<32x8x17x128xf32> loc(#loc7056)
      %6869 = stablehlo.convert %6868 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc7057)
      %6870 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %6871 = stablehlo.compare  EQ, %6869, %6870 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc7058)
      %6872 = stablehlo.not %6871 : tensor<32x8x17x128xi1> loc(#loc7059)
      %6873 = stablehlo.reduce(%6872 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.15796"), %arg1677: tensor<i1> loc("reduce.15796"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc7061)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc7062)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc7060)
      %6874 = stablehlo.reshape %6873 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc7063)
      %6875 = stablehlo.not %6874 : tensor<32x8x17x1xi1> loc(#loc7064)
      %6876 = stablehlo.reshape %6875 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc7065)
      %6877 = stablehlo.broadcast_in_dim %6876, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc7066)
      %6878 = stablehlo.reduce(%6868 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7067)
      %6879 = stablehlo.broadcast_in_dim %6878, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7068)
      %6880 = stablehlo.subtract %6868, %6879 : tensor<32x8x17x128xf32> loc(#loc7069)
      %6881 = stablehlo.exponential %6880 : tensor<32x8x17x128xf32> loc(#loc7070)
      %6882 = stablehlo.reduce(%6881 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7071)
      %6883 = stablehlo.broadcast_in_dim %6882, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7072)
      %6884 = stablehlo.divide %6881, %6883 : tensor<32x8x17x128xf32> loc(#loc7073)
      %6885 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %6886 = stablehlo.select %6877, %6885, %6884 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc7074)
      %6887 = stablehlo.broadcast_in_dim %6820, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7075)
      %6888 = stablehlo.reshape %6887 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7076)
      %6889 = stablehlo.convert %6888 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7077)
      %6890 = stablehlo.dot_general %6886, %6889, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7078)
      %6891 = stablehlo.convert %6890 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc7079)
      %6892 = stablehlo.transpose %6891, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7080)
      %6893 = stablehlo.reshape %6892 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc7081)
      %6894 = stablehlo.reshape %arg1358 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc7082)
      %6895 = stablehlo.reshape %6894 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc7083)
      %6896 = stablehlo.transpose %6895, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc7084)
      %6897 = stablehlo.dot_general %6893, %6896, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7085)
      %6898 = "stablehlo.all_reduce"(%6897) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.15813"), %arg1677: tensor<bf16> loc("dot.15813")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7085)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7085)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7085)
      %6899 = stablehlo.reshape %6898 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7086)
      %6900 = stablehlo.add %6768, %6899 : tensor<32x17x5120xbf16> loc(#loc7087)
      %6901 = stablehlo.reshape %arg1361 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7088)
      %6902 = stablehlo.reshape %6901 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7089)
      %6903 = stablehlo.broadcast_in_dim %6902, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7090)
      %6904 = stablehlo.convert %6900 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7091)
      %6905 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6906 = stablehlo.power %6904, %6905 : tensor<32x17x5120xf32> loc(#loc7092)
      %6907 = stablehlo.reduce(%6906 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7093)
      %6908 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6909 = stablehlo.multiply %6907, %6908 : tensor<32x17xf32> loc(#loc7094)
      %6910 = stablehlo.reshape %6909 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7095)
      %6911 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6912 = stablehlo.add %6910, %6911 : tensor<32x17x1xf32> loc(#loc7096)
      %6913 = stablehlo.rsqrt %6912 : tensor<32x17x1xf32> loc(#loc7097)
      %6914 = stablehlo.reshape %6913 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7098)
      %6915 = stablehlo.broadcast_in_dim %6914, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7099)
      %6916 = stablehlo.multiply %6904, %6915 : tensor<32x17x5120xf32> loc(#loc7100)
      %6917 = stablehlo.convert %6916 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7101)
      %6918 = stablehlo.multiply %6903, %6917 : tensor<32x17x5120xbf16> loc(#loc7102)
      %6919 = stablehlo.reshape %6918 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7103)
      %6920 = stablehlo.reshape %arg1362 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7104)
      %6921 = stablehlo.reshape %6920 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7105)
      %6922 = stablehlo.transpose %6921, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7106)
      %6923 = stablehlo.dot_general %6919, %6922, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7107)
      %6924 = stablehlo.reshape %6923 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7108)
      %6925 = stablehlo.logistic %6924 : tensor<32x17x3200xbf16> loc(#loc7109)
      %6926 = stablehlo.multiply %6924, %6925 : tensor<32x17x3200xbf16> loc(#loc7110)
      %6927 = stablehlo.reshape %arg1357 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7111)
      %6928 = stablehlo.reshape %6927 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7112)
      %6929 = stablehlo.transpose %6928, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7113)
      %6930 = stablehlo.dot_general %6919, %6929, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7114)
      %6931 = stablehlo.reshape %6930 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7115)
      %6932 = stablehlo.multiply %6926, %6931 : tensor<32x17x3200xbf16> loc(#loc7116)
      %6933 = stablehlo.reshape %6932 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7117)
      %6934 = stablehlo.reshape %arg1356 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc7118)
      %6935 = stablehlo.reshape %6934 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc7119)
      %6936 = stablehlo.transpose %6935, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc7120)
      %6937 = stablehlo.dot_general %6933, %6936, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7121)
      %6938 = "stablehlo.all_reduce"(%6937) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.15868"), %arg1677: tensor<bf16> loc("dot.15868")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7121)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7121)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7121)
      %6939 = stablehlo.reshape %6938 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7122)
      %6940 = stablehlo.add %6900, %6939 : tensor<32x17x5120xbf16> loc(#loc7123)
      %6941 = stablehlo.convert %6940 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7124)
      %6942 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %6943 = stablehlo.power %6941, %6942 : tensor<32x17x5120xf32> loc(#loc7125)
      %6944 = stablehlo.reduce(%6943 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7126)
      %6945 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %6946 = stablehlo.multiply %6944, %6945 : tensor<32x17xf32> loc(#loc7127)
      %6947 = stablehlo.reshape %6946 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7128)
      %6948 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6949 = stablehlo.add %6947, %6948 : tensor<32x17x1xf32> loc(#loc7129)
      %6950 = stablehlo.rsqrt %6949 : tensor<32x17x1xf32> loc(#loc7130)
      %6951 = stablehlo.reshape %6950 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7131)
      %6952 = stablehlo.broadcast_in_dim %6951, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7132)
      %6953 = stablehlo.multiply %6941, %6952 : tensor<32x17x5120xf32> loc(#loc7133)
      %6954 = stablehlo.convert %6953 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7134)
      %6955 = stablehlo.multiply %6826, %6954 : tensor<32x17x5120xbf16> loc(#loc7135)
      %6956 = stablehlo.reshape %6955 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7136)
      %6957 = stablehlo.reshape %arg1355 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7137)
      %6958 = stablehlo.reshape %6957 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7138)
      %6959 = stablehlo.transpose %6958, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7139)
      %6960 = stablehlo.dot_general %6956, %6959, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7140)
      %6961 = stablehlo.reshape %6960 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7141)
      %6962 = stablehlo.convert %6961 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc7142)
      %6963 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %6964 = stablehlo.power %6962, %6963 : tensor<32x17x1x128xf32> loc(#loc7143)
      %6965 = stablehlo.reduce(%6964 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc7144)
      %6966 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %6967 = stablehlo.multiply %6965, %6966 : tensor<32x17x1xf32> loc(#loc7145)
      %6968 = stablehlo.reshape %6967 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc7146)
      %6969 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %6970 = stablehlo.add %6968, %6969 : tensor<32x17x1x1xf32> loc(#loc7147)
      %6971 = stablehlo.rsqrt %6970 : tensor<32x17x1x1xf32> loc(#loc7148)
      %6972 = stablehlo.reshape %6971 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc7149)
      %6973 = stablehlo.broadcast_in_dim %6972, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc7150)
      %6974 = stablehlo.multiply %6962, %6973 : tensor<32x17x1x128xf32> loc(#loc7151)
      %6975 = stablehlo.convert %6974 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc7152)
      %6976 = stablehlo.multiply %6823, %6975 : tensor<32x17x1x128xbf16> loc(#loc7153)
      %6977 = stablehlo.transpose %6976, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7154)
      %6978 = stablehlo.multiply %6977, %82 : tensor<32x1x17x128xbf16> loc(#loc7155)
      %6979 = stablehlo.slice %6977 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7156)
      %6980 = stablehlo.negate %6979 : tensor<32x1x17x64xbf16> loc(#loc7157)
      %6981 = stablehlo.slice %6977 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7158)
      %6982 = stablehlo.concatenate %6980, %6981, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7159)
      %6983 = stablehlo.multiply %6982, %91 : tensor<32x1x17x128xbf16> loc(#loc7160)
      %6984 = stablehlo.add %6978, %6983 : tensor<32x1x17x128xbf16> loc(#loc7161)
      %6985 = "stablehlo.scatter"(%arg1365, %21, %6984) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.15980"), %arg1677: tensor<bf16> loc("scatter.15980")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7162)
      %6986 = stablehlo.reshape %arg1366 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7163)
      %6987 = stablehlo.reshape %6986 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7164)
      %6988 = stablehlo.transpose %6987, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7165)
      %6989 = stablehlo.dot_general %6956, %6988, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7166)
      %6990 = stablehlo.reshape %6989 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7167)
      %6991 = stablehlo.transpose %6990, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7168)
      %6992 = "stablehlo.scatter"(%arg1367, %21, %6991) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.16010"), %arg1677: tensor<bf16> loc("scatter.16010")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7169)
      %6993 = stablehlo.reshape %arg1377 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7170)
      %6994 = stablehlo.reshape %6993 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7171)
      %6995 = stablehlo.broadcast_in_dim %6994, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7172)
      %6996 = stablehlo.reshape %arg1376 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7173)
      %6997 = stablehlo.reshape %6996 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7174)
      %6998 = stablehlo.broadcast_in_dim %6997, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7175)
      %6999 = stablehlo.reshape %arg1373 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7176)
      %7000 = stablehlo.reshape %6999 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7177)
      %7001 = stablehlo.broadcast_in_dim %7000, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7178)
      %7002 = stablehlo.reshape %arg1372 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc7179)
      %7003 = stablehlo.reshape %7002 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc7180)
      %7004 = stablehlo.transpose %7003, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc7181)
      %7005 = stablehlo.dot_general %6956, %7004, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc7182)
      %7006 = stablehlo.reshape %7005 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7183)
      %7007 = stablehlo.convert %7006 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc7184)
      %7008 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %7009 = stablehlo.power %7007, %7008 : tensor<32x17x8x128xf32> loc(#loc7185)
      %7010 = stablehlo.reduce(%7009 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc7186)
      %7011 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %7012 = stablehlo.multiply %7010, %7011 : tensor<32x17x8xf32> loc(#loc7187)
      %7013 = stablehlo.reshape %7012 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc7188)
      %7014 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %7015 = stablehlo.add %7013, %7014 : tensor<32x17x8x1xf32> loc(#loc7189)
      %7016 = stablehlo.rsqrt %7015 : tensor<32x17x8x1xf32> loc(#loc7190)
      %7017 = stablehlo.reshape %7016 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc7191)
      %7018 = stablehlo.broadcast_in_dim %7017, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc7192)
      %7019 = stablehlo.multiply %7007, %7018 : tensor<32x17x8x128xf32> loc(#loc7193)
      %7020 = stablehlo.convert %7019 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc7194)
      %7021 = stablehlo.multiply %7001, %7020 : tensor<32x17x8x128xbf16> loc(#loc7195)
      %7022 = stablehlo.transpose %7021, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7196)
      %7023 = stablehlo.multiply %7022, %132 : tensor<32x8x17x128xbf16> loc(#loc7197)
      %7024 = stablehlo.slice %7022 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7198)
      %7025 = stablehlo.negate %7024 : tensor<32x8x17x64xbf16> loc(#loc7199)
      %7026 = stablehlo.slice %7022 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7200)
      %7027 = stablehlo.concatenate %7025, %7026, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7201)
      %7028 = stablehlo.multiply %7027, %138 : tensor<32x8x17x128xbf16> loc(#loc7202)
      %7029 = stablehlo.add %7023, %7028 : tensor<32x8x17x128xbf16> loc(#loc7203)
      %7030 = stablehlo.convert %7029 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc7204)
      %7031 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7032 = stablehlo.multiply %7030, %7031 : tensor<32x8x17x128xf32> loc(#loc7205)
      %7033 = stablehlo.broadcast_in_dim %6985, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7206)
      %7034 = stablehlo.reshape %7033 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7207)
      %7035 = stablehlo.convert %7034 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7208)
      %7036 = stablehlo.transpose %7035, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc7209)
      %7037 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %7038 = stablehlo.multiply %7036, %7037 : tensor<32x8x128x128xf32> loc(#loc7210)
      %7039 = stablehlo.dot_general %7032, %7038, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7211)
      %7040 = stablehlo.add %7039, %159 : tensor<32x8x17x128xf32> loc(#loc7212)
      %7041 = stablehlo.convert %7040 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc7213)
      %7042 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %7043 = stablehlo.compare  EQ, %7041, %7042 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc7214)
      %7044 = stablehlo.not %7043 : tensor<32x8x17x128xi1> loc(#loc7215)
      %7045 = stablehlo.reduce(%7044 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.16191"), %arg1677: tensor<i1> loc("reduce.16191"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc7217)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc7218)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc7216)
      %7046 = stablehlo.reshape %7045 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc7219)
      %7047 = stablehlo.not %7046 : tensor<32x8x17x1xi1> loc(#loc7220)
      %7048 = stablehlo.reshape %7047 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc7221)
      %7049 = stablehlo.broadcast_in_dim %7048, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc7222)
      %7050 = stablehlo.reduce(%7040 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7223)
      %7051 = stablehlo.broadcast_in_dim %7050, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7224)
      %7052 = stablehlo.subtract %7040, %7051 : tensor<32x8x17x128xf32> loc(#loc7225)
      %7053 = stablehlo.exponential %7052 : tensor<32x8x17x128xf32> loc(#loc7226)
      %7054 = stablehlo.reduce(%7053 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7227)
      %7055 = stablehlo.broadcast_in_dim %7054, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7228)
      %7056 = stablehlo.divide %7053, %7055 : tensor<32x8x17x128xf32> loc(#loc7229)
      %7057 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7058 = stablehlo.select %7049, %7057, %7056 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc7230)
      %7059 = stablehlo.broadcast_in_dim %6992, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7231)
      %7060 = stablehlo.reshape %7059 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7232)
      %7061 = stablehlo.convert %7060 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7233)
      %7062 = stablehlo.dot_general %7058, %7061, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7234)
      %7063 = stablehlo.convert %7062 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc7235)
      %7064 = stablehlo.transpose %7063, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7236)
      %7065 = stablehlo.reshape %7064 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc7237)
      %7066 = stablehlo.reshape %arg1371 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc7238)
      %7067 = stablehlo.reshape %7066 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc7239)
      %7068 = stablehlo.transpose %7067, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc7240)
      %7069 = stablehlo.dot_general %7065, %7068, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7241)
      %7070 = "stablehlo.all_reduce"(%7069) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.16208"), %arg1677: tensor<bf16> loc("dot.16208")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7241)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7241)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7241)
      %7071 = stablehlo.reshape %7070 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7242)
      %7072 = stablehlo.add %6940, %7071 : tensor<32x17x5120xbf16> loc(#loc7243)
      %7073 = stablehlo.reshape %arg1374 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7244)
      %7074 = stablehlo.reshape %7073 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7245)
      %7075 = stablehlo.broadcast_in_dim %7074, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7246)
      %7076 = stablehlo.convert %7072 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7247)
      %7077 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7078 = stablehlo.power %7076, %7077 : tensor<32x17x5120xf32> loc(#loc7248)
      %7079 = stablehlo.reduce(%7078 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7249)
      %7080 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7081 = stablehlo.multiply %7079, %7080 : tensor<32x17xf32> loc(#loc7250)
      %7082 = stablehlo.reshape %7081 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7251)
      %7083 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7084 = stablehlo.add %7082, %7083 : tensor<32x17x1xf32> loc(#loc7252)
      %7085 = stablehlo.rsqrt %7084 : tensor<32x17x1xf32> loc(#loc7253)
      %7086 = stablehlo.reshape %7085 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7254)
      %7087 = stablehlo.broadcast_in_dim %7086, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7255)
      %7088 = stablehlo.multiply %7076, %7087 : tensor<32x17x5120xf32> loc(#loc7256)
      %7089 = stablehlo.convert %7088 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7257)
      %7090 = stablehlo.multiply %7075, %7089 : tensor<32x17x5120xbf16> loc(#loc7258)
      %7091 = stablehlo.reshape %7090 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7259)
      %7092 = stablehlo.reshape %arg1375 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7260)
      %7093 = stablehlo.reshape %7092 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7261)
      %7094 = stablehlo.transpose %7093, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7262)
      %7095 = stablehlo.dot_general %7091, %7094, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7263)
      %7096 = stablehlo.reshape %7095 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7264)
      %7097 = stablehlo.logistic %7096 : tensor<32x17x3200xbf16> loc(#loc7265)
      %7098 = stablehlo.multiply %7096, %7097 : tensor<32x17x3200xbf16> loc(#loc7266)
      %7099 = stablehlo.reshape %arg1370 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7267)
      %7100 = stablehlo.reshape %7099 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7268)
      %7101 = stablehlo.transpose %7100, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7269)
      %7102 = stablehlo.dot_general %7091, %7101, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7270)
      %7103 = stablehlo.reshape %7102 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7271)
      %7104 = stablehlo.multiply %7098, %7103 : tensor<32x17x3200xbf16> loc(#loc7272)
      %7105 = stablehlo.reshape %7104 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7273)
      %7106 = stablehlo.reshape %arg1369 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc7274)
      %7107 = stablehlo.reshape %7106 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc7275)
      %7108 = stablehlo.transpose %7107, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc7276)
      %7109 = stablehlo.dot_general %7105, %7108, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7277)
      %7110 = "stablehlo.all_reduce"(%7109) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.16263"), %arg1677: tensor<bf16> loc("dot.16263")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7277)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7277)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7277)
      %7111 = stablehlo.reshape %7110 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7278)
      %7112 = stablehlo.add %7072, %7111 : tensor<32x17x5120xbf16> loc(#loc7279)
      %7113 = stablehlo.convert %7112 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7280)
      %7114 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7115 = stablehlo.power %7113, %7114 : tensor<32x17x5120xf32> loc(#loc7281)
      %7116 = stablehlo.reduce(%7115 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7282)
      %7117 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7118 = stablehlo.multiply %7116, %7117 : tensor<32x17xf32> loc(#loc7283)
      %7119 = stablehlo.reshape %7118 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7284)
      %7120 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7121 = stablehlo.add %7119, %7120 : tensor<32x17x1xf32> loc(#loc7285)
      %7122 = stablehlo.rsqrt %7121 : tensor<32x17x1xf32> loc(#loc7286)
      %7123 = stablehlo.reshape %7122 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7287)
      %7124 = stablehlo.broadcast_in_dim %7123, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7288)
      %7125 = stablehlo.multiply %7113, %7124 : tensor<32x17x5120xf32> loc(#loc7289)
      %7126 = stablehlo.convert %7125 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7290)
      %7127 = stablehlo.multiply %6998, %7126 : tensor<32x17x5120xbf16> loc(#loc7291)
      %7128 = stablehlo.reshape %7127 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7292)
      %7129 = stablehlo.reshape %arg1368 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7293)
      %7130 = stablehlo.reshape %7129 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7294)
      %7131 = stablehlo.transpose %7130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7295)
      %7132 = stablehlo.dot_general %7128, %7131, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7296)
      %7133 = stablehlo.reshape %7132 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7297)
      %7134 = stablehlo.convert %7133 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc7298)
      %7135 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %7136 = stablehlo.power %7134, %7135 : tensor<32x17x1x128xf32> loc(#loc7299)
      %7137 = stablehlo.reduce(%7136 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc7300)
      %7138 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7139 = stablehlo.multiply %7137, %7138 : tensor<32x17x1xf32> loc(#loc7301)
      %7140 = stablehlo.reshape %7139 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc7302)
      %7141 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %7142 = stablehlo.add %7140, %7141 : tensor<32x17x1x1xf32> loc(#loc7303)
      %7143 = stablehlo.rsqrt %7142 : tensor<32x17x1x1xf32> loc(#loc7304)
      %7144 = stablehlo.reshape %7143 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc7305)
      %7145 = stablehlo.broadcast_in_dim %7144, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc7306)
      %7146 = stablehlo.multiply %7134, %7145 : tensor<32x17x1x128xf32> loc(#loc7307)
      %7147 = stablehlo.convert %7146 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc7308)
      %7148 = stablehlo.multiply %6995, %7147 : tensor<32x17x1x128xbf16> loc(#loc7309)
      %7149 = stablehlo.transpose %7148, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7310)
      %7150 = stablehlo.multiply %7149, %82 : tensor<32x1x17x128xbf16> loc(#loc7311)
      %7151 = stablehlo.slice %7149 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7312)
      %7152 = stablehlo.negate %7151 : tensor<32x1x17x64xbf16> loc(#loc7313)
      %7153 = stablehlo.slice %7149 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7314)
      %7154 = stablehlo.concatenate %7152, %7153, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7315)
      %7155 = stablehlo.multiply %7154, %91 : tensor<32x1x17x128xbf16> loc(#loc7316)
      %7156 = stablehlo.add %7150, %7155 : tensor<32x1x17x128xbf16> loc(#loc7317)
      %7157 = "stablehlo.scatter"(%arg1378, %21, %7156) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.16375"), %arg1677: tensor<bf16> loc("scatter.16375")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7318)
      %7158 = stablehlo.reshape %arg1379 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7319)
      %7159 = stablehlo.reshape %7158 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7320)
      %7160 = stablehlo.transpose %7159, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7321)
      %7161 = stablehlo.dot_general %7128, %7160, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7322)
      %7162 = stablehlo.reshape %7161 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7323)
      %7163 = stablehlo.transpose %7162, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7324)
      %7164 = "stablehlo.scatter"(%arg1380, %21, %7163) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.16405"), %arg1677: tensor<bf16> loc("scatter.16405")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7325)
      %7165 = stablehlo.reshape %arg1390 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7326)
      %7166 = stablehlo.reshape %7165 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7327)
      %7167 = stablehlo.broadcast_in_dim %7166, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7328)
      %7168 = stablehlo.reshape %arg1389 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7329)
      %7169 = stablehlo.reshape %7168 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7330)
      %7170 = stablehlo.broadcast_in_dim %7169, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7331)
      %7171 = stablehlo.reshape %arg1386 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7332)
      %7172 = stablehlo.reshape %7171 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7333)
      %7173 = stablehlo.broadcast_in_dim %7172, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7334)
      %7174 = stablehlo.reshape %arg1385 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc7335)
      %7175 = stablehlo.reshape %7174 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc7336)
      %7176 = stablehlo.transpose %7175, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc7337)
      %7177 = stablehlo.dot_general %7128, %7176, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc7338)
      %7178 = stablehlo.reshape %7177 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7339)
      %7179 = stablehlo.convert %7178 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc7340)
      %7180 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %7181 = stablehlo.power %7179, %7180 : tensor<32x17x8x128xf32> loc(#loc7341)
      %7182 = stablehlo.reduce(%7181 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc7342)
      %7183 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %7184 = stablehlo.multiply %7182, %7183 : tensor<32x17x8xf32> loc(#loc7343)
      %7185 = stablehlo.reshape %7184 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc7344)
      %7186 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %7187 = stablehlo.add %7185, %7186 : tensor<32x17x8x1xf32> loc(#loc7345)
      %7188 = stablehlo.rsqrt %7187 : tensor<32x17x8x1xf32> loc(#loc7346)
      %7189 = stablehlo.reshape %7188 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc7347)
      %7190 = stablehlo.broadcast_in_dim %7189, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc7348)
      %7191 = stablehlo.multiply %7179, %7190 : tensor<32x17x8x128xf32> loc(#loc7349)
      %7192 = stablehlo.convert %7191 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc7350)
      %7193 = stablehlo.multiply %7173, %7192 : tensor<32x17x8x128xbf16> loc(#loc7351)
      %7194 = stablehlo.transpose %7193, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7352)
      %7195 = stablehlo.multiply %7194, %132 : tensor<32x8x17x128xbf16> loc(#loc7353)
      %7196 = stablehlo.slice %7194 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7354)
      %7197 = stablehlo.negate %7196 : tensor<32x8x17x64xbf16> loc(#loc7355)
      %7198 = stablehlo.slice %7194 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7356)
      %7199 = stablehlo.concatenate %7197, %7198, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7357)
      %7200 = stablehlo.multiply %7199, %138 : tensor<32x8x17x128xbf16> loc(#loc7358)
      %7201 = stablehlo.add %7195, %7200 : tensor<32x8x17x128xbf16> loc(#loc7359)
      %7202 = stablehlo.convert %7201 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc7360)
      %7203 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7204 = stablehlo.multiply %7202, %7203 : tensor<32x8x17x128xf32> loc(#loc7361)
      %7205 = stablehlo.broadcast_in_dim %7157, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7362)
      %7206 = stablehlo.reshape %7205 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7363)
      %7207 = stablehlo.convert %7206 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7364)
      %7208 = stablehlo.transpose %7207, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc7365)
      %7209 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %7210 = stablehlo.multiply %7208, %7209 : tensor<32x8x128x128xf32> loc(#loc7366)
      %7211 = stablehlo.dot_general %7204, %7210, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7367)
      %7212 = stablehlo.add %7211, %159 : tensor<32x8x17x128xf32> loc(#loc7368)
      %7213 = stablehlo.convert %7212 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc7369)
      %7214 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %7215 = stablehlo.compare  EQ, %7213, %7214 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc7370)
      %7216 = stablehlo.not %7215 : tensor<32x8x17x128xi1> loc(#loc7371)
      %7217 = stablehlo.reduce(%7216 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.16586"), %arg1677: tensor<i1> loc("reduce.16586"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc7373)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc7374)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc7372)
      %7218 = stablehlo.reshape %7217 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc7375)
      %7219 = stablehlo.not %7218 : tensor<32x8x17x1xi1> loc(#loc7376)
      %7220 = stablehlo.reshape %7219 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc7377)
      %7221 = stablehlo.broadcast_in_dim %7220, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc7378)
      %7222 = stablehlo.reduce(%7212 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7379)
      %7223 = stablehlo.broadcast_in_dim %7222, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7380)
      %7224 = stablehlo.subtract %7212, %7223 : tensor<32x8x17x128xf32> loc(#loc7381)
      %7225 = stablehlo.exponential %7224 : tensor<32x8x17x128xf32> loc(#loc7382)
      %7226 = stablehlo.reduce(%7225 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7383)
      %7227 = stablehlo.broadcast_in_dim %7226, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7384)
      %7228 = stablehlo.divide %7225, %7227 : tensor<32x8x17x128xf32> loc(#loc7385)
      %7229 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7230 = stablehlo.select %7221, %7229, %7228 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc7386)
      %7231 = stablehlo.broadcast_in_dim %7164, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7387)
      %7232 = stablehlo.reshape %7231 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7388)
      %7233 = stablehlo.convert %7232 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7389)
      %7234 = stablehlo.dot_general %7230, %7233, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7390)
      %7235 = stablehlo.convert %7234 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc7391)
      %7236 = stablehlo.transpose %7235, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7392)
      %7237 = stablehlo.reshape %7236 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc7393)
      %7238 = stablehlo.reshape %arg1384 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc7394)
      %7239 = stablehlo.reshape %7238 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc7395)
      %7240 = stablehlo.transpose %7239, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc7396)
      %7241 = stablehlo.dot_general %7237, %7240, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7397)
      %7242 = "stablehlo.all_reduce"(%7241) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.16603"), %arg1677: tensor<bf16> loc("dot.16603")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7397)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7397)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7397)
      %7243 = stablehlo.reshape %7242 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7398)
      %7244 = stablehlo.add %7112, %7243 : tensor<32x17x5120xbf16> loc(#loc7399)
      %7245 = stablehlo.reshape %arg1387 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7400)
      %7246 = stablehlo.reshape %7245 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7401)
      %7247 = stablehlo.broadcast_in_dim %7246, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7402)
      %7248 = stablehlo.convert %7244 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7403)
      %7249 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7250 = stablehlo.power %7248, %7249 : tensor<32x17x5120xf32> loc(#loc7404)
      %7251 = stablehlo.reduce(%7250 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7405)
      %7252 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7253 = stablehlo.multiply %7251, %7252 : tensor<32x17xf32> loc(#loc7406)
      %7254 = stablehlo.reshape %7253 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7407)
      %7255 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7256 = stablehlo.add %7254, %7255 : tensor<32x17x1xf32> loc(#loc7408)
      %7257 = stablehlo.rsqrt %7256 : tensor<32x17x1xf32> loc(#loc7409)
      %7258 = stablehlo.reshape %7257 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7410)
      %7259 = stablehlo.broadcast_in_dim %7258, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7411)
      %7260 = stablehlo.multiply %7248, %7259 : tensor<32x17x5120xf32> loc(#loc7412)
      %7261 = stablehlo.convert %7260 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7413)
      %7262 = stablehlo.multiply %7247, %7261 : tensor<32x17x5120xbf16> loc(#loc7414)
      %7263 = stablehlo.reshape %7262 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7415)
      %7264 = stablehlo.reshape %arg1388 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7416)
      %7265 = stablehlo.reshape %7264 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7417)
      %7266 = stablehlo.transpose %7265, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7418)
      %7267 = stablehlo.dot_general %7263, %7266, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7419)
      %7268 = stablehlo.reshape %7267 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7420)
      %7269 = stablehlo.logistic %7268 : tensor<32x17x3200xbf16> loc(#loc7421)
      %7270 = stablehlo.multiply %7268, %7269 : tensor<32x17x3200xbf16> loc(#loc7422)
      %7271 = stablehlo.reshape %arg1383 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7423)
      %7272 = stablehlo.reshape %7271 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7424)
      %7273 = stablehlo.transpose %7272, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7425)
      %7274 = stablehlo.dot_general %7263, %7273, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7426)
      %7275 = stablehlo.reshape %7274 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7427)
      %7276 = stablehlo.multiply %7270, %7275 : tensor<32x17x3200xbf16> loc(#loc7428)
      %7277 = stablehlo.reshape %7276 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7429)
      %7278 = stablehlo.reshape %arg1382 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc7430)
      %7279 = stablehlo.reshape %7278 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc7431)
      %7280 = stablehlo.transpose %7279, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc7432)
      %7281 = stablehlo.dot_general %7277, %7280, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7433)
      %7282 = "stablehlo.all_reduce"(%7281) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.16658"), %arg1677: tensor<bf16> loc("dot.16658")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7433)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7433)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7433)
      %7283 = stablehlo.reshape %7282 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7434)
      %7284 = stablehlo.add %7244, %7283 : tensor<32x17x5120xbf16> loc(#loc7435)
      %7285 = stablehlo.convert %7284 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7436)
      %7286 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7287 = stablehlo.power %7285, %7286 : tensor<32x17x5120xf32> loc(#loc7437)
      %7288 = stablehlo.reduce(%7287 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7438)
      %7289 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7290 = stablehlo.multiply %7288, %7289 : tensor<32x17xf32> loc(#loc7439)
      %7291 = stablehlo.reshape %7290 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7440)
      %7292 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7293 = stablehlo.add %7291, %7292 : tensor<32x17x1xf32> loc(#loc7441)
      %7294 = stablehlo.rsqrt %7293 : tensor<32x17x1xf32> loc(#loc7442)
      %7295 = stablehlo.reshape %7294 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7443)
      %7296 = stablehlo.broadcast_in_dim %7295, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7444)
      %7297 = stablehlo.multiply %7285, %7296 : tensor<32x17x5120xf32> loc(#loc7445)
      %7298 = stablehlo.convert %7297 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7446)
      %7299 = stablehlo.multiply %7170, %7298 : tensor<32x17x5120xbf16> loc(#loc7447)
      %7300 = stablehlo.reshape %7299 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7448)
      %7301 = stablehlo.reshape %arg1381 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7449)
      %7302 = stablehlo.reshape %7301 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7450)
      %7303 = stablehlo.transpose %7302, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7451)
      %7304 = stablehlo.dot_general %7300, %7303, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7452)
      %7305 = stablehlo.reshape %7304 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7453)
      %7306 = stablehlo.convert %7305 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc7454)
      %7307 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %7308 = stablehlo.power %7306, %7307 : tensor<32x17x1x128xf32> loc(#loc7455)
      %7309 = stablehlo.reduce(%7308 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc7456)
      %7310 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7311 = stablehlo.multiply %7309, %7310 : tensor<32x17x1xf32> loc(#loc7457)
      %7312 = stablehlo.reshape %7311 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc7458)
      %7313 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %7314 = stablehlo.add %7312, %7313 : tensor<32x17x1x1xf32> loc(#loc7459)
      %7315 = stablehlo.rsqrt %7314 : tensor<32x17x1x1xf32> loc(#loc7460)
      %7316 = stablehlo.reshape %7315 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc7461)
      %7317 = stablehlo.broadcast_in_dim %7316, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc7462)
      %7318 = stablehlo.multiply %7306, %7317 : tensor<32x17x1x128xf32> loc(#loc7463)
      %7319 = stablehlo.convert %7318 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc7464)
      %7320 = stablehlo.multiply %7167, %7319 : tensor<32x17x1x128xbf16> loc(#loc7465)
      %7321 = stablehlo.transpose %7320, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7466)
      %7322 = stablehlo.multiply %7321, %82 : tensor<32x1x17x128xbf16> loc(#loc7467)
      %7323 = stablehlo.slice %7321 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7468)
      %7324 = stablehlo.negate %7323 : tensor<32x1x17x64xbf16> loc(#loc7469)
      %7325 = stablehlo.slice %7321 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7470)
      %7326 = stablehlo.concatenate %7324, %7325, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7471)
      %7327 = stablehlo.multiply %7326, %91 : tensor<32x1x17x128xbf16> loc(#loc7472)
      %7328 = stablehlo.add %7322, %7327 : tensor<32x1x17x128xbf16> loc(#loc7473)
      %7329 = "stablehlo.scatter"(%arg1391, %21, %7328) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.16770"), %arg1677: tensor<bf16> loc("scatter.16770")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7474)
      %7330 = stablehlo.reshape %arg1392 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7475)
      %7331 = stablehlo.reshape %7330 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7476)
      %7332 = stablehlo.transpose %7331, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7477)
      %7333 = stablehlo.dot_general %7300, %7332, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7478)
      %7334 = stablehlo.reshape %7333 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7479)
      %7335 = stablehlo.transpose %7334, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7480)
      %7336 = "stablehlo.scatter"(%arg1393, %21, %7335) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.16800"), %arg1677: tensor<bf16> loc("scatter.16800")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7481)
      %7337 = stablehlo.reshape %arg1403 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7482)
      %7338 = stablehlo.reshape %7337 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7483)
      %7339 = stablehlo.broadcast_in_dim %7338, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7484)
      %7340 = stablehlo.reshape %arg1402 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7485)
      %7341 = stablehlo.reshape %7340 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7486)
      %7342 = stablehlo.broadcast_in_dim %7341, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7487)
      %7343 = stablehlo.reshape %arg1399 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7488)
      %7344 = stablehlo.reshape %7343 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7489)
      %7345 = stablehlo.broadcast_in_dim %7344, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7490)
      %7346 = stablehlo.reshape %arg1398 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc7491)
      %7347 = stablehlo.reshape %7346 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc7492)
      %7348 = stablehlo.transpose %7347, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc7493)
      %7349 = stablehlo.dot_general %7300, %7348, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc7494)
      %7350 = stablehlo.reshape %7349 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7495)
      %7351 = stablehlo.convert %7350 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc7496)
      %7352 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %7353 = stablehlo.power %7351, %7352 : tensor<32x17x8x128xf32> loc(#loc7497)
      %7354 = stablehlo.reduce(%7353 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc7498)
      %7355 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %7356 = stablehlo.multiply %7354, %7355 : tensor<32x17x8xf32> loc(#loc7499)
      %7357 = stablehlo.reshape %7356 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc7500)
      %7358 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %7359 = stablehlo.add %7357, %7358 : tensor<32x17x8x1xf32> loc(#loc7501)
      %7360 = stablehlo.rsqrt %7359 : tensor<32x17x8x1xf32> loc(#loc7502)
      %7361 = stablehlo.reshape %7360 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc7503)
      %7362 = stablehlo.broadcast_in_dim %7361, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc7504)
      %7363 = stablehlo.multiply %7351, %7362 : tensor<32x17x8x128xf32> loc(#loc7505)
      %7364 = stablehlo.convert %7363 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc7506)
      %7365 = stablehlo.multiply %7345, %7364 : tensor<32x17x8x128xbf16> loc(#loc7507)
      %7366 = stablehlo.transpose %7365, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7508)
      %7367 = stablehlo.multiply %7366, %132 : tensor<32x8x17x128xbf16> loc(#loc7509)
      %7368 = stablehlo.slice %7366 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7510)
      %7369 = stablehlo.negate %7368 : tensor<32x8x17x64xbf16> loc(#loc7511)
      %7370 = stablehlo.slice %7366 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7512)
      %7371 = stablehlo.concatenate %7369, %7370, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7513)
      %7372 = stablehlo.multiply %7371, %138 : tensor<32x8x17x128xbf16> loc(#loc7514)
      %7373 = stablehlo.add %7367, %7372 : tensor<32x8x17x128xbf16> loc(#loc7515)
      %7374 = stablehlo.convert %7373 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc7516)
      %7375 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7376 = stablehlo.multiply %7374, %7375 : tensor<32x8x17x128xf32> loc(#loc7517)
      %7377 = stablehlo.broadcast_in_dim %7329, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7518)
      %7378 = stablehlo.reshape %7377 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7519)
      %7379 = stablehlo.convert %7378 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7520)
      %7380 = stablehlo.transpose %7379, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc7521)
      %7381 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %7382 = stablehlo.multiply %7380, %7381 : tensor<32x8x128x128xf32> loc(#loc7522)
      %7383 = stablehlo.dot_general %7376, %7382, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7523)
      %7384 = stablehlo.add %7383, %159 : tensor<32x8x17x128xf32> loc(#loc7524)
      %7385 = stablehlo.convert %7384 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc7525)
      %7386 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %7387 = stablehlo.compare  EQ, %7385, %7386 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc7526)
      %7388 = stablehlo.not %7387 : tensor<32x8x17x128xi1> loc(#loc7527)
      %7389 = stablehlo.reduce(%7388 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.16981"), %arg1677: tensor<i1> loc("reduce.16981"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc7529)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc7530)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc7528)
      %7390 = stablehlo.reshape %7389 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc7531)
      %7391 = stablehlo.not %7390 : tensor<32x8x17x1xi1> loc(#loc7532)
      %7392 = stablehlo.reshape %7391 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc7533)
      %7393 = stablehlo.broadcast_in_dim %7392, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc7534)
      %7394 = stablehlo.reduce(%7384 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7535)
      %7395 = stablehlo.broadcast_in_dim %7394, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7536)
      %7396 = stablehlo.subtract %7384, %7395 : tensor<32x8x17x128xf32> loc(#loc7537)
      %7397 = stablehlo.exponential %7396 : tensor<32x8x17x128xf32> loc(#loc7538)
      %7398 = stablehlo.reduce(%7397 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7539)
      %7399 = stablehlo.broadcast_in_dim %7398, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7540)
      %7400 = stablehlo.divide %7397, %7399 : tensor<32x8x17x128xf32> loc(#loc7541)
      %7401 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7402 = stablehlo.select %7393, %7401, %7400 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc7542)
      %7403 = stablehlo.broadcast_in_dim %7336, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7543)
      %7404 = stablehlo.reshape %7403 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7544)
      %7405 = stablehlo.convert %7404 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7545)
      %7406 = stablehlo.dot_general %7402, %7405, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7546)
      %7407 = stablehlo.convert %7406 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc7547)
      %7408 = stablehlo.transpose %7407, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7548)
      %7409 = stablehlo.reshape %7408 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc7549)
      %7410 = stablehlo.reshape %arg1397 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc7550)
      %7411 = stablehlo.reshape %7410 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc7551)
      %7412 = stablehlo.transpose %7411, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc7552)
      %7413 = stablehlo.dot_general %7409, %7412, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7553)
      %7414 = "stablehlo.all_reduce"(%7413) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.16998"), %arg1677: tensor<bf16> loc("dot.16998")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7553)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7553)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7553)
      %7415 = stablehlo.reshape %7414 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7554)
      %7416 = stablehlo.add %7284, %7415 : tensor<32x17x5120xbf16> loc(#loc7555)
      %7417 = stablehlo.reshape %arg1400 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7556)
      %7418 = stablehlo.reshape %7417 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7557)
      %7419 = stablehlo.broadcast_in_dim %7418, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7558)
      %7420 = stablehlo.convert %7416 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7559)
      %7421 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7422 = stablehlo.power %7420, %7421 : tensor<32x17x5120xf32> loc(#loc7560)
      %7423 = stablehlo.reduce(%7422 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7561)
      %7424 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7425 = stablehlo.multiply %7423, %7424 : tensor<32x17xf32> loc(#loc7562)
      %7426 = stablehlo.reshape %7425 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7563)
      %7427 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7428 = stablehlo.add %7426, %7427 : tensor<32x17x1xf32> loc(#loc7564)
      %7429 = stablehlo.rsqrt %7428 : tensor<32x17x1xf32> loc(#loc7565)
      %7430 = stablehlo.reshape %7429 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7566)
      %7431 = stablehlo.broadcast_in_dim %7430, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7567)
      %7432 = stablehlo.multiply %7420, %7431 : tensor<32x17x5120xf32> loc(#loc7568)
      %7433 = stablehlo.convert %7432 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7569)
      %7434 = stablehlo.multiply %7419, %7433 : tensor<32x17x5120xbf16> loc(#loc7570)
      %7435 = stablehlo.reshape %7434 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7571)
      %7436 = stablehlo.reshape %arg1401 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7572)
      %7437 = stablehlo.reshape %7436 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7573)
      %7438 = stablehlo.transpose %7437, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7574)
      %7439 = stablehlo.dot_general %7435, %7438, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7575)
      %7440 = stablehlo.reshape %7439 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7576)
      %7441 = stablehlo.logistic %7440 : tensor<32x17x3200xbf16> loc(#loc7577)
      %7442 = stablehlo.multiply %7440, %7441 : tensor<32x17x3200xbf16> loc(#loc7578)
      %7443 = stablehlo.reshape %arg1396 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7579)
      %7444 = stablehlo.reshape %7443 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7580)
      %7445 = stablehlo.transpose %7444, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7581)
      %7446 = stablehlo.dot_general %7435, %7445, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7582)
      %7447 = stablehlo.reshape %7446 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7583)
      %7448 = stablehlo.multiply %7442, %7447 : tensor<32x17x3200xbf16> loc(#loc7584)
      %7449 = stablehlo.reshape %7448 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7585)
      %7450 = stablehlo.reshape %arg1395 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc7586)
      %7451 = stablehlo.reshape %7450 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc7587)
      %7452 = stablehlo.transpose %7451, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc7588)
      %7453 = stablehlo.dot_general %7449, %7452, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7589)
      %7454 = "stablehlo.all_reduce"(%7453) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.17053"), %arg1677: tensor<bf16> loc("dot.17053")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7589)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7589)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7589)
      %7455 = stablehlo.reshape %7454 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7590)
      %7456 = stablehlo.add %7416, %7455 : tensor<32x17x5120xbf16> loc(#loc7591)
      %7457 = stablehlo.convert %7456 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7592)
      %7458 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7459 = stablehlo.power %7457, %7458 : tensor<32x17x5120xf32> loc(#loc7593)
      %7460 = stablehlo.reduce(%7459 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7594)
      %7461 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7462 = stablehlo.multiply %7460, %7461 : tensor<32x17xf32> loc(#loc7595)
      %7463 = stablehlo.reshape %7462 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7596)
      %7464 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7465 = stablehlo.add %7463, %7464 : tensor<32x17x1xf32> loc(#loc7597)
      %7466 = stablehlo.rsqrt %7465 : tensor<32x17x1xf32> loc(#loc7598)
      %7467 = stablehlo.reshape %7466 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7599)
      %7468 = stablehlo.broadcast_in_dim %7467, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7600)
      %7469 = stablehlo.multiply %7457, %7468 : tensor<32x17x5120xf32> loc(#loc7601)
      %7470 = stablehlo.convert %7469 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7602)
      %7471 = stablehlo.multiply %7342, %7470 : tensor<32x17x5120xbf16> loc(#loc7603)
      %7472 = stablehlo.reshape %7471 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7604)
      %7473 = stablehlo.reshape %arg1394 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7605)
      %7474 = stablehlo.reshape %7473 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7606)
      %7475 = stablehlo.transpose %7474, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7607)
      %7476 = stablehlo.dot_general %7472, %7475, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7608)
      %7477 = stablehlo.reshape %7476 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7609)
      %7478 = stablehlo.convert %7477 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc7610)
      %7479 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %7480 = stablehlo.power %7478, %7479 : tensor<32x17x1x128xf32> loc(#loc7611)
      %7481 = stablehlo.reduce(%7480 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc7612)
      %7482 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7483 = stablehlo.multiply %7481, %7482 : tensor<32x17x1xf32> loc(#loc7613)
      %7484 = stablehlo.reshape %7483 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc7614)
      %7485 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %7486 = stablehlo.add %7484, %7485 : tensor<32x17x1x1xf32> loc(#loc7615)
      %7487 = stablehlo.rsqrt %7486 : tensor<32x17x1x1xf32> loc(#loc7616)
      %7488 = stablehlo.reshape %7487 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc7617)
      %7489 = stablehlo.broadcast_in_dim %7488, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc7618)
      %7490 = stablehlo.multiply %7478, %7489 : tensor<32x17x1x128xf32> loc(#loc7619)
      %7491 = stablehlo.convert %7490 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc7620)
      %7492 = stablehlo.multiply %7339, %7491 : tensor<32x17x1x128xbf16> loc(#loc7621)
      %7493 = stablehlo.transpose %7492, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7622)
      %7494 = stablehlo.multiply %7493, %82 : tensor<32x1x17x128xbf16> loc(#loc7623)
      %7495 = stablehlo.slice %7493 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7624)
      %7496 = stablehlo.negate %7495 : tensor<32x1x17x64xbf16> loc(#loc7625)
      %7497 = stablehlo.slice %7493 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7626)
      %7498 = stablehlo.concatenate %7496, %7497, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7627)
      %7499 = stablehlo.multiply %7498, %91 : tensor<32x1x17x128xbf16> loc(#loc7628)
      %7500 = stablehlo.add %7494, %7499 : tensor<32x1x17x128xbf16> loc(#loc7629)
      %7501 = "stablehlo.scatter"(%arg1404, %21, %7500) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.17165"), %arg1677: tensor<bf16> loc("scatter.17165")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7630)
      %7502 = stablehlo.reshape %arg1405 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7631)
      %7503 = stablehlo.reshape %7502 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7632)
      %7504 = stablehlo.transpose %7503, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7633)
      %7505 = stablehlo.dot_general %7472, %7504, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7634)
      %7506 = stablehlo.reshape %7505 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7635)
      %7507 = stablehlo.transpose %7506, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7636)
      %7508 = "stablehlo.scatter"(%arg1406, %21, %7507) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.17195"), %arg1677: tensor<bf16> loc("scatter.17195")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7637)
      %7509 = stablehlo.reshape %arg1416 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7638)
      %7510 = stablehlo.reshape %7509 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7639)
      %7511 = stablehlo.broadcast_in_dim %7510, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7640)
      %7512 = stablehlo.reshape %arg1415 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7641)
      %7513 = stablehlo.reshape %7512 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7642)
      %7514 = stablehlo.broadcast_in_dim %7513, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7643)
      %7515 = stablehlo.reshape %arg1412 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7644)
      %7516 = stablehlo.reshape %7515 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7645)
      %7517 = stablehlo.broadcast_in_dim %7516, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7646)
      %7518 = stablehlo.reshape %arg1411 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc7647)
      %7519 = stablehlo.reshape %7518 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc7648)
      %7520 = stablehlo.transpose %7519, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc7649)
      %7521 = stablehlo.dot_general %7472, %7520, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc7650)
      %7522 = stablehlo.reshape %7521 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7651)
      %7523 = stablehlo.convert %7522 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc7652)
      %7524 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %7525 = stablehlo.power %7523, %7524 : tensor<32x17x8x128xf32> loc(#loc7653)
      %7526 = stablehlo.reduce(%7525 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc7654)
      %7527 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %7528 = stablehlo.multiply %7526, %7527 : tensor<32x17x8xf32> loc(#loc7655)
      %7529 = stablehlo.reshape %7528 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc7656)
      %7530 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %7531 = stablehlo.add %7529, %7530 : tensor<32x17x8x1xf32> loc(#loc7657)
      %7532 = stablehlo.rsqrt %7531 : tensor<32x17x8x1xf32> loc(#loc7658)
      %7533 = stablehlo.reshape %7532 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc7659)
      %7534 = stablehlo.broadcast_in_dim %7533, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc7660)
      %7535 = stablehlo.multiply %7523, %7534 : tensor<32x17x8x128xf32> loc(#loc7661)
      %7536 = stablehlo.convert %7535 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc7662)
      %7537 = stablehlo.multiply %7517, %7536 : tensor<32x17x8x128xbf16> loc(#loc7663)
      %7538 = stablehlo.transpose %7537, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7664)
      %7539 = stablehlo.multiply %7538, %132 : tensor<32x8x17x128xbf16> loc(#loc7665)
      %7540 = stablehlo.slice %7538 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7666)
      %7541 = stablehlo.negate %7540 : tensor<32x8x17x64xbf16> loc(#loc7667)
      %7542 = stablehlo.slice %7538 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7668)
      %7543 = stablehlo.concatenate %7541, %7542, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7669)
      %7544 = stablehlo.multiply %7543, %138 : tensor<32x8x17x128xbf16> loc(#loc7670)
      %7545 = stablehlo.add %7539, %7544 : tensor<32x8x17x128xbf16> loc(#loc7671)
      %7546 = stablehlo.convert %7545 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc7672)
      %7547 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7548 = stablehlo.multiply %7546, %7547 : tensor<32x8x17x128xf32> loc(#loc7673)
      %7549 = stablehlo.broadcast_in_dim %7501, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7674)
      %7550 = stablehlo.reshape %7549 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7675)
      %7551 = stablehlo.convert %7550 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7676)
      %7552 = stablehlo.transpose %7551, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc7677)
      %7553 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %7554 = stablehlo.multiply %7552, %7553 : tensor<32x8x128x128xf32> loc(#loc7678)
      %7555 = stablehlo.dot_general %7548, %7554, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7679)
      %7556 = stablehlo.add %7555, %159 : tensor<32x8x17x128xf32> loc(#loc7680)
      %7557 = stablehlo.convert %7556 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc7681)
      %7558 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %7559 = stablehlo.compare  EQ, %7557, %7558 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc7682)
      %7560 = stablehlo.not %7559 : tensor<32x8x17x128xi1> loc(#loc7683)
      %7561 = stablehlo.reduce(%7560 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.17376"), %arg1677: tensor<i1> loc("reduce.17376"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc7685)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc7686)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc7684)
      %7562 = stablehlo.reshape %7561 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc7687)
      %7563 = stablehlo.not %7562 : tensor<32x8x17x1xi1> loc(#loc7688)
      %7564 = stablehlo.reshape %7563 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc7689)
      %7565 = stablehlo.broadcast_in_dim %7564, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc7690)
      %7566 = stablehlo.reduce(%7556 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7691)
      %7567 = stablehlo.broadcast_in_dim %7566, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7692)
      %7568 = stablehlo.subtract %7556, %7567 : tensor<32x8x17x128xf32> loc(#loc7693)
      %7569 = stablehlo.exponential %7568 : tensor<32x8x17x128xf32> loc(#loc7694)
      %7570 = stablehlo.reduce(%7569 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7695)
      %7571 = stablehlo.broadcast_in_dim %7570, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7696)
      %7572 = stablehlo.divide %7569, %7571 : tensor<32x8x17x128xf32> loc(#loc7697)
      %7573 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7574 = stablehlo.select %7565, %7573, %7572 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc7698)
      %7575 = stablehlo.broadcast_in_dim %7508, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7699)
      %7576 = stablehlo.reshape %7575 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7700)
      %7577 = stablehlo.convert %7576 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7701)
      %7578 = stablehlo.dot_general %7574, %7577, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7702)
      %7579 = stablehlo.convert %7578 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc7703)
      %7580 = stablehlo.transpose %7579, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7704)
      %7581 = stablehlo.reshape %7580 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc7705)
      %7582 = stablehlo.reshape %arg1410 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc7706)
      %7583 = stablehlo.reshape %7582 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc7707)
      %7584 = stablehlo.transpose %7583, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc7708)
      %7585 = stablehlo.dot_general %7581, %7584, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7709)
      %7586 = "stablehlo.all_reduce"(%7585) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.17393"), %arg1677: tensor<bf16> loc("dot.17393")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7709)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7709)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7709)
      %7587 = stablehlo.reshape %7586 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7710)
      %7588 = stablehlo.add %7456, %7587 : tensor<32x17x5120xbf16> loc(#loc7711)
      %7589 = stablehlo.reshape %arg1413 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7712)
      %7590 = stablehlo.reshape %7589 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7713)
      %7591 = stablehlo.broadcast_in_dim %7590, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7714)
      %7592 = stablehlo.convert %7588 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7715)
      %7593 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7594 = stablehlo.power %7592, %7593 : tensor<32x17x5120xf32> loc(#loc7716)
      %7595 = stablehlo.reduce(%7594 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7717)
      %7596 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7597 = stablehlo.multiply %7595, %7596 : tensor<32x17xf32> loc(#loc7718)
      %7598 = stablehlo.reshape %7597 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7719)
      %7599 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7600 = stablehlo.add %7598, %7599 : tensor<32x17x1xf32> loc(#loc7720)
      %7601 = stablehlo.rsqrt %7600 : tensor<32x17x1xf32> loc(#loc7721)
      %7602 = stablehlo.reshape %7601 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7722)
      %7603 = stablehlo.broadcast_in_dim %7602, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7723)
      %7604 = stablehlo.multiply %7592, %7603 : tensor<32x17x5120xf32> loc(#loc7724)
      %7605 = stablehlo.convert %7604 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7725)
      %7606 = stablehlo.multiply %7591, %7605 : tensor<32x17x5120xbf16> loc(#loc7726)
      %7607 = stablehlo.reshape %7606 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7727)
      %7608 = stablehlo.reshape %arg1414 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7728)
      %7609 = stablehlo.reshape %7608 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7729)
      %7610 = stablehlo.transpose %7609, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7730)
      %7611 = stablehlo.dot_general %7607, %7610, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7731)
      %7612 = stablehlo.reshape %7611 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7732)
      %7613 = stablehlo.logistic %7612 : tensor<32x17x3200xbf16> loc(#loc7733)
      %7614 = stablehlo.multiply %7612, %7613 : tensor<32x17x3200xbf16> loc(#loc7734)
      %7615 = stablehlo.reshape %arg1409 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7735)
      %7616 = stablehlo.reshape %7615 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7736)
      %7617 = stablehlo.transpose %7616, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7737)
      %7618 = stablehlo.dot_general %7607, %7617, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7738)
      %7619 = stablehlo.reshape %7618 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7739)
      %7620 = stablehlo.multiply %7614, %7619 : tensor<32x17x3200xbf16> loc(#loc7740)
      %7621 = stablehlo.reshape %7620 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7741)
      %7622 = stablehlo.reshape %arg1408 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc7742)
      %7623 = stablehlo.reshape %7622 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc7743)
      %7624 = stablehlo.transpose %7623, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc7744)
      %7625 = stablehlo.dot_general %7621, %7624, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7745)
      %7626 = "stablehlo.all_reduce"(%7625) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.17448"), %arg1677: tensor<bf16> loc("dot.17448")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7745)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7745)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7745)
      %7627 = stablehlo.reshape %7626 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7746)
      %7628 = stablehlo.add %7588, %7627 : tensor<32x17x5120xbf16> loc(#loc7747)
      %7629 = stablehlo.convert %7628 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7748)
      %7630 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7631 = stablehlo.power %7629, %7630 : tensor<32x17x5120xf32> loc(#loc7749)
      %7632 = stablehlo.reduce(%7631 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7750)
      %7633 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7634 = stablehlo.multiply %7632, %7633 : tensor<32x17xf32> loc(#loc7751)
      %7635 = stablehlo.reshape %7634 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7752)
      %7636 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7637 = stablehlo.add %7635, %7636 : tensor<32x17x1xf32> loc(#loc7753)
      %7638 = stablehlo.rsqrt %7637 : tensor<32x17x1xf32> loc(#loc7754)
      %7639 = stablehlo.reshape %7638 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7755)
      %7640 = stablehlo.broadcast_in_dim %7639, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7756)
      %7641 = stablehlo.multiply %7629, %7640 : tensor<32x17x5120xf32> loc(#loc7757)
      %7642 = stablehlo.convert %7641 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7758)
      %7643 = stablehlo.multiply %7514, %7642 : tensor<32x17x5120xbf16> loc(#loc7759)
      %7644 = stablehlo.reshape %7643 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7760)
      %7645 = stablehlo.reshape %arg1407 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7761)
      %7646 = stablehlo.reshape %7645 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7762)
      %7647 = stablehlo.transpose %7646, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7763)
      %7648 = stablehlo.dot_general %7644, %7647, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7764)
      %7649 = stablehlo.reshape %7648 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7765)
      %7650 = stablehlo.convert %7649 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc7766)
      %7651 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %7652 = stablehlo.power %7650, %7651 : tensor<32x17x1x128xf32> loc(#loc7767)
      %7653 = stablehlo.reduce(%7652 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc7768)
      %7654 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7655 = stablehlo.multiply %7653, %7654 : tensor<32x17x1xf32> loc(#loc7769)
      %7656 = stablehlo.reshape %7655 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc7770)
      %7657 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %7658 = stablehlo.add %7656, %7657 : tensor<32x17x1x1xf32> loc(#loc7771)
      %7659 = stablehlo.rsqrt %7658 : tensor<32x17x1x1xf32> loc(#loc7772)
      %7660 = stablehlo.reshape %7659 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc7773)
      %7661 = stablehlo.broadcast_in_dim %7660, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc7774)
      %7662 = stablehlo.multiply %7650, %7661 : tensor<32x17x1x128xf32> loc(#loc7775)
      %7663 = stablehlo.convert %7662 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc7776)
      %7664 = stablehlo.multiply %7511, %7663 : tensor<32x17x1x128xbf16> loc(#loc7777)
      %7665 = stablehlo.transpose %7664, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7778)
      %7666 = stablehlo.multiply %7665, %82 : tensor<32x1x17x128xbf16> loc(#loc7779)
      %7667 = stablehlo.slice %7665 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7780)
      %7668 = stablehlo.negate %7667 : tensor<32x1x17x64xbf16> loc(#loc7781)
      %7669 = stablehlo.slice %7665 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7782)
      %7670 = stablehlo.concatenate %7668, %7669, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7783)
      %7671 = stablehlo.multiply %7670, %91 : tensor<32x1x17x128xbf16> loc(#loc7784)
      %7672 = stablehlo.add %7666, %7671 : tensor<32x1x17x128xbf16> loc(#loc7785)
      %7673 = "stablehlo.scatter"(%arg1417, %21, %7672) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.17560"), %arg1677: tensor<bf16> loc("scatter.17560")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7786)
      %7674 = stablehlo.reshape %arg1418 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7787)
      %7675 = stablehlo.reshape %7674 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7788)
      %7676 = stablehlo.transpose %7675, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7789)
      %7677 = stablehlo.dot_general %7644, %7676, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7790)
      %7678 = stablehlo.reshape %7677 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7791)
      %7679 = stablehlo.transpose %7678, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7792)
      %7680 = "stablehlo.scatter"(%arg1419, %21, %7679) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.17590"), %arg1677: tensor<bf16> loc("scatter.17590")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7793)
      %7681 = stablehlo.reshape %arg1429 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7794)
      %7682 = stablehlo.reshape %7681 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7795)
      %7683 = stablehlo.broadcast_in_dim %7682, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7796)
      %7684 = stablehlo.reshape %arg1428 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7797)
      %7685 = stablehlo.reshape %7684 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7798)
      %7686 = stablehlo.broadcast_in_dim %7685, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7799)
      %7687 = stablehlo.reshape %arg1425 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7800)
      %7688 = stablehlo.reshape %7687 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7801)
      %7689 = stablehlo.broadcast_in_dim %7688, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7802)
      %7690 = stablehlo.reshape %arg1424 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc7803)
      %7691 = stablehlo.reshape %7690 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc7804)
      %7692 = stablehlo.transpose %7691, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc7805)
      %7693 = stablehlo.dot_general %7644, %7692, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc7806)
      %7694 = stablehlo.reshape %7693 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7807)
      %7695 = stablehlo.convert %7694 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc7808)
      %7696 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %7697 = stablehlo.power %7695, %7696 : tensor<32x17x8x128xf32> loc(#loc7809)
      %7698 = stablehlo.reduce(%7697 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc7810)
      %7699 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %7700 = stablehlo.multiply %7698, %7699 : tensor<32x17x8xf32> loc(#loc7811)
      %7701 = stablehlo.reshape %7700 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc7812)
      %7702 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %7703 = stablehlo.add %7701, %7702 : tensor<32x17x8x1xf32> loc(#loc7813)
      %7704 = stablehlo.rsqrt %7703 : tensor<32x17x8x1xf32> loc(#loc7814)
      %7705 = stablehlo.reshape %7704 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc7815)
      %7706 = stablehlo.broadcast_in_dim %7705, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc7816)
      %7707 = stablehlo.multiply %7695, %7706 : tensor<32x17x8x128xf32> loc(#loc7817)
      %7708 = stablehlo.convert %7707 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc7818)
      %7709 = stablehlo.multiply %7689, %7708 : tensor<32x17x8x128xbf16> loc(#loc7819)
      %7710 = stablehlo.transpose %7709, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7820)
      %7711 = stablehlo.multiply %7710, %132 : tensor<32x8x17x128xbf16> loc(#loc7821)
      %7712 = stablehlo.slice %7710 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7822)
      %7713 = stablehlo.negate %7712 : tensor<32x8x17x64xbf16> loc(#loc7823)
      %7714 = stablehlo.slice %7710 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7824)
      %7715 = stablehlo.concatenate %7713, %7714, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7825)
      %7716 = stablehlo.multiply %7715, %138 : tensor<32x8x17x128xbf16> loc(#loc7826)
      %7717 = stablehlo.add %7711, %7716 : tensor<32x8x17x128xbf16> loc(#loc7827)
      %7718 = stablehlo.convert %7717 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc7828)
      %7719 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7720 = stablehlo.multiply %7718, %7719 : tensor<32x8x17x128xf32> loc(#loc7829)
      %7721 = stablehlo.broadcast_in_dim %7673, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7830)
      %7722 = stablehlo.reshape %7721 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7831)
      %7723 = stablehlo.convert %7722 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7832)
      %7724 = stablehlo.transpose %7723, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc7833)
      %7725 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %7726 = stablehlo.multiply %7724, %7725 : tensor<32x8x128x128xf32> loc(#loc7834)
      %7727 = stablehlo.dot_general %7720, %7726, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7835)
      %7728 = stablehlo.add %7727, %159 : tensor<32x8x17x128xf32> loc(#loc7836)
      %7729 = stablehlo.convert %7728 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc7837)
      %7730 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %7731 = stablehlo.compare  EQ, %7729, %7730 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc7838)
      %7732 = stablehlo.not %7731 : tensor<32x8x17x128xi1> loc(#loc7839)
      %7733 = stablehlo.reduce(%7732 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.17771"), %arg1677: tensor<i1> loc("reduce.17771"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc7841)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc7842)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc7840)
      %7734 = stablehlo.reshape %7733 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc7843)
      %7735 = stablehlo.not %7734 : tensor<32x8x17x1xi1> loc(#loc7844)
      %7736 = stablehlo.reshape %7735 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc7845)
      %7737 = stablehlo.broadcast_in_dim %7736, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc7846)
      %7738 = stablehlo.reduce(%7728 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7847)
      %7739 = stablehlo.broadcast_in_dim %7738, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7848)
      %7740 = stablehlo.subtract %7728, %7739 : tensor<32x8x17x128xf32> loc(#loc7849)
      %7741 = stablehlo.exponential %7740 : tensor<32x8x17x128xf32> loc(#loc7850)
      %7742 = stablehlo.reduce(%7741 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc7851)
      %7743 = stablehlo.broadcast_in_dim %7742, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc7852)
      %7744 = stablehlo.divide %7741, %7743 : tensor<32x8x17x128xf32> loc(#loc7853)
      %7745 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7746 = stablehlo.select %7737, %7745, %7744 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc7854)
      %7747 = stablehlo.broadcast_in_dim %7680, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7855)
      %7748 = stablehlo.reshape %7747 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7856)
      %7749 = stablehlo.convert %7748 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7857)
      %7750 = stablehlo.dot_general %7746, %7749, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7858)
      %7751 = stablehlo.convert %7750 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc7859)
      %7752 = stablehlo.transpose %7751, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7860)
      %7753 = stablehlo.reshape %7752 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc7861)
      %7754 = stablehlo.reshape %arg1423 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc7862)
      %7755 = stablehlo.reshape %7754 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc7863)
      %7756 = stablehlo.transpose %7755, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc7864)
      %7757 = stablehlo.dot_general %7753, %7756, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7865)
      %7758 = "stablehlo.all_reduce"(%7757) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.17788"), %arg1677: tensor<bf16> loc("dot.17788")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7865)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7865)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7865)
      %7759 = stablehlo.reshape %7758 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7866)
      %7760 = stablehlo.add %7628, %7759 : tensor<32x17x5120xbf16> loc(#loc7867)
      %7761 = stablehlo.reshape %arg1426 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7868)
      %7762 = stablehlo.reshape %7761 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7869)
      %7763 = stablehlo.broadcast_in_dim %7762, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7870)
      %7764 = stablehlo.convert %7760 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7871)
      %7765 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7766 = stablehlo.power %7764, %7765 : tensor<32x17x5120xf32> loc(#loc7872)
      %7767 = stablehlo.reduce(%7766 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7873)
      %7768 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7769 = stablehlo.multiply %7767, %7768 : tensor<32x17xf32> loc(#loc7874)
      %7770 = stablehlo.reshape %7769 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7875)
      %7771 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7772 = stablehlo.add %7770, %7771 : tensor<32x17x1xf32> loc(#loc7876)
      %7773 = stablehlo.rsqrt %7772 : tensor<32x17x1xf32> loc(#loc7877)
      %7774 = stablehlo.reshape %7773 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7878)
      %7775 = stablehlo.broadcast_in_dim %7774, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7879)
      %7776 = stablehlo.multiply %7764, %7775 : tensor<32x17x5120xf32> loc(#loc7880)
      %7777 = stablehlo.convert %7776 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7881)
      %7778 = stablehlo.multiply %7763, %7777 : tensor<32x17x5120xbf16> loc(#loc7882)
      %7779 = stablehlo.reshape %7778 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7883)
      %7780 = stablehlo.reshape %arg1427 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7884)
      %7781 = stablehlo.reshape %7780 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7885)
      %7782 = stablehlo.transpose %7781, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7886)
      %7783 = stablehlo.dot_general %7779, %7782, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7887)
      %7784 = stablehlo.reshape %7783 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7888)
      %7785 = stablehlo.logistic %7784 : tensor<32x17x3200xbf16> loc(#loc7889)
      %7786 = stablehlo.multiply %7784, %7785 : tensor<32x17x3200xbf16> loc(#loc7890)
      %7787 = stablehlo.reshape %arg1422 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc7891)
      %7788 = stablehlo.reshape %7787 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc7892)
      %7789 = stablehlo.transpose %7788, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc7893)
      %7790 = stablehlo.dot_general %7779, %7789, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7894)
      %7791 = stablehlo.reshape %7790 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc7895)
      %7792 = stablehlo.multiply %7786, %7791 : tensor<32x17x3200xbf16> loc(#loc7896)
      %7793 = stablehlo.reshape %7792 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc7897)
      %7794 = stablehlo.reshape %arg1421 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc7898)
      %7795 = stablehlo.reshape %7794 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc7899)
      %7796 = stablehlo.transpose %7795, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc7900)
      %7797 = stablehlo.dot_general %7793, %7796, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7901)
      %7798 = "stablehlo.all_reduce"(%7797) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.17843"), %arg1677: tensor<bf16> loc("dot.17843")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc7901)
        stablehlo.return %11074 : tensor<bf16> loc(#loc7901)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7901)
      %7799 = stablehlo.reshape %7798 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7902)
      %7800 = stablehlo.add %7760, %7799 : tensor<32x17x5120xbf16> loc(#loc7903)
      %7801 = stablehlo.convert %7800 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc7904)
      %7802 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7803 = stablehlo.power %7801, %7802 : tensor<32x17x5120xf32> loc(#loc7905)
      %7804 = stablehlo.reduce(%7803 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc7906)
      %7805 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7806 = stablehlo.multiply %7804, %7805 : tensor<32x17xf32> loc(#loc7907)
      %7807 = stablehlo.reshape %7806 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc7908)
      %7808 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7809 = stablehlo.add %7807, %7808 : tensor<32x17x1xf32> loc(#loc7909)
      %7810 = stablehlo.rsqrt %7809 : tensor<32x17x1xf32> loc(#loc7910)
      %7811 = stablehlo.reshape %7810 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc7911)
      %7812 = stablehlo.broadcast_in_dim %7811, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc7912)
      %7813 = stablehlo.multiply %7801, %7812 : tensor<32x17x5120xf32> loc(#loc7913)
      %7814 = stablehlo.convert %7813 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc7914)
      %7815 = stablehlo.multiply %7686, %7814 : tensor<32x17x5120xbf16> loc(#loc7915)
      %7816 = stablehlo.reshape %7815 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc7916)
      %7817 = stablehlo.reshape %arg1420 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7917)
      %7818 = stablehlo.reshape %7817 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7918)
      %7819 = stablehlo.transpose %7818, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7919)
      %7820 = stablehlo.dot_general %7816, %7819, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7920)
      %7821 = stablehlo.reshape %7820 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7921)
      %7822 = stablehlo.convert %7821 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc7922)
      %7823 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %7824 = stablehlo.power %7822, %7823 : tensor<32x17x1x128xf32> loc(#loc7923)
      %7825 = stablehlo.reduce(%7824 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc7924)
      %7826 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7827 = stablehlo.multiply %7825, %7826 : tensor<32x17x1xf32> loc(#loc7925)
      %7828 = stablehlo.reshape %7827 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc7926)
      %7829 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %7830 = stablehlo.add %7828, %7829 : tensor<32x17x1x1xf32> loc(#loc7927)
      %7831 = stablehlo.rsqrt %7830 : tensor<32x17x1x1xf32> loc(#loc7928)
      %7832 = stablehlo.reshape %7831 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc7929)
      %7833 = stablehlo.broadcast_in_dim %7832, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc7930)
      %7834 = stablehlo.multiply %7822, %7833 : tensor<32x17x1x128xf32> loc(#loc7931)
      %7835 = stablehlo.convert %7834 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc7932)
      %7836 = stablehlo.multiply %7683, %7835 : tensor<32x17x1x128xbf16> loc(#loc7933)
      %7837 = stablehlo.transpose %7836, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7934)
      %7838 = stablehlo.multiply %7837, %82 : tensor<32x1x17x128xbf16> loc(#loc7935)
      %7839 = stablehlo.slice %7837 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7936)
      %7840 = stablehlo.negate %7839 : tensor<32x1x17x64xbf16> loc(#loc7937)
      %7841 = stablehlo.slice %7837 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc7938)
      %7842 = stablehlo.concatenate %7840, %7841, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7939)
      %7843 = stablehlo.multiply %7842, %91 : tensor<32x1x17x128xbf16> loc(#loc7940)
      %7844 = stablehlo.add %7838, %7843 : tensor<32x1x17x128xbf16> loc(#loc7941)
      %7845 = "stablehlo.scatter"(%arg1430, %21, %7844) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.17955"), %arg1677: tensor<bf16> loc("scatter.17955")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7942)
      %7846 = stablehlo.reshape %arg1431 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc7943)
      %7847 = stablehlo.reshape %7846 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc7944)
      %7848 = stablehlo.transpose %7847, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc7945)
      %7849 = stablehlo.dot_general %7816, %7848, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc7946)
      %7850 = stablehlo.reshape %7849 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7947)
      %7851 = stablehlo.transpose %7850, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc7948)
      %7852 = "stablehlo.scatter"(%arg1432, %21, %7851) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.17985"), %arg1677: tensor<bf16> loc("scatter.17985")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc7949)
      %7853 = stablehlo.reshape %arg1442 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7950)
      %7854 = stablehlo.reshape %7853 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7951)
      %7855 = stablehlo.broadcast_in_dim %7854, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc7952)
      %7856 = stablehlo.reshape %arg1441 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc7953)
      %7857 = stablehlo.reshape %7856 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc7954)
      %7858 = stablehlo.broadcast_in_dim %7857, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc7955)
      %7859 = stablehlo.reshape %arg1438 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc7956)
      %7860 = stablehlo.reshape %7859 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc7957)
      %7861 = stablehlo.broadcast_in_dim %7860, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7958)
      %7862 = stablehlo.reshape %arg1437 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc7959)
      %7863 = stablehlo.reshape %7862 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc7960)
      %7864 = stablehlo.transpose %7863, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc7961)
      %7865 = stablehlo.dot_general %7816, %7864, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc7962)
      %7866 = stablehlo.reshape %7865 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc7963)
      %7867 = stablehlo.convert %7866 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc7964)
      %7868 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %7869 = stablehlo.power %7867, %7868 : tensor<32x17x8x128xf32> loc(#loc7965)
      %7870 = stablehlo.reduce(%7869 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc7966)
      %7871 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %7872 = stablehlo.multiply %7870, %7871 : tensor<32x17x8xf32> loc(#loc7967)
      %7873 = stablehlo.reshape %7872 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc7968)
      %7874 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %7875 = stablehlo.add %7873, %7874 : tensor<32x17x8x1xf32> loc(#loc7969)
      %7876 = stablehlo.rsqrt %7875 : tensor<32x17x8x1xf32> loc(#loc7970)
      %7877 = stablehlo.reshape %7876 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc7971)
      %7878 = stablehlo.broadcast_in_dim %7877, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc7972)
      %7879 = stablehlo.multiply %7867, %7878 : tensor<32x17x8x128xf32> loc(#loc7973)
      %7880 = stablehlo.convert %7879 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc7974)
      %7881 = stablehlo.multiply %7861, %7880 : tensor<32x17x8x128xbf16> loc(#loc7975)
      %7882 = stablehlo.transpose %7881, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7976)
      %7883 = stablehlo.multiply %7882, %132 : tensor<32x8x17x128xbf16> loc(#loc7977)
      %7884 = stablehlo.slice %7882 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7978)
      %7885 = stablehlo.negate %7884 : tensor<32x8x17x64xbf16> loc(#loc7979)
      %7886 = stablehlo.slice %7882 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc7980)
      %7887 = stablehlo.concatenate %7885, %7886, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc7981)
      %7888 = stablehlo.multiply %7887, %138 : tensor<32x8x17x128xbf16> loc(#loc7982)
      %7889 = stablehlo.add %7883, %7888 : tensor<32x8x17x128xbf16> loc(#loc7983)
      %7890 = stablehlo.convert %7889 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc7984)
      %7891 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7892 = stablehlo.multiply %7890, %7891 : tensor<32x8x17x128xf32> loc(#loc7985)
      %7893 = stablehlo.broadcast_in_dim %7845, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc7986)
      %7894 = stablehlo.reshape %7893 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc7987)
      %7895 = stablehlo.convert %7894 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc7988)
      %7896 = stablehlo.transpose %7895, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc7989)
      %7897 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %7898 = stablehlo.multiply %7896, %7897 : tensor<32x8x128x128xf32> loc(#loc7990)
      %7899 = stablehlo.dot_general %7892, %7898, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc7991)
      %7900 = stablehlo.add %7899, %159 : tensor<32x8x17x128xf32> loc(#loc7992)
      %7901 = stablehlo.convert %7900 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc7993)
      %7902 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %7903 = stablehlo.compare  EQ, %7901, %7902 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc7994)
      %7904 = stablehlo.not %7903 : tensor<32x8x17x128xi1> loc(#loc7995)
      %7905 = stablehlo.reduce(%7904 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.18166"), %arg1677: tensor<i1> loc("reduce.18166"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc7997)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc7998)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc7996)
      %7906 = stablehlo.reshape %7905 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc7999)
      %7907 = stablehlo.not %7906 : tensor<32x8x17x1xi1> loc(#loc8000)
      %7908 = stablehlo.reshape %7907 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc8001)
      %7909 = stablehlo.broadcast_in_dim %7908, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc8002)
      %7910 = stablehlo.reduce(%7900 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8003)
      %7911 = stablehlo.broadcast_in_dim %7910, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8004)
      %7912 = stablehlo.subtract %7900, %7911 : tensor<32x8x17x128xf32> loc(#loc8005)
      %7913 = stablehlo.exponential %7912 : tensor<32x8x17x128xf32> loc(#loc8006)
      %7914 = stablehlo.reduce(%7913 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8007)
      %7915 = stablehlo.broadcast_in_dim %7914, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8008)
      %7916 = stablehlo.divide %7913, %7915 : tensor<32x8x17x128xf32> loc(#loc8009)
      %7917 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %7918 = stablehlo.select %7909, %7917, %7916 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc8010)
      %7919 = stablehlo.broadcast_in_dim %7852, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8011)
      %7920 = stablehlo.reshape %7919 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8012)
      %7921 = stablehlo.convert %7920 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8013)
      %7922 = stablehlo.dot_general %7918, %7921, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8014)
      %7923 = stablehlo.convert %7922 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc8015)
      %7924 = stablehlo.transpose %7923, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8016)
      %7925 = stablehlo.reshape %7924 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc8017)
      %7926 = stablehlo.reshape %arg1436 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc8018)
      %7927 = stablehlo.reshape %7926 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc8019)
      %7928 = stablehlo.transpose %7927, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc8020)
      %7929 = stablehlo.dot_general %7925, %7928, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8021)
      %7930 = "stablehlo.all_reduce"(%7929) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.18183"), %arg1677: tensor<bf16> loc("dot.18183")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8021)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8021)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8021)
      %7931 = stablehlo.reshape %7930 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8022)
      %7932 = stablehlo.add %7800, %7931 : tensor<32x17x5120xbf16> loc(#loc8023)
      %7933 = stablehlo.reshape %arg1439 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8024)
      %7934 = stablehlo.reshape %7933 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8025)
      %7935 = stablehlo.broadcast_in_dim %7934, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8026)
      %7936 = stablehlo.convert %7932 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8027)
      %7937 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7938 = stablehlo.power %7936, %7937 : tensor<32x17x5120xf32> loc(#loc8028)
      %7939 = stablehlo.reduce(%7938 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8029)
      %7940 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7941 = stablehlo.multiply %7939, %7940 : tensor<32x17xf32> loc(#loc8030)
      %7942 = stablehlo.reshape %7941 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8031)
      %7943 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7944 = stablehlo.add %7942, %7943 : tensor<32x17x1xf32> loc(#loc8032)
      %7945 = stablehlo.rsqrt %7944 : tensor<32x17x1xf32> loc(#loc8033)
      %7946 = stablehlo.reshape %7945 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8034)
      %7947 = stablehlo.broadcast_in_dim %7946, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8035)
      %7948 = stablehlo.multiply %7936, %7947 : tensor<32x17x5120xf32> loc(#loc8036)
      %7949 = stablehlo.convert %7948 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8037)
      %7950 = stablehlo.multiply %7935, %7949 : tensor<32x17x5120xbf16> loc(#loc8038)
      %7951 = stablehlo.reshape %7950 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8039)
      %7952 = stablehlo.reshape %arg1440 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8040)
      %7953 = stablehlo.reshape %7952 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8041)
      %7954 = stablehlo.transpose %7953, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8042)
      %7955 = stablehlo.dot_general %7951, %7954, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8043)
      %7956 = stablehlo.reshape %7955 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8044)
      %7957 = stablehlo.logistic %7956 : tensor<32x17x3200xbf16> loc(#loc8045)
      %7958 = stablehlo.multiply %7956, %7957 : tensor<32x17x3200xbf16> loc(#loc8046)
      %7959 = stablehlo.reshape %arg1435 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8047)
      %7960 = stablehlo.reshape %7959 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8048)
      %7961 = stablehlo.transpose %7960, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8049)
      %7962 = stablehlo.dot_general %7951, %7961, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8050)
      %7963 = stablehlo.reshape %7962 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8051)
      %7964 = stablehlo.multiply %7958, %7963 : tensor<32x17x3200xbf16> loc(#loc8052)
      %7965 = stablehlo.reshape %7964 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8053)
      %7966 = stablehlo.reshape %arg1434 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc8054)
      %7967 = stablehlo.reshape %7966 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc8055)
      %7968 = stablehlo.transpose %7967, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc8056)
      %7969 = stablehlo.dot_general %7965, %7968, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8057)
      %7970 = "stablehlo.all_reduce"(%7969) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.18238"), %arg1677: tensor<bf16> loc("dot.18238")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8057)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8057)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8057)
      %7971 = stablehlo.reshape %7970 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8058)
      %7972 = stablehlo.add %7932, %7971 : tensor<32x17x5120xbf16> loc(#loc8059)
      %7973 = stablehlo.convert %7972 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8060)
      %7974 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %7975 = stablehlo.power %7973, %7974 : tensor<32x17x5120xf32> loc(#loc8061)
      %7976 = stablehlo.reduce(%7975 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8062)
      %7977 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %7978 = stablehlo.multiply %7976, %7977 : tensor<32x17xf32> loc(#loc8063)
      %7979 = stablehlo.reshape %7978 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8064)
      %7980 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7981 = stablehlo.add %7979, %7980 : tensor<32x17x1xf32> loc(#loc8065)
      %7982 = stablehlo.rsqrt %7981 : tensor<32x17x1xf32> loc(#loc8066)
      %7983 = stablehlo.reshape %7982 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8067)
      %7984 = stablehlo.broadcast_in_dim %7983, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8068)
      %7985 = stablehlo.multiply %7973, %7984 : tensor<32x17x5120xf32> loc(#loc8069)
      %7986 = stablehlo.convert %7985 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8070)
      %7987 = stablehlo.multiply %7858, %7986 : tensor<32x17x5120xbf16> loc(#loc8071)
      %7988 = stablehlo.reshape %7987 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8072)
      %7989 = stablehlo.reshape %arg1433 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8073)
      %7990 = stablehlo.reshape %7989 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8074)
      %7991 = stablehlo.transpose %7990, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8075)
      %7992 = stablehlo.dot_general %7988, %7991, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8076)
      %7993 = stablehlo.reshape %7992 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8077)
      %7994 = stablehlo.convert %7993 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc8078)
      %7995 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %7996 = stablehlo.power %7994, %7995 : tensor<32x17x1x128xf32> loc(#loc8079)
      %7997 = stablehlo.reduce(%7996 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc8080)
      %7998 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %7999 = stablehlo.multiply %7997, %7998 : tensor<32x17x1xf32> loc(#loc8081)
      %8000 = stablehlo.reshape %7999 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc8082)
      %8001 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %8002 = stablehlo.add %8000, %8001 : tensor<32x17x1x1xf32> loc(#loc8083)
      %8003 = stablehlo.rsqrt %8002 : tensor<32x17x1x1xf32> loc(#loc8084)
      %8004 = stablehlo.reshape %8003 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc8085)
      %8005 = stablehlo.broadcast_in_dim %8004, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc8086)
      %8006 = stablehlo.multiply %7994, %8005 : tensor<32x17x1x128xf32> loc(#loc8087)
      %8007 = stablehlo.convert %8006 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc8088)
      %8008 = stablehlo.multiply %7855, %8007 : tensor<32x17x1x128xbf16> loc(#loc8089)
      %8009 = stablehlo.transpose %8008, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8090)
      %8010 = stablehlo.multiply %8009, %82 : tensor<32x1x17x128xbf16> loc(#loc8091)
      %8011 = stablehlo.slice %8009 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8092)
      %8012 = stablehlo.negate %8011 : tensor<32x1x17x64xbf16> loc(#loc8093)
      %8013 = stablehlo.slice %8009 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8094)
      %8014 = stablehlo.concatenate %8012, %8013, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8095)
      %8015 = stablehlo.multiply %8014, %91 : tensor<32x1x17x128xbf16> loc(#loc8096)
      %8016 = stablehlo.add %8010, %8015 : tensor<32x1x17x128xbf16> loc(#loc8097)
      %8017 = "stablehlo.scatter"(%arg1443, %21, %8016) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.18350"), %arg1677: tensor<bf16> loc("scatter.18350")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8098)
      %8018 = stablehlo.reshape %arg1444 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8099)
      %8019 = stablehlo.reshape %8018 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8100)
      %8020 = stablehlo.transpose %8019, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8101)
      %8021 = stablehlo.dot_general %7988, %8020, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8102)
      %8022 = stablehlo.reshape %8021 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8103)
      %8023 = stablehlo.transpose %8022, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8104)
      %8024 = "stablehlo.scatter"(%arg1445, %21, %8023) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.18380"), %arg1677: tensor<bf16> loc("scatter.18380")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8105)
      %8025 = stablehlo.reshape %arg1455 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8106)
      %8026 = stablehlo.reshape %8025 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8107)
      %8027 = stablehlo.broadcast_in_dim %8026, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8108)
      %8028 = stablehlo.reshape %arg1454 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8109)
      %8029 = stablehlo.reshape %8028 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8110)
      %8030 = stablehlo.broadcast_in_dim %8029, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8111)
      %8031 = stablehlo.reshape %arg1451 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8112)
      %8032 = stablehlo.reshape %8031 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8113)
      %8033 = stablehlo.broadcast_in_dim %8032, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8114)
      %8034 = stablehlo.reshape %arg1450 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc8115)
      %8035 = stablehlo.reshape %8034 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc8116)
      %8036 = stablehlo.transpose %8035, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc8117)
      %8037 = stablehlo.dot_general %7988, %8036, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc8118)
      %8038 = stablehlo.reshape %8037 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8119)
      %8039 = stablehlo.convert %8038 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc8120)
      %8040 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %8041 = stablehlo.power %8039, %8040 : tensor<32x17x8x128xf32> loc(#loc8121)
      %8042 = stablehlo.reduce(%8041 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc8122)
      %8043 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %8044 = stablehlo.multiply %8042, %8043 : tensor<32x17x8xf32> loc(#loc8123)
      %8045 = stablehlo.reshape %8044 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc8124)
      %8046 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %8047 = stablehlo.add %8045, %8046 : tensor<32x17x8x1xf32> loc(#loc8125)
      %8048 = stablehlo.rsqrt %8047 : tensor<32x17x8x1xf32> loc(#loc8126)
      %8049 = stablehlo.reshape %8048 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc8127)
      %8050 = stablehlo.broadcast_in_dim %8049, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc8128)
      %8051 = stablehlo.multiply %8039, %8050 : tensor<32x17x8x128xf32> loc(#loc8129)
      %8052 = stablehlo.convert %8051 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc8130)
      %8053 = stablehlo.multiply %8033, %8052 : tensor<32x17x8x128xbf16> loc(#loc8131)
      %8054 = stablehlo.transpose %8053, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8132)
      %8055 = stablehlo.multiply %8054, %132 : tensor<32x8x17x128xbf16> loc(#loc8133)
      %8056 = stablehlo.slice %8054 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8134)
      %8057 = stablehlo.negate %8056 : tensor<32x8x17x64xbf16> loc(#loc8135)
      %8058 = stablehlo.slice %8054 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8136)
      %8059 = stablehlo.concatenate %8057, %8058, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8137)
      %8060 = stablehlo.multiply %8059, %138 : tensor<32x8x17x128xbf16> loc(#loc8138)
      %8061 = stablehlo.add %8055, %8060 : tensor<32x8x17x128xbf16> loc(#loc8139)
      %8062 = stablehlo.convert %8061 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc8140)
      %8063 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8064 = stablehlo.multiply %8062, %8063 : tensor<32x8x17x128xf32> loc(#loc8141)
      %8065 = stablehlo.broadcast_in_dim %8017, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8142)
      %8066 = stablehlo.reshape %8065 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8143)
      %8067 = stablehlo.convert %8066 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8144)
      %8068 = stablehlo.transpose %8067, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc8145)
      %8069 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %8070 = stablehlo.multiply %8068, %8069 : tensor<32x8x128x128xf32> loc(#loc8146)
      %8071 = stablehlo.dot_general %8064, %8070, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8147)
      %8072 = stablehlo.add %8071, %159 : tensor<32x8x17x128xf32> loc(#loc8148)
      %8073 = stablehlo.convert %8072 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc8149)
      %8074 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %8075 = stablehlo.compare  EQ, %8073, %8074 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc8150)
      %8076 = stablehlo.not %8075 : tensor<32x8x17x128xi1> loc(#loc8151)
      %8077 = stablehlo.reduce(%8076 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.18561"), %arg1677: tensor<i1> loc("reduce.18561"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc8153)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc8154)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc8152)
      %8078 = stablehlo.reshape %8077 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc8155)
      %8079 = stablehlo.not %8078 : tensor<32x8x17x1xi1> loc(#loc8156)
      %8080 = stablehlo.reshape %8079 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc8157)
      %8081 = stablehlo.broadcast_in_dim %8080, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc8158)
      %8082 = stablehlo.reduce(%8072 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8159)
      %8083 = stablehlo.broadcast_in_dim %8082, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8160)
      %8084 = stablehlo.subtract %8072, %8083 : tensor<32x8x17x128xf32> loc(#loc8161)
      %8085 = stablehlo.exponential %8084 : tensor<32x8x17x128xf32> loc(#loc8162)
      %8086 = stablehlo.reduce(%8085 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8163)
      %8087 = stablehlo.broadcast_in_dim %8086, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8164)
      %8088 = stablehlo.divide %8085, %8087 : tensor<32x8x17x128xf32> loc(#loc8165)
      %8089 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8090 = stablehlo.select %8081, %8089, %8088 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc8166)
      %8091 = stablehlo.broadcast_in_dim %8024, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8167)
      %8092 = stablehlo.reshape %8091 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8168)
      %8093 = stablehlo.convert %8092 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8169)
      %8094 = stablehlo.dot_general %8090, %8093, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8170)
      %8095 = stablehlo.convert %8094 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc8171)
      %8096 = stablehlo.transpose %8095, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8172)
      %8097 = stablehlo.reshape %8096 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc8173)
      %8098 = stablehlo.reshape %arg1449 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc8174)
      %8099 = stablehlo.reshape %8098 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc8175)
      %8100 = stablehlo.transpose %8099, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc8176)
      %8101 = stablehlo.dot_general %8097, %8100, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8177)
      %8102 = "stablehlo.all_reduce"(%8101) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.18578"), %arg1677: tensor<bf16> loc("dot.18578")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8177)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8177)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8177)
      %8103 = stablehlo.reshape %8102 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8178)
      %8104 = stablehlo.add %7972, %8103 : tensor<32x17x5120xbf16> loc(#loc8179)
      %8105 = stablehlo.reshape %arg1452 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8180)
      %8106 = stablehlo.reshape %8105 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8181)
      %8107 = stablehlo.broadcast_in_dim %8106, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8182)
      %8108 = stablehlo.convert %8104 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8183)
      %8109 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8110 = stablehlo.power %8108, %8109 : tensor<32x17x5120xf32> loc(#loc8184)
      %8111 = stablehlo.reduce(%8110 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8185)
      %8112 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8113 = stablehlo.multiply %8111, %8112 : tensor<32x17xf32> loc(#loc8186)
      %8114 = stablehlo.reshape %8113 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8187)
      %8115 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8116 = stablehlo.add %8114, %8115 : tensor<32x17x1xf32> loc(#loc8188)
      %8117 = stablehlo.rsqrt %8116 : tensor<32x17x1xf32> loc(#loc8189)
      %8118 = stablehlo.reshape %8117 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8190)
      %8119 = stablehlo.broadcast_in_dim %8118, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8191)
      %8120 = stablehlo.multiply %8108, %8119 : tensor<32x17x5120xf32> loc(#loc8192)
      %8121 = stablehlo.convert %8120 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8193)
      %8122 = stablehlo.multiply %8107, %8121 : tensor<32x17x5120xbf16> loc(#loc8194)
      %8123 = stablehlo.reshape %8122 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8195)
      %8124 = stablehlo.reshape %arg1453 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8196)
      %8125 = stablehlo.reshape %8124 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8197)
      %8126 = stablehlo.transpose %8125, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8198)
      %8127 = stablehlo.dot_general %8123, %8126, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8199)
      %8128 = stablehlo.reshape %8127 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8200)
      %8129 = stablehlo.logistic %8128 : tensor<32x17x3200xbf16> loc(#loc8201)
      %8130 = stablehlo.multiply %8128, %8129 : tensor<32x17x3200xbf16> loc(#loc8202)
      %8131 = stablehlo.reshape %arg1448 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8203)
      %8132 = stablehlo.reshape %8131 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8204)
      %8133 = stablehlo.transpose %8132, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8205)
      %8134 = stablehlo.dot_general %8123, %8133, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8206)
      %8135 = stablehlo.reshape %8134 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8207)
      %8136 = stablehlo.multiply %8130, %8135 : tensor<32x17x3200xbf16> loc(#loc8208)
      %8137 = stablehlo.reshape %8136 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8209)
      %8138 = stablehlo.reshape %arg1447 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc8210)
      %8139 = stablehlo.reshape %8138 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc8211)
      %8140 = stablehlo.transpose %8139, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc8212)
      %8141 = stablehlo.dot_general %8137, %8140, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8213)
      %8142 = "stablehlo.all_reduce"(%8141) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.18633"), %arg1677: tensor<bf16> loc("dot.18633")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8213)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8213)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8213)
      %8143 = stablehlo.reshape %8142 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8214)
      %8144 = stablehlo.add %8104, %8143 : tensor<32x17x5120xbf16> loc(#loc8215)
      %8145 = stablehlo.convert %8144 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8216)
      %8146 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8147 = stablehlo.power %8145, %8146 : tensor<32x17x5120xf32> loc(#loc8217)
      %8148 = stablehlo.reduce(%8147 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8218)
      %8149 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8150 = stablehlo.multiply %8148, %8149 : tensor<32x17xf32> loc(#loc8219)
      %8151 = stablehlo.reshape %8150 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8220)
      %8152 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8153 = stablehlo.add %8151, %8152 : tensor<32x17x1xf32> loc(#loc8221)
      %8154 = stablehlo.rsqrt %8153 : tensor<32x17x1xf32> loc(#loc8222)
      %8155 = stablehlo.reshape %8154 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8223)
      %8156 = stablehlo.broadcast_in_dim %8155, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8224)
      %8157 = stablehlo.multiply %8145, %8156 : tensor<32x17x5120xf32> loc(#loc8225)
      %8158 = stablehlo.convert %8157 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8226)
      %8159 = stablehlo.multiply %8030, %8158 : tensor<32x17x5120xbf16> loc(#loc8227)
      %8160 = stablehlo.reshape %8159 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8228)
      %8161 = stablehlo.reshape %arg1446 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8229)
      %8162 = stablehlo.reshape %8161 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8230)
      %8163 = stablehlo.transpose %8162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8231)
      %8164 = stablehlo.dot_general %8160, %8163, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8232)
      %8165 = stablehlo.reshape %8164 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8233)
      %8166 = stablehlo.convert %8165 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc8234)
      %8167 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %8168 = stablehlo.power %8166, %8167 : tensor<32x17x1x128xf32> loc(#loc8235)
      %8169 = stablehlo.reduce(%8168 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc8236)
      %8170 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8171 = stablehlo.multiply %8169, %8170 : tensor<32x17x1xf32> loc(#loc8237)
      %8172 = stablehlo.reshape %8171 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc8238)
      %8173 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %8174 = stablehlo.add %8172, %8173 : tensor<32x17x1x1xf32> loc(#loc8239)
      %8175 = stablehlo.rsqrt %8174 : tensor<32x17x1x1xf32> loc(#loc8240)
      %8176 = stablehlo.reshape %8175 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc8241)
      %8177 = stablehlo.broadcast_in_dim %8176, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc8242)
      %8178 = stablehlo.multiply %8166, %8177 : tensor<32x17x1x128xf32> loc(#loc8243)
      %8179 = stablehlo.convert %8178 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc8244)
      %8180 = stablehlo.multiply %8027, %8179 : tensor<32x17x1x128xbf16> loc(#loc8245)
      %8181 = stablehlo.transpose %8180, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8246)
      %8182 = stablehlo.multiply %8181, %82 : tensor<32x1x17x128xbf16> loc(#loc8247)
      %8183 = stablehlo.slice %8181 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8248)
      %8184 = stablehlo.negate %8183 : tensor<32x1x17x64xbf16> loc(#loc8249)
      %8185 = stablehlo.slice %8181 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8250)
      %8186 = stablehlo.concatenate %8184, %8185, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8251)
      %8187 = stablehlo.multiply %8186, %91 : tensor<32x1x17x128xbf16> loc(#loc8252)
      %8188 = stablehlo.add %8182, %8187 : tensor<32x1x17x128xbf16> loc(#loc8253)
      %8189 = "stablehlo.scatter"(%arg1456, %21, %8188) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.18745"), %arg1677: tensor<bf16> loc("scatter.18745")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8254)
      %8190 = stablehlo.reshape %arg1457 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8255)
      %8191 = stablehlo.reshape %8190 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8256)
      %8192 = stablehlo.transpose %8191, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8257)
      %8193 = stablehlo.dot_general %8160, %8192, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8258)
      %8194 = stablehlo.reshape %8193 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8259)
      %8195 = stablehlo.transpose %8194, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8260)
      %8196 = "stablehlo.scatter"(%arg1458, %21, %8195) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.18775"), %arg1677: tensor<bf16> loc("scatter.18775")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8261)
      %8197 = stablehlo.reshape %arg1468 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8262)
      %8198 = stablehlo.reshape %8197 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8263)
      %8199 = stablehlo.broadcast_in_dim %8198, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8264)
      %8200 = stablehlo.reshape %arg1467 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8265)
      %8201 = stablehlo.reshape %8200 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8266)
      %8202 = stablehlo.broadcast_in_dim %8201, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8267)
      %8203 = stablehlo.reshape %arg1464 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8268)
      %8204 = stablehlo.reshape %8203 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8269)
      %8205 = stablehlo.broadcast_in_dim %8204, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8270)
      %8206 = stablehlo.reshape %arg1463 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc8271)
      %8207 = stablehlo.reshape %8206 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc8272)
      %8208 = stablehlo.transpose %8207, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc8273)
      %8209 = stablehlo.dot_general %8160, %8208, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc8274)
      %8210 = stablehlo.reshape %8209 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8275)
      %8211 = stablehlo.convert %8210 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc8276)
      %8212 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %8213 = stablehlo.power %8211, %8212 : tensor<32x17x8x128xf32> loc(#loc8277)
      %8214 = stablehlo.reduce(%8213 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc8278)
      %8215 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %8216 = stablehlo.multiply %8214, %8215 : tensor<32x17x8xf32> loc(#loc8279)
      %8217 = stablehlo.reshape %8216 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc8280)
      %8218 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %8219 = stablehlo.add %8217, %8218 : tensor<32x17x8x1xf32> loc(#loc8281)
      %8220 = stablehlo.rsqrt %8219 : tensor<32x17x8x1xf32> loc(#loc8282)
      %8221 = stablehlo.reshape %8220 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc8283)
      %8222 = stablehlo.broadcast_in_dim %8221, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc8284)
      %8223 = stablehlo.multiply %8211, %8222 : tensor<32x17x8x128xf32> loc(#loc8285)
      %8224 = stablehlo.convert %8223 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc8286)
      %8225 = stablehlo.multiply %8205, %8224 : tensor<32x17x8x128xbf16> loc(#loc8287)
      %8226 = stablehlo.transpose %8225, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8288)
      %8227 = stablehlo.multiply %8226, %132 : tensor<32x8x17x128xbf16> loc(#loc8289)
      %8228 = stablehlo.slice %8226 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8290)
      %8229 = stablehlo.negate %8228 : tensor<32x8x17x64xbf16> loc(#loc8291)
      %8230 = stablehlo.slice %8226 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8292)
      %8231 = stablehlo.concatenate %8229, %8230, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8293)
      %8232 = stablehlo.multiply %8231, %138 : tensor<32x8x17x128xbf16> loc(#loc8294)
      %8233 = stablehlo.add %8227, %8232 : tensor<32x8x17x128xbf16> loc(#loc8295)
      %8234 = stablehlo.convert %8233 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc8296)
      %8235 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8236 = stablehlo.multiply %8234, %8235 : tensor<32x8x17x128xf32> loc(#loc8297)
      %8237 = stablehlo.broadcast_in_dim %8189, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8298)
      %8238 = stablehlo.reshape %8237 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8299)
      %8239 = stablehlo.convert %8238 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8300)
      %8240 = stablehlo.transpose %8239, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc8301)
      %8241 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %8242 = stablehlo.multiply %8240, %8241 : tensor<32x8x128x128xf32> loc(#loc8302)
      %8243 = stablehlo.dot_general %8236, %8242, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8303)
      %8244 = stablehlo.add %8243, %159 : tensor<32x8x17x128xf32> loc(#loc8304)
      %8245 = stablehlo.convert %8244 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc8305)
      %8246 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %8247 = stablehlo.compare  EQ, %8245, %8246 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc8306)
      %8248 = stablehlo.not %8247 : tensor<32x8x17x128xi1> loc(#loc8307)
      %8249 = stablehlo.reduce(%8248 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.18956"), %arg1677: tensor<i1> loc("reduce.18956"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc8309)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc8310)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc8308)
      %8250 = stablehlo.reshape %8249 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc8311)
      %8251 = stablehlo.not %8250 : tensor<32x8x17x1xi1> loc(#loc8312)
      %8252 = stablehlo.reshape %8251 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc8313)
      %8253 = stablehlo.broadcast_in_dim %8252, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc8314)
      %8254 = stablehlo.reduce(%8244 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8315)
      %8255 = stablehlo.broadcast_in_dim %8254, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8316)
      %8256 = stablehlo.subtract %8244, %8255 : tensor<32x8x17x128xf32> loc(#loc8317)
      %8257 = stablehlo.exponential %8256 : tensor<32x8x17x128xf32> loc(#loc8318)
      %8258 = stablehlo.reduce(%8257 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8319)
      %8259 = stablehlo.broadcast_in_dim %8258, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8320)
      %8260 = stablehlo.divide %8257, %8259 : tensor<32x8x17x128xf32> loc(#loc8321)
      %8261 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8262 = stablehlo.select %8253, %8261, %8260 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc8322)
      %8263 = stablehlo.broadcast_in_dim %8196, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8323)
      %8264 = stablehlo.reshape %8263 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8324)
      %8265 = stablehlo.convert %8264 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8325)
      %8266 = stablehlo.dot_general %8262, %8265, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8326)
      %8267 = stablehlo.convert %8266 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc8327)
      %8268 = stablehlo.transpose %8267, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8328)
      %8269 = stablehlo.reshape %8268 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc8329)
      %8270 = stablehlo.reshape %arg1462 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc8330)
      %8271 = stablehlo.reshape %8270 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc8331)
      %8272 = stablehlo.transpose %8271, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc8332)
      %8273 = stablehlo.dot_general %8269, %8272, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8333)
      %8274 = "stablehlo.all_reduce"(%8273) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.18973"), %arg1677: tensor<bf16> loc("dot.18973")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8333)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8333)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8333)
      %8275 = stablehlo.reshape %8274 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8334)
      %8276 = stablehlo.add %8144, %8275 : tensor<32x17x5120xbf16> loc(#loc8335)
      %8277 = stablehlo.reshape %arg1465 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8336)
      %8278 = stablehlo.reshape %8277 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8337)
      %8279 = stablehlo.broadcast_in_dim %8278, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8338)
      %8280 = stablehlo.convert %8276 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8339)
      %8281 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8282 = stablehlo.power %8280, %8281 : tensor<32x17x5120xf32> loc(#loc8340)
      %8283 = stablehlo.reduce(%8282 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8341)
      %8284 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8285 = stablehlo.multiply %8283, %8284 : tensor<32x17xf32> loc(#loc8342)
      %8286 = stablehlo.reshape %8285 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8343)
      %8287 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8288 = stablehlo.add %8286, %8287 : tensor<32x17x1xf32> loc(#loc8344)
      %8289 = stablehlo.rsqrt %8288 : tensor<32x17x1xf32> loc(#loc8345)
      %8290 = stablehlo.reshape %8289 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8346)
      %8291 = stablehlo.broadcast_in_dim %8290, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8347)
      %8292 = stablehlo.multiply %8280, %8291 : tensor<32x17x5120xf32> loc(#loc8348)
      %8293 = stablehlo.convert %8292 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8349)
      %8294 = stablehlo.multiply %8279, %8293 : tensor<32x17x5120xbf16> loc(#loc8350)
      %8295 = stablehlo.reshape %8294 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8351)
      %8296 = stablehlo.reshape %arg1466 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8352)
      %8297 = stablehlo.reshape %8296 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8353)
      %8298 = stablehlo.transpose %8297, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8354)
      %8299 = stablehlo.dot_general %8295, %8298, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8355)
      %8300 = stablehlo.reshape %8299 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8356)
      %8301 = stablehlo.logistic %8300 : tensor<32x17x3200xbf16> loc(#loc8357)
      %8302 = stablehlo.multiply %8300, %8301 : tensor<32x17x3200xbf16> loc(#loc8358)
      %8303 = stablehlo.reshape %arg1461 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8359)
      %8304 = stablehlo.reshape %8303 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8360)
      %8305 = stablehlo.transpose %8304, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8361)
      %8306 = stablehlo.dot_general %8295, %8305, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8362)
      %8307 = stablehlo.reshape %8306 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8363)
      %8308 = stablehlo.multiply %8302, %8307 : tensor<32x17x3200xbf16> loc(#loc8364)
      %8309 = stablehlo.reshape %8308 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8365)
      %8310 = stablehlo.reshape %arg1460 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc8366)
      %8311 = stablehlo.reshape %8310 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc8367)
      %8312 = stablehlo.transpose %8311, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc8368)
      %8313 = stablehlo.dot_general %8309, %8312, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8369)
      %8314 = "stablehlo.all_reduce"(%8313) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.19028"), %arg1677: tensor<bf16> loc("dot.19028")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8369)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8369)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8369)
      %8315 = stablehlo.reshape %8314 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8370)
      %8316 = stablehlo.add %8276, %8315 : tensor<32x17x5120xbf16> loc(#loc8371)
      %8317 = stablehlo.convert %8316 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8372)
      %8318 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8319 = stablehlo.power %8317, %8318 : tensor<32x17x5120xf32> loc(#loc8373)
      %8320 = stablehlo.reduce(%8319 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8374)
      %8321 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8322 = stablehlo.multiply %8320, %8321 : tensor<32x17xf32> loc(#loc8375)
      %8323 = stablehlo.reshape %8322 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8376)
      %8324 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8325 = stablehlo.add %8323, %8324 : tensor<32x17x1xf32> loc(#loc8377)
      %8326 = stablehlo.rsqrt %8325 : tensor<32x17x1xf32> loc(#loc8378)
      %8327 = stablehlo.reshape %8326 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8379)
      %8328 = stablehlo.broadcast_in_dim %8327, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8380)
      %8329 = stablehlo.multiply %8317, %8328 : tensor<32x17x5120xf32> loc(#loc8381)
      %8330 = stablehlo.convert %8329 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8382)
      %8331 = stablehlo.multiply %8202, %8330 : tensor<32x17x5120xbf16> loc(#loc8383)
      %8332 = stablehlo.reshape %8331 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8384)
      %8333 = stablehlo.reshape %arg1459 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8385)
      %8334 = stablehlo.reshape %8333 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8386)
      %8335 = stablehlo.transpose %8334, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8387)
      %8336 = stablehlo.dot_general %8332, %8335, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8388)
      %8337 = stablehlo.reshape %8336 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8389)
      %8338 = stablehlo.convert %8337 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc8390)
      %8339 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %8340 = stablehlo.power %8338, %8339 : tensor<32x17x1x128xf32> loc(#loc8391)
      %8341 = stablehlo.reduce(%8340 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc8392)
      %8342 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8343 = stablehlo.multiply %8341, %8342 : tensor<32x17x1xf32> loc(#loc8393)
      %8344 = stablehlo.reshape %8343 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc8394)
      %8345 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %8346 = stablehlo.add %8344, %8345 : tensor<32x17x1x1xf32> loc(#loc8395)
      %8347 = stablehlo.rsqrt %8346 : tensor<32x17x1x1xf32> loc(#loc8396)
      %8348 = stablehlo.reshape %8347 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc8397)
      %8349 = stablehlo.broadcast_in_dim %8348, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc8398)
      %8350 = stablehlo.multiply %8338, %8349 : tensor<32x17x1x128xf32> loc(#loc8399)
      %8351 = stablehlo.convert %8350 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc8400)
      %8352 = stablehlo.multiply %8199, %8351 : tensor<32x17x1x128xbf16> loc(#loc8401)
      %8353 = stablehlo.transpose %8352, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8402)
      %8354 = stablehlo.multiply %8353, %82 : tensor<32x1x17x128xbf16> loc(#loc8403)
      %8355 = stablehlo.slice %8353 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8404)
      %8356 = stablehlo.negate %8355 : tensor<32x1x17x64xbf16> loc(#loc8405)
      %8357 = stablehlo.slice %8353 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8406)
      %8358 = stablehlo.concatenate %8356, %8357, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8407)
      %8359 = stablehlo.multiply %8358, %91 : tensor<32x1x17x128xbf16> loc(#loc8408)
      %8360 = stablehlo.add %8354, %8359 : tensor<32x1x17x128xbf16> loc(#loc8409)
      %8361 = "stablehlo.scatter"(%arg1469, %21, %8360) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.19140"), %arg1677: tensor<bf16> loc("scatter.19140")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8410)
      %8362 = stablehlo.reshape %arg1470 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8411)
      %8363 = stablehlo.reshape %8362 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8412)
      %8364 = stablehlo.transpose %8363, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8413)
      %8365 = stablehlo.dot_general %8332, %8364, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8414)
      %8366 = stablehlo.reshape %8365 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8415)
      %8367 = stablehlo.transpose %8366, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8416)
      %8368 = "stablehlo.scatter"(%arg1471, %21, %8367) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.19170"), %arg1677: tensor<bf16> loc("scatter.19170")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8417)
      %8369 = stablehlo.reshape %arg1481 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8418)
      %8370 = stablehlo.reshape %8369 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8419)
      %8371 = stablehlo.broadcast_in_dim %8370, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8420)
      %8372 = stablehlo.reshape %arg1480 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8421)
      %8373 = stablehlo.reshape %8372 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8422)
      %8374 = stablehlo.broadcast_in_dim %8373, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8423)
      %8375 = stablehlo.reshape %arg1477 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8424)
      %8376 = stablehlo.reshape %8375 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8425)
      %8377 = stablehlo.broadcast_in_dim %8376, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8426)
      %8378 = stablehlo.reshape %arg1476 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc8427)
      %8379 = stablehlo.reshape %8378 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc8428)
      %8380 = stablehlo.transpose %8379, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc8429)
      %8381 = stablehlo.dot_general %8332, %8380, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc8430)
      %8382 = stablehlo.reshape %8381 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8431)
      %8383 = stablehlo.convert %8382 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc8432)
      %8384 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %8385 = stablehlo.power %8383, %8384 : tensor<32x17x8x128xf32> loc(#loc8433)
      %8386 = stablehlo.reduce(%8385 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc8434)
      %8387 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %8388 = stablehlo.multiply %8386, %8387 : tensor<32x17x8xf32> loc(#loc8435)
      %8389 = stablehlo.reshape %8388 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc8436)
      %8390 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %8391 = stablehlo.add %8389, %8390 : tensor<32x17x8x1xf32> loc(#loc8437)
      %8392 = stablehlo.rsqrt %8391 : tensor<32x17x8x1xf32> loc(#loc8438)
      %8393 = stablehlo.reshape %8392 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc8439)
      %8394 = stablehlo.broadcast_in_dim %8393, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc8440)
      %8395 = stablehlo.multiply %8383, %8394 : tensor<32x17x8x128xf32> loc(#loc8441)
      %8396 = stablehlo.convert %8395 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc8442)
      %8397 = stablehlo.multiply %8377, %8396 : tensor<32x17x8x128xbf16> loc(#loc8443)
      %8398 = stablehlo.transpose %8397, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8444)
      %8399 = stablehlo.multiply %8398, %132 : tensor<32x8x17x128xbf16> loc(#loc8445)
      %8400 = stablehlo.slice %8398 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8446)
      %8401 = stablehlo.negate %8400 : tensor<32x8x17x64xbf16> loc(#loc8447)
      %8402 = stablehlo.slice %8398 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8448)
      %8403 = stablehlo.concatenate %8401, %8402, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8449)
      %8404 = stablehlo.multiply %8403, %138 : tensor<32x8x17x128xbf16> loc(#loc8450)
      %8405 = stablehlo.add %8399, %8404 : tensor<32x8x17x128xbf16> loc(#loc8451)
      %8406 = stablehlo.convert %8405 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc8452)
      %8407 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8408 = stablehlo.multiply %8406, %8407 : tensor<32x8x17x128xf32> loc(#loc8453)
      %8409 = stablehlo.broadcast_in_dim %8361, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8454)
      %8410 = stablehlo.reshape %8409 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8455)
      %8411 = stablehlo.convert %8410 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8456)
      %8412 = stablehlo.transpose %8411, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc8457)
      %8413 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %8414 = stablehlo.multiply %8412, %8413 : tensor<32x8x128x128xf32> loc(#loc8458)
      %8415 = stablehlo.dot_general %8408, %8414, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8459)
      %8416 = stablehlo.add %8415, %159 : tensor<32x8x17x128xf32> loc(#loc8460)
      %8417 = stablehlo.convert %8416 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc8461)
      %8418 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %8419 = stablehlo.compare  EQ, %8417, %8418 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc8462)
      %8420 = stablehlo.not %8419 : tensor<32x8x17x128xi1> loc(#loc8463)
      %8421 = stablehlo.reduce(%8420 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.19351"), %arg1677: tensor<i1> loc("reduce.19351"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc8465)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc8466)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc8464)
      %8422 = stablehlo.reshape %8421 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc8467)
      %8423 = stablehlo.not %8422 : tensor<32x8x17x1xi1> loc(#loc8468)
      %8424 = stablehlo.reshape %8423 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc8469)
      %8425 = stablehlo.broadcast_in_dim %8424, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc8470)
      %8426 = stablehlo.reduce(%8416 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8471)
      %8427 = stablehlo.broadcast_in_dim %8426, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8472)
      %8428 = stablehlo.subtract %8416, %8427 : tensor<32x8x17x128xf32> loc(#loc8473)
      %8429 = stablehlo.exponential %8428 : tensor<32x8x17x128xf32> loc(#loc8474)
      %8430 = stablehlo.reduce(%8429 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8475)
      %8431 = stablehlo.broadcast_in_dim %8430, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8476)
      %8432 = stablehlo.divide %8429, %8431 : tensor<32x8x17x128xf32> loc(#loc8477)
      %8433 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8434 = stablehlo.select %8425, %8433, %8432 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc8478)
      %8435 = stablehlo.broadcast_in_dim %8368, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8479)
      %8436 = stablehlo.reshape %8435 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8480)
      %8437 = stablehlo.convert %8436 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8481)
      %8438 = stablehlo.dot_general %8434, %8437, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8482)
      %8439 = stablehlo.convert %8438 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc8483)
      %8440 = stablehlo.transpose %8439, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8484)
      %8441 = stablehlo.reshape %8440 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc8485)
      %8442 = stablehlo.reshape %arg1475 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc8486)
      %8443 = stablehlo.reshape %8442 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc8487)
      %8444 = stablehlo.transpose %8443, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc8488)
      %8445 = stablehlo.dot_general %8441, %8444, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8489)
      %8446 = "stablehlo.all_reduce"(%8445) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.19368"), %arg1677: tensor<bf16> loc("dot.19368")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8489)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8489)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8489)
      %8447 = stablehlo.reshape %8446 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8490)
      %8448 = stablehlo.add %8316, %8447 : tensor<32x17x5120xbf16> loc(#loc8491)
      %8449 = stablehlo.reshape %arg1478 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8492)
      %8450 = stablehlo.reshape %8449 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8493)
      %8451 = stablehlo.broadcast_in_dim %8450, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8494)
      %8452 = stablehlo.convert %8448 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8495)
      %8453 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8454 = stablehlo.power %8452, %8453 : tensor<32x17x5120xf32> loc(#loc8496)
      %8455 = stablehlo.reduce(%8454 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8497)
      %8456 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8457 = stablehlo.multiply %8455, %8456 : tensor<32x17xf32> loc(#loc8498)
      %8458 = stablehlo.reshape %8457 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8499)
      %8459 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8460 = stablehlo.add %8458, %8459 : tensor<32x17x1xf32> loc(#loc8500)
      %8461 = stablehlo.rsqrt %8460 : tensor<32x17x1xf32> loc(#loc8501)
      %8462 = stablehlo.reshape %8461 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8502)
      %8463 = stablehlo.broadcast_in_dim %8462, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8503)
      %8464 = stablehlo.multiply %8452, %8463 : tensor<32x17x5120xf32> loc(#loc8504)
      %8465 = stablehlo.convert %8464 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8505)
      %8466 = stablehlo.multiply %8451, %8465 : tensor<32x17x5120xbf16> loc(#loc8506)
      %8467 = stablehlo.reshape %8466 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8507)
      %8468 = stablehlo.reshape %arg1479 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8508)
      %8469 = stablehlo.reshape %8468 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8509)
      %8470 = stablehlo.transpose %8469, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8510)
      %8471 = stablehlo.dot_general %8467, %8470, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8511)
      %8472 = stablehlo.reshape %8471 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8512)
      %8473 = stablehlo.logistic %8472 : tensor<32x17x3200xbf16> loc(#loc8513)
      %8474 = stablehlo.multiply %8472, %8473 : tensor<32x17x3200xbf16> loc(#loc8514)
      %8475 = stablehlo.reshape %arg1474 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8515)
      %8476 = stablehlo.reshape %8475 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8516)
      %8477 = stablehlo.transpose %8476, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8517)
      %8478 = stablehlo.dot_general %8467, %8477, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8518)
      %8479 = stablehlo.reshape %8478 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8519)
      %8480 = stablehlo.multiply %8474, %8479 : tensor<32x17x3200xbf16> loc(#loc8520)
      %8481 = stablehlo.reshape %8480 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8521)
      %8482 = stablehlo.reshape %arg1473 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc8522)
      %8483 = stablehlo.reshape %8482 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc8523)
      %8484 = stablehlo.transpose %8483, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc8524)
      %8485 = stablehlo.dot_general %8481, %8484, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8525)
      %8486 = "stablehlo.all_reduce"(%8485) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.19423"), %arg1677: tensor<bf16> loc("dot.19423")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8525)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8525)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8525)
      %8487 = stablehlo.reshape %8486 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8526)
      %8488 = stablehlo.add %8448, %8487 : tensor<32x17x5120xbf16> loc(#loc8527)
      %8489 = stablehlo.convert %8488 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8528)
      %8490 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8491 = stablehlo.power %8489, %8490 : tensor<32x17x5120xf32> loc(#loc8529)
      %8492 = stablehlo.reduce(%8491 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8530)
      %8493 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8494 = stablehlo.multiply %8492, %8493 : tensor<32x17xf32> loc(#loc8531)
      %8495 = stablehlo.reshape %8494 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8532)
      %8496 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8497 = stablehlo.add %8495, %8496 : tensor<32x17x1xf32> loc(#loc8533)
      %8498 = stablehlo.rsqrt %8497 : tensor<32x17x1xf32> loc(#loc8534)
      %8499 = stablehlo.reshape %8498 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8535)
      %8500 = stablehlo.broadcast_in_dim %8499, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8536)
      %8501 = stablehlo.multiply %8489, %8500 : tensor<32x17x5120xf32> loc(#loc8537)
      %8502 = stablehlo.convert %8501 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8538)
      %8503 = stablehlo.multiply %8374, %8502 : tensor<32x17x5120xbf16> loc(#loc8539)
      %8504 = stablehlo.reshape %8503 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8540)
      %8505 = stablehlo.reshape %arg1472 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8541)
      %8506 = stablehlo.reshape %8505 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8542)
      %8507 = stablehlo.transpose %8506, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8543)
      %8508 = stablehlo.dot_general %8504, %8507, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8544)
      %8509 = stablehlo.reshape %8508 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8545)
      %8510 = stablehlo.convert %8509 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc8546)
      %8511 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %8512 = stablehlo.power %8510, %8511 : tensor<32x17x1x128xf32> loc(#loc8547)
      %8513 = stablehlo.reduce(%8512 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc8548)
      %8514 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8515 = stablehlo.multiply %8513, %8514 : tensor<32x17x1xf32> loc(#loc8549)
      %8516 = stablehlo.reshape %8515 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc8550)
      %8517 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %8518 = stablehlo.add %8516, %8517 : tensor<32x17x1x1xf32> loc(#loc8551)
      %8519 = stablehlo.rsqrt %8518 : tensor<32x17x1x1xf32> loc(#loc8552)
      %8520 = stablehlo.reshape %8519 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc8553)
      %8521 = stablehlo.broadcast_in_dim %8520, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc8554)
      %8522 = stablehlo.multiply %8510, %8521 : tensor<32x17x1x128xf32> loc(#loc8555)
      %8523 = stablehlo.convert %8522 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc8556)
      %8524 = stablehlo.multiply %8371, %8523 : tensor<32x17x1x128xbf16> loc(#loc8557)
      %8525 = stablehlo.transpose %8524, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8558)
      %8526 = stablehlo.multiply %8525, %82 : tensor<32x1x17x128xbf16> loc(#loc8559)
      %8527 = stablehlo.slice %8525 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8560)
      %8528 = stablehlo.negate %8527 : tensor<32x1x17x64xbf16> loc(#loc8561)
      %8529 = stablehlo.slice %8525 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8562)
      %8530 = stablehlo.concatenate %8528, %8529, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8563)
      %8531 = stablehlo.multiply %8530, %91 : tensor<32x1x17x128xbf16> loc(#loc8564)
      %8532 = stablehlo.add %8526, %8531 : tensor<32x1x17x128xbf16> loc(#loc8565)
      %8533 = "stablehlo.scatter"(%arg1482, %21, %8532) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.19535"), %arg1677: tensor<bf16> loc("scatter.19535")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8566)
      %8534 = stablehlo.reshape %arg1483 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8567)
      %8535 = stablehlo.reshape %8534 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8568)
      %8536 = stablehlo.transpose %8535, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8569)
      %8537 = stablehlo.dot_general %8504, %8536, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8570)
      %8538 = stablehlo.reshape %8537 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8571)
      %8539 = stablehlo.transpose %8538, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8572)
      %8540 = "stablehlo.scatter"(%arg1484, %21, %8539) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.19565"), %arg1677: tensor<bf16> loc("scatter.19565")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8573)
      %8541 = stablehlo.reshape %arg1494 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8574)
      %8542 = stablehlo.reshape %8541 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8575)
      %8543 = stablehlo.broadcast_in_dim %8542, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8576)
      %8544 = stablehlo.reshape %arg1493 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8577)
      %8545 = stablehlo.reshape %8544 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8578)
      %8546 = stablehlo.broadcast_in_dim %8545, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8579)
      %8547 = stablehlo.reshape %arg1490 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8580)
      %8548 = stablehlo.reshape %8547 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8581)
      %8549 = stablehlo.broadcast_in_dim %8548, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8582)
      %8550 = stablehlo.reshape %arg1489 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc8583)
      %8551 = stablehlo.reshape %8550 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc8584)
      %8552 = stablehlo.transpose %8551, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc8585)
      %8553 = stablehlo.dot_general %8504, %8552, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc8586)
      %8554 = stablehlo.reshape %8553 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8587)
      %8555 = stablehlo.convert %8554 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc8588)
      %8556 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %8557 = stablehlo.power %8555, %8556 : tensor<32x17x8x128xf32> loc(#loc8589)
      %8558 = stablehlo.reduce(%8557 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc8590)
      %8559 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %8560 = stablehlo.multiply %8558, %8559 : tensor<32x17x8xf32> loc(#loc8591)
      %8561 = stablehlo.reshape %8560 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc8592)
      %8562 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %8563 = stablehlo.add %8561, %8562 : tensor<32x17x8x1xf32> loc(#loc8593)
      %8564 = stablehlo.rsqrt %8563 : tensor<32x17x8x1xf32> loc(#loc8594)
      %8565 = stablehlo.reshape %8564 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc8595)
      %8566 = stablehlo.broadcast_in_dim %8565, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc8596)
      %8567 = stablehlo.multiply %8555, %8566 : tensor<32x17x8x128xf32> loc(#loc8597)
      %8568 = stablehlo.convert %8567 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc8598)
      %8569 = stablehlo.multiply %8549, %8568 : tensor<32x17x8x128xbf16> loc(#loc8599)
      %8570 = stablehlo.transpose %8569, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8600)
      %8571 = stablehlo.multiply %8570, %132 : tensor<32x8x17x128xbf16> loc(#loc8601)
      %8572 = stablehlo.slice %8570 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8602)
      %8573 = stablehlo.negate %8572 : tensor<32x8x17x64xbf16> loc(#loc8603)
      %8574 = stablehlo.slice %8570 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8604)
      %8575 = stablehlo.concatenate %8573, %8574, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8605)
      %8576 = stablehlo.multiply %8575, %138 : tensor<32x8x17x128xbf16> loc(#loc8606)
      %8577 = stablehlo.add %8571, %8576 : tensor<32x8x17x128xbf16> loc(#loc8607)
      %8578 = stablehlo.convert %8577 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc8608)
      %8579 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8580 = stablehlo.multiply %8578, %8579 : tensor<32x8x17x128xf32> loc(#loc8609)
      %8581 = stablehlo.broadcast_in_dim %8533, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8610)
      %8582 = stablehlo.reshape %8581 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8611)
      %8583 = stablehlo.convert %8582 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8612)
      %8584 = stablehlo.transpose %8583, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc8613)
      %8585 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %8586 = stablehlo.multiply %8584, %8585 : tensor<32x8x128x128xf32> loc(#loc8614)
      %8587 = stablehlo.dot_general %8580, %8586, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8615)
      %8588 = stablehlo.add %8587, %159 : tensor<32x8x17x128xf32> loc(#loc8616)
      %8589 = stablehlo.convert %8588 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc8617)
      %8590 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %8591 = stablehlo.compare  EQ, %8589, %8590 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc8618)
      %8592 = stablehlo.not %8591 : tensor<32x8x17x128xi1> loc(#loc8619)
      %8593 = stablehlo.reduce(%8592 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.19746"), %arg1677: tensor<i1> loc("reduce.19746"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc8621)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc8622)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc8620)
      %8594 = stablehlo.reshape %8593 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc8623)
      %8595 = stablehlo.not %8594 : tensor<32x8x17x1xi1> loc(#loc8624)
      %8596 = stablehlo.reshape %8595 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc8625)
      %8597 = stablehlo.broadcast_in_dim %8596, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc8626)
      %8598 = stablehlo.reduce(%8588 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8627)
      %8599 = stablehlo.broadcast_in_dim %8598, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8628)
      %8600 = stablehlo.subtract %8588, %8599 : tensor<32x8x17x128xf32> loc(#loc8629)
      %8601 = stablehlo.exponential %8600 : tensor<32x8x17x128xf32> loc(#loc8630)
      %8602 = stablehlo.reduce(%8601 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8631)
      %8603 = stablehlo.broadcast_in_dim %8602, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8632)
      %8604 = stablehlo.divide %8601, %8603 : tensor<32x8x17x128xf32> loc(#loc8633)
      %8605 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8606 = stablehlo.select %8597, %8605, %8604 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc8634)
      %8607 = stablehlo.broadcast_in_dim %8540, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8635)
      %8608 = stablehlo.reshape %8607 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8636)
      %8609 = stablehlo.convert %8608 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8637)
      %8610 = stablehlo.dot_general %8606, %8609, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8638)
      %8611 = stablehlo.convert %8610 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc8639)
      %8612 = stablehlo.transpose %8611, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8640)
      %8613 = stablehlo.reshape %8612 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc8641)
      %8614 = stablehlo.reshape %arg1488 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc8642)
      %8615 = stablehlo.reshape %8614 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc8643)
      %8616 = stablehlo.transpose %8615, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc8644)
      %8617 = stablehlo.dot_general %8613, %8616, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8645)
      %8618 = "stablehlo.all_reduce"(%8617) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.19763"), %arg1677: tensor<bf16> loc("dot.19763")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8645)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8645)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8645)
      %8619 = stablehlo.reshape %8618 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8646)
      %8620 = stablehlo.add %8488, %8619 : tensor<32x17x5120xbf16> loc(#loc8647)
      %8621 = stablehlo.reshape %arg1491 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8648)
      %8622 = stablehlo.reshape %8621 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8649)
      %8623 = stablehlo.broadcast_in_dim %8622, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8650)
      %8624 = stablehlo.convert %8620 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8651)
      %8625 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8626 = stablehlo.power %8624, %8625 : tensor<32x17x5120xf32> loc(#loc8652)
      %8627 = stablehlo.reduce(%8626 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8653)
      %8628 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8629 = stablehlo.multiply %8627, %8628 : tensor<32x17xf32> loc(#loc8654)
      %8630 = stablehlo.reshape %8629 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8655)
      %8631 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8632 = stablehlo.add %8630, %8631 : tensor<32x17x1xf32> loc(#loc8656)
      %8633 = stablehlo.rsqrt %8632 : tensor<32x17x1xf32> loc(#loc8657)
      %8634 = stablehlo.reshape %8633 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8658)
      %8635 = stablehlo.broadcast_in_dim %8634, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8659)
      %8636 = stablehlo.multiply %8624, %8635 : tensor<32x17x5120xf32> loc(#loc8660)
      %8637 = stablehlo.convert %8636 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8661)
      %8638 = stablehlo.multiply %8623, %8637 : tensor<32x17x5120xbf16> loc(#loc8662)
      %8639 = stablehlo.reshape %8638 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8663)
      %8640 = stablehlo.reshape %arg1492 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8664)
      %8641 = stablehlo.reshape %8640 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8665)
      %8642 = stablehlo.transpose %8641, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8666)
      %8643 = stablehlo.dot_general %8639, %8642, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8667)
      %8644 = stablehlo.reshape %8643 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8668)
      %8645 = stablehlo.logistic %8644 : tensor<32x17x3200xbf16> loc(#loc8669)
      %8646 = stablehlo.multiply %8644, %8645 : tensor<32x17x3200xbf16> loc(#loc8670)
      %8647 = stablehlo.reshape %arg1487 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8671)
      %8648 = stablehlo.reshape %8647 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8672)
      %8649 = stablehlo.transpose %8648, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8673)
      %8650 = stablehlo.dot_general %8639, %8649, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8674)
      %8651 = stablehlo.reshape %8650 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8675)
      %8652 = stablehlo.multiply %8646, %8651 : tensor<32x17x3200xbf16> loc(#loc8676)
      %8653 = stablehlo.reshape %8652 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8677)
      %8654 = stablehlo.reshape %arg1486 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc8678)
      %8655 = stablehlo.reshape %8654 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc8679)
      %8656 = stablehlo.transpose %8655, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc8680)
      %8657 = stablehlo.dot_general %8653, %8656, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8681)
      %8658 = "stablehlo.all_reduce"(%8657) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.19818"), %arg1677: tensor<bf16> loc("dot.19818")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8681)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8681)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8681)
      %8659 = stablehlo.reshape %8658 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8682)
      %8660 = stablehlo.add %8620, %8659 : tensor<32x17x5120xbf16> loc(#loc8683)
      %8661 = stablehlo.convert %8660 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8684)
      %8662 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8663 = stablehlo.power %8661, %8662 : tensor<32x17x5120xf32> loc(#loc8685)
      %8664 = stablehlo.reduce(%8663 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8686)
      %8665 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8666 = stablehlo.multiply %8664, %8665 : tensor<32x17xf32> loc(#loc8687)
      %8667 = stablehlo.reshape %8666 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8688)
      %8668 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8669 = stablehlo.add %8667, %8668 : tensor<32x17x1xf32> loc(#loc8689)
      %8670 = stablehlo.rsqrt %8669 : tensor<32x17x1xf32> loc(#loc8690)
      %8671 = stablehlo.reshape %8670 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8691)
      %8672 = stablehlo.broadcast_in_dim %8671, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8692)
      %8673 = stablehlo.multiply %8661, %8672 : tensor<32x17x5120xf32> loc(#loc8693)
      %8674 = stablehlo.convert %8673 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8694)
      %8675 = stablehlo.multiply %8546, %8674 : tensor<32x17x5120xbf16> loc(#loc8695)
      %8676 = stablehlo.reshape %8675 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8696)
      %8677 = stablehlo.reshape %arg1485 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8697)
      %8678 = stablehlo.reshape %8677 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8698)
      %8679 = stablehlo.transpose %8678, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8699)
      %8680 = stablehlo.dot_general %8676, %8679, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8700)
      %8681 = stablehlo.reshape %8680 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8701)
      %8682 = stablehlo.convert %8681 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc8702)
      %8683 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %8684 = stablehlo.power %8682, %8683 : tensor<32x17x1x128xf32> loc(#loc8703)
      %8685 = stablehlo.reduce(%8684 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc8704)
      %8686 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8687 = stablehlo.multiply %8685, %8686 : tensor<32x17x1xf32> loc(#loc8705)
      %8688 = stablehlo.reshape %8687 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc8706)
      %8689 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %8690 = stablehlo.add %8688, %8689 : tensor<32x17x1x1xf32> loc(#loc8707)
      %8691 = stablehlo.rsqrt %8690 : tensor<32x17x1x1xf32> loc(#loc8708)
      %8692 = stablehlo.reshape %8691 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc8709)
      %8693 = stablehlo.broadcast_in_dim %8692, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc8710)
      %8694 = stablehlo.multiply %8682, %8693 : tensor<32x17x1x128xf32> loc(#loc8711)
      %8695 = stablehlo.convert %8694 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc8712)
      %8696 = stablehlo.multiply %8543, %8695 : tensor<32x17x1x128xbf16> loc(#loc8713)
      %8697 = stablehlo.transpose %8696, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8714)
      %8698 = stablehlo.multiply %8697, %82 : tensor<32x1x17x128xbf16> loc(#loc8715)
      %8699 = stablehlo.slice %8697 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8716)
      %8700 = stablehlo.negate %8699 : tensor<32x1x17x64xbf16> loc(#loc8717)
      %8701 = stablehlo.slice %8697 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8718)
      %8702 = stablehlo.concatenate %8700, %8701, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8719)
      %8703 = stablehlo.multiply %8702, %91 : tensor<32x1x17x128xbf16> loc(#loc8720)
      %8704 = stablehlo.add %8698, %8703 : tensor<32x1x17x128xbf16> loc(#loc8721)
      %8705 = "stablehlo.scatter"(%arg1495, %21, %8704) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.19930"), %arg1677: tensor<bf16> loc("scatter.19930")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8722)
      %8706 = stablehlo.reshape %arg1496 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8723)
      %8707 = stablehlo.reshape %8706 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8724)
      %8708 = stablehlo.transpose %8707, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8725)
      %8709 = stablehlo.dot_general %8676, %8708, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8726)
      %8710 = stablehlo.reshape %8709 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8727)
      %8711 = stablehlo.transpose %8710, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8728)
      %8712 = "stablehlo.scatter"(%arg1497, %21, %8711) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.19960"), %arg1677: tensor<bf16> loc("scatter.19960")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8729)
      %8713 = stablehlo.reshape %arg1507 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8730)
      %8714 = stablehlo.reshape %8713 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8731)
      %8715 = stablehlo.broadcast_in_dim %8714, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8732)
      %8716 = stablehlo.reshape %arg1506 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8733)
      %8717 = stablehlo.reshape %8716 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8734)
      %8718 = stablehlo.broadcast_in_dim %8717, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8735)
      %8719 = stablehlo.reshape %arg1503 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8736)
      %8720 = stablehlo.reshape %8719 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8737)
      %8721 = stablehlo.broadcast_in_dim %8720, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8738)
      %8722 = stablehlo.reshape %arg1502 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc8739)
      %8723 = stablehlo.reshape %8722 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc8740)
      %8724 = stablehlo.transpose %8723, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc8741)
      %8725 = stablehlo.dot_general %8676, %8724, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc8742)
      %8726 = stablehlo.reshape %8725 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8743)
      %8727 = stablehlo.convert %8726 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc8744)
      %8728 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %8729 = stablehlo.power %8727, %8728 : tensor<32x17x8x128xf32> loc(#loc8745)
      %8730 = stablehlo.reduce(%8729 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc8746)
      %8731 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %8732 = stablehlo.multiply %8730, %8731 : tensor<32x17x8xf32> loc(#loc8747)
      %8733 = stablehlo.reshape %8732 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc8748)
      %8734 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %8735 = stablehlo.add %8733, %8734 : tensor<32x17x8x1xf32> loc(#loc8749)
      %8736 = stablehlo.rsqrt %8735 : tensor<32x17x8x1xf32> loc(#loc8750)
      %8737 = stablehlo.reshape %8736 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc8751)
      %8738 = stablehlo.broadcast_in_dim %8737, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc8752)
      %8739 = stablehlo.multiply %8727, %8738 : tensor<32x17x8x128xf32> loc(#loc8753)
      %8740 = stablehlo.convert %8739 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc8754)
      %8741 = stablehlo.multiply %8721, %8740 : tensor<32x17x8x128xbf16> loc(#loc8755)
      %8742 = stablehlo.transpose %8741, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8756)
      %8743 = stablehlo.multiply %8742, %132 : tensor<32x8x17x128xbf16> loc(#loc8757)
      %8744 = stablehlo.slice %8742 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8758)
      %8745 = stablehlo.negate %8744 : tensor<32x8x17x64xbf16> loc(#loc8759)
      %8746 = stablehlo.slice %8742 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8760)
      %8747 = stablehlo.concatenate %8745, %8746, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8761)
      %8748 = stablehlo.multiply %8747, %138 : tensor<32x8x17x128xbf16> loc(#loc8762)
      %8749 = stablehlo.add %8743, %8748 : tensor<32x8x17x128xbf16> loc(#loc8763)
      %8750 = stablehlo.convert %8749 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc8764)
      %8751 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8752 = stablehlo.multiply %8750, %8751 : tensor<32x8x17x128xf32> loc(#loc8765)
      %8753 = stablehlo.broadcast_in_dim %8705, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8766)
      %8754 = stablehlo.reshape %8753 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8767)
      %8755 = stablehlo.convert %8754 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8768)
      %8756 = stablehlo.transpose %8755, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc8769)
      %8757 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %8758 = stablehlo.multiply %8756, %8757 : tensor<32x8x128x128xf32> loc(#loc8770)
      %8759 = stablehlo.dot_general %8752, %8758, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8771)
      %8760 = stablehlo.add %8759, %159 : tensor<32x8x17x128xf32> loc(#loc8772)
      %8761 = stablehlo.convert %8760 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc8773)
      %8762 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %8763 = stablehlo.compare  EQ, %8761, %8762 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc8774)
      %8764 = stablehlo.not %8763 : tensor<32x8x17x128xi1> loc(#loc8775)
      %8765 = stablehlo.reduce(%8764 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.20141"), %arg1677: tensor<i1> loc("reduce.20141"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc8777)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc8778)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc8776)
      %8766 = stablehlo.reshape %8765 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc8779)
      %8767 = stablehlo.not %8766 : tensor<32x8x17x1xi1> loc(#loc8780)
      %8768 = stablehlo.reshape %8767 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc8781)
      %8769 = stablehlo.broadcast_in_dim %8768, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc8782)
      %8770 = stablehlo.reduce(%8760 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8783)
      %8771 = stablehlo.broadcast_in_dim %8770, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8784)
      %8772 = stablehlo.subtract %8760, %8771 : tensor<32x8x17x128xf32> loc(#loc8785)
      %8773 = stablehlo.exponential %8772 : tensor<32x8x17x128xf32> loc(#loc8786)
      %8774 = stablehlo.reduce(%8773 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8787)
      %8775 = stablehlo.broadcast_in_dim %8774, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8788)
      %8776 = stablehlo.divide %8773, %8775 : tensor<32x8x17x128xf32> loc(#loc8789)
      %8777 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8778 = stablehlo.select %8769, %8777, %8776 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc8790)
      %8779 = stablehlo.broadcast_in_dim %8712, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8791)
      %8780 = stablehlo.reshape %8779 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8792)
      %8781 = stablehlo.convert %8780 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8793)
      %8782 = stablehlo.dot_general %8778, %8781, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8794)
      %8783 = stablehlo.convert %8782 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc8795)
      %8784 = stablehlo.transpose %8783, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8796)
      %8785 = stablehlo.reshape %8784 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc8797)
      %8786 = stablehlo.reshape %arg1501 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc8798)
      %8787 = stablehlo.reshape %8786 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc8799)
      %8788 = stablehlo.transpose %8787, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc8800)
      %8789 = stablehlo.dot_general %8785, %8788, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8801)
      %8790 = "stablehlo.all_reduce"(%8789) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.20158"), %arg1677: tensor<bf16> loc("dot.20158")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8801)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8801)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8801)
      %8791 = stablehlo.reshape %8790 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8802)
      %8792 = stablehlo.add %8660, %8791 : tensor<32x17x5120xbf16> loc(#loc8803)
      %8793 = stablehlo.reshape %arg1504 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8804)
      %8794 = stablehlo.reshape %8793 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8805)
      %8795 = stablehlo.broadcast_in_dim %8794, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8806)
      %8796 = stablehlo.convert %8792 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8807)
      %8797 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8798 = stablehlo.power %8796, %8797 : tensor<32x17x5120xf32> loc(#loc8808)
      %8799 = stablehlo.reduce(%8798 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8809)
      %8800 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8801 = stablehlo.multiply %8799, %8800 : tensor<32x17xf32> loc(#loc8810)
      %8802 = stablehlo.reshape %8801 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8811)
      %8803 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8804 = stablehlo.add %8802, %8803 : tensor<32x17x1xf32> loc(#loc8812)
      %8805 = stablehlo.rsqrt %8804 : tensor<32x17x1xf32> loc(#loc8813)
      %8806 = stablehlo.reshape %8805 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8814)
      %8807 = stablehlo.broadcast_in_dim %8806, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8815)
      %8808 = stablehlo.multiply %8796, %8807 : tensor<32x17x5120xf32> loc(#loc8816)
      %8809 = stablehlo.convert %8808 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8817)
      %8810 = stablehlo.multiply %8795, %8809 : tensor<32x17x5120xbf16> loc(#loc8818)
      %8811 = stablehlo.reshape %8810 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8819)
      %8812 = stablehlo.reshape %arg1505 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8820)
      %8813 = stablehlo.reshape %8812 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8821)
      %8814 = stablehlo.transpose %8813, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8822)
      %8815 = stablehlo.dot_general %8811, %8814, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8823)
      %8816 = stablehlo.reshape %8815 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8824)
      %8817 = stablehlo.logistic %8816 : tensor<32x17x3200xbf16> loc(#loc8825)
      %8818 = stablehlo.multiply %8816, %8817 : tensor<32x17x3200xbf16> loc(#loc8826)
      %8819 = stablehlo.reshape %arg1500 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8827)
      %8820 = stablehlo.reshape %8819 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8828)
      %8821 = stablehlo.transpose %8820, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8829)
      %8822 = stablehlo.dot_general %8811, %8821, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8830)
      %8823 = stablehlo.reshape %8822 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8831)
      %8824 = stablehlo.multiply %8818, %8823 : tensor<32x17x3200xbf16> loc(#loc8832)
      %8825 = stablehlo.reshape %8824 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8833)
      %8826 = stablehlo.reshape %arg1499 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc8834)
      %8827 = stablehlo.reshape %8826 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc8835)
      %8828 = stablehlo.transpose %8827, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc8836)
      %8829 = stablehlo.dot_general %8825, %8828, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8837)
      %8830 = "stablehlo.all_reduce"(%8829) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.20213"), %arg1677: tensor<bf16> loc("dot.20213")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8837)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8837)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8837)
      %8831 = stablehlo.reshape %8830 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8838)
      %8832 = stablehlo.add %8792, %8831 : tensor<32x17x5120xbf16> loc(#loc8839)
      %8833 = stablehlo.convert %8832 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8840)
      %8834 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8835 = stablehlo.power %8833, %8834 : tensor<32x17x5120xf32> loc(#loc8841)
      %8836 = stablehlo.reduce(%8835 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8842)
      %8837 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8838 = stablehlo.multiply %8836, %8837 : tensor<32x17xf32> loc(#loc8843)
      %8839 = stablehlo.reshape %8838 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8844)
      %8840 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8841 = stablehlo.add %8839, %8840 : tensor<32x17x1xf32> loc(#loc8845)
      %8842 = stablehlo.rsqrt %8841 : tensor<32x17x1xf32> loc(#loc8846)
      %8843 = stablehlo.reshape %8842 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8847)
      %8844 = stablehlo.broadcast_in_dim %8843, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8848)
      %8845 = stablehlo.multiply %8833, %8844 : tensor<32x17x5120xf32> loc(#loc8849)
      %8846 = stablehlo.convert %8845 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8850)
      %8847 = stablehlo.multiply %8718, %8846 : tensor<32x17x5120xbf16> loc(#loc8851)
      %8848 = stablehlo.reshape %8847 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8852)
      %8849 = stablehlo.reshape %arg1498 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8853)
      %8850 = stablehlo.reshape %8849 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8854)
      %8851 = stablehlo.transpose %8850, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8855)
      %8852 = stablehlo.dot_general %8848, %8851, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8856)
      %8853 = stablehlo.reshape %8852 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8857)
      %8854 = stablehlo.convert %8853 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc8858)
      %8855 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %8856 = stablehlo.power %8854, %8855 : tensor<32x17x1x128xf32> loc(#loc8859)
      %8857 = stablehlo.reduce(%8856 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc8860)
      %8858 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8859 = stablehlo.multiply %8857, %8858 : tensor<32x17x1xf32> loc(#loc8861)
      %8860 = stablehlo.reshape %8859 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc8862)
      %8861 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %8862 = stablehlo.add %8860, %8861 : tensor<32x17x1x1xf32> loc(#loc8863)
      %8863 = stablehlo.rsqrt %8862 : tensor<32x17x1x1xf32> loc(#loc8864)
      %8864 = stablehlo.reshape %8863 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc8865)
      %8865 = stablehlo.broadcast_in_dim %8864, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc8866)
      %8866 = stablehlo.multiply %8854, %8865 : tensor<32x17x1x128xf32> loc(#loc8867)
      %8867 = stablehlo.convert %8866 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc8868)
      %8868 = stablehlo.multiply %8715, %8867 : tensor<32x17x1x128xbf16> loc(#loc8869)
      %8869 = stablehlo.transpose %8868, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8870)
      %8870 = stablehlo.multiply %8869, %82 : tensor<32x1x17x128xbf16> loc(#loc8871)
      %8871 = stablehlo.slice %8869 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8872)
      %8872 = stablehlo.negate %8871 : tensor<32x1x17x64xbf16> loc(#loc8873)
      %8873 = stablehlo.slice %8869 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc8874)
      %8874 = stablehlo.concatenate %8872, %8873, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8875)
      %8875 = stablehlo.multiply %8874, %91 : tensor<32x1x17x128xbf16> loc(#loc8876)
      %8876 = stablehlo.add %8870, %8875 : tensor<32x1x17x128xbf16> loc(#loc8877)
      %8877 = "stablehlo.scatter"(%arg1508, %21, %8876) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.20325"), %arg1677: tensor<bf16> loc("scatter.20325")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8878)
      %8878 = stablehlo.reshape %arg1509 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc8879)
      %8879 = stablehlo.reshape %8878 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc8880)
      %8880 = stablehlo.transpose %8879, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc8881)
      %8881 = stablehlo.dot_general %8848, %8880, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc8882)
      %8882 = stablehlo.reshape %8881 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8883)
      %8883 = stablehlo.transpose %8882, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc8884)
      %8884 = "stablehlo.scatter"(%arg1510, %21, %8883) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.20355"), %arg1677: tensor<bf16> loc("scatter.20355")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc8885)
      %8885 = stablehlo.reshape %arg1520 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8886)
      %8886 = stablehlo.reshape %8885 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8887)
      %8887 = stablehlo.broadcast_in_dim %8886, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc8888)
      %8888 = stablehlo.reshape %arg1519 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8889)
      %8889 = stablehlo.reshape %8888 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8890)
      %8890 = stablehlo.broadcast_in_dim %8889, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8891)
      %8891 = stablehlo.reshape %arg1516 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc8892)
      %8892 = stablehlo.reshape %8891 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc8893)
      %8893 = stablehlo.broadcast_in_dim %8892, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8894)
      %8894 = stablehlo.reshape %arg1515 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc8895)
      %8895 = stablehlo.reshape %8894 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc8896)
      %8896 = stablehlo.transpose %8895, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc8897)
      %8897 = stablehlo.dot_general %8848, %8896, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc8898)
      %8898 = stablehlo.reshape %8897 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8899)
      %8899 = stablehlo.convert %8898 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc8900)
      %8900 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %8901 = stablehlo.power %8899, %8900 : tensor<32x17x8x128xf32> loc(#loc8901)
      %8902 = stablehlo.reduce(%8901 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc8902)
      %8903 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %8904 = stablehlo.multiply %8902, %8903 : tensor<32x17x8xf32> loc(#loc8903)
      %8905 = stablehlo.reshape %8904 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc8904)
      %8906 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %8907 = stablehlo.add %8905, %8906 : tensor<32x17x8x1xf32> loc(#loc8905)
      %8908 = stablehlo.rsqrt %8907 : tensor<32x17x8x1xf32> loc(#loc8906)
      %8909 = stablehlo.reshape %8908 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc8907)
      %8910 = stablehlo.broadcast_in_dim %8909, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc8908)
      %8911 = stablehlo.multiply %8899, %8910 : tensor<32x17x8x128xf32> loc(#loc8909)
      %8912 = stablehlo.convert %8911 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc8910)
      %8913 = stablehlo.multiply %8893, %8912 : tensor<32x17x8x128xbf16> loc(#loc8911)
      %8914 = stablehlo.transpose %8913, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8912)
      %8915 = stablehlo.multiply %8914, %132 : tensor<32x8x17x128xbf16> loc(#loc8913)
      %8916 = stablehlo.slice %8914 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8914)
      %8917 = stablehlo.negate %8916 : tensor<32x8x17x64xbf16> loc(#loc8915)
      %8918 = stablehlo.slice %8914 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc8916)
      %8919 = stablehlo.concatenate %8917, %8918, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc8917)
      %8920 = stablehlo.multiply %8919, %138 : tensor<32x8x17x128xbf16> loc(#loc8918)
      %8921 = stablehlo.add %8915, %8920 : tensor<32x8x17x128xbf16> loc(#loc8919)
      %8922 = stablehlo.convert %8921 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc8920)
      %8923 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8924 = stablehlo.multiply %8922, %8923 : tensor<32x8x17x128xf32> loc(#loc8921)
      %8925 = stablehlo.broadcast_in_dim %8877, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8922)
      %8926 = stablehlo.reshape %8925 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8923)
      %8927 = stablehlo.convert %8926 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8924)
      %8928 = stablehlo.transpose %8927, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc8925)
      %8929 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %8930 = stablehlo.multiply %8928, %8929 : tensor<32x8x128x128xf32> loc(#loc8926)
      %8931 = stablehlo.dot_general %8924, %8930, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8927)
      %8932 = stablehlo.add %8931, %159 : tensor<32x8x17x128xf32> loc(#loc8928)
      %8933 = stablehlo.convert %8932 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc8929)
      %8934 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %8935 = stablehlo.compare  EQ, %8933, %8934 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc8930)
      %8936 = stablehlo.not %8935 : tensor<32x8x17x128xi1> loc(#loc8931)
      %8937 = stablehlo.reduce(%8936 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.20536"), %arg1677: tensor<i1> loc("reduce.20536"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc8933)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc8934)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc8932)
      %8938 = stablehlo.reshape %8937 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc8935)
      %8939 = stablehlo.not %8938 : tensor<32x8x17x1xi1> loc(#loc8936)
      %8940 = stablehlo.reshape %8939 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc8937)
      %8941 = stablehlo.broadcast_in_dim %8940, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc8938)
      %8942 = stablehlo.reduce(%8932 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8939)
      %8943 = stablehlo.broadcast_in_dim %8942, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8940)
      %8944 = stablehlo.subtract %8932, %8943 : tensor<32x8x17x128xf32> loc(#loc8941)
      %8945 = stablehlo.exponential %8944 : tensor<32x8x17x128xf32> loc(#loc8942)
      %8946 = stablehlo.reduce(%8945 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc8943)
      %8947 = stablehlo.broadcast_in_dim %8946, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc8944)
      %8948 = stablehlo.divide %8945, %8947 : tensor<32x8x17x128xf32> loc(#loc8945)
      %8949 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %8950 = stablehlo.select %8941, %8949, %8948 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc8946)
      %8951 = stablehlo.broadcast_in_dim %8884, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc8947)
      %8952 = stablehlo.reshape %8951 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc8948)
      %8953 = stablehlo.convert %8952 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc8949)
      %8954 = stablehlo.dot_general %8950, %8953, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc8950)
      %8955 = stablehlo.convert %8954 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc8951)
      %8956 = stablehlo.transpose %8955, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc8952)
      %8957 = stablehlo.reshape %8956 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc8953)
      %8958 = stablehlo.reshape %arg1514 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc8954)
      %8959 = stablehlo.reshape %8958 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc8955)
      %8960 = stablehlo.transpose %8959, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc8956)
      %8961 = stablehlo.dot_general %8957, %8960, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8957)
      %8962 = "stablehlo.all_reduce"(%8961) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.20553"), %arg1677: tensor<bf16> loc("dot.20553")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8957)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8957)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8957)
      %8963 = stablehlo.reshape %8962 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8958)
      %8964 = stablehlo.add %8832, %8963 : tensor<32x17x5120xbf16> loc(#loc8959)
      %8965 = stablehlo.reshape %arg1517 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc8960)
      %8966 = stablehlo.reshape %8965 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc8961)
      %8967 = stablehlo.broadcast_in_dim %8966, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8962)
      %8968 = stablehlo.convert %8964 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8963)
      %8969 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %8970 = stablehlo.power %8968, %8969 : tensor<32x17x5120xf32> loc(#loc8964)
      %8971 = stablehlo.reduce(%8970 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8965)
      %8972 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %8973 = stablehlo.multiply %8971, %8972 : tensor<32x17xf32> loc(#loc8966)
      %8974 = stablehlo.reshape %8973 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc8967)
      %8975 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %8976 = stablehlo.add %8974, %8975 : tensor<32x17x1xf32> loc(#loc8968)
      %8977 = stablehlo.rsqrt %8976 : tensor<32x17x1xf32> loc(#loc8969)
      %8978 = stablehlo.reshape %8977 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc8970)
      %8979 = stablehlo.broadcast_in_dim %8978, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc8971)
      %8980 = stablehlo.multiply %8968, %8979 : tensor<32x17x5120xf32> loc(#loc8972)
      %8981 = stablehlo.convert %8980 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc8973)
      %8982 = stablehlo.multiply %8967, %8981 : tensor<32x17x5120xbf16> loc(#loc8974)
      %8983 = stablehlo.reshape %8982 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8975)
      %8984 = stablehlo.reshape %arg1518 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8976)
      %8985 = stablehlo.reshape %8984 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8977)
      %8986 = stablehlo.transpose %8985, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8978)
      %8987 = stablehlo.dot_general %8983, %8986, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8979)
      %8988 = stablehlo.reshape %8987 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8980)
      %8989 = stablehlo.logistic %8988 : tensor<32x17x3200xbf16> loc(#loc8981)
      %8990 = stablehlo.multiply %8988, %8989 : tensor<32x17x3200xbf16> loc(#loc8982)
      %8991 = stablehlo.reshape %arg1513 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc8983)
      %8992 = stablehlo.reshape %8991 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc8984)
      %8993 = stablehlo.transpose %8992, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc8985)
      %8994 = stablehlo.dot_general %8983, %8993, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8986)
      %8995 = stablehlo.reshape %8994 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc8987)
      %8996 = stablehlo.multiply %8990, %8995 : tensor<32x17x3200xbf16> loc(#loc8988)
      %8997 = stablehlo.reshape %8996 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc8989)
      %8998 = stablehlo.reshape %arg1512 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc8990)
      %8999 = stablehlo.reshape %8998 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc8991)
      %9000 = stablehlo.transpose %8999, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc8992)
      %9001 = stablehlo.dot_general %8997, %9000, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8993)
      %9002 = "stablehlo.all_reduce"(%9001) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.20608"), %arg1677: tensor<bf16> loc("dot.20608")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc8993)
        stablehlo.return %11074 : tensor<bf16> loc(#loc8993)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc8993)
      %9003 = stablehlo.reshape %9002 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc8994)
      %9004 = stablehlo.add %8964, %9003 : tensor<32x17x5120xbf16> loc(#loc8995)
      %9005 = stablehlo.convert %9004 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc8996)
      %9006 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9007 = stablehlo.power %9005, %9006 : tensor<32x17x5120xf32> loc(#loc8997)
      %9008 = stablehlo.reduce(%9007 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc8998)
      %9009 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9010 = stablehlo.multiply %9008, %9009 : tensor<32x17xf32> loc(#loc8999)
      %9011 = stablehlo.reshape %9010 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9000)
      %9012 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9013 = stablehlo.add %9011, %9012 : tensor<32x17x1xf32> loc(#loc9001)
      %9014 = stablehlo.rsqrt %9013 : tensor<32x17x1xf32> loc(#loc9002)
      %9015 = stablehlo.reshape %9014 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9003)
      %9016 = stablehlo.broadcast_in_dim %9015, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9004)
      %9017 = stablehlo.multiply %9005, %9016 : tensor<32x17x5120xf32> loc(#loc9005)
      %9018 = stablehlo.convert %9017 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9006)
      %9019 = stablehlo.multiply %8890, %9018 : tensor<32x17x5120xbf16> loc(#loc9007)
      %9020 = stablehlo.reshape %9019 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9008)
      %9021 = stablehlo.reshape %arg1511 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9009)
      %9022 = stablehlo.reshape %9021 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9010)
      %9023 = stablehlo.transpose %9022, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9011)
      %9024 = stablehlo.dot_general %9020, %9023, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9012)
      %9025 = stablehlo.reshape %9024 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9013)
      %9026 = stablehlo.convert %9025 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc9014)
      %9027 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %9028 = stablehlo.power %9026, %9027 : tensor<32x17x1x128xf32> loc(#loc9015)
      %9029 = stablehlo.reduce(%9028 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc9016)
      %9030 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9031 = stablehlo.multiply %9029, %9030 : tensor<32x17x1xf32> loc(#loc9017)
      %9032 = stablehlo.reshape %9031 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc9018)
      %9033 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %9034 = stablehlo.add %9032, %9033 : tensor<32x17x1x1xf32> loc(#loc9019)
      %9035 = stablehlo.rsqrt %9034 : tensor<32x17x1x1xf32> loc(#loc9020)
      %9036 = stablehlo.reshape %9035 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc9021)
      %9037 = stablehlo.broadcast_in_dim %9036, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc9022)
      %9038 = stablehlo.multiply %9026, %9037 : tensor<32x17x1x128xf32> loc(#loc9023)
      %9039 = stablehlo.convert %9038 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc9024)
      %9040 = stablehlo.multiply %8887, %9039 : tensor<32x17x1x128xbf16> loc(#loc9025)
      %9041 = stablehlo.transpose %9040, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9026)
      %9042 = stablehlo.multiply %9041, %82 : tensor<32x1x17x128xbf16> loc(#loc9027)
      %9043 = stablehlo.slice %9041 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9028)
      %9044 = stablehlo.negate %9043 : tensor<32x1x17x64xbf16> loc(#loc9029)
      %9045 = stablehlo.slice %9041 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9030)
      %9046 = stablehlo.concatenate %9044, %9045, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9031)
      %9047 = stablehlo.multiply %9046, %91 : tensor<32x1x17x128xbf16> loc(#loc9032)
      %9048 = stablehlo.add %9042, %9047 : tensor<32x1x17x128xbf16> loc(#loc9033)
      %9049 = "stablehlo.scatter"(%arg1521, %21, %9048) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.20720"), %arg1677: tensor<bf16> loc("scatter.20720")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9034)
      %9050 = stablehlo.reshape %arg1522 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9035)
      %9051 = stablehlo.reshape %9050 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9036)
      %9052 = stablehlo.transpose %9051, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9037)
      %9053 = stablehlo.dot_general %9020, %9052, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9038)
      %9054 = stablehlo.reshape %9053 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9039)
      %9055 = stablehlo.transpose %9054, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9040)
      %9056 = "stablehlo.scatter"(%arg1523, %21, %9055) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.20750"), %arg1677: tensor<bf16> loc("scatter.20750")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9041)
      %9057 = stablehlo.reshape %arg1533 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9042)
      %9058 = stablehlo.reshape %9057 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9043)
      %9059 = stablehlo.broadcast_in_dim %9058, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9044)
      %9060 = stablehlo.reshape %arg1532 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9045)
      %9061 = stablehlo.reshape %9060 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9046)
      %9062 = stablehlo.broadcast_in_dim %9061, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9047)
      %9063 = stablehlo.reshape %arg1529 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9048)
      %9064 = stablehlo.reshape %9063 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9049)
      %9065 = stablehlo.broadcast_in_dim %9064, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9050)
      %9066 = stablehlo.reshape %arg1528 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc9051)
      %9067 = stablehlo.reshape %9066 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc9052)
      %9068 = stablehlo.transpose %9067, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc9053)
      %9069 = stablehlo.dot_general %9020, %9068, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc9054)
      %9070 = stablehlo.reshape %9069 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9055)
      %9071 = stablehlo.convert %9070 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc9056)
      %9072 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %9073 = stablehlo.power %9071, %9072 : tensor<32x17x8x128xf32> loc(#loc9057)
      %9074 = stablehlo.reduce(%9073 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc9058)
      %9075 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %9076 = stablehlo.multiply %9074, %9075 : tensor<32x17x8xf32> loc(#loc9059)
      %9077 = stablehlo.reshape %9076 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc9060)
      %9078 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %9079 = stablehlo.add %9077, %9078 : tensor<32x17x8x1xf32> loc(#loc9061)
      %9080 = stablehlo.rsqrt %9079 : tensor<32x17x8x1xf32> loc(#loc9062)
      %9081 = stablehlo.reshape %9080 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc9063)
      %9082 = stablehlo.broadcast_in_dim %9081, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc9064)
      %9083 = stablehlo.multiply %9071, %9082 : tensor<32x17x8x128xf32> loc(#loc9065)
      %9084 = stablehlo.convert %9083 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc9066)
      %9085 = stablehlo.multiply %9065, %9084 : tensor<32x17x8x128xbf16> loc(#loc9067)
      %9086 = stablehlo.transpose %9085, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9068)
      %9087 = stablehlo.multiply %9086, %132 : tensor<32x8x17x128xbf16> loc(#loc9069)
      %9088 = stablehlo.slice %9086 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9070)
      %9089 = stablehlo.negate %9088 : tensor<32x8x17x64xbf16> loc(#loc9071)
      %9090 = stablehlo.slice %9086 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9072)
      %9091 = stablehlo.concatenate %9089, %9090, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9073)
      %9092 = stablehlo.multiply %9091, %138 : tensor<32x8x17x128xbf16> loc(#loc9074)
      %9093 = stablehlo.add %9087, %9092 : tensor<32x8x17x128xbf16> loc(#loc9075)
      %9094 = stablehlo.convert %9093 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc9076)
      %9095 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9096 = stablehlo.multiply %9094, %9095 : tensor<32x8x17x128xf32> loc(#loc9077)
      %9097 = stablehlo.broadcast_in_dim %9049, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9078)
      %9098 = stablehlo.reshape %9097 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9079)
      %9099 = stablehlo.convert %9098 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9080)
      %9100 = stablehlo.transpose %9099, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc9081)
      %9101 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %9102 = stablehlo.multiply %9100, %9101 : tensor<32x8x128x128xf32> loc(#loc9082)
      %9103 = stablehlo.dot_general %9096, %9102, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9083)
      %9104 = stablehlo.add %9103, %159 : tensor<32x8x17x128xf32> loc(#loc9084)
      %9105 = stablehlo.convert %9104 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc9085)
      %9106 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %9107 = stablehlo.compare  EQ, %9105, %9106 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc9086)
      %9108 = stablehlo.not %9107 : tensor<32x8x17x128xi1> loc(#loc9087)
      %9109 = stablehlo.reduce(%9108 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.20931"), %arg1677: tensor<i1> loc("reduce.20931"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc9089)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc9090)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc9088)
      %9110 = stablehlo.reshape %9109 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc9091)
      %9111 = stablehlo.not %9110 : tensor<32x8x17x1xi1> loc(#loc9092)
      %9112 = stablehlo.reshape %9111 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc9093)
      %9113 = stablehlo.broadcast_in_dim %9112, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc9094)
      %9114 = stablehlo.reduce(%9104 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9095)
      %9115 = stablehlo.broadcast_in_dim %9114, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9096)
      %9116 = stablehlo.subtract %9104, %9115 : tensor<32x8x17x128xf32> loc(#loc9097)
      %9117 = stablehlo.exponential %9116 : tensor<32x8x17x128xf32> loc(#loc9098)
      %9118 = stablehlo.reduce(%9117 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9099)
      %9119 = stablehlo.broadcast_in_dim %9118, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9100)
      %9120 = stablehlo.divide %9117, %9119 : tensor<32x8x17x128xf32> loc(#loc9101)
      %9121 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9122 = stablehlo.select %9113, %9121, %9120 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc9102)
      %9123 = stablehlo.broadcast_in_dim %9056, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9103)
      %9124 = stablehlo.reshape %9123 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9104)
      %9125 = stablehlo.convert %9124 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9105)
      %9126 = stablehlo.dot_general %9122, %9125, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9106)
      %9127 = stablehlo.convert %9126 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc9107)
      %9128 = stablehlo.transpose %9127, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9108)
      %9129 = stablehlo.reshape %9128 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc9109)
      %9130 = stablehlo.reshape %arg1527 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc9110)
      %9131 = stablehlo.reshape %9130 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc9111)
      %9132 = stablehlo.transpose %9131, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc9112)
      %9133 = stablehlo.dot_general %9129, %9132, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9113)
      %9134 = "stablehlo.all_reduce"(%9133) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.20948"), %arg1677: tensor<bf16> loc("dot.20948")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9113)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9113)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9113)
      %9135 = stablehlo.reshape %9134 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9114)
      %9136 = stablehlo.add %9004, %9135 : tensor<32x17x5120xbf16> loc(#loc9115)
      %9137 = stablehlo.reshape %arg1530 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9116)
      %9138 = stablehlo.reshape %9137 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9117)
      %9139 = stablehlo.broadcast_in_dim %9138, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9118)
      %9140 = stablehlo.convert %9136 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9119)
      %9141 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9142 = stablehlo.power %9140, %9141 : tensor<32x17x5120xf32> loc(#loc9120)
      %9143 = stablehlo.reduce(%9142 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9121)
      %9144 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9145 = stablehlo.multiply %9143, %9144 : tensor<32x17xf32> loc(#loc9122)
      %9146 = stablehlo.reshape %9145 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9123)
      %9147 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9148 = stablehlo.add %9146, %9147 : tensor<32x17x1xf32> loc(#loc9124)
      %9149 = stablehlo.rsqrt %9148 : tensor<32x17x1xf32> loc(#loc9125)
      %9150 = stablehlo.reshape %9149 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9126)
      %9151 = stablehlo.broadcast_in_dim %9150, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9127)
      %9152 = stablehlo.multiply %9140, %9151 : tensor<32x17x5120xf32> loc(#loc9128)
      %9153 = stablehlo.convert %9152 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9129)
      %9154 = stablehlo.multiply %9139, %9153 : tensor<32x17x5120xbf16> loc(#loc9130)
      %9155 = stablehlo.reshape %9154 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9131)
      %9156 = stablehlo.reshape %arg1531 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9132)
      %9157 = stablehlo.reshape %9156 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9133)
      %9158 = stablehlo.transpose %9157, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9134)
      %9159 = stablehlo.dot_general %9155, %9158, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9135)
      %9160 = stablehlo.reshape %9159 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9136)
      %9161 = stablehlo.logistic %9160 : tensor<32x17x3200xbf16> loc(#loc9137)
      %9162 = stablehlo.multiply %9160, %9161 : tensor<32x17x3200xbf16> loc(#loc9138)
      %9163 = stablehlo.reshape %arg1526 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9139)
      %9164 = stablehlo.reshape %9163 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9140)
      %9165 = stablehlo.transpose %9164, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9141)
      %9166 = stablehlo.dot_general %9155, %9165, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9142)
      %9167 = stablehlo.reshape %9166 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9143)
      %9168 = stablehlo.multiply %9162, %9167 : tensor<32x17x3200xbf16> loc(#loc9144)
      %9169 = stablehlo.reshape %9168 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9145)
      %9170 = stablehlo.reshape %arg1525 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc9146)
      %9171 = stablehlo.reshape %9170 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc9147)
      %9172 = stablehlo.transpose %9171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc9148)
      %9173 = stablehlo.dot_general %9169, %9172, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9149)
      %9174 = "stablehlo.all_reduce"(%9173) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.21003"), %arg1677: tensor<bf16> loc("dot.21003")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9149)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9149)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9149)
      %9175 = stablehlo.reshape %9174 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9150)
      %9176 = stablehlo.add %9136, %9175 : tensor<32x17x5120xbf16> loc(#loc9151)
      %9177 = stablehlo.convert %9176 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9152)
      %9178 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9179 = stablehlo.power %9177, %9178 : tensor<32x17x5120xf32> loc(#loc9153)
      %9180 = stablehlo.reduce(%9179 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9154)
      %9181 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9182 = stablehlo.multiply %9180, %9181 : tensor<32x17xf32> loc(#loc9155)
      %9183 = stablehlo.reshape %9182 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9156)
      %9184 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9185 = stablehlo.add %9183, %9184 : tensor<32x17x1xf32> loc(#loc9157)
      %9186 = stablehlo.rsqrt %9185 : tensor<32x17x1xf32> loc(#loc9158)
      %9187 = stablehlo.reshape %9186 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9159)
      %9188 = stablehlo.broadcast_in_dim %9187, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9160)
      %9189 = stablehlo.multiply %9177, %9188 : tensor<32x17x5120xf32> loc(#loc9161)
      %9190 = stablehlo.convert %9189 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9162)
      %9191 = stablehlo.multiply %9062, %9190 : tensor<32x17x5120xbf16> loc(#loc9163)
      %9192 = stablehlo.reshape %9191 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9164)
      %9193 = stablehlo.reshape %arg1524 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9165)
      %9194 = stablehlo.reshape %9193 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9166)
      %9195 = stablehlo.transpose %9194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9167)
      %9196 = stablehlo.dot_general %9192, %9195, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9168)
      %9197 = stablehlo.reshape %9196 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9169)
      %9198 = stablehlo.convert %9197 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc9170)
      %9199 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %9200 = stablehlo.power %9198, %9199 : tensor<32x17x1x128xf32> loc(#loc9171)
      %9201 = stablehlo.reduce(%9200 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc9172)
      %9202 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9203 = stablehlo.multiply %9201, %9202 : tensor<32x17x1xf32> loc(#loc9173)
      %9204 = stablehlo.reshape %9203 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc9174)
      %9205 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %9206 = stablehlo.add %9204, %9205 : tensor<32x17x1x1xf32> loc(#loc9175)
      %9207 = stablehlo.rsqrt %9206 : tensor<32x17x1x1xf32> loc(#loc9176)
      %9208 = stablehlo.reshape %9207 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc9177)
      %9209 = stablehlo.broadcast_in_dim %9208, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc9178)
      %9210 = stablehlo.multiply %9198, %9209 : tensor<32x17x1x128xf32> loc(#loc9179)
      %9211 = stablehlo.convert %9210 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc9180)
      %9212 = stablehlo.multiply %9059, %9211 : tensor<32x17x1x128xbf16> loc(#loc9181)
      %9213 = stablehlo.transpose %9212, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9182)
      %9214 = stablehlo.multiply %9213, %82 : tensor<32x1x17x128xbf16> loc(#loc9183)
      %9215 = stablehlo.slice %9213 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9184)
      %9216 = stablehlo.negate %9215 : tensor<32x1x17x64xbf16> loc(#loc9185)
      %9217 = stablehlo.slice %9213 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9186)
      %9218 = stablehlo.concatenate %9216, %9217, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9187)
      %9219 = stablehlo.multiply %9218, %91 : tensor<32x1x17x128xbf16> loc(#loc9188)
      %9220 = stablehlo.add %9214, %9219 : tensor<32x1x17x128xbf16> loc(#loc9189)
      %9221 = "stablehlo.scatter"(%arg1534, %21, %9220) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.21115"), %arg1677: tensor<bf16> loc("scatter.21115")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9190)
      %9222 = stablehlo.reshape %arg1535 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9191)
      %9223 = stablehlo.reshape %9222 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9192)
      %9224 = stablehlo.transpose %9223, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9193)
      %9225 = stablehlo.dot_general %9192, %9224, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9194)
      %9226 = stablehlo.reshape %9225 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9195)
      %9227 = stablehlo.transpose %9226, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9196)
      %9228 = "stablehlo.scatter"(%arg1536, %21, %9227) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.21145"), %arg1677: tensor<bf16> loc("scatter.21145")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9197)
      %9229 = stablehlo.reshape %arg1546 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9198)
      %9230 = stablehlo.reshape %9229 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9199)
      %9231 = stablehlo.broadcast_in_dim %9230, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9200)
      %9232 = stablehlo.reshape %arg1545 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9201)
      %9233 = stablehlo.reshape %9232 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9202)
      %9234 = stablehlo.broadcast_in_dim %9233, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9203)
      %9235 = stablehlo.reshape %arg1542 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9204)
      %9236 = stablehlo.reshape %9235 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9205)
      %9237 = stablehlo.broadcast_in_dim %9236, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9206)
      %9238 = stablehlo.reshape %arg1541 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc9207)
      %9239 = stablehlo.reshape %9238 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc9208)
      %9240 = stablehlo.transpose %9239, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc9209)
      %9241 = stablehlo.dot_general %9192, %9240, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc9210)
      %9242 = stablehlo.reshape %9241 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9211)
      %9243 = stablehlo.convert %9242 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc9212)
      %9244 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %9245 = stablehlo.power %9243, %9244 : tensor<32x17x8x128xf32> loc(#loc9213)
      %9246 = stablehlo.reduce(%9245 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc9214)
      %9247 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %9248 = stablehlo.multiply %9246, %9247 : tensor<32x17x8xf32> loc(#loc9215)
      %9249 = stablehlo.reshape %9248 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc9216)
      %9250 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %9251 = stablehlo.add %9249, %9250 : tensor<32x17x8x1xf32> loc(#loc9217)
      %9252 = stablehlo.rsqrt %9251 : tensor<32x17x8x1xf32> loc(#loc9218)
      %9253 = stablehlo.reshape %9252 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc9219)
      %9254 = stablehlo.broadcast_in_dim %9253, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc9220)
      %9255 = stablehlo.multiply %9243, %9254 : tensor<32x17x8x128xf32> loc(#loc9221)
      %9256 = stablehlo.convert %9255 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc9222)
      %9257 = stablehlo.multiply %9237, %9256 : tensor<32x17x8x128xbf16> loc(#loc9223)
      %9258 = stablehlo.transpose %9257, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9224)
      %9259 = stablehlo.multiply %9258, %132 : tensor<32x8x17x128xbf16> loc(#loc9225)
      %9260 = stablehlo.slice %9258 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9226)
      %9261 = stablehlo.negate %9260 : tensor<32x8x17x64xbf16> loc(#loc9227)
      %9262 = stablehlo.slice %9258 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9228)
      %9263 = stablehlo.concatenate %9261, %9262, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9229)
      %9264 = stablehlo.multiply %9263, %138 : tensor<32x8x17x128xbf16> loc(#loc9230)
      %9265 = stablehlo.add %9259, %9264 : tensor<32x8x17x128xbf16> loc(#loc9231)
      %9266 = stablehlo.convert %9265 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc9232)
      %9267 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9268 = stablehlo.multiply %9266, %9267 : tensor<32x8x17x128xf32> loc(#loc9233)
      %9269 = stablehlo.broadcast_in_dim %9221, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9234)
      %9270 = stablehlo.reshape %9269 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9235)
      %9271 = stablehlo.convert %9270 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9236)
      %9272 = stablehlo.transpose %9271, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc9237)
      %9273 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %9274 = stablehlo.multiply %9272, %9273 : tensor<32x8x128x128xf32> loc(#loc9238)
      %9275 = stablehlo.dot_general %9268, %9274, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9239)
      %9276 = stablehlo.add %9275, %159 : tensor<32x8x17x128xf32> loc(#loc9240)
      %9277 = stablehlo.convert %9276 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc9241)
      %9278 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %9279 = stablehlo.compare  EQ, %9277, %9278 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc9242)
      %9280 = stablehlo.not %9279 : tensor<32x8x17x128xi1> loc(#loc9243)
      %9281 = stablehlo.reduce(%9280 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.21326"), %arg1677: tensor<i1> loc("reduce.21326"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc9245)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc9246)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc9244)
      %9282 = stablehlo.reshape %9281 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc9247)
      %9283 = stablehlo.not %9282 : tensor<32x8x17x1xi1> loc(#loc9248)
      %9284 = stablehlo.reshape %9283 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc9249)
      %9285 = stablehlo.broadcast_in_dim %9284, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc9250)
      %9286 = stablehlo.reduce(%9276 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9251)
      %9287 = stablehlo.broadcast_in_dim %9286, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9252)
      %9288 = stablehlo.subtract %9276, %9287 : tensor<32x8x17x128xf32> loc(#loc9253)
      %9289 = stablehlo.exponential %9288 : tensor<32x8x17x128xf32> loc(#loc9254)
      %9290 = stablehlo.reduce(%9289 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9255)
      %9291 = stablehlo.broadcast_in_dim %9290, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9256)
      %9292 = stablehlo.divide %9289, %9291 : tensor<32x8x17x128xf32> loc(#loc9257)
      %9293 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9294 = stablehlo.select %9285, %9293, %9292 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc9258)
      %9295 = stablehlo.broadcast_in_dim %9228, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9259)
      %9296 = stablehlo.reshape %9295 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9260)
      %9297 = stablehlo.convert %9296 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9261)
      %9298 = stablehlo.dot_general %9294, %9297, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9262)
      %9299 = stablehlo.convert %9298 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc9263)
      %9300 = stablehlo.transpose %9299, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9264)
      %9301 = stablehlo.reshape %9300 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc9265)
      %9302 = stablehlo.reshape %arg1540 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc9266)
      %9303 = stablehlo.reshape %9302 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc9267)
      %9304 = stablehlo.transpose %9303, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc9268)
      %9305 = stablehlo.dot_general %9301, %9304, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9269)
      %9306 = "stablehlo.all_reduce"(%9305) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.21343"), %arg1677: tensor<bf16> loc("dot.21343")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9269)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9269)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9269)
      %9307 = stablehlo.reshape %9306 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9270)
      %9308 = stablehlo.add %9176, %9307 : tensor<32x17x5120xbf16> loc(#loc9271)
      %9309 = stablehlo.reshape %arg1543 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9272)
      %9310 = stablehlo.reshape %9309 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9273)
      %9311 = stablehlo.broadcast_in_dim %9310, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9274)
      %9312 = stablehlo.convert %9308 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9275)
      %9313 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9314 = stablehlo.power %9312, %9313 : tensor<32x17x5120xf32> loc(#loc9276)
      %9315 = stablehlo.reduce(%9314 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9277)
      %9316 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9317 = stablehlo.multiply %9315, %9316 : tensor<32x17xf32> loc(#loc9278)
      %9318 = stablehlo.reshape %9317 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9279)
      %9319 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9320 = stablehlo.add %9318, %9319 : tensor<32x17x1xf32> loc(#loc9280)
      %9321 = stablehlo.rsqrt %9320 : tensor<32x17x1xf32> loc(#loc9281)
      %9322 = stablehlo.reshape %9321 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9282)
      %9323 = stablehlo.broadcast_in_dim %9322, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9283)
      %9324 = stablehlo.multiply %9312, %9323 : tensor<32x17x5120xf32> loc(#loc9284)
      %9325 = stablehlo.convert %9324 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9285)
      %9326 = stablehlo.multiply %9311, %9325 : tensor<32x17x5120xbf16> loc(#loc9286)
      %9327 = stablehlo.reshape %9326 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9287)
      %9328 = stablehlo.reshape %arg1544 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9288)
      %9329 = stablehlo.reshape %9328 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9289)
      %9330 = stablehlo.transpose %9329, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9290)
      %9331 = stablehlo.dot_general %9327, %9330, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9291)
      %9332 = stablehlo.reshape %9331 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9292)
      %9333 = stablehlo.logistic %9332 : tensor<32x17x3200xbf16> loc(#loc9293)
      %9334 = stablehlo.multiply %9332, %9333 : tensor<32x17x3200xbf16> loc(#loc9294)
      %9335 = stablehlo.reshape %arg1539 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9295)
      %9336 = stablehlo.reshape %9335 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9296)
      %9337 = stablehlo.transpose %9336, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9297)
      %9338 = stablehlo.dot_general %9327, %9337, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9298)
      %9339 = stablehlo.reshape %9338 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9299)
      %9340 = stablehlo.multiply %9334, %9339 : tensor<32x17x3200xbf16> loc(#loc9300)
      %9341 = stablehlo.reshape %9340 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9301)
      %9342 = stablehlo.reshape %arg1538 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc9302)
      %9343 = stablehlo.reshape %9342 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc9303)
      %9344 = stablehlo.transpose %9343, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc9304)
      %9345 = stablehlo.dot_general %9341, %9344, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9305)
      %9346 = "stablehlo.all_reduce"(%9345) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.21398"), %arg1677: tensor<bf16> loc("dot.21398")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9305)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9305)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9305)
      %9347 = stablehlo.reshape %9346 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9306)
      %9348 = stablehlo.add %9308, %9347 : tensor<32x17x5120xbf16> loc(#loc9307)
      %9349 = stablehlo.convert %9348 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9308)
      %9350 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9351 = stablehlo.power %9349, %9350 : tensor<32x17x5120xf32> loc(#loc9309)
      %9352 = stablehlo.reduce(%9351 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9310)
      %9353 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9354 = stablehlo.multiply %9352, %9353 : tensor<32x17xf32> loc(#loc9311)
      %9355 = stablehlo.reshape %9354 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9312)
      %9356 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9357 = stablehlo.add %9355, %9356 : tensor<32x17x1xf32> loc(#loc9313)
      %9358 = stablehlo.rsqrt %9357 : tensor<32x17x1xf32> loc(#loc9314)
      %9359 = stablehlo.reshape %9358 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9315)
      %9360 = stablehlo.broadcast_in_dim %9359, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9316)
      %9361 = stablehlo.multiply %9349, %9360 : tensor<32x17x5120xf32> loc(#loc9317)
      %9362 = stablehlo.convert %9361 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9318)
      %9363 = stablehlo.multiply %9234, %9362 : tensor<32x17x5120xbf16> loc(#loc9319)
      %9364 = stablehlo.reshape %9363 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9320)
      %9365 = stablehlo.reshape %arg1537 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9321)
      %9366 = stablehlo.reshape %9365 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9322)
      %9367 = stablehlo.transpose %9366, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9323)
      %9368 = stablehlo.dot_general %9364, %9367, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9324)
      %9369 = stablehlo.reshape %9368 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9325)
      %9370 = stablehlo.convert %9369 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc9326)
      %9371 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %9372 = stablehlo.power %9370, %9371 : tensor<32x17x1x128xf32> loc(#loc9327)
      %9373 = stablehlo.reduce(%9372 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc9328)
      %9374 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9375 = stablehlo.multiply %9373, %9374 : tensor<32x17x1xf32> loc(#loc9329)
      %9376 = stablehlo.reshape %9375 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc9330)
      %9377 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %9378 = stablehlo.add %9376, %9377 : tensor<32x17x1x1xf32> loc(#loc9331)
      %9379 = stablehlo.rsqrt %9378 : tensor<32x17x1x1xf32> loc(#loc9332)
      %9380 = stablehlo.reshape %9379 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc9333)
      %9381 = stablehlo.broadcast_in_dim %9380, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc9334)
      %9382 = stablehlo.multiply %9370, %9381 : tensor<32x17x1x128xf32> loc(#loc9335)
      %9383 = stablehlo.convert %9382 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc9336)
      %9384 = stablehlo.multiply %9231, %9383 : tensor<32x17x1x128xbf16> loc(#loc9337)
      %9385 = stablehlo.transpose %9384, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9338)
      %9386 = stablehlo.multiply %9385, %82 : tensor<32x1x17x128xbf16> loc(#loc9339)
      %9387 = stablehlo.slice %9385 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9340)
      %9388 = stablehlo.negate %9387 : tensor<32x1x17x64xbf16> loc(#loc9341)
      %9389 = stablehlo.slice %9385 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9342)
      %9390 = stablehlo.concatenate %9388, %9389, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9343)
      %9391 = stablehlo.multiply %9390, %91 : tensor<32x1x17x128xbf16> loc(#loc9344)
      %9392 = stablehlo.add %9386, %9391 : tensor<32x1x17x128xbf16> loc(#loc9345)
      %9393 = "stablehlo.scatter"(%arg1547, %21, %9392) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.21510"), %arg1677: tensor<bf16> loc("scatter.21510")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9346)
      %9394 = stablehlo.reshape %arg1548 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9347)
      %9395 = stablehlo.reshape %9394 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9348)
      %9396 = stablehlo.transpose %9395, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9349)
      %9397 = stablehlo.dot_general %9364, %9396, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9350)
      %9398 = stablehlo.reshape %9397 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9351)
      %9399 = stablehlo.transpose %9398, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9352)
      %9400 = "stablehlo.scatter"(%arg1549, %21, %9399) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.21540"), %arg1677: tensor<bf16> loc("scatter.21540")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9353)
      %9401 = stablehlo.reshape %arg1559 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9354)
      %9402 = stablehlo.reshape %9401 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9355)
      %9403 = stablehlo.broadcast_in_dim %9402, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9356)
      %9404 = stablehlo.reshape %arg1558 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9357)
      %9405 = stablehlo.reshape %9404 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9358)
      %9406 = stablehlo.broadcast_in_dim %9405, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9359)
      %9407 = stablehlo.reshape %arg1555 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9360)
      %9408 = stablehlo.reshape %9407 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9361)
      %9409 = stablehlo.broadcast_in_dim %9408, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9362)
      %9410 = stablehlo.reshape %arg1554 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc9363)
      %9411 = stablehlo.reshape %9410 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc9364)
      %9412 = stablehlo.transpose %9411, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc9365)
      %9413 = stablehlo.dot_general %9364, %9412, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc9366)
      %9414 = stablehlo.reshape %9413 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9367)
      %9415 = stablehlo.convert %9414 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc9368)
      %9416 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %9417 = stablehlo.power %9415, %9416 : tensor<32x17x8x128xf32> loc(#loc9369)
      %9418 = stablehlo.reduce(%9417 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc9370)
      %9419 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %9420 = stablehlo.multiply %9418, %9419 : tensor<32x17x8xf32> loc(#loc9371)
      %9421 = stablehlo.reshape %9420 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc9372)
      %9422 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %9423 = stablehlo.add %9421, %9422 : tensor<32x17x8x1xf32> loc(#loc9373)
      %9424 = stablehlo.rsqrt %9423 : tensor<32x17x8x1xf32> loc(#loc9374)
      %9425 = stablehlo.reshape %9424 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc9375)
      %9426 = stablehlo.broadcast_in_dim %9425, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc9376)
      %9427 = stablehlo.multiply %9415, %9426 : tensor<32x17x8x128xf32> loc(#loc9377)
      %9428 = stablehlo.convert %9427 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc9378)
      %9429 = stablehlo.multiply %9409, %9428 : tensor<32x17x8x128xbf16> loc(#loc9379)
      %9430 = stablehlo.transpose %9429, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9380)
      %9431 = stablehlo.multiply %9430, %132 : tensor<32x8x17x128xbf16> loc(#loc9381)
      %9432 = stablehlo.slice %9430 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9382)
      %9433 = stablehlo.negate %9432 : tensor<32x8x17x64xbf16> loc(#loc9383)
      %9434 = stablehlo.slice %9430 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9384)
      %9435 = stablehlo.concatenate %9433, %9434, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9385)
      %9436 = stablehlo.multiply %9435, %138 : tensor<32x8x17x128xbf16> loc(#loc9386)
      %9437 = stablehlo.add %9431, %9436 : tensor<32x8x17x128xbf16> loc(#loc9387)
      %9438 = stablehlo.convert %9437 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc9388)
      %9439 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9440 = stablehlo.multiply %9438, %9439 : tensor<32x8x17x128xf32> loc(#loc9389)
      %9441 = stablehlo.broadcast_in_dim %9393, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9390)
      %9442 = stablehlo.reshape %9441 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9391)
      %9443 = stablehlo.convert %9442 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9392)
      %9444 = stablehlo.transpose %9443, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc9393)
      %9445 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %9446 = stablehlo.multiply %9444, %9445 : tensor<32x8x128x128xf32> loc(#loc9394)
      %9447 = stablehlo.dot_general %9440, %9446, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9395)
      %9448 = stablehlo.add %9447, %159 : tensor<32x8x17x128xf32> loc(#loc9396)
      %9449 = stablehlo.convert %9448 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc9397)
      %9450 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %9451 = stablehlo.compare  EQ, %9449, %9450 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc9398)
      %9452 = stablehlo.not %9451 : tensor<32x8x17x128xi1> loc(#loc9399)
      %9453 = stablehlo.reduce(%9452 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.21721"), %arg1677: tensor<i1> loc("reduce.21721"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc9401)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc9402)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc9400)
      %9454 = stablehlo.reshape %9453 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc9403)
      %9455 = stablehlo.not %9454 : tensor<32x8x17x1xi1> loc(#loc9404)
      %9456 = stablehlo.reshape %9455 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc9405)
      %9457 = stablehlo.broadcast_in_dim %9456, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc9406)
      %9458 = stablehlo.reduce(%9448 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9407)
      %9459 = stablehlo.broadcast_in_dim %9458, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9408)
      %9460 = stablehlo.subtract %9448, %9459 : tensor<32x8x17x128xf32> loc(#loc9409)
      %9461 = stablehlo.exponential %9460 : tensor<32x8x17x128xf32> loc(#loc9410)
      %9462 = stablehlo.reduce(%9461 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9411)
      %9463 = stablehlo.broadcast_in_dim %9462, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9412)
      %9464 = stablehlo.divide %9461, %9463 : tensor<32x8x17x128xf32> loc(#loc9413)
      %9465 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9466 = stablehlo.select %9457, %9465, %9464 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc9414)
      %9467 = stablehlo.broadcast_in_dim %9400, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9415)
      %9468 = stablehlo.reshape %9467 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9416)
      %9469 = stablehlo.convert %9468 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9417)
      %9470 = stablehlo.dot_general %9466, %9469, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9418)
      %9471 = stablehlo.convert %9470 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc9419)
      %9472 = stablehlo.transpose %9471, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9420)
      %9473 = stablehlo.reshape %9472 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc9421)
      %9474 = stablehlo.reshape %arg1553 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc9422)
      %9475 = stablehlo.reshape %9474 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc9423)
      %9476 = stablehlo.transpose %9475, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc9424)
      %9477 = stablehlo.dot_general %9473, %9476, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9425)
      %9478 = "stablehlo.all_reduce"(%9477) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.21738"), %arg1677: tensor<bf16> loc("dot.21738")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9425)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9425)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9425)
      %9479 = stablehlo.reshape %9478 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9426)
      %9480 = stablehlo.add %9348, %9479 : tensor<32x17x5120xbf16> loc(#loc9427)
      %9481 = stablehlo.reshape %arg1556 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9428)
      %9482 = stablehlo.reshape %9481 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9429)
      %9483 = stablehlo.broadcast_in_dim %9482, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9430)
      %9484 = stablehlo.convert %9480 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9431)
      %9485 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9486 = stablehlo.power %9484, %9485 : tensor<32x17x5120xf32> loc(#loc9432)
      %9487 = stablehlo.reduce(%9486 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9433)
      %9488 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9489 = stablehlo.multiply %9487, %9488 : tensor<32x17xf32> loc(#loc9434)
      %9490 = stablehlo.reshape %9489 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9435)
      %9491 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9492 = stablehlo.add %9490, %9491 : tensor<32x17x1xf32> loc(#loc9436)
      %9493 = stablehlo.rsqrt %9492 : tensor<32x17x1xf32> loc(#loc9437)
      %9494 = stablehlo.reshape %9493 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9438)
      %9495 = stablehlo.broadcast_in_dim %9494, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9439)
      %9496 = stablehlo.multiply %9484, %9495 : tensor<32x17x5120xf32> loc(#loc9440)
      %9497 = stablehlo.convert %9496 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9441)
      %9498 = stablehlo.multiply %9483, %9497 : tensor<32x17x5120xbf16> loc(#loc9442)
      %9499 = stablehlo.reshape %9498 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9443)
      %9500 = stablehlo.reshape %arg1557 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9444)
      %9501 = stablehlo.reshape %9500 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9445)
      %9502 = stablehlo.transpose %9501, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9446)
      %9503 = stablehlo.dot_general %9499, %9502, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9447)
      %9504 = stablehlo.reshape %9503 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9448)
      %9505 = stablehlo.logistic %9504 : tensor<32x17x3200xbf16> loc(#loc9449)
      %9506 = stablehlo.multiply %9504, %9505 : tensor<32x17x3200xbf16> loc(#loc9450)
      %9507 = stablehlo.reshape %arg1552 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9451)
      %9508 = stablehlo.reshape %9507 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9452)
      %9509 = stablehlo.transpose %9508, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9453)
      %9510 = stablehlo.dot_general %9499, %9509, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9454)
      %9511 = stablehlo.reshape %9510 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9455)
      %9512 = stablehlo.multiply %9506, %9511 : tensor<32x17x3200xbf16> loc(#loc9456)
      %9513 = stablehlo.reshape %9512 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9457)
      %9514 = stablehlo.reshape %arg1551 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc9458)
      %9515 = stablehlo.reshape %9514 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc9459)
      %9516 = stablehlo.transpose %9515, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc9460)
      %9517 = stablehlo.dot_general %9513, %9516, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9461)
      %9518 = "stablehlo.all_reduce"(%9517) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.21793"), %arg1677: tensor<bf16> loc("dot.21793")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9461)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9461)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9461)
      %9519 = stablehlo.reshape %9518 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9462)
      %9520 = stablehlo.add %9480, %9519 : tensor<32x17x5120xbf16> loc(#loc9463)
      %9521 = stablehlo.convert %9520 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9464)
      %9522 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9523 = stablehlo.power %9521, %9522 : tensor<32x17x5120xf32> loc(#loc9465)
      %9524 = stablehlo.reduce(%9523 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9466)
      %9525 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9526 = stablehlo.multiply %9524, %9525 : tensor<32x17xf32> loc(#loc9467)
      %9527 = stablehlo.reshape %9526 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9468)
      %9528 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9529 = stablehlo.add %9527, %9528 : tensor<32x17x1xf32> loc(#loc9469)
      %9530 = stablehlo.rsqrt %9529 : tensor<32x17x1xf32> loc(#loc9470)
      %9531 = stablehlo.reshape %9530 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9471)
      %9532 = stablehlo.broadcast_in_dim %9531, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9472)
      %9533 = stablehlo.multiply %9521, %9532 : tensor<32x17x5120xf32> loc(#loc9473)
      %9534 = stablehlo.convert %9533 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9474)
      %9535 = stablehlo.multiply %9406, %9534 : tensor<32x17x5120xbf16> loc(#loc9475)
      %9536 = stablehlo.reshape %9535 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9476)
      %9537 = stablehlo.reshape %arg1550 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9477)
      %9538 = stablehlo.reshape %9537 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9478)
      %9539 = stablehlo.transpose %9538, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9479)
      %9540 = stablehlo.dot_general %9536, %9539, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9480)
      %9541 = stablehlo.reshape %9540 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9481)
      %9542 = stablehlo.convert %9541 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc9482)
      %9543 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %9544 = stablehlo.power %9542, %9543 : tensor<32x17x1x128xf32> loc(#loc9483)
      %9545 = stablehlo.reduce(%9544 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc9484)
      %9546 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9547 = stablehlo.multiply %9545, %9546 : tensor<32x17x1xf32> loc(#loc9485)
      %9548 = stablehlo.reshape %9547 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc9486)
      %9549 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %9550 = stablehlo.add %9548, %9549 : tensor<32x17x1x1xf32> loc(#loc9487)
      %9551 = stablehlo.rsqrt %9550 : tensor<32x17x1x1xf32> loc(#loc9488)
      %9552 = stablehlo.reshape %9551 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc9489)
      %9553 = stablehlo.broadcast_in_dim %9552, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc9490)
      %9554 = stablehlo.multiply %9542, %9553 : tensor<32x17x1x128xf32> loc(#loc9491)
      %9555 = stablehlo.convert %9554 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc9492)
      %9556 = stablehlo.multiply %9403, %9555 : tensor<32x17x1x128xbf16> loc(#loc9493)
      %9557 = stablehlo.transpose %9556, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9494)
      %9558 = stablehlo.multiply %9557, %82 : tensor<32x1x17x128xbf16> loc(#loc9495)
      %9559 = stablehlo.slice %9557 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9496)
      %9560 = stablehlo.negate %9559 : tensor<32x1x17x64xbf16> loc(#loc9497)
      %9561 = stablehlo.slice %9557 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9498)
      %9562 = stablehlo.concatenate %9560, %9561, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9499)
      %9563 = stablehlo.multiply %9562, %91 : tensor<32x1x17x128xbf16> loc(#loc9500)
      %9564 = stablehlo.add %9558, %9563 : tensor<32x1x17x128xbf16> loc(#loc9501)
      %9565 = "stablehlo.scatter"(%arg1560, %21, %9564) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.21905"), %arg1677: tensor<bf16> loc("scatter.21905")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9502)
      %9566 = stablehlo.reshape %arg1561 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9503)
      %9567 = stablehlo.reshape %9566 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9504)
      %9568 = stablehlo.transpose %9567, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9505)
      %9569 = stablehlo.dot_general %9536, %9568, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9506)
      %9570 = stablehlo.reshape %9569 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9507)
      %9571 = stablehlo.transpose %9570, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9508)
      %9572 = "stablehlo.scatter"(%arg1562, %21, %9571) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.21935"), %arg1677: tensor<bf16> loc("scatter.21935")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9509)
      %9573 = stablehlo.reshape %arg1572 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9510)
      %9574 = stablehlo.reshape %9573 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9511)
      %9575 = stablehlo.broadcast_in_dim %9574, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9512)
      %9576 = stablehlo.reshape %arg1571 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9513)
      %9577 = stablehlo.reshape %9576 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9514)
      %9578 = stablehlo.broadcast_in_dim %9577, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9515)
      %9579 = stablehlo.reshape %arg1568 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9516)
      %9580 = stablehlo.reshape %9579 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9517)
      %9581 = stablehlo.broadcast_in_dim %9580, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9518)
      %9582 = stablehlo.reshape %arg1567 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc9519)
      %9583 = stablehlo.reshape %9582 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc9520)
      %9584 = stablehlo.transpose %9583, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc9521)
      %9585 = stablehlo.dot_general %9536, %9584, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc9522)
      %9586 = stablehlo.reshape %9585 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9523)
      %9587 = stablehlo.convert %9586 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc9524)
      %9588 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %9589 = stablehlo.power %9587, %9588 : tensor<32x17x8x128xf32> loc(#loc9525)
      %9590 = stablehlo.reduce(%9589 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc9526)
      %9591 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %9592 = stablehlo.multiply %9590, %9591 : tensor<32x17x8xf32> loc(#loc9527)
      %9593 = stablehlo.reshape %9592 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc9528)
      %9594 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %9595 = stablehlo.add %9593, %9594 : tensor<32x17x8x1xf32> loc(#loc9529)
      %9596 = stablehlo.rsqrt %9595 : tensor<32x17x8x1xf32> loc(#loc9530)
      %9597 = stablehlo.reshape %9596 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc9531)
      %9598 = stablehlo.broadcast_in_dim %9597, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc9532)
      %9599 = stablehlo.multiply %9587, %9598 : tensor<32x17x8x128xf32> loc(#loc9533)
      %9600 = stablehlo.convert %9599 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc9534)
      %9601 = stablehlo.multiply %9581, %9600 : tensor<32x17x8x128xbf16> loc(#loc9535)
      %9602 = stablehlo.transpose %9601, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9536)
      %9603 = stablehlo.multiply %9602, %132 : tensor<32x8x17x128xbf16> loc(#loc9537)
      %9604 = stablehlo.slice %9602 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9538)
      %9605 = stablehlo.negate %9604 : tensor<32x8x17x64xbf16> loc(#loc9539)
      %9606 = stablehlo.slice %9602 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9540)
      %9607 = stablehlo.concatenate %9605, %9606, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9541)
      %9608 = stablehlo.multiply %9607, %138 : tensor<32x8x17x128xbf16> loc(#loc9542)
      %9609 = stablehlo.add %9603, %9608 : tensor<32x8x17x128xbf16> loc(#loc9543)
      %9610 = stablehlo.convert %9609 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc9544)
      %9611 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9612 = stablehlo.multiply %9610, %9611 : tensor<32x8x17x128xf32> loc(#loc9545)
      %9613 = stablehlo.broadcast_in_dim %9565, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9546)
      %9614 = stablehlo.reshape %9613 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9547)
      %9615 = stablehlo.convert %9614 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9548)
      %9616 = stablehlo.transpose %9615, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc9549)
      %9617 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %9618 = stablehlo.multiply %9616, %9617 : tensor<32x8x128x128xf32> loc(#loc9550)
      %9619 = stablehlo.dot_general %9612, %9618, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9551)
      %9620 = stablehlo.add %9619, %159 : tensor<32x8x17x128xf32> loc(#loc9552)
      %9621 = stablehlo.convert %9620 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc9553)
      %9622 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %9623 = stablehlo.compare  EQ, %9621, %9622 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc9554)
      %9624 = stablehlo.not %9623 : tensor<32x8x17x128xi1> loc(#loc9555)
      %9625 = stablehlo.reduce(%9624 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.22116"), %arg1677: tensor<i1> loc("reduce.22116"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc9557)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc9558)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc9556)
      %9626 = stablehlo.reshape %9625 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc9559)
      %9627 = stablehlo.not %9626 : tensor<32x8x17x1xi1> loc(#loc9560)
      %9628 = stablehlo.reshape %9627 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc9561)
      %9629 = stablehlo.broadcast_in_dim %9628, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc9562)
      %9630 = stablehlo.reduce(%9620 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9563)
      %9631 = stablehlo.broadcast_in_dim %9630, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9564)
      %9632 = stablehlo.subtract %9620, %9631 : tensor<32x8x17x128xf32> loc(#loc9565)
      %9633 = stablehlo.exponential %9632 : tensor<32x8x17x128xf32> loc(#loc9566)
      %9634 = stablehlo.reduce(%9633 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9567)
      %9635 = stablehlo.broadcast_in_dim %9634, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9568)
      %9636 = stablehlo.divide %9633, %9635 : tensor<32x8x17x128xf32> loc(#loc9569)
      %9637 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9638 = stablehlo.select %9629, %9637, %9636 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc9570)
      %9639 = stablehlo.broadcast_in_dim %9572, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9571)
      %9640 = stablehlo.reshape %9639 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9572)
      %9641 = stablehlo.convert %9640 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9573)
      %9642 = stablehlo.dot_general %9638, %9641, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9574)
      %9643 = stablehlo.convert %9642 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc9575)
      %9644 = stablehlo.transpose %9643, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9576)
      %9645 = stablehlo.reshape %9644 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc9577)
      %9646 = stablehlo.reshape %arg1566 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc9578)
      %9647 = stablehlo.reshape %9646 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc9579)
      %9648 = stablehlo.transpose %9647, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc9580)
      %9649 = stablehlo.dot_general %9645, %9648, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9581)
      %9650 = "stablehlo.all_reduce"(%9649) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.22133"), %arg1677: tensor<bf16> loc("dot.22133")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9581)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9581)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9581)
      %9651 = stablehlo.reshape %9650 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9582)
      %9652 = stablehlo.add %9520, %9651 : tensor<32x17x5120xbf16> loc(#loc9583)
      %9653 = stablehlo.reshape %arg1569 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9584)
      %9654 = stablehlo.reshape %9653 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9585)
      %9655 = stablehlo.broadcast_in_dim %9654, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9586)
      %9656 = stablehlo.convert %9652 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9587)
      %9657 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9658 = stablehlo.power %9656, %9657 : tensor<32x17x5120xf32> loc(#loc9588)
      %9659 = stablehlo.reduce(%9658 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9589)
      %9660 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9661 = stablehlo.multiply %9659, %9660 : tensor<32x17xf32> loc(#loc9590)
      %9662 = stablehlo.reshape %9661 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9591)
      %9663 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9664 = stablehlo.add %9662, %9663 : tensor<32x17x1xf32> loc(#loc9592)
      %9665 = stablehlo.rsqrt %9664 : tensor<32x17x1xf32> loc(#loc9593)
      %9666 = stablehlo.reshape %9665 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9594)
      %9667 = stablehlo.broadcast_in_dim %9666, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9595)
      %9668 = stablehlo.multiply %9656, %9667 : tensor<32x17x5120xf32> loc(#loc9596)
      %9669 = stablehlo.convert %9668 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9597)
      %9670 = stablehlo.multiply %9655, %9669 : tensor<32x17x5120xbf16> loc(#loc9598)
      %9671 = stablehlo.reshape %9670 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9599)
      %9672 = stablehlo.reshape %arg1570 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9600)
      %9673 = stablehlo.reshape %9672 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9601)
      %9674 = stablehlo.transpose %9673, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9602)
      %9675 = stablehlo.dot_general %9671, %9674, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9603)
      %9676 = stablehlo.reshape %9675 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9604)
      %9677 = stablehlo.logistic %9676 : tensor<32x17x3200xbf16> loc(#loc9605)
      %9678 = stablehlo.multiply %9676, %9677 : tensor<32x17x3200xbf16> loc(#loc9606)
      %9679 = stablehlo.reshape %arg1565 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9607)
      %9680 = stablehlo.reshape %9679 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9608)
      %9681 = stablehlo.transpose %9680, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9609)
      %9682 = stablehlo.dot_general %9671, %9681, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9610)
      %9683 = stablehlo.reshape %9682 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9611)
      %9684 = stablehlo.multiply %9678, %9683 : tensor<32x17x3200xbf16> loc(#loc9612)
      %9685 = stablehlo.reshape %9684 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9613)
      %9686 = stablehlo.reshape %arg1564 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc9614)
      %9687 = stablehlo.reshape %9686 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc9615)
      %9688 = stablehlo.transpose %9687, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc9616)
      %9689 = stablehlo.dot_general %9685, %9688, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9617)
      %9690 = "stablehlo.all_reduce"(%9689) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.22188"), %arg1677: tensor<bf16> loc("dot.22188")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9617)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9617)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9617)
      %9691 = stablehlo.reshape %9690 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9618)
      %9692 = stablehlo.add %9652, %9691 : tensor<32x17x5120xbf16> loc(#loc9619)
      %9693 = stablehlo.convert %9692 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9620)
      %9694 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9695 = stablehlo.power %9693, %9694 : tensor<32x17x5120xf32> loc(#loc9621)
      %9696 = stablehlo.reduce(%9695 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9622)
      %9697 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9698 = stablehlo.multiply %9696, %9697 : tensor<32x17xf32> loc(#loc9623)
      %9699 = stablehlo.reshape %9698 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9624)
      %9700 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9701 = stablehlo.add %9699, %9700 : tensor<32x17x1xf32> loc(#loc9625)
      %9702 = stablehlo.rsqrt %9701 : tensor<32x17x1xf32> loc(#loc9626)
      %9703 = stablehlo.reshape %9702 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9627)
      %9704 = stablehlo.broadcast_in_dim %9703, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9628)
      %9705 = stablehlo.multiply %9693, %9704 : tensor<32x17x5120xf32> loc(#loc9629)
      %9706 = stablehlo.convert %9705 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9630)
      %9707 = stablehlo.multiply %9578, %9706 : tensor<32x17x5120xbf16> loc(#loc9631)
      %9708 = stablehlo.reshape %9707 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9632)
      %9709 = stablehlo.reshape %arg1563 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9633)
      %9710 = stablehlo.reshape %9709 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9634)
      %9711 = stablehlo.transpose %9710, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9635)
      %9712 = stablehlo.dot_general %9708, %9711, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9636)
      %9713 = stablehlo.reshape %9712 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9637)
      %9714 = stablehlo.convert %9713 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc9638)
      %9715 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %9716 = stablehlo.power %9714, %9715 : tensor<32x17x1x128xf32> loc(#loc9639)
      %9717 = stablehlo.reduce(%9716 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc9640)
      %9718 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9719 = stablehlo.multiply %9717, %9718 : tensor<32x17x1xf32> loc(#loc9641)
      %9720 = stablehlo.reshape %9719 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc9642)
      %9721 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %9722 = stablehlo.add %9720, %9721 : tensor<32x17x1x1xf32> loc(#loc9643)
      %9723 = stablehlo.rsqrt %9722 : tensor<32x17x1x1xf32> loc(#loc9644)
      %9724 = stablehlo.reshape %9723 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc9645)
      %9725 = stablehlo.broadcast_in_dim %9724, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc9646)
      %9726 = stablehlo.multiply %9714, %9725 : tensor<32x17x1x128xf32> loc(#loc9647)
      %9727 = stablehlo.convert %9726 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc9648)
      %9728 = stablehlo.multiply %9575, %9727 : tensor<32x17x1x128xbf16> loc(#loc9649)
      %9729 = stablehlo.transpose %9728, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9650)
      %9730 = stablehlo.multiply %9729, %82 : tensor<32x1x17x128xbf16> loc(#loc9651)
      %9731 = stablehlo.slice %9729 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9652)
      %9732 = stablehlo.negate %9731 : tensor<32x1x17x64xbf16> loc(#loc9653)
      %9733 = stablehlo.slice %9729 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9654)
      %9734 = stablehlo.concatenate %9732, %9733, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9655)
      %9735 = stablehlo.multiply %9734, %91 : tensor<32x1x17x128xbf16> loc(#loc9656)
      %9736 = stablehlo.add %9730, %9735 : tensor<32x1x17x128xbf16> loc(#loc9657)
      %9737 = "stablehlo.scatter"(%arg1573, %21, %9736) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.22300"), %arg1677: tensor<bf16> loc("scatter.22300")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9658)
      %9738 = stablehlo.reshape %arg1574 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9659)
      %9739 = stablehlo.reshape %9738 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9660)
      %9740 = stablehlo.transpose %9739, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9661)
      %9741 = stablehlo.dot_general %9708, %9740, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9662)
      %9742 = stablehlo.reshape %9741 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9663)
      %9743 = stablehlo.transpose %9742, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9664)
      %9744 = "stablehlo.scatter"(%arg1575, %21, %9743) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.22330"), %arg1677: tensor<bf16> loc("scatter.22330")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9665)
      %9745 = stablehlo.reshape %arg1585 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9666)
      %9746 = stablehlo.reshape %9745 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9667)
      %9747 = stablehlo.broadcast_in_dim %9746, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9668)
      %9748 = stablehlo.reshape %arg1584 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9669)
      %9749 = stablehlo.reshape %9748 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9670)
      %9750 = stablehlo.broadcast_in_dim %9749, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9671)
      %9751 = stablehlo.reshape %arg1581 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9672)
      %9752 = stablehlo.reshape %9751 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9673)
      %9753 = stablehlo.broadcast_in_dim %9752, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9674)
      %9754 = stablehlo.reshape %arg1580 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc9675)
      %9755 = stablehlo.reshape %9754 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc9676)
      %9756 = stablehlo.transpose %9755, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc9677)
      %9757 = stablehlo.dot_general %9708, %9756, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc9678)
      %9758 = stablehlo.reshape %9757 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9679)
      %9759 = stablehlo.convert %9758 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc9680)
      %9760 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %9761 = stablehlo.power %9759, %9760 : tensor<32x17x8x128xf32> loc(#loc9681)
      %9762 = stablehlo.reduce(%9761 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc9682)
      %9763 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %9764 = stablehlo.multiply %9762, %9763 : tensor<32x17x8xf32> loc(#loc9683)
      %9765 = stablehlo.reshape %9764 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc9684)
      %9766 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %9767 = stablehlo.add %9765, %9766 : tensor<32x17x8x1xf32> loc(#loc9685)
      %9768 = stablehlo.rsqrt %9767 : tensor<32x17x8x1xf32> loc(#loc9686)
      %9769 = stablehlo.reshape %9768 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc9687)
      %9770 = stablehlo.broadcast_in_dim %9769, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc9688)
      %9771 = stablehlo.multiply %9759, %9770 : tensor<32x17x8x128xf32> loc(#loc9689)
      %9772 = stablehlo.convert %9771 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc9690)
      %9773 = stablehlo.multiply %9753, %9772 : tensor<32x17x8x128xbf16> loc(#loc9691)
      %9774 = stablehlo.transpose %9773, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9692)
      %9775 = stablehlo.multiply %9774, %132 : tensor<32x8x17x128xbf16> loc(#loc9693)
      %9776 = stablehlo.slice %9774 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9694)
      %9777 = stablehlo.negate %9776 : tensor<32x8x17x64xbf16> loc(#loc9695)
      %9778 = stablehlo.slice %9774 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9696)
      %9779 = stablehlo.concatenate %9777, %9778, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9697)
      %9780 = stablehlo.multiply %9779, %138 : tensor<32x8x17x128xbf16> loc(#loc9698)
      %9781 = stablehlo.add %9775, %9780 : tensor<32x8x17x128xbf16> loc(#loc9699)
      %9782 = stablehlo.convert %9781 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc9700)
      %9783 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9784 = stablehlo.multiply %9782, %9783 : tensor<32x8x17x128xf32> loc(#loc9701)
      %9785 = stablehlo.broadcast_in_dim %9737, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9702)
      %9786 = stablehlo.reshape %9785 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9703)
      %9787 = stablehlo.convert %9786 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9704)
      %9788 = stablehlo.transpose %9787, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc9705)
      %9789 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %9790 = stablehlo.multiply %9788, %9789 : tensor<32x8x128x128xf32> loc(#loc9706)
      %9791 = stablehlo.dot_general %9784, %9790, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9707)
      %9792 = stablehlo.add %9791, %159 : tensor<32x8x17x128xf32> loc(#loc9708)
      %9793 = stablehlo.convert %9792 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc9709)
      %9794 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %9795 = stablehlo.compare  EQ, %9793, %9794 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc9710)
      %9796 = stablehlo.not %9795 : tensor<32x8x17x128xi1> loc(#loc9711)
      %9797 = stablehlo.reduce(%9796 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.22511"), %arg1677: tensor<i1> loc("reduce.22511"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc9713)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc9714)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc9712)
      %9798 = stablehlo.reshape %9797 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc9715)
      %9799 = stablehlo.not %9798 : tensor<32x8x17x1xi1> loc(#loc9716)
      %9800 = stablehlo.reshape %9799 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc9717)
      %9801 = stablehlo.broadcast_in_dim %9800, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc9718)
      %9802 = stablehlo.reduce(%9792 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9719)
      %9803 = stablehlo.broadcast_in_dim %9802, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9720)
      %9804 = stablehlo.subtract %9792, %9803 : tensor<32x8x17x128xf32> loc(#loc9721)
      %9805 = stablehlo.exponential %9804 : tensor<32x8x17x128xf32> loc(#loc9722)
      %9806 = stablehlo.reduce(%9805 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9723)
      %9807 = stablehlo.broadcast_in_dim %9806, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9724)
      %9808 = stablehlo.divide %9805, %9807 : tensor<32x8x17x128xf32> loc(#loc9725)
      %9809 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9810 = stablehlo.select %9801, %9809, %9808 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc9726)
      %9811 = stablehlo.broadcast_in_dim %9744, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9727)
      %9812 = stablehlo.reshape %9811 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9728)
      %9813 = stablehlo.convert %9812 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9729)
      %9814 = stablehlo.dot_general %9810, %9813, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9730)
      %9815 = stablehlo.convert %9814 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc9731)
      %9816 = stablehlo.transpose %9815, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9732)
      %9817 = stablehlo.reshape %9816 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc9733)
      %9818 = stablehlo.reshape %arg1579 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc9734)
      %9819 = stablehlo.reshape %9818 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc9735)
      %9820 = stablehlo.transpose %9819, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc9736)
      %9821 = stablehlo.dot_general %9817, %9820, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9737)
      %9822 = "stablehlo.all_reduce"(%9821) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.22528"), %arg1677: tensor<bf16> loc("dot.22528")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9737)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9737)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9737)
      %9823 = stablehlo.reshape %9822 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9738)
      %9824 = stablehlo.add %9692, %9823 : tensor<32x17x5120xbf16> loc(#loc9739)
      %9825 = stablehlo.reshape %arg1582 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9740)
      %9826 = stablehlo.reshape %9825 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9741)
      %9827 = stablehlo.broadcast_in_dim %9826, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9742)
      %9828 = stablehlo.convert %9824 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9743)
      %9829 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9830 = stablehlo.power %9828, %9829 : tensor<32x17x5120xf32> loc(#loc9744)
      %9831 = stablehlo.reduce(%9830 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9745)
      %9832 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9833 = stablehlo.multiply %9831, %9832 : tensor<32x17xf32> loc(#loc9746)
      %9834 = stablehlo.reshape %9833 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9747)
      %9835 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9836 = stablehlo.add %9834, %9835 : tensor<32x17x1xf32> loc(#loc9748)
      %9837 = stablehlo.rsqrt %9836 : tensor<32x17x1xf32> loc(#loc9749)
      %9838 = stablehlo.reshape %9837 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9750)
      %9839 = stablehlo.broadcast_in_dim %9838, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9751)
      %9840 = stablehlo.multiply %9828, %9839 : tensor<32x17x5120xf32> loc(#loc9752)
      %9841 = stablehlo.convert %9840 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9753)
      %9842 = stablehlo.multiply %9827, %9841 : tensor<32x17x5120xbf16> loc(#loc9754)
      %9843 = stablehlo.reshape %9842 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9755)
      %9844 = stablehlo.reshape %arg1583 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9756)
      %9845 = stablehlo.reshape %9844 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9757)
      %9846 = stablehlo.transpose %9845, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9758)
      %9847 = stablehlo.dot_general %9843, %9846, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9759)
      %9848 = stablehlo.reshape %9847 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9760)
      %9849 = stablehlo.logistic %9848 : tensor<32x17x3200xbf16> loc(#loc9761)
      %9850 = stablehlo.multiply %9848, %9849 : tensor<32x17x3200xbf16> loc(#loc9762)
      %9851 = stablehlo.reshape %arg1578 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9763)
      %9852 = stablehlo.reshape %9851 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9764)
      %9853 = stablehlo.transpose %9852, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9765)
      %9854 = stablehlo.dot_general %9843, %9853, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9766)
      %9855 = stablehlo.reshape %9854 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9767)
      %9856 = stablehlo.multiply %9850, %9855 : tensor<32x17x3200xbf16> loc(#loc9768)
      %9857 = stablehlo.reshape %9856 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9769)
      %9858 = stablehlo.reshape %arg1577 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc9770)
      %9859 = stablehlo.reshape %9858 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc9771)
      %9860 = stablehlo.transpose %9859, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc9772)
      %9861 = stablehlo.dot_general %9857, %9860, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9773)
      %9862 = "stablehlo.all_reduce"(%9861) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.22583"), %arg1677: tensor<bf16> loc("dot.22583")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9773)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9773)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9773)
      %9863 = stablehlo.reshape %9862 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9774)
      %9864 = stablehlo.add %9824, %9863 : tensor<32x17x5120xbf16> loc(#loc9775)
      %9865 = stablehlo.convert %9864 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9776)
      %9866 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %9867 = stablehlo.power %9865, %9866 : tensor<32x17x5120xf32> loc(#loc9777)
      %9868 = stablehlo.reduce(%9867 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9778)
      %9869 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %9870 = stablehlo.multiply %9868, %9869 : tensor<32x17xf32> loc(#loc9779)
      %9871 = stablehlo.reshape %9870 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9780)
      %9872 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9873 = stablehlo.add %9871, %9872 : tensor<32x17x1xf32> loc(#loc9781)
      %9874 = stablehlo.rsqrt %9873 : tensor<32x17x1xf32> loc(#loc9782)
      %9875 = stablehlo.reshape %9874 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9783)
      %9876 = stablehlo.broadcast_in_dim %9875, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9784)
      %9877 = stablehlo.multiply %9865, %9876 : tensor<32x17x5120xf32> loc(#loc9785)
      %9878 = stablehlo.convert %9877 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9786)
      %9879 = stablehlo.multiply %9750, %9878 : tensor<32x17x5120xbf16> loc(#loc9787)
      %9880 = stablehlo.reshape %9879 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9788)
      %9881 = stablehlo.reshape %arg1576 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9789)
      %9882 = stablehlo.reshape %9881 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9790)
      %9883 = stablehlo.transpose %9882, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9791)
      %9884 = stablehlo.dot_general %9880, %9883, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9792)
      %9885 = stablehlo.reshape %9884 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9793)
      %9886 = stablehlo.convert %9885 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc9794)
      %9887 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %9888 = stablehlo.power %9886, %9887 : tensor<32x17x1x128xf32> loc(#loc9795)
      %9889 = stablehlo.reduce(%9888 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc9796)
      %9890 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %9891 = stablehlo.multiply %9889, %9890 : tensor<32x17x1xf32> loc(#loc9797)
      %9892 = stablehlo.reshape %9891 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc9798)
      %9893 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %9894 = stablehlo.add %9892, %9893 : tensor<32x17x1x1xf32> loc(#loc9799)
      %9895 = stablehlo.rsqrt %9894 : tensor<32x17x1x1xf32> loc(#loc9800)
      %9896 = stablehlo.reshape %9895 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc9801)
      %9897 = stablehlo.broadcast_in_dim %9896, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc9802)
      %9898 = stablehlo.multiply %9886, %9897 : tensor<32x17x1x128xf32> loc(#loc9803)
      %9899 = stablehlo.convert %9898 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc9804)
      %9900 = stablehlo.multiply %9747, %9899 : tensor<32x17x1x128xbf16> loc(#loc9805)
      %9901 = stablehlo.transpose %9900, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9806)
      %9902 = stablehlo.multiply %9901, %82 : tensor<32x1x17x128xbf16> loc(#loc9807)
      %9903 = stablehlo.slice %9901 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9808)
      %9904 = stablehlo.negate %9903 : tensor<32x1x17x64xbf16> loc(#loc9809)
      %9905 = stablehlo.slice %9901 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9810)
      %9906 = stablehlo.concatenate %9904, %9905, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9811)
      %9907 = stablehlo.multiply %9906, %91 : tensor<32x1x17x128xbf16> loc(#loc9812)
      %9908 = stablehlo.add %9902, %9907 : tensor<32x1x17x128xbf16> loc(#loc9813)
      %9909 = "stablehlo.scatter"(%arg1586, %21, %9908) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.22695"), %arg1677: tensor<bf16> loc("scatter.22695")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9814)
      %9910 = stablehlo.reshape %arg1587 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9815)
      %9911 = stablehlo.reshape %9910 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9816)
      %9912 = stablehlo.transpose %9911, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9817)
      %9913 = stablehlo.dot_general %9880, %9912, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9818)
      %9914 = stablehlo.reshape %9913 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9819)
      %9915 = stablehlo.transpose %9914, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9820)
      %9916 = "stablehlo.scatter"(%arg1588, %21, %9915) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.22725"), %arg1677: tensor<bf16> loc("scatter.22725")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9821)
      %9917 = stablehlo.reshape %arg1598 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9822)
      %9918 = stablehlo.reshape %9917 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9823)
      %9919 = stablehlo.broadcast_in_dim %9918, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9824)
      %9920 = stablehlo.reshape %arg1597 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9825)
      %9921 = stablehlo.reshape %9920 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9826)
      %9922 = stablehlo.broadcast_in_dim %9921, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9827)
      %9923 = stablehlo.reshape %arg1594 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9828)
      %9924 = stablehlo.reshape %9923 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9829)
      %9925 = stablehlo.broadcast_in_dim %9924, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9830)
      %9926 = stablehlo.reshape %arg1593 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc9831)
      %9927 = stablehlo.reshape %9926 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc9832)
      %9928 = stablehlo.transpose %9927, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc9833)
      %9929 = stablehlo.dot_general %9880, %9928, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc9834)
      %9930 = stablehlo.reshape %9929 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9835)
      %9931 = stablehlo.convert %9930 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc9836)
      %9932 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %9933 = stablehlo.power %9931, %9932 : tensor<32x17x8x128xf32> loc(#loc9837)
      %9934 = stablehlo.reduce(%9933 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc9838)
      %9935 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %9936 = stablehlo.multiply %9934, %9935 : tensor<32x17x8xf32> loc(#loc9839)
      %9937 = stablehlo.reshape %9936 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc9840)
      %9938 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %9939 = stablehlo.add %9937, %9938 : tensor<32x17x8x1xf32> loc(#loc9841)
      %9940 = stablehlo.rsqrt %9939 : tensor<32x17x8x1xf32> loc(#loc9842)
      %9941 = stablehlo.reshape %9940 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc9843)
      %9942 = stablehlo.broadcast_in_dim %9941, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc9844)
      %9943 = stablehlo.multiply %9931, %9942 : tensor<32x17x8x128xf32> loc(#loc9845)
      %9944 = stablehlo.convert %9943 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc9846)
      %9945 = stablehlo.multiply %9925, %9944 : tensor<32x17x8x128xbf16> loc(#loc9847)
      %9946 = stablehlo.transpose %9945, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9848)
      %9947 = stablehlo.multiply %9946, %132 : tensor<32x8x17x128xbf16> loc(#loc9849)
      %9948 = stablehlo.slice %9946 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9850)
      %9949 = stablehlo.negate %9948 : tensor<32x8x17x64xbf16> loc(#loc9851)
      %9950 = stablehlo.slice %9946 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc9852)
      %9951 = stablehlo.concatenate %9949, %9950, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc9853)
      %9952 = stablehlo.multiply %9951, %138 : tensor<32x8x17x128xbf16> loc(#loc9854)
      %9953 = stablehlo.add %9947, %9952 : tensor<32x8x17x128xbf16> loc(#loc9855)
      %9954 = stablehlo.convert %9953 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc9856)
      %9955 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9956 = stablehlo.multiply %9954, %9955 : tensor<32x8x17x128xf32> loc(#loc9857)
      %9957 = stablehlo.broadcast_in_dim %9909, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9858)
      %9958 = stablehlo.reshape %9957 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9859)
      %9959 = stablehlo.convert %9958 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9860)
      %9960 = stablehlo.transpose %9959, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc9861)
      %9961 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %9962 = stablehlo.multiply %9960, %9961 : tensor<32x8x128x128xf32> loc(#loc9862)
      %9963 = stablehlo.dot_general %9956, %9962, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9863)
      %9964 = stablehlo.add %9963, %159 : tensor<32x8x17x128xf32> loc(#loc9864)
      %9965 = stablehlo.convert %9964 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc9865)
      %9966 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %9967 = stablehlo.compare  EQ, %9965, %9966 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc9866)
      %9968 = stablehlo.not %9967 : tensor<32x8x17x128xi1> loc(#loc9867)
      %9969 = stablehlo.reduce(%9968 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.22906"), %arg1677: tensor<i1> loc("reduce.22906"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc9869)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc9870)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc9868)
      %9970 = stablehlo.reshape %9969 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc9871)
      %9971 = stablehlo.not %9970 : tensor<32x8x17x1xi1> loc(#loc9872)
      %9972 = stablehlo.reshape %9971 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc9873)
      %9973 = stablehlo.broadcast_in_dim %9972, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc9874)
      %9974 = stablehlo.reduce(%9964 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9875)
      %9975 = stablehlo.broadcast_in_dim %9974, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9876)
      %9976 = stablehlo.subtract %9964, %9975 : tensor<32x8x17x128xf32> loc(#loc9877)
      %9977 = stablehlo.exponential %9976 : tensor<32x8x17x128xf32> loc(#loc9878)
      %9978 = stablehlo.reduce(%9977 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc9879)
      %9979 = stablehlo.broadcast_in_dim %9978, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc9880)
      %9980 = stablehlo.divide %9977, %9979 : tensor<32x8x17x128xf32> loc(#loc9881)
      %9981 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %9982 = stablehlo.select %9973, %9981, %9980 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc9882)
      %9983 = stablehlo.broadcast_in_dim %9916, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc9883)
      %9984 = stablehlo.reshape %9983 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc9884)
      %9985 = stablehlo.convert %9984 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc9885)
      %9986 = stablehlo.dot_general %9982, %9985, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc9886)
      %9987 = stablehlo.convert %9986 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc9887)
      %9988 = stablehlo.transpose %9987, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9888)
      %9989 = stablehlo.reshape %9988 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc9889)
      %9990 = stablehlo.reshape %arg1592 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc9890)
      %9991 = stablehlo.reshape %9990 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc9891)
      %9992 = stablehlo.transpose %9991, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc9892)
      %9993 = stablehlo.dot_general %9989, %9992, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9893)
      %9994 = "stablehlo.all_reduce"(%9993) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.22923"), %arg1677: tensor<bf16> loc("dot.22923")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9893)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9893)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9893)
      %9995 = stablehlo.reshape %9994 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9894)
      %9996 = stablehlo.add %9864, %9995 : tensor<32x17x5120xbf16> loc(#loc9895)
      %9997 = stablehlo.reshape %arg1595 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9896)
      %9998 = stablehlo.reshape %9997 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9897)
      %9999 = stablehlo.broadcast_in_dim %9998, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9898)
      %10000 = stablehlo.convert %9996 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9899)
      %10001 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10002 = stablehlo.power %10000, %10001 : tensor<32x17x5120xf32> loc(#loc9900)
      %10003 = stablehlo.reduce(%10002 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9901)
      %10004 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10005 = stablehlo.multiply %10003, %10004 : tensor<32x17xf32> loc(#loc9902)
      %10006 = stablehlo.reshape %10005 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9903)
      %10007 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10008 = stablehlo.add %10006, %10007 : tensor<32x17x1xf32> loc(#loc9904)
      %10009 = stablehlo.rsqrt %10008 : tensor<32x17x1xf32> loc(#loc9905)
      %10010 = stablehlo.reshape %10009 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9906)
      %10011 = stablehlo.broadcast_in_dim %10010, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9907)
      %10012 = stablehlo.multiply %10000, %10011 : tensor<32x17x5120xf32> loc(#loc9908)
      %10013 = stablehlo.convert %10012 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9909)
      %10014 = stablehlo.multiply %9999, %10013 : tensor<32x17x5120xbf16> loc(#loc9910)
      %10015 = stablehlo.reshape %10014 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9911)
      %10016 = stablehlo.reshape %arg1596 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9912)
      %10017 = stablehlo.reshape %10016 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9913)
      %10018 = stablehlo.transpose %10017, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9914)
      %10019 = stablehlo.dot_general %10015, %10018, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9915)
      %10020 = stablehlo.reshape %10019 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9916)
      %10021 = stablehlo.logistic %10020 : tensor<32x17x3200xbf16> loc(#loc9917)
      %10022 = stablehlo.multiply %10020, %10021 : tensor<32x17x3200xbf16> loc(#loc9918)
      %10023 = stablehlo.reshape %arg1591 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc9919)
      %10024 = stablehlo.reshape %10023 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc9920)
      %10025 = stablehlo.transpose %10024, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc9921)
      %10026 = stablehlo.dot_general %10015, %10025, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9922)
      %10027 = stablehlo.reshape %10026 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc9923)
      %10028 = stablehlo.multiply %10022, %10027 : tensor<32x17x3200xbf16> loc(#loc9924)
      %10029 = stablehlo.reshape %10028 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc9925)
      %10030 = stablehlo.reshape %arg1590 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc9926)
      %10031 = stablehlo.reshape %10030 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc9927)
      %10032 = stablehlo.transpose %10031, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc9928)
      %10033 = stablehlo.dot_general %10029, %10032, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9929)
      %10034 = "stablehlo.all_reduce"(%10033) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.22978"), %arg1677: tensor<bf16> loc("dot.22978")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc9929)
        stablehlo.return %11074 : tensor<bf16> loc(#loc9929)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9929)
      %10035 = stablehlo.reshape %10034 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9930)
      %10036 = stablehlo.add %9996, %10035 : tensor<32x17x5120xbf16> loc(#loc9931)
      %10037 = stablehlo.convert %10036 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc9932)
      %10038 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10039 = stablehlo.power %10037, %10038 : tensor<32x17x5120xf32> loc(#loc9933)
      %10040 = stablehlo.reduce(%10039 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc9934)
      %10041 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10042 = stablehlo.multiply %10040, %10041 : tensor<32x17xf32> loc(#loc9935)
      %10043 = stablehlo.reshape %10042 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc9936)
      %10044 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10045 = stablehlo.add %10043, %10044 : tensor<32x17x1xf32> loc(#loc9937)
      %10046 = stablehlo.rsqrt %10045 : tensor<32x17x1xf32> loc(#loc9938)
      %10047 = stablehlo.reshape %10046 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc9939)
      %10048 = stablehlo.broadcast_in_dim %10047, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc9940)
      %10049 = stablehlo.multiply %10037, %10048 : tensor<32x17x5120xf32> loc(#loc9941)
      %10050 = stablehlo.convert %10049 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc9942)
      %10051 = stablehlo.multiply %9922, %10050 : tensor<32x17x5120xbf16> loc(#loc9943)
      %10052 = stablehlo.reshape %10051 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc9944)
      %10053 = stablehlo.reshape %arg1589 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9945)
      %10054 = stablehlo.reshape %10053 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9946)
      %10055 = stablehlo.transpose %10054, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9947)
      %10056 = stablehlo.dot_general %10052, %10055, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9948)
      %10057 = stablehlo.reshape %10056 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9949)
      %10058 = stablehlo.convert %10057 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc9950)
      %10059 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %10060 = stablehlo.power %10058, %10059 : tensor<32x17x1x128xf32> loc(#loc9951)
      %10061 = stablehlo.reduce(%10060 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc9952)
      %10062 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10063 = stablehlo.multiply %10061, %10062 : tensor<32x17x1xf32> loc(#loc9953)
      %10064 = stablehlo.reshape %10063 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc9954)
      %10065 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %10066 = stablehlo.add %10064, %10065 : tensor<32x17x1x1xf32> loc(#loc9955)
      %10067 = stablehlo.rsqrt %10066 : tensor<32x17x1x1xf32> loc(#loc9956)
      %10068 = stablehlo.reshape %10067 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc9957)
      %10069 = stablehlo.broadcast_in_dim %10068, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc9958)
      %10070 = stablehlo.multiply %10058, %10069 : tensor<32x17x1x128xf32> loc(#loc9959)
      %10071 = stablehlo.convert %10070 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc9960)
      %10072 = stablehlo.multiply %9919, %10071 : tensor<32x17x1x128xbf16> loc(#loc9961)
      %10073 = stablehlo.transpose %10072, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9962)
      %10074 = stablehlo.multiply %10073, %82 : tensor<32x1x17x128xbf16> loc(#loc9963)
      %10075 = stablehlo.slice %10073 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9964)
      %10076 = stablehlo.negate %10075 : tensor<32x1x17x64xbf16> loc(#loc9965)
      %10077 = stablehlo.slice %10073 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc9966)
      %10078 = stablehlo.concatenate %10076, %10077, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9967)
      %10079 = stablehlo.multiply %10078, %91 : tensor<32x1x17x128xbf16> loc(#loc9968)
      %10080 = stablehlo.add %10074, %10079 : tensor<32x1x17x128xbf16> loc(#loc9969)
      %10081 = "stablehlo.scatter"(%arg1599, %21, %10080) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.23090"), %arg1677: tensor<bf16> loc("scatter.23090")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9970)
      %10082 = stablehlo.reshape %arg1600 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc9971)
      %10083 = stablehlo.reshape %10082 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc9972)
      %10084 = stablehlo.transpose %10083, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc9973)
      %10085 = stablehlo.dot_general %10052, %10084, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc9974)
      %10086 = stablehlo.reshape %10085 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9975)
      %10087 = stablehlo.transpose %10086, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc9976)
      %10088 = "stablehlo.scatter"(%arg1601, %21, %10087) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.23120"), %arg1677: tensor<bf16> loc("scatter.23120")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc9977)
      %10089 = stablehlo.reshape %arg1611 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9978)
      %10090 = stablehlo.reshape %10089 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9979)
      %10091 = stablehlo.broadcast_in_dim %10090, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc9980)
      %10092 = stablehlo.reshape %arg1610 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc9981)
      %10093 = stablehlo.reshape %10092 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc9982)
      %10094 = stablehlo.broadcast_in_dim %10093, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc9983)
      %10095 = stablehlo.reshape %arg1607 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc9984)
      %10096 = stablehlo.reshape %10095 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc9985)
      %10097 = stablehlo.broadcast_in_dim %10096, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9986)
      %10098 = stablehlo.reshape %arg1606 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc9987)
      %10099 = stablehlo.reshape %10098 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc9988)
      %10100 = stablehlo.transpose %10099, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc9989)
      %10101 = stablehlo.dot_general %10052, %10100, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc9990)
      %10102 = stablehlo.reshape %10101 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc9991)
      %10103 = stablehlo.convert %10102 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc9992)
      %10104 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %10105 = stablehlo.power %10103, %10104 : tensor<32x17x8x128xf32> loc(#loc9993)
      %10106 = stablehlo.reduce(%10105 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc9994)
      %10107 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %10108 = stablehlo.multiply %10106, %10107 : tensor<32x17x8xf32> loc(#loc9995)
      %10109 = stablehlo.reshape %10108 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc9996)
      %10110 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %10111 = stablehlo.add %10109, %10110 : tensor<32x17x8x1xf32> loc(#loc9997)
      %10112 = stablehlo.rsqrt %10111 : tensor<32x17x8x1xf32> loc(#loc9998)
      %10113 = stablehlo.reshape %10112 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc9999)
      %10114 = stablehlo.broadcast_in_dim %10113, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc10000)
      %10115 = stablehlo.multiply %10103, %10114 : tensor<32x17x8x128xf32> loc(#loc10001)
      %10116 = stablehlo.convert %10115 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc10002)
      %10117 = stablehlo.multiply %10097, %10116 : tensor<32x17x8x128xbf16> loc(#loc10003)
      %10118 = stablehlo.transpose %10117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10004)
      %10119 = stablehlo.multiply %10118, %132 : tensor<32x8x17x128xbf16> loc(#loc10005)
      %10120 = stablehlo.slice %10118 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10006)
      %10121 = stablehlo.negate %10120 : tensor<32x8x17x64xbf16> loc(#loc10007)
      %10122 = stablehlo.slice %10118 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10008)
      %10123 = stablehlo.concatenate %10121, %10122, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10009)
      %10124 = stablehlo.multiply %10123, %138 : tensor<32x8x17x128xbf16> loc(#loc10010)
      %10125 = stablehlo.add %10119, %10124 : tensor<32x8x17x128xbf16> loc(#loc10011)
      %10126 = stablehlo.convert %10125 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc10012)
      %10127 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10128 = stablehlo.multiply %10126, %10127 : tensor<32x8x17x128xf32> loc(#loc10013)
      %10129 = stablehlo.broadcast_in_dim %10081, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10014)
      %10130 = stablehlo.reshape %10129 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10015)
      %10131 = stablehlo.convert %10130 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10016)
      %10132 = stablehlo.transpose %10131, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc10017)
      %10133 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %10134 = stablehlo.multiply %10132, %10133 : tensor<32x8x128x128xf32> loc(#loc10018)
      %10135 = stablehlo.dot_general %10128, %10134, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10019)
      %10136 = stablehlo.add %10135, %159 : tensor<32x8x17x128xf32> loc(#loc10020)
      %10137 = stablehlo.convert %10136 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc10021)
      %10138 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %10139 = stablehlo.compare  EQ, %10137, %10138 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc10022)
      %10140 = stablehlo.not %10139 : tensor<32x8x17x128xi1> loc(#loc10023)
      %10141 = stablehlo.reduce(%10140 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.23301"), %arg1677: tensor<i1> loc("reduce.23301"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc10025)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc10026)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc10024)
      %10142 = stablehlo.reshape %10141 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc10027)
      %10143 = stablehlo.not %10142 : tensor<32x8x17x1xi1> loc(#loc10028)
      %10144 = stablehlo.reshape %10143 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc10029)
      %10145 = stablehlo.broadcast_in_dim %10144, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc10030)
      %10146 = stablehlo.reduce(%10136 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10031)
      %10147 = stablehlo.broadcast_in_dim %10146, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10032)
      %10148 = stablehlo.subtract %10136, %10147 : tensor<32x8x17x128xf32> loc(#loc10033)
      %10149 = stablehlo.exponential %10148 : tensor<32x8x17x128xf32> loc(#loc10034)
      %10150 = stablehlo.reduce(%10149 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10035)
      %10151 = stablehlo.broadcast_in_dim %10150, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10036)
      %10152 = stablehlo.divide %10149, %10151 : tensor<32x8x17x128xf32> loc(#loc10037)
      %10153 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10154 = stablehlo.select %10145, %10153, %10152 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc10038)
      %10155 = stablehlo.broadcast_in_dim %10088, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10039)
      %10156 = stablehlo.reshape %10155 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10040)
      %10157 = stablehlo.convert %10156 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10041)
      %10158 = stablehlo.dot_general %10154, %10157, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10042)
      %10159 = stablehlo.convert %10158 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc10043)
      %10160 = stablehlo.transpose %10159, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10044)
      %10161 = stablehlo.reshape %10160 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc10045)
      %10162 = stablehlo.reshape %arg1605 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc10046)
      %10163 = stablehlo.reshape %10162 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc10047)
      %10164 = stablehlo.transpose %10163, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc10048)
      %10165 = stablehlo.dot_general %10161, %10164, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10049)
      %10166 = "stablehlo.all_reduce"(%10165) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.23318"), %arg1677: tensor<bf16> loc("dot.23318")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10049)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10049)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10049)
      %10167 = stablehlo.reshape %10166 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10050)
      %10168 = stablehlo.add %10036, %10167 : tensor<32x17x5120xbf16> loc(#loc10051)
      %10169 = stablehlo.reshape %arg1608 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10052)
      %10170 = stablehlo.reshape %10169 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10053)
      %10171 = stablehlo.broadcast_in_dim %10170, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10054)
      %10172 = stablehlo.convert %10168 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10055)
      %10173 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10174 = stablehlo.power %10172, %10173 : tensor<32x17x5120xf32> loc(#loc10056)
      %10175 = stablehlo.reduce(%10174 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10057)
      %10176 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10177 = stablehlo.multiply %10175, %10176 : tensor<32x17xf32> loc(#loc10058)
      %10178 = stablehlo.reshape %10177 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10059)
      %10179 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10180 = stablehlo.add %10178, %10179 : tensor<32x17x1xf32> loc(#loc10060)
      %10181 = stablehlo.rsqrt %10180 : tensor<32x17x1xf32> loc(#loc10061)
      %10182 = stablehlo.reshape %10181 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10062)
      %10183 = stablehlo.broadcast_in_dim %10182, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10063)
      %10184 = stablehlo.multiply %10172, %10183 : tensor<32x17x5120xf32> loc(#loc10064)
      %10185 = stablehlo.convert %10184 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10065)
      %10186 = stablehlo.multiply %10171, %10185 : tensor<32x17x5120xbf16> loc(#loc10066)
      %10187 = stablehlo.reshape %10186 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10067)
      %10188 = stablehlo.reshape %arg1609 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10068)
      %10189 = stablehlo.reshape %10188 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10069)
      %10190 = stablehlo.transpose %10189, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10070)
      %10191 = stablehlo.dot_general %10187, %10190, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10071)
      %10192 = stablehlo.reshape %10191 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10072)
      %10193 = stablehlo.logistic %10192 : tensor<32x17x3200xbf16> loc(#loc10073)
      %10194 = stablehlo.multiply %10192, %10193 : tensor<32x17x3200xbf16> loc(#loc10074)
      %10195 = stablehlo.reshape %arg1604 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10075)
      %10196 = stablehlo.reshape %10195 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10076)
      %10197 = stablehlo.transpose %10196, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10077)
      %10198 = stablehlo.dot_general %10187, %10197, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10078)
      %10199 = stablehlo.reshape %10198 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10079)
      %10200 = stablehlo.multiply %10194, %10199 : tensor<32x17x3200xbf16> loc(#loc10080)
      %10201 = stablehlo.reshape %10200 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10081)
      %10202 = stablehlo.reshape %arg1603 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc10082)
      %10203 = stablehlo.reshape %10202 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc10083)
      %10204 = stablehlo.transpose %10203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc10084)
      %10205 = stablehlo.dot_general %10201, %10204, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10085)
      %10206 = "stablehlo.all_reduce"(%10205) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.23373"), %arg1677: tensor<bf16> loc("dot.23373")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10085)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10085)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10085)
      %10207 = stablehlo.reshape %10206 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10086)
      %10208 = stablehlo.add %10168, %10207 : tensor<32x17x5120xbf16> loc(#loc10087)
      %10209 = stablehlo.convert %10208 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10088)
      %10210 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10211 = stablehlo.power %10209, %10210 : tensor<32x17x5120xf32> loc(#loc10089)
      %10212 = stablehlo.reduce(%10211 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10090)
      %10213 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10214 = stablehlo.multiply %10212, %10213 : tensor<32x17xf32> loc(#loc10091)
      %10215 = stablehlo.reshape %10214 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10092)
      %10216 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10217 = stablehlo.add %10215, %10216 : tensor<32x17x1xf32> loc(#loc10093)
      %10218 = stablehlo.rsqrt %10217 : tensor<32x17x1xf32> loc(#loc10094)
      %10219 = stablehlo.reshape %10218 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10095)
      %10220 = stablehlo.broadcast_in_dim %10219, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10096)
      %10221 = stablehlo.multiply %10209, %10220 : tensor<32x17x5120xf32> loc(#loc10097)
      %10222 = stablehlo.convert %10221 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10098)
      %10223 = stablehlo.multiply %10094, %10222 : tensor<32x17x5120xbf16> loc(#loc10099)
      %10224 = stablehlo.reshape %10223 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10100)
      %10225 = stablehlo.reshape %arg1602 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10101)
      %10226 = stablehlo.reshape %10225 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10102)
      %10227 = stablehlo.transpose %10226, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10103)
      %10228 = stablehlo.dot_general %10224, %10227, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10104)
      %10229 = stablehlo.reshape %10228 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10105)
      %10230 = stablehlo.convert %10229 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc10106)
      %10231 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %10232 = stablehlo.power %10230, %10231 : tensor<32x17x1x128xf32> loc(#loc10107)
      %10233 = stablehlo.reduce(%10232 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc10108)
      %10234 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10235 = stablehlo.multiply %10233, %10234 : tensor<32x17x1xf32> loc(#loc10109)
      %10236 = stablehlo.reshape %10235 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc10110)
      %10237 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %10238 = stablehlo.add %10236, %10237 : tensor<32x17x1x1xf32> loc(#loc10111)
      %10239 = stablehlo.rsqrt %10238 : tensor<32x17x1x1xf32> loc(#loc10112)
      %10240 = stablehlo.reshape %10239 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc10113)
      %10241 = stablehlo.broadcast_in_dim %10240, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc10114)
      %10242 = stablehlo.multiply %10230, %10241 : tensor<32x17x1x128xf32> loc(#loc10115)
      %10243 = stablehlo.convert %10242 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc10116)
      %10244 = stablehlo.multiply %10091, %10243 : tensor<32x17x1x128xbf16> loc(#loc10117)
      %10245 = stablehlo.transpose %10244, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10118)
      %10246 = stablehlo.multiply %10245, %82 : tensor<32x1x17x128xbf16> loc(#loc10119)
      %10247 = stablehlo.slice %10245 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10120)
      %10248 = stablehlo.negate %10247 : tensor<32x1x17x64xbf16> loc(#loc10121)
      %10249 = stablehlo.slice %10245 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10122)
      %10250 = stablehlo.concatenate %10248, %10249, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10123)
      %10251 = stablehlo.multiply %10250, %91 : tensor<32x1x17x128xbf16> loc(#loc10124)
      %10252 = stablehlo.add %10246, %10251 : tensor<32x1x17x128xbf16> loc(#loc10125)
      %10253 = "stablehlo.scatter"(%arg1612, %21, %10252) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.23485"), %arg1677: tensor<bf16> loc("scatter.23485")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10126)
      %10254 = stablehlo.reshape %arg1613 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10127)
      %10255 = stablehlo.reshape %10254 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10128)
      %10256 = stablehlo.transpose %10255, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10129)
      %10257 = stablehlo.dot_general %10224, %10256, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10130)
      %10258 = stablehlo.reshape %10257 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10131)
      %10259 = stablehlo.transpose %10258, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10132)
      %10260 = "stablehlo.scatter"(%arg1614, %21, %10259) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.23515"), %arg1677: tensor<bf16> loc("scatter.23515")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10133)
      %10261 = stablehlo.reshape %arg1624 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10134)
      %10262 = stablehlo.reshape %10261 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10135)
      %10263 = stablehlo.broadcast_in_dim %10262, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10136)
      %10264 = stablehlo.reshape %arg1623 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10137)
      %10265 = stablehlo.reshape %10264 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10138)
      %10266 = stablehlo.broadcast_in_dim %10265, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10139)
      %10267 = stablehlo.reshape %arg1620 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10140)
      %10268 = stablehlo.reshape %10267 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10141)
      %10269 = stablehlo.broadcast_in_dim %10268, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10142)
      %10270 = stablehlo.reshape %arg1619 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc10143)
      %10271 = stablehlo.reshape %10270 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc10144)
      %10272 = stablehlo.transpose %10271, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc10145)
      %10273 = stablehlo.dot_general %10224, %10272, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc10146)
      %10274 = stablehlo.reshape %10273 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10147)
      %10275 = stablehlo.convert %10274 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc10148)
      %10276 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %10277 = stablehlo.power %10275, %10276 : tensor<32x17x8x128xf32> loc(#loc10149)
      %10278 = stablehlo.reduce(%10277 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc10150)
      %10279 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %10280 = stablehlo.multiply %10278, %10279 : tensor<32x17x8xf32> loc(#loc10151)
      %10281 = stablehlo.reshape %10280 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc10152)
      %10282 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %10283 = stablehlo.add %10281, %10282 : tensor<32x17x8x1xf32> loc(#loc10153)
      %10284 = stablehlo.rsqrt %10283 : tensor<32x17x8x1xf32> loc(#loc10154)
      %10285 = stablehlo.reshape %10284 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc10155)
      %10286 = stablehlo.broadcast_in_dim %10285, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc10156)
      %10287 = stablehlo.multiply %10275, %10286 : tensor<32x17x8x128xf32> loc(#loc10157)
      %10288 = stablehlo.convert %10287 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc10158)
      %10289 = stablehlo.multiply %10269, %10288 : tensor<32x17x8x128xbf16> loc(#loc10159)
      %10290 = stablehlo.transpose %10289, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10160)
      %10291 = stablehlo.multiply %10290, %132 : tensor<32x8x17x128xbf16> loc(#loc10161)
      %10292 = stablehlo.slice %10290 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10162)
      %10293 = stablehlo.negate %10292 : tensor<32x8x17x64xbf16> loc(#loc10163)
      %10294 = stablehlo.slice %10290 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10164)
      %10295 = stablehlo.concatenate %10293, %10294, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10165)
      %10296 = stablehlo.multiply %10295, %138 : tensor<32x8x17x128xbf16> loc(#loc10166)
      %10297 = stablehlo.add %10291, %10296 : tensor<32x8x17x128xbf16> loc(#loc10167)
      %10298 = stablehlo.convert %10297 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc10168)
      %10299 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10300 = stablehlo.multiply %10298, %10299 : tensor<32x8x17x128xf32> loc(#loc10169)
      %10301 = stablehlo.broadcast_in_dim %10253, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10170)
      %10302 = stablehlo.reshape %10301 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10171)
      %10303 = stablehlo.convert %10302 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10172)
      %10304 = stablehlo.transpose %10303, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc10173)
      %10305 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %10306 = stablehlo.multiply %10304, %10305 : tensor<32x8x128x128xf32> loc(#loc10174)
      %10307 = stablehlo.dot_general %10300, %10306, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10175)
      %10308 = stablehlo.add %10307, %159 : tensor<32x8x17x128xf32> loc(#loc10176)
      %10309 = stablehlo.convert %10308 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc10177)
      %10310 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %10311 = stablehlo.compare  EQ, %10309, %10310 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc10178)
      %10312 = stablehlo.not %10311 : tensor<32x8x17x128xi1> loc(#loc10179)
      %10313 = stablehlo.reduce(%10312 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.23696"), %arg1677: tensor<i1> loc("reduce.23696"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc10181)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc10182)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc10180)
      %10314 = stablehlo.reshape %10313 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc10183)
      %10315 = stablehlo.not %10314 : tensor<32x8x17x1xi1> loc(#loc10184)
      %10316 = stablehlo.reshape %10315 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc10185)
      %10317 = stablehlo.broadcast_in_dim %10316, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc10186)
      %10318 = stablehlo.reduce(%10308 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10187)
      %10319 = stablehlo.broadcast_in_dim %10318, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10188)
      %10320 = stablehlo.subtract %10308, %10319 : tensor<32x8x17x128xf32> loc(#loc10189)
      %10321 = stablehlo.exponential %10320 : tensor<32x8x17x128xf32> loc(#loc10190)
      %10322 = stablehlo.reduce(%10321 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10191)
      %10323 = stablehlo.broadcast_in_dim %10322, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10192)
      %10324 = stablehlo.divide %10321, %10323 : tensor<32x8x17x128xf32> loc(#loc10193)
      %10325 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10326 = stablehlo.select %10317, %10325, %10324 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc10194)
      %10327 = stablehlo.broadcast_in_dim %10260, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10195)
      %10328 = stablehlo.reshape %10327 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10196)
      %10329 = stablehlo.convert %10328 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10197)
      %10330 = stablehlo.dot_general %10326, %10329, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10198)
      %10331 = stablehlo.convert %10330 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc10199)
      %10332 = stablehlo.transpose %10331, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10200)
      %10333 = stablehlo.reshape %10332 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc10201)
      %10334 = stablehlo.reshape %arg1618 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc10202)
      %10335 = stablehlo.reshape %10334 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc10203)
      %10336 = stablehlo.transpose %10335, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc10204)
      %10337 = stablehlo.dot_general %10333, %10336, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10205)
      %10338 = "stablehlo.all_reduce"(%10337) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.23713"), %arg1677: tensor<bf16> loc("dot.23713")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10205)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10205)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10205)
      %10339 = stablehlo.reshape %10338 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10206)
      %10340 = stablehlo.add %10208, %10339 : tensor<32x17x5120xbf16> loc(#loc10207)
      %10341 = stablehlo.reshape %arg1621 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10208)
      %10342 = stablehlo.reshape %10341 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10209)
      %10343 = stablehlo.broadcast_in_dim %10342, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10210)
      %10344 = stablehlo.convert %10340 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10211)
      %10345 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10346 = stablehlo.power %10344, %10345 : tensor<32x17x5120xf32> loc(#loc10212)
      %10347 = stablehlo.reduce(%10346 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10213)
      %10348 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10349 = stablehlo.multiply %10347, %10348 : tensor<32x17xf32> loc(#loc10214)
      %10350 = stablehlo.reshape %10349 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10215)
      %10351 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10352 = stablehlo.add %10350, %10351 : tensor<32x17x1xf32> loc(#loc10216)
      %10353 = stablehlo.rsqrt %10352 : tensor<32x17x1xf32> loc(#loc10217)
      %10354 = stablehlo.reshape %10353 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10218)
      %10355 = stablehlo.broadcast_in_dim %10354, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10219)
      %10356 = stablehlo.multiply %10344, %10355 : tensor<32x17x5120xf32> loc(#loc10220)
      %10357 = stablehlo.convert %10356 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10221)
      %10358 = stablehlo.multiply %10343, %10357 : tensor<32x17x5120xbf16> loc(#loc10222)
      %10359 = stablehlo.reshape %10358 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10223)
      %10360 = stablehlo.reshape %arg1622 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10224)
      %10361 = stablehlo.reshape %10360 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10225)
      %10362 = stablehlo.transpose %10361, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10226)
      %10363 = stablehlo.dot_general %10359, %10362, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10227)
      %10364 = stablehlo.reshape %10363 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10228)
      %10365 = stablehlo.logistic %10364 : tensor<32x17x3200xbf16> loc(#loc10229)
      %10366 = stablehlo.multiply %10364, %10365 : tensor<32x17x3200xbf16> loc(#loc10230)
      %10367 = stablehlo.reshape %arg1617 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10231)
      %10368 = stablehlo.reshape %10367 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10232)
      %10369 = stablehlo.transpose %10368, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10233)
      %10370 = stablehlo.dot_general %10359, %10369, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10234)
      %10371 = stablehlo.reshape %10370 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10235)
      %10372 = stablehlo.multiply %10366, %10371 : tensor<32x17x3200xbf16> loc(#loc10236)
      %10373 = stablehlo.reshape %10372 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10237)
      %10374 = stablehlo.reshape %arg1616 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc10238)
      %10375 = stablehlo.reshape %10374 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc10239)
      %10376 = stablehlo.transpose %10375, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc10240)
      %10377 = stablehlo.dot_general %10373, %10376, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10241)
      %10378 = "stablehlo.all_reduce"(%10377) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.23768"), %arg1677: tensor<bf16> loc("dot.23768")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10241)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10241)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10241)
      %10379 = stablehlo.reshape %10378 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10242)
      %10380 = stablehlo.add %10340, %10379 : tensor<32x17x5120xbf16> loc(#loc10243)
      %10381 = stablehlo.convert %10380 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10244)
      %10382 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10383 = stablehlo.power %10381, %10382 : tensor<32x17x5120xf32> loc(#loc10245)
      %10384 = stablehlo.reduce(%10383 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10246)
      %10385 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10386 = stablehlo.multiply %10384, %10385 : tensor<32x17xf32> loc(#loc10247)
      %10387 = stablehlo.reshape %10386 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10248)
      %10388 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10389 = stablehlo.add %10387, %10388 : tensor<32x17x1xf32> loc(#loc10249)
      %10390 = stablehlo.rsqrt %10389 : tensor<32x17x1xf32> loc(#loc10250)
      %10391 = stablehlo.reshape %10390 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10251)
      %10392 = stablehlo.broadcast_in_dim %10391, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10252)
      %10393 = stablehlo.multiply %10381, %10392 : tensor<32x17x5120xf32> loc(#loc10253)
      %10394 = stablehlo.convert %10393 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10254)
      %10395 = stablehlo.multiply %10266, %10394 : tensor<32x17x5120xbf16> loc(#loc10255)
      %10396 = stablehlo.reshape %10395 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10256)
      %10397 = stablehlo.reshape %arg1615 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10257)
      %10398 = stablehlo.reshape %10397 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10258)
      %10399 = stablehlo.transpose %10398, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10259)
      %10400 = stablehlo.dot_general %10396, %10399, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10260)
      %10401 = stablehlo.reshape %10400 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10261)
      %10402 = stablehlo.convert %10401 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc10262)
      %10403 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %10404 = stablehlo.power %10402, %10403 : tensor<32x17x1x128xf32> loc(#loc10263)
      %10405 = stablehlo.reduce(%10404 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc10264)
      %10406 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10407 = stablehlo.multiply %10405, %10406 : tensor<32x17x1xf32> loc(#loc10265)
      %10408 = stablehlo.reshape %10407 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc10266)
      %10409 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %10410 = stablehlo.add %10408, %10409 : tensor<32x17x1x1xf32> loc(#loc10267)
      %10411 = stablehlo.rsqrt %10410 : tensor<32x17x1x1xf32> loc(#loc10268)
      %10412 = stablehlo.reshape %10411 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc10269)
      %10413 = stablehlo.broadcast_in_dim %10412, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc10270)
      %10414 = stablehlo.multiply %10402, %10413 : tensor<32x17x1x128xf32> loc(#loc10271)
      %10415 = stablehlo.convert %10414 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc10272)
      %10416 = stablehlo.multiply %10263, %10415 : tensor<32x17x1x128xbf16> loc(#loc10273)
      %10417 = stablehlo.transpose %10416, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10274)
      %10418 = stablehlo.multiply %10417, %82 : tensor<32x1x17x128xbf16> loc(#loc10275)
      %10419 = stablehlo.slice %10417 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10276)
      %10420 = stablehlo.negate %10419 : tensor<32x1x17x64xbf16> loc(#loc10277)
      %10421 = stablehlo.slice %10417 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10278)
      %10422 = stablehlo.concatenate %10420, %10421, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10279)
      %10423 = stablehlo.multiply %10422, %91 : tensor<32x1x17x128xbf16> loc(#loc10280)
      %10424 = stablehlo.add %10418, %10423 : tensor<32x1x17x128xbf16> loc(#loc10281)
      %10425 = "stablehlo.scatter"(%arg1625, %21, %10424) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.23880"), %arg1677: tensor<bf16> loc("scatter.23880")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10282)
      %10426 = stablehlo.reshape %arg1626 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10283)
      %10427 = stablehlo.reshape %10426 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10284)
      %10428 = stablehlo.transpose %10427, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10285)
      %10429 = stablehlo.dot_general %10396, %10428, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10286)
      %10430 = stablehlo.reshape %10429 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10287)
      %10431 = stablehlo.transpose %10430, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10288)
      %10432 = "stablehlo.scatter"(%arg1627, %21, %10431) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.23910"), %arg1677: tensor<bf16> loc("scatter.23910")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10289)
      %10433 = stablehlo.reshape %arg1637 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10290)
      %10434 = stablehlo.reshape %10433 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10291)
      %10435 = stablehlo.broadcast_in_dim %10434, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10292)
      %10436 = stablehlo.reshape %arg1636 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10293)
      %10437 = stablehlo.reshape %10436 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10294)
      %10438 = stablehlo.broadcast_in_dim %10437, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10295)
      %10439 = stablehlo.reshape %arg1633 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10296)
      %10440 = stablehlo.reshape %10439 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10297)
      %10441 = stablehlo.broadcast_in_dim %10440, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10298)
      %10442 = stablehlo.reshape %arg1632 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc10299)
      %10443 = stablehlo.reshape %10442 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc10300)
      %10444 = stablehlo.transpose %10443, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc10301)
      %10445 = stablehlo.dot_general %10396, %10444, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc10302)
      %10446 = stablehlo.reshape %10445 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10303)
      %10447 = stablehlo.convert %10446 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc10304)
      %10448 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %10449 = stablehlo.power %10447, %10448 : tensor<32x17x8x128xf32> loc(#loc10305)
      %10450 = stablehlo.reduce(%10449 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc10306)
      %10451 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %10452 = stablehlo.multiply %10450, %10451 : tensor<32x17x8xf32> loc(#loc10307)
      %10453 = stablehlo.reshape %10452 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc10308)
      %10454 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %10455 = stablehlo.add %10453, %10454 : tensor<32x17x8x1xf32> loc(#loc10309)
      %10456 = stablehlo.rsqrt %10455 : tensor<32x17x8x1xf32> loc(#loc10310)
      %10457 = stablehlo.reshape %10456 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc10311)
      %10458 = stablehlo.broadcast_in_dim %10457, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc10312)
      %10459 = stablehlo.multiply %10447, %10458 : tensor<32x17x8x128xf32> loc(#loc10313)
      %10460 = stablehlo.convert %10459 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc10314)
      %10461 = stablehlo.multiply %10441, %10460 : tensor<32x17x8x128xbf16> loc(#loc10315)
      %10462 = stablehlo.transpose %10461, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10316)
      %10463 = stablehlo.multiply %10462, %132 : tensor<32x8x17x128xbf16> loc(#loc10317)
      %10464 = stablehlo.slice %10462 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10318)
      %10465 = stablehlo.negate %10464 : tensor<32x8x17x64xbf16> loc(#loc10319)
      %10466 = stablehlo.slice %10462 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10320)
      %10467 = stablehlo.concatenate %10465, %10466, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10321)
      %10468 = stablehlo.multiply %10467, %138 : tensor<32x8x17x128xbf16> loc(#loc10322)
      %10469 = stablehlo.add %10463, %10468 : tensor<32x8x17x128xbf16> loc(#loc10323)
      %10470 = stablehlo.convert %10469 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc10324)
      %10471 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10472 = stablehlo.multiply %10470, %10471 : tensor<32x8x17x128xf32> loc(#loc10325)
      %10473 = stablehlo.broadcast_in_dim %10425, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10326)
      %10474 = stablehlo.reshape %10473 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10327)
      %10475 = stablehlo.convert %10474 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10328)
      %10476 = stablehlo.transpose %10475, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc10329)
      %10477 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %10478 = stablehlo.multiply %10476, %10477 : tensor<32x8x128x128xf32> loc(#loc10330)
      %10479 = stablehlo.dot_general %10472, %10478, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10331)
      %10480 = stablehlo.add %10479, %159 : tensor<32x8x17x128xf32> loc(#loc10332)
      %10481 = stablehlo.convert %10480 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc10333)
      %10482 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %10483 = stablehlo.compare  EQ, %10481, %10482 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc10334)
      %10484 = stablehlo.not %10483 : tensor<32x8x17x128xi1> loc(#loc10335)
      %10485 = stablehlo.reduce(%10484 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.24091"), %arg1677: tensor<i1> loc("reduce.24091"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc10337)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc10338)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc10336)
      %10486 = stablehlo.reshape %10485 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc10339)
      %10487 = stablehlo.not %10486 : tensor<32x8x17x1xi1> loc(#loc10340)
      %10488 = stablehlo.reshape %10487 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc10341)
      %10489 = stablehlo.broadcast_in_dim %10488, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc10342)
      %10490 = stablehlo.reduce(%10480 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10343)
      %10491 = stablehlo.broadcast_in_dim %10490, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10344)
      %10492 = stablehlo.subtract %10480, %10491 : tensor<32x8x17x128xf32> loc(#loc10345)
      %10493 = stablehlo.exponential %10492 : tensor<32x8x17x128xf32> loc(#loc10346)
      %10494 = stablehlo.reduce(%10493 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10347)
      %10495 = stablehlo.broadcast_in_dim %10494, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10348)
      %10496 = stablehlo.divide %10493, %10495 : tensor<32x8x17x128xf32> loc(#loc10349)
      %10497 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10498 = stablehlo.select %10489, %10497, %10496 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc10350)
      %10499 = stablehlo.broadcast_in_dim %10432, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10351)
      %10500 = stablehlo.reshape %10499 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10352)
      %10501 = stablehlo.convert %10500 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10353)
      %10502 = stablehlo.dot_general %10498, %10501, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10354)
      %10503 = stablehlo.convert %10502 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc10355)
      %10504 = stablehlo.transpose %10503, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10356)
      %10505 = stablehlo.reshape %10504 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc10357)
      %10506 = stablehlo.reshape %arg1631 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc10358)
      %10507 = stablehlo.reshape %10506 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc10359)
      %10508 = stablehlo.transpose %10507, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc10360)
      %10509 = stablehlo.dot_general %10505, %10508, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10361)
      %10510 = "stablehlo.all_reduce"(%10509) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.24108"), %arg1677: tensor<bf16> loc("dot.24108")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10361)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10361)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10361)
      %10511 = stablehlo.reshape %10510 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10362)
      %10512 = stablehlo.add %10380, %10511 : tensor<32x17x5120xbf16> loc(#loc10363)
      %10513 = stablehlo.reshape %arg1634 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10364)
      %10514 = stablehlo.reshape %10513 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10365)
      %10515 = stablehlo.broadcast_in_dim %10514, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10366)
      %10516 = stablehlo.convert %10512 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10367)
      %10517 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10518 = stablehlo.power %10516, %10517 : tensor<32x17x5120xf32> loc(#loc10368)
      %10519 = stablehlo.reduce(%10518 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10369)
      %10520 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10521 = stablehlo.multiply %10519, %10520 : tensor<32x17xf32> loc(#loc10370)
      %10522 = stablehlo.reshape %10521 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10371)
      %10523 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10524 = stablehlo.add %10522, %10523 : tensor<32x17x1xf32> loc(#loc10372)
      %10525 = stablehlo.rsqrt %10524 : tensor<32x17x1xf32> loc(#loc10373)
      %10526 = stablehlo.reshape %10525 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10374)
      %10527 = stablehlo.broadcast_in_dim %10526, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10375)
      %10528 = stablehlo.multiply %10516, %10527 : tensor<32x17x5120xf32> loc(#loc10376)
      %10529 = stablehlo.convert %10528 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10377)
      %10530 = stablehlo.multiply %10515, %10529 : tensor<32x17x5120xbf16> loc(#loc10378)
      %10531 = stablehlo.reshape %10530 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10379)
      %10532 = stablehlo.reshape %arg1635 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10380)
      %10533 = stablehlo.reshape %10532 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10381)
      %10534 = stablehlo.transpose %10533, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10382)
      %10535 = stablehlo.dot_general %10531, %10534, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10383)
      %10536 = stablehlo.reshape %10535 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10384)
      %10537 = stablehlo.logistic %10536 : tensor<32x17x3200xbf16> loc(#loc10385)
      %10538 = stablehlo.multiply %10536, %10537 : tensor<32x17x3200xbf16> loc(#loc10386)
      %10539 = stablehlo.reshape %arg1630 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10387)
      %10540 = stablehlo.reshape %10539 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10388)
      %10541 = stablehlo.transpose %10540, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10389)
      %10542 = stablehlo.dot_general %10531, %10541, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10390)
      %10543 = stablehlo.reshape %10542 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10391)
      %10544 = stablehlo.multiply %10538, %10543 : tensor<32x17x3200xbf16> loc(#loc10392)
      %10545 = stablehlo.reshape %10544 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10393)
      %10546 = stablehlo.reshape %arg1629 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc10394)
      %10547 = stablehlo.reshape %10546 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc10395)
      %10548 = stablehlo.transpose %10547, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc10396)
      %10549 = stablehlo.dot_general %10545, %10548, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10397)
      %10550 = "stablehlo.all_reduce"(%10549) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.24163"), %arg1677: tensor<bf16> loc("dot.24163")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10397)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10397)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10397)
      %10551 = stablehlo.reshape %10550 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10398)
      %10552 = stablehlo.add %10512, %10551 : tensor<32x17x5120xbf16> loc(#loc10399)
      %10553 = stablehlo.convert %10552 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10400)
      %10554 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10555 = stablehlo.power %10553, %10554 : tensor<32x17x5120xf32> loc(#loc10401)
      %10556 = stablehlo.reduce(%10555 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10402)
      %10557 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10558 = stablehlo.multiply %10556, %10557 : tensor<32x17xf32> loc(#loc10403)
      %10559 = stablehlo.reshape %10558 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10404)
      %10560 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10561 = stablehlo.add %10559, %10560 : tensor<32x17x1xf32> loc(#loc10405)
      %10562 = stablehlo.rsqrt %10561 : tensor<32x17x1xf32> loc(#loc10406)
      %10563 = stablehlo.reshape %10562 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10407)
      %10564 = stablehlo.broadcast_in_dim %10563, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10408)
      %10565 = stablehlo.multiply %10553, %10564 : tensor<32x17x5120xf32> loc(#loc10409)
      %10566 = stablehlo.convert %10565 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10410)
      %10567 = stablehlo.multiply %10438, %10566 : tensor<32x17x5120xbf16> loc(#loc10411)
      %10568 = stablehlo.reshape %10567 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10412)
      %10569 = stablehlo.reshape %arg1628 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10413)
      %10570 = stablehlo.reshape %10569 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10414)
      %10571 = stablehlo.transpose %10570, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10415)
      %10572 = stablehlo.dot_general %10568, %10571, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10416)
      %10573 = stablehlo.reshape %10572 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10417)
      %10574 = stablehlo.convert %10573 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc10418)
      %10575 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %10576 = stablehlo.power %10574, %10575 : tensor<32x17x1x128xf32> loc(#loc10419)
      %10577 = stablehlo.reduce(%10576 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc10420)
      %10578 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10579 = stablehlo.multiply %10577, %10578 : tensor<32x17x1xf32> loc(#loc10421)
      %10580 = stablehlo.reshape %10579 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc10422)
      %10581 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %10582 = stablehlo.add %10580, %10581 : tensor<32x17x1x1xf32> loc(#loc10423)
      %10583 = stablehlo.rsqrt %10582 : tensor<32x17x1x1xf32> loc(#loc10424)
      %10584 = stablehlo.reshape %10583 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc10425)
      %10585 = stablehlo.broadcast_in_dim %10584, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc10426)
      %10586 = stablehlo.multiply %10574, %10585 : tensor<32x17x1x128xf32> loc(#loc10427)
      %10587 = stablehlo.convert %10586 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc10428)
      %10588 = stablehlo.multiply %10435, %10587 : tensor<32x17x1x128xbf16> loc(#loc10429)
      %10589 = stablehlo.transpose %10588, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10430)
      %10590 = stablehlo.multiply %10589, %82 : tensor<32x1x17x128xbf16> loc(#loc10431)
      %10591 = stablehlo.slice %10589 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10432)
      %10592 = stablehlo.negate %10591 : tensor<32x1x17x64xbf16> loc(#loc10433)
      %10593 = stablehlo.slice %10589 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10434)
      %10594 = stablehlo.concatenate %10592, %10593, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10435)
      %10595 = stablehlo.multiply %10594, %91 : tensor<32x1x17x128xbf16> loc(#loc10436)
      %10596 = stablehlo.add %10590, %10595 : tensor<32x1x17x128xbf16> loc(#loc10437)
      %10597 = "stablehlo.scatter"(%arg1638, %21, %10596) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.24275"), %arg1677: tensor<bf16> loc("scatter.24275")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10438)
      %10598 = stablehlo.reshape %arg1639 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10439)
      %10599 = stablehlo.reshape %10598 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10440)
      %10600 = stablehlo.transpose %10599, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10441)
      %10601 = stablehlo.dot_general %10568, %10600, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10442)
      %10602 = stablehlo.reshape %10601 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10443)
      %10603 = stablehlo.transpose %10602, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10444)
      %10604 = "stablehlo.scatter"(%arg1640, %21, %10603) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.24305"), %arg1677: tensor<bf16> loc("scatter.24305")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10445)
      %10605 = stablehlo.reshape %arg1650 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10446)
      %10606 = stablehlo.reshape %10605 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10447)
      %10607 = stablehlo.broadcast_in_dim %10606, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10448)
      %10608 = stablehlo.reshape %arg1649 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10449)
      %10609 = stablehlo.reshape %10608 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10450)
      %10610 = stablehlo.broadcast_in_dim %10609, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10451)
      %10611 = stablehlo.reshape %arg1646 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10452)
      %10612 = stablehlo.reshape %10611 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10453)
      %10613 = stablehlo.broadcast_in_dim %10612, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10454)
      %10614 = stablehlo.reshape %arg1645 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc10455)
      %10615 = stablehlo.reshape %10614 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc10456)
      %10616 = stablehlo.transpose %10615, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc10457)
      %10617 = stablehlo.dot_general %10568, %10616, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc10458)
      %10618 = stablehlo.reshape %10617 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10459)
      %10619 = stablehlo.convert %10618 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc10460)
      %10620 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %10621 = stablehlo.power %10619, %10620 : tensor<32x17x8x128xf32> loc(#loc10461)
      %10622 = stablehlo.reduce(%10621 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc10462)
      %10623 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %10624 = stablehlo.multiply %10622, %10623 : tensor<32x17x8xf32> loc(#loc10463)
      %10625 = stablehlo.reshape %10624 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc10464)
      %10626 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %10627 = stablehlo.add %10625, %10626 : tensor<32x17x8x1xf32> loc(#loc10465)
      %10628 = stablehlo.rsqrt %10627 : tensor<32x17x8x1xf32> loc(#loc10466)
      %10629 = stablehlo.reshape %10628 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc10467)
      %10630 = stablehlo.broadcast_in_dim %10629, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc10468)
      %10631 = stablehlo.multiply %10619, %10630 : tensor<32x17x8x128xf32> loc(#loc10469)
      %10632 = stablehlo.convert %10631 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc10470)
      %10633 = stablehlo.multiply %10613, %10632 : tensor<32x17x8x128xbf16> loc(#loc10471)
      %10634 = stablehlo.transpose %10633, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10472)
      %10635 = stablehlo.multiply %10634, %132 : tensor<32x8x17x128xbf16> loc(#loc10473)
      %10636 = stablehlo.slice %10634 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10474)
      %10637 = stablehlo.negate %10636 : tensor<32x8x17x64xbf16> loc(#loc10475)
      %10638 = stablehlo.slice %10634 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10476)
      %10639 = stablehlo.concatenate %10637, %10638, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10477)
      %10640 = stablehlo.multiply %10639, %138 : tensor<32x8x17x128xbf16> loc(#loc10478)
      %10641 = stablehlo.add %10635, %10640 : tensor<32x8x17x128xbf16> loc(#loc10479)
      %10642 = stablehlo.convert %10641 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc10480)
      %10643 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10644 = stablehlo.multiply %10642, %10643 : tensor<32x8x17x128xf32> loc(#loc10481)
      %10645 = stablehlo.broadcast_in_dim %10597, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10482)
      %10646 = stablehlo.reshape %10645 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10483)
      %10647 = stablehlo.convert %10646 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10484)
      %10648 = stablehlo.transpose %10647, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc10485)
      %10649 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %10650 = stablehlo.multiply %10648, %10649 : tensor<32x8x128x128xf32> loc(#loc10486)
      %10651 = stablehlo.dot_general %10644, %10650, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10487)
      %10652 = stablehlo.add %10651, %159 : tensor<32x8x17x128xf32> loc(#loc10488)
      %10653 = stablehlo.convert %10652 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc10489)
      %10654 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %10655 = stablehlo.compare  EQ, %10653, %10654 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc10490)
      %10656 = stablehlo.not %10655 : tensor<32x8x17x128xi1> loc(#loc10491)
      %10657 = stablehlo.reduce(%10656 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.24486"), %arg1677: tensor<i1> loc("reduce.24486"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc10493)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc10494)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc10492)
      %10658 = stablehlo.reshape %10657 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc10495)
      %10659 = stablehlo.not %10658 : tensor<32x8x17x1xi1> loc(#loc10496)
      %10660 = stablehlo.reshape %10659 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc10497)
      %10661 = stablehlo.broadcast_in_dim %10660, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc10498)
      %10662 = stablehlo.reduce(%10652 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10499)
      %10663 = stablehlo.broadcast_in_dim %10662, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10500)
      %10664 = stablehlo.subtract %10652, %10663 : tensor<32x8x17x128xf32> loc(#loc10501)
      %10665 = stablehlo.exponential %10664 : tensor<32x8x17x128xf32> loc(#loc10502)
      %10666 = stablehlo.reduce(%10665 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10503)
      %10667 = stablehlo.broadcast_in_dim %10666, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10504)
      %10668 = stablehlo.divide %10665, %10667 : tensor<32x8x17x128xf32> loc(#loc10505)
      %10669 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10670 = stablehlo.select %10661, %10669, %10668 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc10506)
      %10671 = stablehlo.broadcast_in_dim %10604, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10507)
      %10672 = stablehlo.reshape %10671 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10508)
      %10673 = stablehlo.convert %10672 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10509)
      %10674 = stablehlo.dot_general %10670, %10673, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10510)
      %10675 = stablehlo.convert %10674 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc10511)
      %10676 = stablehlo.transpose %10675, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10512)
      %10677 = stablehlo.reshape %10676 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc10513)
      %10678 = stablehlo.reshape %arg1644 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc10514)
      %10679 = stablehlo.reshape %10678 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc10515)
      %10680 = stablehlo.transpose %10679, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc10516)
      %10681 = stablehlo.dot_general %10677, %10680, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10517)
      %10682 = "stablehlo.all_reduce"(%10681) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.24503"), %arg1677: tensor<bf16> loc("dot.24503")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10517)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10517)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10517)
      %10683 = stablehlo.reshape %10682 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10518)
      %10684 = stablehlo.add %10552, %10683 : tensor<32x17x5120xbf16> loc(#loc10519)
      %10685 = stablehlo.reshape %arg1647 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10520)
      %10686 = stablehlo.reshape %10685 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10521)
      %10687 = stablehlo.broadcast_in_dim %10686, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10522)
      %10688 = stablehlo.convert %10684 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10523)
      %10689 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10690 = stablehlo.power %10688, %10689 : tensor<32x17x5120xf32> loc(#loc10524)
      %10691 = stablehlo.reduce(%10690 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10525)
      %10692 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10693 = stablehlo.multiply %10691, %10692 : tensor<32x17xf32> loc(#loc10526)
      %10694 = stablehlo.reshape %10693 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10527)
      %10695 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10696 = stablehlo.add %10694, %10695 : tensor<32x17x1xf32> loc(#loc10528)
      %10697 = stablehlo.rsqrt %10696 : tensor<32x17x1xf32> loc(#loc10529)
      %10698 = stablehlo.reshape %10697 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10530)
      %10699 = stablehlo.broadcast_in_dim %10698, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10531)
      %10700 = stablehlo.multiply %10688, %10699 : tensor<32x17x5120xf32> loc(#loc10532)
      %10701 = stablehlo.convert %10700 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10533)
      %10702 = stablehlo.multiply %10687, %10701 : tensor<32x17x5120xbf16> loc(#loc10534)
      %10703 = stablehlo.reshape %10702 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10535)
      %10704 = stablehlo.reshape %arg1648 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10536)
      %10705 = stablehlo.reshape %10704 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10537)
      %10706 = stablehlo.transpose %10705, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10538)
      %10707 = stablehlo.dot_general %10703, %10706, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10539)
      %10708 = stablehlo.reshape %10707 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10540)
      %10709 = stablehlo.logistic %10708 : tensor<32x17x3200xbf16> loc(#loc10541)
      %10710 = stablehlo.multiply %10708, %10709 : tensor<32x17x3200xbf16> loc(#loc10542)
      %10711 = stablehlo.reshape %arg1643 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10543)
      %10712 = stablehlo.reshape %10711 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10544)
      %10713 = stablehlo.transpose %10712, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10545)
      %10714 = stablehlo.dot_general %10703, %10713, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10546)
      %10715 = stablehlo.reshape %10714 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10547)
      %10716 = stablehlo.multiply %10710, %10715 : tensor<32x17x3200xbf16> loc(#loc10548)
      %10717 = stablehlo.reshape %10716 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10549)
      %10718 = stablehlo.reshape %arg1642 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc10550)
      %10719 = stablehlo.reshape %10718 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc10551)
      %10720 = stablehlo.transpose %10719, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc10552)
      %10721 = stablehlo.dot_general %10717, %10720, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10553)
      %10722 = "stablehlo.all_reduce"(%10721) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.24558"), %arg1677: tensor<bf16> loc("dot.24558")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10553)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10553)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10553)
      %10723 = stablehlo.reshape %10722 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10554)
      %10724 = stablehlo.add %10684, %10723 : tensor<32x17x5120xbf16> loc(#loc10555)
      %10725 = stablehlo.convert %10724 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10556)
      %10726 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10727 = stablehlo.power %10725, %10726 : tensor<32x17x5120xf32> loc(#loc10557)
      %10728 = stablehlo.reduce(%10727 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10558)
      %10729 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10730 = stablehlo.multiply %10728, %10729 : tensor<32x17xf32> loc(#loc10559)
      %10731 = stablehlo.reshape %10730 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10560)
      %10732 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10733 = stablehlo.add %10731, %10732 : tensor<32x17x1xf32> loc(#loc10561)
      %10734 = stablehlo.rsqrt %10733 : tensor<32x17x1xf32> loc(#loc10562)
      %10735 = stablehlo.reshape %10734 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10563)
      %10736 = stablehlo.broadcast_in_dim %10735, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10564)
      %10737 = stablehlo.multiply %10725, %10736 : tensor<32x17x5120xf32> loc(#loc10565)
      %10738 = stablehlo.convert %10737 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10566)
      %10739 = stablehlo.multiply %10610, %10738 : tensor<32x17x5120xbf16> loc(#loc10567)
      %10740 = stablehlo.reshape %10739 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10568)
      %10741 = stablehlo.reshape %arg1641 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10569)
      %10742 = stablehlo.reshape %10741 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10570)
      %10743 = stablehlo.transpose %10742, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10571)
      %10744 = stablehlo.dot_general %10740, %10743, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10572)
      %10745 = stablehlo.reshape %10744 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10573)
      %10746 = stablehlo.convert %10745 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc10574)
      %10747 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x1x128xf32> loc(#loc)
      %10748 = stablehlo.power %10746, %10747 : tensor<32x17x1x128xf32> loc(#loc10575)
      %10749 = stablehlo.reduce(%10748 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc10576)
      %10750 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10751 = stablehlo.multiply %10749, %10750 : tensor<32x17x1xf32> loc(#loc10577)
      %10752 = stablehlo.reshape %10751 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc10578)
      %10753 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1x1xf32> loc(#loc)
      %10754 = stablehlo.add %10752, %10753 : tensor<32x17x1x1xf32> loc(#loc10579)
      %10755 = stablehlo.rsqrt %10754 : tensor<32x17x1x1xf32> loc(#loc10580)
      %10756 = stablehlo.reshape %10755 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc10581)
      %10757 = stablehlo.broadcast_in_dim %10756, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc10582)
      %10758 = stablehlo.multiply %10746, %10757 : tensor<32x17x1x128xf32> loc(#loc10583)
      %10759 = stablehlo.convert %10758 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc10584)
      %10760 = stablehlo.multiply %10607, %10759 : tensor<32x17x1x128xbf16> loc(#loc10585)
      %10761 = stablehlo.transpose %10760, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10586)
      %10762 = stablehlo.multiply %10761, %82 : tensor<32x1x17x128xbf16> loc(#loc10587)
      %10763 = stablehlo.slice %10761 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10588)
      %10764 = stablehlo.negate %10763 : tensor<32x1x17x64xbf16> loc(#loc10589)
      %10765 = stablehlo.slice %10761 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10590)
      %10766 = stablehlo.concatenate %10764, %10765, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10591)
      %10767 = stablehlo.multiply %10766, %91 : tensor<32x1x17x128xbf16> loc(#loc10592)
      %10768 = stablehlo.add %10762, %10767 : tensor<32x1x17x128xbf16> loc(#loc10593)
      %10769 = "stablehlo.scatter"(%arg1651, %21, %10768) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.24670"), %arg1677: tensor<bf16> loc("scatter.24670")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10594)
      %10770 = stablehlo.reshape %arg1652 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10595)
      %10771 = stablehlo.reshape %10770 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10596)
      %10772 = stablehlo.transpose %10771, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10597)
      %10773 = stablehlo.dot_general %10740, %10772, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10598)
      %10774 = stablehlo.reshape %10773 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10599)
      %10775 = stablehlo.transpose %10774, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10600)
      %10776 = "stablehlo.scatter"(%arg1653, %21, %10775) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.24700"), %arg1677: tensor<bf16> loc("scatter.24700")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10601)
      %10777 = stablehlo.reshape %arg1663 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10602)
      %10778 = stablehlo.reshape %10777 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10603)
      %10779 = stablehlo.broadcast_in_dim %10778, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10604)
      %10780 = stablehlo.reshape %arg1662 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10605)
      %10781 = stablehlo.reshape %10780 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10606)
      %10782 = stablehlo.broadcast_in_dim %10781, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10607)
      %10783 = stablehlo.reshape %arg1659 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10608)
      %10784 = stablehlo.reshape %10783 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10609)
      %10785 = stablehlo.broadcast_in_dim %10784, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10610)
      %10786 = stablehlo.reshape %arg1658 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc10611)
      %10787 = stablehlo.reshape %10786 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc10612)
      %10788 = stablehlo.transpose %10787, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc10613)
      %10789 = stablehlo.dot_general %10740, %10788, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc10614)
      %10790 = stablehlo.reshape %10789 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10615)
      %10791 = stablehlo.convert %10790 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc10616)
      %10792 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x8x128xf32> loc(#loc)
      %10793 = stablehlo.power %10791, %10792 : tensor<32x17x8x128xf32> loc(#loc10617)
      %10794 = stablehlo.reduce(%10793 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc10618)
      %10795 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<32x17x8xf32> loc(#loc)
      %10796 = stablehlo.multiply %10794, %10795 : tensor<32x17x8xf32> loc(#loc10619)
      %10797 = stablehlo.reshape %10796 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc10620)
      %10798 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x8x1xf32> loc(#loc)
      %10799 = stablehlo.add %10797, %10798 : tensor<32x17x8x1xf32> loc(#loc10621)
      %10800 = stablehlo.rsqrt %10799 : tensor<32x17x8x1xf32> loc(#loc10622)
      %10801 = stablehlo.reshape %10800 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc10623)
      %10802 = stablehlo.broadcast_in_dim %10801, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc10624)
      %10803 = stablehlo.multiply %10791, %10802 : tensor<32x17x8x128xf32> loc(#loc10625)
      %10804 = stablehlo.convert %10803 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc10626)
      %10805 = stablehlo.multiply %10785, %10804 : tensor<32x17x8x128xbf16> loc(#loc10627)
      %10806 = stablehlo.transpose %10805, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10628)
      %10807 = stablehlo.multiply %10806, %132 : tensor<32x8x17x128xbf16> loc(#loc10629)
      %10808 = stablehlo.slice %10806 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10630)
      %10809 = stablehlo.negate %10808 : tensor<32x8x17x64xbf16> loc(#loc10631)
      %10810 = stablehlo.slice %10806 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10632)
      %10811 = stablehlo.concatenate %10809, %10810, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10633)
      %10812 = stablehlo.multiply %10811, %138 : tensor<32x8x17x128xbf16> loc(#loc10634)
      %10813 = stablehlo.add %10807, %10812 : tensor<32x8x17x128xbf16> loc(#loc10635)
      %10814 = stablehlo.convert %10813 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc10636)
      %10815 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10816 = stablehlo.multiply %10814, %10815 : tensor<32x8x17x128xf32> loc(#loc10637)
      %10817 = stablehlo.broadcast_in_dim %10769, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10638)
      %10818 = stablehlo.reshape %10817 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10639)
      %10819 = stablehlo.convert %10818 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10640)
      %10820 = stablehlo.transpose %10819, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc10641)
      %10821 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<32x8x128x128xf32> loc(#loc)
      %10822 = stablehlo.multiply %10820, %10821 : tensor<32x8x128x128xf32> loc(#loc10642)
      %10823 = stablehlo.dot_general %10816, %10822, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10643)
      %10824 = stablehlo.add %10823, %159 : tensor<32x8x17x128xf32> loc(#loc10644)
      %10825 = stablehlo.convert %10824 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc10645)
      %10826 = stablehlo.broadcast_in_dim %cst_11, dims = [] : (tensor<f64>) -> tensor<32x8x17x128xf64> loc(#loc)
      %10827 = stablehlo.compare  EQ, %10825, %10826 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc10646)
      %10828 = stablehlo.not %10827 : tensor<32x8x17x128xi1> loc(#loc10647)
      %10829 = stablehlo.reduce(%10828 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.24881"), %arg1677: tensor<i1> loc("reduce.24881"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc10649)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc10650)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc10648)
      %10830 = stablehlo.reshape %10829 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc10651)
      %10831 = stablehlo.not %10830 : tensor<32x8x17x1xi1> loc(#loc10652)
      %10832 = stablehlo.reshape %10831 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc10653)
      %10833 = stablehlo.broadcast_in_dim %10832, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc10654)
      %10834 = stablehlo.reduce(%10824 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10655)
      %10835 = stablehlo.broadcast_in_dim %10834, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10656)
      %10836 = stablehlo.subtract %10824, %10835 : tensor<32x8x17x128xf32> loc(#loc10657)
      %10837 = stablehlo.exponential %10836 : tensor<32x8x17x128xf32> loc(#loc10658)
      %10838 = stablehlo.reduce(%10837 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10659)
      %10839 = stablehlo.broadcast_in_dim %10838, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10660)
      %10840 = stablehlo.divide %10837, %10839 : tensor<32x8x17x128xf32> loc(#loc10661)
      %10841 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<32x8x17x128xf32> loc(#loc)
      %10842 = stablehlo.select %10833, %10841, %10840 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc10662)
      %10843 = stablehlo.broadcast_in_dim %10776, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10663)
      %10844 = stablehlo.reshape %10843 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10664)
      %10845 = stablehlo.convert %10844 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10665)
      %10846 = stablehlo.dot_general %10842, %10845, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10666)
      %10847 = stablehlo.convert %10846 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc10667)
      %10848 = stablehlo.transpose %10847, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10668)
      %10849 = stablehlo.reshape %10848 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc10669)
      %10850 = stablehlo.reshape %arg1657 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc10670)
      %10851 = stablehlo.reshape %10850 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc10671)
      %10852 = stablehlo.transpose %10851, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc10672)
      %10853 = stablehlo.dot_general %10849, %10852, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10673)
      %10854 = "stablehlo.all_reduce"(%10853) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.24898"), %arg1677: tensor<bf16> loc("dot.24898")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10673)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10673)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10673)
      %10855 = stablehlo.reshape %10854 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10674)
      %10856 = stablehlo.add %10724, %10855 : tensor<32x17x5120xbf16> loc(#loc10675)
      %10857 = stablehlo.reshape %arg1660 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10676)
      %10858 = stablehlo.reshape %10857 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10677)
      %10859 = stablehlo.broadcast_in_dim %10858, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10678)
      %10860 = stablehlo.convert %10856 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10679)
      %10861 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10862 = stablehlo.power %10860, %10861 : tensor<32x17x5120xf32> loc(#loc10680)
      %10863 = stablehlo.reduce(%10862 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10681)
      %10864 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10865 = stablehlo.multiply %10863, %10864 : tensor<32x17xf32> loc(#loc10682)
      %10866 = stablehlo.reshape %10865 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10683)
      %10867 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10868 = stablehlo.add %10866, %10867 : tensor<32x17x1xf32> loc(#loc10684)
      %10869 = stablehlo.rsqrt %10868 : tensor<32x17x1xf32> loc(#loc10685)
      %10870 = stablehlo.reshape %10869 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10686)
      %10871 = stablehlo.broadcast_in_dim %10870, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10687)
      %10872 = stablehlo.multiply %10860, %10871 : tensor<32x17x5120xf32> loc(#loc10688)
      %10873 = stablehlo.convert %10872 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10689)
      %10874 = stablehlo.multiply %10859, %10873 : tensor<32x17x5120xbf16> loc(#loc10690)
      %10875 = stablehlo.reshape %10874 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10691)
      %10876 = stablehlo.reshape %arg1661 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10692)
      %10877 = stablehlo.reshape %10876 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10693)
      %10878 = stablehlo.transpose %10877, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10694)
      %10879 = stablehlo.dot_general %10875, %10878, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10695)
      %10880 = stablehlo.reshape %10879 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10696)
      %10881 = stablehlo.logistic %10880 : tensor<32x17x3200xbf16> loc(#loc10697)
      %10882 = stablehlo.multiply %10880, %10881 : tensor<32x17x3200xbf16> loc(#loc10698)
      %10883 = stablehlo.reshape %arg1656 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10699)
      %10884 = stablehlo.reshape %10883 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10700)
      %10885 = stablehlo.transpose %10884, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10701)
      %10886 = stablehlo.dot_general %10875, %10885, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10702)
      %10887 = stablehlo.reshape %10886 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10703)
      %10888 = stablehlo.multiply %10882, %10887 : tensor<32x17x3200xbf16> loc(#loc10704)
      %10889 = stablehlo.reshape %10888 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10705)
      %10890 = stablehlo.reshape %arg1655 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc10706)
      %10891 = stablehlo.reshape %10890 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc10707)
      %10892 = stablehlo.transpose %10891, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc10708)
      %10893 = stablehlo.dot_general %10889, %10892, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10709)
      %10894 = "stablehlo.all_reduce"(%10893) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.24953"), %arg1677: tensor<bf16> loc("dot.24953")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10709)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10709)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10709)
      %10895 = stablehlo.reshape %10894 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10710)
      %10896 = stablehlo.add %10856, %10895 : tensor<32x17x5120xbf16> loc(#loc10711)
      %10897 = stablehlo.convert %10896 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10712)
      %10898 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %10899 = stablehlo.power %10897, %10898 : tensor<32x17x5120xf32> loc(#loc10713)
      %10900 = stablehlo.reduce(%10899 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10714)
      %10901 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %10902 = stablehlo.multiply %10900, %10901 : tensor<32x17xf32> loc(#loc10715)
      %10903 = stablehlo.reshape %10902 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10716)
      %10904 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %10905 = stablehlo.add %10903, %10904 : tensor<32x17x1xf32> loc(#loc10717)
      %10906 = stablehlo.rsqrt %10905 : tensor<32x17x1xf32> loc(#loc10718)
      %10907 = stablehlo.reshape %10906 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10719)
      %10908 = stablehlo.broadcast_in_dim %10907, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10720)
      %10909 = stablehlo.multiply %10897, %10908 : tensor<32x17x5120xf32> loc(#loc10721)
      %10910 = stablehlo.convert %10909 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10722)
      %10911 = stablehlo.multiply %10782, %10910 : tensor<32x17x5120xbf16> loc(#loc10723)
      %10912 = stablehlo.reshape %10911 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10724)
      %10913 = stablehlo.reshape %arg1654 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10725)
      %10914 = stablehlo.reshape %10913 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10726)
      %10915 = stablehlo.transpose %10914, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10727)
      %10916 = stablehlo.dot_general %10912, %10915, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10728)
      %10917 = stablehlo.reshape %10916 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10729)
      %10918 = stablehlo.convert %10917 : (tensor<32x17x1x128xbf16>) -> tensor<32x17x1x128xf32> loc(#loc10730)
      %10919 = stablehlo.power %10918, %12 : tensor<32x17x1x128xf32> loc(#loc10731)
      %10920 = stablehlo.reduce(%10919 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x1x128xf32>, tensor<f32>) -> tensor<32x17x1xf32> loc(#loc10732)
      %10921 = stablehlo.multiply %10920, %11 : tensor<32x17x1xf32> loc(#loc10733)
      %10922 = stablehlo.reshape %10921 : (tensor<32x17x1xf32>) -> tensor<32x17x1x1xf32> loc(#loc10734)
      %10923 = stablehlo.add %10922, %10 : tensor<32x17x1x1xf32> loc(#loc10735)
      %10924 = stablehlo.rsqrt %10923 : tensor<32x17x1x1xf32> loc(#loc10736)
      %10925 = stablehlo.reshape %10924 : (tensor<32x17x1x1xf32>) -> tensor<32x17x1xf32> loc(#loc10737)
      %10926 = stablehlo.broadcast_in_dim %10925, dims = [0, 1, 2] : (tensor<32x17x1xf32>) -> tensor<32x17x1x128xf32> loc(#loc10738)
      %10927 = stablehlo.multiply %10918, %10926 : tensor<32x17x1x128xf32> loc(#loc10739)
      %10928 = stablehlo.convert %10927 : (tensor<32x17x1x128xf32>) -> tensor<32x17x1x128xbf16> loc(#loc10740)
      %10929 = stablehlo.multiply %10779, %10928 : tensor<32x17x1x128xbf16> loc(#loc10741)
      %10930 = stablehlo.transpose %10929, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10742)
      %10931 = stablehlo.multiply %10930, %82 : tensor<32x1x17x128xbf16> loc(#loc10743)
      %10932 = stablehlo.slice %10930 [0:32, 0:1, 0:17, 64:128] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10744)
      %10933 = stablehlo.negate %10932 : tensor<32x1x17x64xbf16> loc(#loc10745)
      %10934 = stablehlo.slice %10930 [0:32, 0:1, 0:17, 0:64] : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x64xbf16> loc(#loc10746)
      %10935 = stablehlo.concatenate %10933, %10934, dim = 3 : (tensor<32x1x17x64xbf16>, tensor<32x1x17x64xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10747)
      %10936 = stablehlo.multiply %10935, %91 : tensor<32x1x17x128xbf16> loc(#loc10748)
      %10937 = stablehlo.add %10931, %10936 : tensor<32x1x17x128xbf16> loc(#loc10749)
      %10938 = "stablehlo.scatter"(%arg1664, %21, %10937) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.25065"), %arg1677: tensor<bf16> loc("scatter.25065")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10750)
      %10939 = stablehlo.reshape %arg1665 : (tensor<128x5120xbf16>) -> tensor<1x128x5120xbf16> loc(#loc10751)
      %10940 = stablehlo.reshape %10939 : (tensor<1x128x5120xbf16>) -> tensor<128x5120xbf16> loc(#loc10752)
      %10941 = stablehlo.transpose %10940, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,1024]{0,1}"} : (tensor<128x5120xbf16>) -> tensor<5120x128xbf16> loc(#loc10753)
      %10942 = stablehlo.dot_general %10912, %10941, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x128xbf16>) -> tensor<544x128xbf16> loc(#loc10754)
      %10943 = stablehlo.reshape %10942 : (tensor<544x128xbf16>) -> tensor<32x17x1x128xbf16> loc(#loc10755)
      %10944 = stablehlo.transpose %10943, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,8,17,128]{3,1,2,0}"} : (tensor<32x17x1x128xbf16>) -> tensor<32x1x17x128xbf16> loc(#loc10756)
      %10945 = "stablehlo.scatter"(%arg1666, %21, %10944) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("scatter.25095"), %arg1677: tensor<bf16> loc("scatter.25095")):
        stablehlo.return %arg1677 : tensor<bf16> loc(#loc)
      }) : (tensor<32x1x128x128xbf16>, tensor<17x1xi64>, tensor<32x1x17x128xbf16>) -> tensor<32x1x128x128xbf16> loc(#loc10757)
      %10946 = stablehlo.reshape %arg1675 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10758)
      %10947 = stablehlo.reshape %10946 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10759)
      %10948 = stablehlo.broadcast_in_dim %10947, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10760)
      %10949 = stablehlo.reshape %arg1672 : (tensor<128xbf16>) -> tensor<1x1x128xbf16> loc(#loc10761)
      %10950 = stablehlo.reshape %10949 : (tensor<1x1x128xbf16>) -> tensor<128xbf16> loc(#loc10762)
      %10951 = stablehlo.broadcast_in_dim %10950, dims = [3] : (tensor<128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10763)
      %10952 = stablehlo.reshape %arg1671 : (tensor<1024x5120xbf16>) -> tensor<1x1024x5120xbf16> loc(#loc10764)
      %10953 = stablehlo.reshape %10952 : (tensor<1x1024x5120xbf16>) -> tensor<1024x5120xbf16> loc(#loc10765)
      %10954 = stablehlo.transpose %10953, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,8192]{0,1}"} : (tensor<1024x5120xbf16>) -> tensor<5120x1024xbf16> loc(#loc10766)
      %10955 = stablehlo.dot_general %10912, %10954, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x1024xbf16>) -> tensor<544x1024xbf16> loc(#loc10767)
      %10956 = stablehlo.reshape %10955 : (tensor<544x1024xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10768)
      %10957 = stablehlo.convert %10956 : (tensor<32x17x8x128xbf16>) -> tensor<32x17x8x128xf32> loc(#loc10769)
      %10958 = stablehlo.power %10957, %9 : tensor<32x17x8x128xf32> loc(#loc10770)
      %10959 = stablehlo.reduce(%10958 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x17x8x128xf32>, tensor<f32>) -> tensor<32x17x8xf32> loc(#loc10771)
      %10960 = stablehlo.multiply %10959, %8 : tensor<32x17x8xf32> loc(#loc10772)
      %10961 = stablehlo.reshape %10960 : (tensor<32x17x8xf32>) -> tensor<32x17x8x1xf32> loc(#loc10773)
      %10962 = stablehlo.add %10961, %7 : tensor<32x17x8x1xf32> loc(#loc10774)
      %10963 = stablehlo.rsqrt %10962 : tensor<32x17x8x1xf32> loc(#loc10775)
      %10964 = stablehlo.reshape %10963 : (tensor<32x17x8x1xf32>) -> tensor<32x17x8xf32> loc(#loc10776)
      %10965 = stablehlo.broadcast_in_dim %10964, dims = [0, 1, 2] : (tensor<32x17x8xf32>) -> tensor<32x17x8x128xf32> loc(#loc10777)
      %10966 = stablehlo.multiply %10957, %10965 : tensor<32x17x8x128xf32> loc(#loc10778)
      %10967 = stablehlo.convert %10966 : (tensor<32x17x8x128xf32>) -> tensor<32x17x8x128xbf16> loc(#loc10779)
      %10968 = stablehlo.multiply %10951, %10967 : tensor<32x17x8x128xbf16> loc(#loc10780)
      %10969 = stablehlo.transpose %10968, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,64,17,128]{3,1,2,0}"} : (tensor<32x17x8x128xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10781)
      %10970 = stablehlo.multiply %10969, %132 : tensor<32x8x17x128xbf16> loc(#loc10782)
      %10971 = stablehlo.slice %10969 [0:32, 0:8, 0:17, 64:128] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10783)
      %10972 = stablehlo.negate %10971 : tensor<32x8x17x64xbf16> loc(#loc10784)
      %10973 = stablehlo.slice %10969 [0:32, 0:8, 0:17, 0:64] : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x64xbf16> loc(#loc10785)
      %10974 = stablehlo.concatenate %10972, %10973, dim = 3 : (tensor<32x8x17x64xbf16>, tensor<32x8x17x64xbf16>) -> tensor<32x8x17x128xbf16> loc(#loc10786)
      %10975 = stablehlo.multiply %10974, %138 : tensor<32x8x17x128xbf16> loc(#loc10787)
      %10976 = stablehlo.add %10970, %10975 : tensor<32x8x17x128xbf16> loc(#loc10788)
      %10977 = stablehlo.convert %10976 : (tensor<32x8x17x128xbf16>) -> tensor<32x8x17x128xf32> loc(#loc10789)
      %10978 = stablehlo.multiply %10977, %6 : tensor<32x8x17x128xf32> loc(#loc10790)
      %10979 = stablehlo.broadcast_in_dim %10938, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10791)
      %10980 = stablehlo.reshape %10979 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10792)
      %10981 = stablehlo.convert %10980 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10793)
      %10982 = stablehlo.transpose %10981, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "f32[32,64,128,128]{2,3,1,0}"} : (tensor<32x8x128x128xf32>) -> tensor<32x8x128x128xf32> loc(#loc10794)
      %10983 = stablehlo.multiply %10982, %5 : tensor<32x8x128x128xf32> loc(#loc10795)
      %10984 = stablehlo.dot_general %10978, %10983, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10796)
      %10985 = stablehlo.add %10984, %159 : tensor<32x8x17x128xf32> loc(#loc10797)
      %10986 = stablehlo.convert %10985 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xf64> loc(#loc10798)
      %10987 = stablehlo.compare  EQ, %10986, %2 : (tensor<32x8x17x128xf64>, tensor<32x8x17x128xf64>) -> tensor<32x8x17x128xi1> loc(#loc10799)
      %10988 = stablehlo.not %10987 : tensor<32x8x17x128xi1> loc(#loc10800)
      %10989 = stablehlo.reduce(%10988 init: %c_1) across dimensions = [3] : (tensor<32x8x17x128xi1>, tensor<i1>) -> tensor<32x8x17xi1>
       reducer(%arg1676: tensor<i1> loc("reduce.25271"), %arg1677: tensor<i1> loc("reduce.25271"))  {
        %11074 = stablehlo.or %arg1676, %arg1677 : tensor<i1> loc(#loc10802)
        %11075 = stablehlo.select %11074, %c, %c_1 : tensor<i1>, tensor<i1> loc(#loc10803)
        stablehlo.return %11075 : tensor<i1> loc(#loc)
      } loc(#loc10801)
      %10990 = stablehlo.reshape %10989 : (tensor<32x8x17xi1>) -> tensor<32x8x17x1xi1> loc(#loc10804)
      %10991 = stablehlo.not %10990 : tensor<32x8x17x1xi1> loc(#loc10805)
      %10992 = stablehlo.reshape %10991 : (tensor<32x8x17x1xi1>) -> tensor<32x8x17xi1> loc(#loc10806)
      %10993 = stablehlo.broadcast_in_dim %10992, dims = [0, 1, 2] : (tensor<32x8x17xi1>) -> tensor<32x8x17x128xi1> loc(#loc10807)
      %10994 = stablehlo.reduce(%10985 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10808)
      %10995 = stablehlo.broadcast_in_dim %10994, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10809)
      %10996 = stablehlo.subtract %10985, %10995 : tensor<32x8x17x128xf32> loc(#loc10810)
      %10997 = stablehlo.exponential %10996 : tensor<32x8x17x128xf32> loc(#loc10811)
      %10998 = stablehlo.reduce(%10997 init: %cst_12) applies stablehlo.add across dimensions = [3] : (tensor<32x8x17x128xf32>, tensor<f32>) -> tensor<32x8x17xf32> loc(#loc10812)
      %10999 = stablehlo.broadcast_in_dim %10998, dims = [0, 1, 2] : (tensor<32x8x17xf32>) -> tensor<32x8x17x128xf32> loc(#loc10813)
      %11000 = stablehlo.divide %10997, %10999 : tensor<32x8x17x128xf32> loc(#loc10814)
      %11001 = stablehlo.select %10993, %1, %11000 : tensor<32x8x17x128xi1>, tensor<32x8x17x128xf32> loc(#loc10815)
      %11002 = stablehlo.broadcast_in_dim %10945, dims = [0, 1, 3, 4] : (tensor<32x1x128x128xbf16>) -> tensor<32x1x8x128x128xbf16> loc(#loc10816)
      %11003 = stablehlo.reshape %11002 : (tensor<32x1x8x128x128xbf16>) -> tensor<32x8x128x128xbf16> loc(#loc10817)
      %11004 = stablehlo.convert %11003 : (tensor<32x8x128x128xbf16>) -> tensor<32x8x128x128xf32> loc(#loc10818)
      %11005 = stablehlo.dot_general %11001, %11004, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<32x8x17x128xf32>, tensor<32x8x128x128xf32>) -> tensor<32x8x17x128xf32> loc(#loc10819)
      %11006 = stablehlo.convert %11005 : (tensor<32x8x17x128xf32>) -> tensor<32x8x17x128xbf16> loc(#loc10820)
      %11007 = stablehlo.transpose %11006, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[32,17,64,128]{3,1,2,0}"} : (tensor<32x8x17x128xbf16>) -> tensor<32x17x8x128xbf16> loc(#loc10821)
      %11008 = stablehlo.reshape %11007 : (tensor<32x17x8x128xbf16>) -> tensor<544x1024xbf16> loc(#loc10822)
      %11009 = stablehlo.reshape %arg1670 : (tensor<5120x1024xbf16>) -> tensor<1x5120x1024xbf16> loc(#loc10823)
      %11010 = stablehlo.reshape %11009 : (tensor<1x5120x1024xbf16>) -> tensor<5120x1024xbf16> loc(#loc10824)
      %11011 = stablehlo.transpose %11010, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,5120]{0,1}"} : (tensor<5120x1024xbf16>) -> tensor<1024x5120xbf16> loc(#loc10825)
      %11012 = stablehlo.dot_general %11008, %11011, contracting_dims = [1] x [0] : (tensor<544x1024xbf16>, tensor<1024x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10826)
      %11013 = "stablehlo.all_reduce"(%11012) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.25288"), %arg1677: tensor<bf16> loc("dot.25288")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10826)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10826)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10826)
      %11014 = stablehlo.reshape %11013 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10827)
      %11015 = stablehlo.add %10896, %11014 : tensor<32x17x5120xbf16> loc(#loc10828)
      %11016 = stablehlo.reshape %arg1673 : (tensor<5120xbf16>) -> tensor<1x1x5120xbf16> loc(#loc10829)
      %11017 = stablehlo.reshape %11016 : (tensor<1x1x5120xbf16>) -> tensor<5120xbf16> loc(#loc10830)
      %11018 = stablehlo.broadcast_in_dim %11017, dims = [2] : (tensor<5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10831)
      %11019 = stablehlo.convert %11015 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10832)
      %11020 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<32x17x5120xf32> loc(#loc)
      %11021 = stablehlo.power %11019, %11020 : tensor<32x17x5120xf32> loc(#loc10833)
      %11022 = stablehlo.reduce(%11021 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10834)
      %11023 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<32x17xf32> loc(#loc)
      %11024 = stablehlo.multiply %11022, %11023 : tensor<32x17xf32> loc(#loc10835)
      %11025 = stablehlo.reshape %11024 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10836)
      %11026 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<32x17x1xf32> loc(#loc)
      %11027 = stablehlo.add %11025, %11026 : tensor<32x17x1xf32> loc(#loc10837)
      %11028 = stablehlo.rsqrt %11027 : tensor<32x17x1xf32> loc(#loc10838)
      %11029 = stablehlo.reshape %11028 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10839)
      %11030 = stablehlo.broadcast_in_dim %11029, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10840)
      %11031 = stablehlo.multiply %11019, %11030 : tensor<32x17x5120xf32> loc(#loc10841)
      %11032 = stablehlo.convert %11031 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10842)
      %11033 = stablehlo.multiply %11018, %11032 : tensor<32x17x5120xbf16> loc(#loc10843)
      %11034 = stablehlo.reshape %11033 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10844)
      %11035 = stablehlo.reshape %arg1674 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10845)
      %11036 = stablehlo.reshape %11035 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10846)
      %11037 = stablehlo.transpose %11036, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10847)
      %11038 = stablehlo.dot_general %11034, %11037, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10848)
      %11039 = stablehlo.reshape %11038 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10849)
      %11040 = stablehlo.logistic %11039 : tensor<32x17x3200xbf16> loc(#loc10850)
      %11041 = stablehlo.multiply %11039, %11040 : tensor<32x17x3200xbf16> loc(#loc10851)
      %11042 = stablehlo.reshape %arg1669 : (tensor<3200x5120xbf16>) -> tensor<1x3200x5120xbf16> loc(#loc10852)
      %11043 = stablehlo.reshape %11042 : (tensor<1x3200x5120xbf16>) -> tensor<3200x5120xbf16> loc(#loc10853)
      %11044 = stablehlo.transpose %11043, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,25600]{0,1}"} : (tensor<3200x5120xbf16>) -> tensor<5120x3200xbf16> loc(#loc10854)
      %11045 = stablehlo.dot_general %11034, %11044, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10855)
      %11046 = stablehlo.reshape %11045 : (tensor<544x3200xbf16>) -> tensor<32x17x3200xbf16> loc(#loc10856)
      %11047 = stablehlo.multiply %11041, %11046 : tensor<32x17x3200xbf16> loc(#loc10857)
      %11048 = stablehlo.reshape %11047 : (tensor<32x17x3200xbf16>) -> tensor<544x3200xbf16> loc(#loc10858)
      %11049 = stablehlo.reshape %arg1668 : (tensor<5120x3200xbf16>) -> tensor<1x5120x3200xbf16> loc(#loc10859)
      %11050 = stablehlo.reshape %11049 : (tensor<1x5120x3200xbf16>) -> tensor<5120x3200xbf16> loc(#loc10860)
      %11051 = stablehlo.transpose %11050, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[25600,5120]{0,1}"} : (tensor<5120x3200xbf16>) -> tensor<3200x5120xbf16> loc(#loc10861)
      %11052 = stablehlo.dot_general %11048, %11051, contracting_dims = [1] x [0] : (tensor<544x3200xbf16>, tensor<3200x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10862)
      %11053 = "stablehlo.all_reduce"(%11052) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>}> ({
      ^bb0(%arg1676: tensor<bf16> loc("dot.25343"), %arg1677: tensor<bf16> loc("dot.25343")):
        %11074 = stablehlo.add %arg1676, %arg1677 : tensor<bf16> loc(#loc10862)
        stablehlo.return %11074 : tensor<bf16> loc(#loc10862)
      }) : (tensor<544x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10862)
      %11054 = stablehlo.reshape %11053 : (tensor<544x5120xbf16>) -> tensor<32x17x5120xbf16> loc(#loc10863)
      %11055 = stablehlo.add %11015, %11054 : tensor<32x17x5120xbf16> loc(#loc10864)
      %11056 = stablehlo.convert %11055 : (tensor<32x17x5120xbf16>) -> tensor<32x17x5120xf32> loc(#loc10865)
      %11057 = stablehlo.power %11056, %15 : tensor<32x17x5120xf32> loc(#loc10866)
      %11058 = stablehlo.reduce(%11057 init: %cst_12) applies stablehlo.add across dimensions = [2] : (tensor<32x17x5120xf32>, tensor<f32>) -> tensor<32x17xf32> loc(#loc10867)
      %11059 = stablehlo.multiply %11058, %14 : tensor<32x17xf32> loc(#loc10868)
      %11060 = stablehlo.reshape %11059 : (tensor<32x17xf32>) -> tensor<32x17x1xf32> loc(#loc10869)
      %11061 = stablehlo.add %11060, %13 : tensor<32x17x1xf32> loc(#loc10870)
      %11062 = stablehlo.rsqrt %11061 : tensor<32x17x1xf32> loc(#loc10871)
      %11063 = stablehlo.reshape %11062 : (tensor<32x17x1xf32>) -> tensor<32x17xf32> loc(#loc10872)
      %11064 = stablehlo.broadcast_in_dim %11063, dims = [0, 1] : (tensor<32x17xf32>) -> tensor<32x17x5120xf32> loc(#loc10873)
      %11065 = stablehlo.multiply %11056, %11064 : tensor<32x17x5120xf32> loc(#loc10874)
      %11066 = stablehlo.convert %11065 : (tensor<32x17x5120xf32>) -> tensor<32x17x5120xbf16> loc(#loc10875)
      %11067 = stablehlo.multiply %10948, %11066 : tensor<32x17x5120xbf16> loc(#loc10876)
      %11068 = stablehlo.reshape %11067 : (tensor<32x17x5120xbf16>) -> tensor<544x5120xbf16> loc(#loc10877)
      %11069 = stablehlo.reshape %arg1667 : (tensor<18992x5120xbf16>) -> tensor<1x18992x5120xbf16> loc(#loc10878)
      %11070 = stablehlo.reshape %11069 : (tensor<1x18992x5120xbf16>) -> tensor<18992x5120xbf16> loc(#loc10879)
      %11071 = stablehlo.transpose %11070, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[5120,151936]{0,1}"} : (tensor<18992x5120xbf16>) -> tensor<5120x18992xbf16> loc(#loc10880)
      %11072 = stablehlo.dot_general %11068, %11071, contracting_dims = [1] x [0] : (tensor<544x5120xbf16>, tensor<5120x18992xbf16>) -> tensor<544x18992xbf16> loc(#loc10881)
      %11073 = stablehlo.reshape %11072 : (tensor<544x18992xbf16>) -> tensor<32x17x18992xbf16> loc(#loc10882)
      sdy.return %94, %101, %277, %284, %449, %456, %621, %628, %793, %800, %965, %972, %1137, %1144, %1309, %1316, %1481, %1488, %1653, %1660, %1825, %1832, %1997, %2004, %2169, %2176, %2341, %2348, %2513, %2520, %2685, %2692, %2857, %2864, %3029, %3036, %3201, %3208, %3373, %3380, %3545, %3552, %3717, %3724, %3889, %3896, %4061, %4068, %4233, %4240, %4405, %4412, %4577, %4584, %4749, %4756, %4921, %4928, %5093, %5100, %5265, %5272, %5437, %5444, %5609, %5616, %5781, %5788, %5953, %5960, %6125, %6132, %6297, %6304, %6469, %6476, %6641, %6648, %6813, %6820, %6985, %6992, %7157, %7164, %7329, %7336, %7501, %7508, %7673, %7680, %7845, %7852, %8017, %8024, %8189, %8196, %8361, %8368, %8533, %8540, %8705, %8712, %8877, %8884, %9049, %9056, %9221, %9228, %9393, %9400, %9565, %9572, %9737, %9744, %9909, %9916, %10081, %10088, %10253, %10260, %10425, %10432, %10597, %10604, %10769, %10776, %10938, %10945, %11073 : tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x1x128x128xbf16>, tensor<32x17x18992xbf16> loc(#loc)
    } : (tensor<17xi64>, tensor<64xbf16>, tensor<1024x5120xbf16>, tensor<32x17xi64>, tensor<151936x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>, tensor<128xbf16>, tensor<32x8x128x128xbf16>, tensor<1024x5120xbf16>, tensor<32x8x128x128xbf16>, tensor<151936x5120xbf16>, tensor<5120x25600xbf16>, tensor<25600x5120xbf16>, tensor<5120x8192xbf16>, tensor<8192x5120xbf16>, tensor<128xbf16>, tensor<5120xbf16>, tensor<25600x5120xbf16>, tensor<5120xbf16>) -> (tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x17x151936xbf16>) loc(#loc)
    return %0#0, %0#1, %0#2, %0#3, %0#4, %0#5, %0#6, %0#7, %0#8, %0#9, %0#10, %0#11, %0#12, %0#13, %0#14, %0#15, %0#16, %0#17, %0#18, %0#19, %0#20, %0#21, %0#22, %0#23, %0#24, %0#25, %0#26, %0#27, %0#28, %0#29, %0#30, %0#31, %0#32, %0#33, %0#34, %0#35, %0#36, %0#37, %0#38, %0#39, %0#40, %0#41, %0#42, %0#43, %0#44, %0#45, %0#46, %0#47, %0#48, %0#49, %0#50, %0#51, %0#52, %0#53, %0#54, %0#55, %0#56, %0#57, %0#58, %0#59, %0#60, %0#61, %0#62, %0#63, %0#64, %0#65, %0#66, %0#67, %0#68, %0#69, %0#70, %0#71, %0#72, %0#73, %0#74, %0#75, %0#76, %0#77, %0#78, %0#79, %0#80, %0#81, %0#82, %0#83, %0#84, %0#85, %0#86, %0#87, %0#88, %0#89, %0#90, %0#91, %0#92, %0#93, %0#94, %0#95, %0#96, %0#97, %0#98, %0#99, %0#100, %0#101, %0#102, %0#103, %0#104, %0#105, %0#106, %0#107, %0#108, %0#109, %0#110, %0#111, %0#112, %0#113, %0#114, %0#115, %0#116, %0#117, %0#118, %0#119, %0#120, %0#121, %0#122, %0#123, %0#124, %0#125, %0#126, %0#127, %0#128 : tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x8x128x128xbf16>, tensor<32x17x151936xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc839 = loc("reshape.4")
#loc840 = loc("reshape.6")
#loc841 = loc("compare.153")
#loc842 = loc("add.150")
#loc843 = loc("select.154")
#loc844 = loc("reshape.155")
#loc845 = loc("reshape.120")
#loc846 = loc("reshape.122")
#loc847 = loc("broadcast.123")
#loc848 = loc("reshape.81")
#loc849 = loc("reshape.83")
#loc850 = loc("broadcast.84")
#loc851 = loc("reshape.45")
#loc852 = loc("reshape.47")
#loc853 = loc("reshape.40")
#loc854 = loc("reshape.43")
#loc855 = loc("convert.48")
#loc856 = loc("gather.49")
#loc857 = loc("reshape.50")
#loc858 = loc("convert.51")
#loc859 = loc("power.53")
#loc860 = loc("reduce.60")
#loc861 = loc("multiply.69")
#loc862 = loc("reshape.70")
#loc863 = loc("add.74")
#loc864 = loc("rsqrt.75")
#loc865 = loc("reshape.76")
#loc866 = loc("broadcast.77")
#loc867 = loc("multiply.78")
#loc868 = loc("convert.79")
#loc869 = loc("multiply.85")
#loc870 = loc("reshape.86")
#loc871 = loc("reshape.32")
#loc872 = loc("reshape.34")
#loc873 = loc("transpose.35")
#loc874 = loc("dot.87")
#loc875 = loc("reshape.89")
#loc876 = loc("convert.90")
#loc877 = loc("power.92")
#loc878 = loc("reduce.99")
#loc879 = loc("multiply.108")
#loc880 = loc("reshape.109")
#loc881 = loc("add.113")
#loc882 = loc("rsqrt.114")
#loc883 = loc("reshape.115")
#loc884 = loc("broadcast.116")
#loc885 = loc("multiply.117")
#loc886 = loc("convert.118")
#loc887 = loc("multiply.124")
#loc888 = loc("transpose.125")
#loc889 = loc("reshape.12")
#loc890 = loc("reshape.16")
#loc891 = loc("convert.17")
#loc892 = loc("convert.9")
#loc893 = loc("dot.20")
#loc894 = loc("transpose.21")
#loc895 = loc("concatenate.22")
#loc896 = loc("cosine.134")
#loc897 = loc("convert.137")
#loc898 = loc("reshape.139")
#loc899 = loc("broadcast.140")
#loc900 = loc("multiply.141")
#loc901 = loc("slice.127")
#loc902 = loc("negate.128")
#loc903 = loc("slice.126")
#loc904 = loc("concatenate.129")
#loc905 = loc("sine.23")
#loc906 = loc("convert.26")
#loc907 = loc("reshape.130")
#loc908 = loc("broadcast.131")
#loc909 = loc("multiply.132")
#loc910 = loc("add.144")
#loc912 = loc("reshape.166")
#loc913 = loc("reshape.168")
#loc914 = loc("transpose.169")
#loc915 = loc("dot.171")
#loc916 = loc("reshape.173")
#loc917 = loc("transpose.174")
#loc919 = loc("reshape.537")
#loc920 = loc("reshape.539")
#loc921 = loc("broadcast.540")
#loc922 = loc("reshape.498")
#loc923 = loc("reshape.500")
#loc924 = loc("broadcast.501")
#loc925 = loc("reshape.320")
#loc926 = loc("reshape.322")
#loc927 = loc("broadcast.323")
#loc928 = loc("reshape.282")
#loc929 = loc("reshape.284")
#loc930 = loc("transpose.285")
#loc931 = loc("dot.287")
#loc932 = loc("reshape.289")
#loc933 = loc("convert.290")
#loc934 = loc("power.292")
#loc935 = loc("reduce.299")
#loc936 = loc("multiply.308")
#loc937 = loc("reshape.309")
#loc938 = loc("add.313")
#loc939 = loc("rsqrt.314")
#loc940 = loc("reshape.315")
#loc941 = loc("broadcast.316")
#loc942 = loc("multiply.317")
#loc943 = loc("convert.318")
#loc944 = loc("multiply.324")
#loc945 = loc("transpose.325")
#loc946 = loc("broadcast.334")
#loc947 = loc("multiply.335")
#loc948 = loc("slice.327")
#loc949 = loc("negate.328")
#loc950 = loc("slice.326")
#loc951 = loc("concatenate.329")
#loc952 = loc("broadcast.331")
#loc953 = loc("multiply.332")
#loc954 = loc("add.338")
#loc955 = loc("convert.339")
#loc956 = loc("multiply.341")
#loc957 = loc("broadcast.270")
#loc958 = loc("reshape.271")
#loc959 = loc("convert.272")
#loc960 = loc("transpose.273")
#loc961 = loc("multiply.275")
#loc962 = loc("dot.342")
#loc963 = loc("broadcast.254")
#loc964 = loc("broadcast.256")
#loc965 = loc("compare.257")
#loc966 = loc("reshape.258")
#loc967 = loc("broadcast.263")
#loc968 = loc("select.265")
#loc969 = loc("convert.344")
#loc970 = loc("reshape.347")
#loc971 = loc("broadcast.348")
#loc972 = loc("add.349")
#loc973 = loc("convert.375")
#loc974 = loc("compare.377")
#loc975 = loc("not.379")
#loc977 = loc("or.389")
#loc978 = loc("select.390")
#loc979 = loc("reshape.395")
#loc980 = loc("not.397")
#loc981 = loc("reshape.399")
#loc982 = loc("broadcast.400")
#loc983 = loc("reduce.355")
#loc984 = loc("broadcast.356")
#loc985 = loc("subtract.357")
#loc986 = loc("exponential.358")
#loc987 = loc("reduce.364")
#loc988 = loc("broadcast.365")
#loc989 = loc("divide.366")
#loc990 = loc("select.401")
#loc991 = loc("broadcast.231")
#loc992 = loc("reshape.232")
#loc993 = loc("convert.233")
#loc994 = loc("dot.402")
#loc995 = loc("convert.404")
#loc996 = loc("transpose.405")
#loc997 = loc("reshape.407")
#loc998 = loc("reshape.224")
#loc999 = loc("reshape.226")
#loc1000 = loc("transpose.227")
#loc1002 = loc("reshape.409")
#loc1003 = loc("add.412")
#loc1004 = loc("reshape.443")
#loc1005 = loc("reshape.445")
#loc1006 = loc("broadcast.446")
#loc1007 = loc("convert.413")
#loc1008 = loc("power.415")
#loc1009 = loc("reduce.422")
#loc1010 = loc("multiply.431")
#loc1011 = loc("reshape.432")
#loc1012 = loc("add.436")
#loc1013 = loc("rsqrt.437")
#loc1014 = loc("reshape.438")
#loc1015 = loc("broadcast.439")
#loc1016 = loc("multiply.440")
#loc1017 = loc("convert.441")
#loc1018 = loc("multiply.447")
#loc1019 = loc("reshape.456")
#loc1020 = loc("reshape.452")
#loc1021 = loc("reshape.454")
#loc1022 = loc("transpose.455")
#loc1023 = loc("dot.457")
#loc1024 = loc("reshape.458")
#loc1025 = loc("logistic.459")
#loc1026 = loc("multiply.460")
#loc1027 = loc("reshape.215")
#loc1028 = loc("reshape.217")
#loc1029 = loc("transpose.218")
#loc1030 = loc("dot.449")
#loc1031 = loc("reshape.450")
#loc1032 = loc("multiply.461")
#loc1033 = loc("reshape.462")
#loc1034 = loc("reshape.210")
#loc1035 = loc("reshape.212")
#loc1036 = loc("transpose.213")
#loc1038 = loc("reshape.464")
#loc1039 = loc("add.467")
#loc1040 = loc("convert.468")
#loc1041 = loc("power.470")
#loc1042 = loc("reduce.477")
#loc1043 = loc("multiply.486")
#loc1044 = loc("reshape.487")
#loc1045 = loc("add.491")
#loc1046 = loc("rsqrt.492")
#loc1047 = loc("reshape.493")
#loc1048 = loc("broadcast.494")
#loc1049 = loc("multiply.495")
#loc1050 = loc("convert.496")
#loc1051 = loc("multiply.502")
#loc1052 = loc("reshape.503")
#loc1053 = loc("reshape.201")
#loc1054 = loc("reshape.203")
#loc1055 = loc("transpose.204")
#loc1056 = loc("dot.504")
#loc1057 = loc("reshape.506")
#loc1058 = loc("convert.507")
#loc1059 = loc("power.509")
#loc1060 = loc("reduce.516")
#loc1061 = loc("multiply.525")
#loc1062 = loc("reshape.526")
#loc1063 = loc("add.530")
#loc1064 = loc("rsqrt.531")
#loc1065 = loc("reshape.532")
#loc1066 = loc("broadcast.533")
#loc1067 = loc("multiply.534")
#loc1068 = loc("convert.535")
#loc1069 = loc("multiply.541")
#loc1070 = loc("transpose.542")
#loc1071 = loc("multiply.553")
#loc1072 = loc("slice.544")
#loc1073 = loc("negate.545")
#loc1074 = loc("slice.543")
#loc1075 = loc("concatenate.546")
#loc1076 = loc("multiply.549")
#loc1077 = loc("add.556")
#loc1079 = loc("reshape.578")
#loc1080 = loc("reshape.580")
#loc1081 = loc("transpose.581")
#loc1082 = loc("dot.583")
#loc1083 = loc("reshape.585")
#loc1084 = loc("transpose.586")
#loc1086 = loc("reshape.932")
#loc1087 = loc("reshape.934")
#loc1088 = loc("broadcast.935")
#loc1089 = loc("reshape.893")
#loc1090 = loc("reshape.895")
#loc1091 = loc("broadcast.896")
#loc1092 = loc("reshape.715")
#loc1093 = loc("reshape.717")
#loc1094 = loc("broadcast.718")
#loc1095 = loc("reshape.677")
#loc1096 = loc("reshape.679")
#loc1097 = loc("transpose.680")
#loc1098 = loc("dot.682")
#loc1099 = loc("reshape.684")
#loc1100 = loc("convert.685")
#loc1101 = loc("power.687")
#loc1102 = loc("reduce.694")
#loc1103 = loc("multiply.703")
#loc1104 = loc("reshape.704")
#loc1105 = loc("add.708")
#loc1106 = loc("rsqrt.709")
#loc1107 = loc("reshape.710")
#loc1108 = loc("broadcast.711")
#loc1109 = loc("multiply.712")
#loc1110 = loc("convert.713")
#loc1111 = loc("multiply.719")
#loc1112 = loc("transpose.720")
#loc1113 = loc("multiply.730")
#loc1114 = loc("slice.722")
#loc1115 = loc("negate.723")
#loc1116 = loc("slice.721")
#loc1117 = loc("concatenate.724")
#loc1118 = loc("multiply.727")
#loc1119 = loc("add.733")
#loc1120 = loc("convert.734")
#loc1121 = loc("multiply.736")
#loc1122 = loc("broadcast.665")
#loc1123 = loc("reshape.666")
#loc1124 = loc("convert.667")
#loc1125 = loc("transpose.668")
#loc1126 = loc("multiply.670")
#loc1127 = loc("dot.737")
#loc1128 = loc("add.744")
#loc1129 = loc("convert.770")
#loc1130 = loc("compare.772")
#loc1131 = loc("not.774")
#loc1133 = loc("or.784")
#loc1134 = loc("select.785")
#loc1135 = loc("reshape.790")
#loc1136 = loc("not.792")
#loc1137 = loc("reshape.794")
#loc1138 = loc("broadcast.795")
#loc1139 = loc("reduce.750")
#loc1140 = loc("broadcast.751")
#loc1141 = loc("subtract.752")
#loc1142 = loc("exponential.753")
#loc1143 = loc("reduce.759")
#loc1144 = loc("broadcast.760")
#loc1145 = loc("divide.761")
#loc1146 = loc("select.796")
#loc1147 = loc("broadcast.643")
#loc1148 = loc("reshape.644")
#loc1149 = loc("convert.645")
#loc1150 = loc("dot.797")
#loc1151 = loc("convert.799")
#loc1152 = loc("transpose.800")
#loc1153 = loc("reshape.802")
#loc1154 = loc("reshape.636")
#loc1155 = loc("reshape.638")
#loc1156 = loc("transpose.639")
#loc1158 = loc("reshape.804")
#loc1159 = loc("add.807")
#loc1160 = loc("reshape.838")
#loc1161 = loc("reshape.840")
#loc1162 = loc("broadcast.841")
#loc1163 = loc("convert.808")
#loc1164 = loc("power.810")
#loc1165 = loc("reduce.817")
#loc1166 = loc("multiply.826")
#loc1167 = loc("reshape.827")
#loc1168 = loc("add.831")
#loc1169 = loc("rsqrt.832")
#loc1170 = loc("reshape.833")
#loc1171 = loc("broadcast.834")
#loc1172 = loc("multiply.835")
#loc1173 = loc("convert.836")
#loc1174 = loc("multiply.842")
#loc1175 = loc("reshape.851")
#loc1176 = loc("reshape.847")
#loc1177 = loc("reshape.849")
#loc1178 = loc("transpose.850")
#loc1179 = loc("dot.852")
#loc1180 = loc("reshape.853")
#loc1181 = loc("logistic.854")
#loc1182 = loc("multiply.855")
#loc1183 = loc("reshape.627")
#loc1184 = loc("reshape.629")
#loc1185 = loc("transpose.630")
#loc1186 = loc("dot.844")
#loc1187 = loc("reshape.845")
#loc1188 = loc("multiply.856")
#loc1189 = loc("reshape.857")
#loc1190 = loc("reshape.622")
#loc1191 = loc("reshape.624")
#loc1192 = loc("transpose.625")
#loc1194 = loc("reshape.859")
#loc1195 = loc("add.862")
#loc1196 = loc("convert.863")
#loc1197 = loc("power.865")
#loc1198 = loc("reduce.872")
#loc1199 = loc("multiply.881")
#loc1200 = loc("reshape.882")
#loc1201 = loc("add.886")
#loc1202 = loc("rsqrt.887")
#loc1203 = loc("reshape.888")
#loc1204 = loc("broadcast.889")
#loc1205 = loc("multiply.890")
#loc1206 = loc("convert.891")
#loc1207 = loc("multiply.897")
#loc1208 = loc("reshape.898")
#loc1209 = loc("reshape.613")
#loc1210 = loc("reshape.615")
#loc1211 = loc("transpose.616")
#loc1212 = loc("dot.899")
#loc1213 = loc("reshape.901")
#loc1214 = loc("convert.902")
#loc1215 = loc("power.904")
#loc1216 = loc("reduce.911")
#loc1217 = loc("multiply.920")
#loc1218 = loc("reshape.921")
#loc1219 = loc("add.925")
#loc1220 = loc("rsqrt.926")
#loc1221 = loc("reshape.927")
#loc1222 = loc("broadcast.928")
#loc1223 = loc("multiply.929")
#loc1224 = loc("convert.930")
#loc1225 = loc("multiply.936")
#loc1226 = loc("transpose.937")
#loc1227 = loc("multiply.948")
#loc1228 = loc("slice.939")
#loc1229 = loc("negate.940")
#loc1230 = loc("slice.938")
#loc1231 = loc("concatenate.941")
#loc1232 = loc("multiply.944")
#loc1233 = loc("add.951")
#loc1235 = loc("reshape.973")
#loc1236 = loc("reshape.975")
#loc1237 = loc("transpose.976")
#loc1238 = loc("dot.978")
#loc1239 = loc("reshape.980")
#loc1240 = loc("transpose.981")
#loc1242 = loc("reshape.1327")
#loc1243 = loc("reshape.1329")
#loc1244 = loc("broadcast.1330")
#loc1245 = loc("reshape.1288")
#loc1246 = loc("reshape.1290")
#loc1247 = loc("broadcast.1291")
#loc1248 = loc("reshape.1110")
#loc1249 = loc("reshape.1112")
#loc1250 = loc("broadcast.1113")
#loc1251 = loc("reshape.1072")
#loc1252 = loc("reshape.1074")
#loc1253 = loc("transpose.1075")
#loc1254 = loc("dot.1077")
#loc1255 = loc("reshape.1079")
#loc1256 = loc("convert.1080")
#loc1257 = loc("power.1082")
#loc1258 = loc("reduce.1089")
#loc1259 = loc("multiply.1098")
#loc1260 = loc("reshape.1099")
#loc1261 = loc("add.1103")
#loc1262 = loc("rsqrt.1104")
#loc1263 = loc("reshape.1105")
#loc1264 = loc("broadcast.1106")
#loc1265 = loc("multiply.1107")
#loc1266 = loc("convert.1108")
#loc1267 = loc("multiply.1114")
#loc1268 = loc("transpose.1115")
#loc1269 = loc("multiply.1125")
#loc1270 = loc("slice.1117")
#loc1271 = loc("negate.1118")
#loc1272 = loc("slice.1116")
#loc1273 = loc("concatenate.1119")
#loc1274 = loc("multiply.1122")
#loc1275 = loc("add.1128")
#loc1276 = loc("convert.1129")
#loc1277 = loc("multiply.1131")
#loc1278 = loc("broadcast.1060")
#loc1279 = loc("reshape.1061")
#loc1280 = loc("convert.1062")
#loc1281 = loc("transpose.1063")
#loc1282 = loc("multiply.1065")
#loc1283 = loc("dot.1132")
#loc1284 = loc("add.1139")
#loc1285 = loc("convert.1165")
#loc1286 = loc("compare.1167")
#loc1287 = loc("not.1169")
#loc1289 = loc("or.1179")
#loc1290 = loc("select.1180")
#loc1291 = loc("reshape.1185")
#loc1292 = loc("not.1187")
#loc1293 = loc("reshape.1189")
#loc1294 = loc("broadcast.1190")
#loc1295 = loc("reduce.1145")
#loc1296 = loc("broadcast.1146")
#loc1297 = loc("subtract.1147")
#loc1298 = loc("exponential.1148")
#loc1299 = loc("reduce.1154")
#loc1300 = loc("broadcast.1155")
#loc1301 = loc("divide.1156")
#loc1302 = loc("select.1191")
#loc1303 = loc("broadcast.1038")
#loc1304 = loc("reshape.1039")
#loc1305 = loc("convert.1040")
#loc1306 = loc("dot.1192")
#loc1307 = loc("convert.1194")
#loc1308 = loc("transpose.1195")
#loc1309 = loc("reshape.1197")
#loc1310 = loc("reshape.1031")
#loc1311 = loc("reshape.1033")
#loc1312 = loc("transpose.1034")
#loc1314 = loc("reshape.1199")
#loc1315 = loc("add.1202")
#loc1316 = loc("reshape.1233")
#loc1317 = loc("reshape.1235")
#loc1318 = loc("broadcast.1236")
#loc1319 = loc("convert.1203")
#loc1320 = loc("power.1205")
#loc1321 = loc("reduce.1212")
#loc1322 = loc("multiply.1221")
#loc1323 = loc("reshape.1222")
#loc1324 = loc("add.1226")
#loc1325 = loc("rsqrt.1227")
#loc1326 = loc("reshape.1228")
#loc1327 = loc("broadcast.1229")
#loc1328 = loc("multiply.1230")
#loc1329 = loc("convert.1231")
#loc1330 = loc("multiply.1237")
#loc1331 = loc("reshape.1246")
#loc1332 = loc("reshape.1242")
#loc1333 = loc("reshape.1244")
#loc1334 = loc("transpose.1245")
#loc1335 = loc("dot.1247")
#loc1336 = loc("reshape.1248")
#loc1337 = loc("logistic.1249")
#loc1338 = loc("multiply.1250")
#loc1339 = loc("reshape.1022")
#loc1340 = loc("reshape.1024")
#loc1341 = loc("transpose.1025")
#loc1342 = loc("dot.1239")
#loc1343 = loc("reshape.1240")
#loc1344 = loc("multiply.1251")
#loc1345 = loc("reshape.1252")
#loc1346 = loc("reshape.1017")
#loc1347 = loc("reshape.1019")
#loc1348 = loc("transpose.1020")
#loc1350 = loc("reshape.1254")
#loc1351 = loc("add.1257")
#loc1352 = loc("convert.1258")
#loc1353 = loc("power.1260")
#loc1354 = loc("reduce.1267")
#loc1355 = loc("multiply.1276")
#loc1356 = loc("reshape.1277")
#loc1357 = loc("add.1281")
#loc1358 = loc("rsqrt.1282")
#loc1359 = loc("reshape.1283")
#loc1360 = loc("broadcast.1284")
#loc1361 = loc("multiply.1285")
#loc1362 = loc("convert.1286")
#loc1363 = loc("multiply.1292")
#loc1364 = loc("reshape.1293")
#loc1365 = loc("reshape.1008")
#loc1366 = loc("reshape.1010")
#loc1367 = loc("transpose.1011")
#loc1368 = loc("dot.1294")
#loc1369 = loc("reshape.1296")
#loc1370 = loc("convert.1297")
#loc1371 = loc("power.1299")
#loc1372 = loc("reduce.1306")
#loc1373 = loc("multiply.1315")
#loc1374 = loc("reshape.1316")
#loc1375 = loc("add.1320")
#loc1376 = loc("rsqrt.1321")
#loc1377 = loc("reshape.1322")
#loc1378 = loc("broadcast.1323")
#loc1379 = loc("multiply.1324")
#loc1380 = loc("convert.1325")
#loc1381 = loc("multiply.1331")
#loc1382 = loc("transpose.1332")
#loc1383 = loc("multiply.1343")
#loc1384 = loc("slice.1334")
#loc1385 = loc("negate.1335")
#loc1386 = loc("slice.1333")
#loc1387 = loc("concatenate.1336")
#loc1388 = loc("multiply.1339")
#loc1389 = loc("add.1346")
#loc1391 = loc("reshape.1368")
#loc1392 = loc("reshape.1370")
#loc1393 = loc("transpose.1371")
#loc1394 = loc("dot.1373")
#loc1395 = loc("reshape.1375")
#loc1396 = loc("transpose.1376")
#loc1398 = loc("reshape.1722")
#loc1399 = loc("reshape.1724")
#loc1400 = loc("broadcast.1725")
#loc1401 = loc("reshape.1683")
#loc1402 = loc("reshape.1685")
#loc1403 = loc("broadcast.1686")
#loc1404 = loc("reshape.1505")
#loc1405 = loc("reshape.1507")
#loc1406 = loc("broadcast.1508")
#loc1407 = loc("reshape.1467")
#loc1408 = loc("reshape.1469")
#loc1409 = loc("transpose.1470")
#loc1410 = loc("dot.1472")
#loc1411 = loc("reshape.1474")
#loc1412 = loc("convert.1475")
#loc1413 = loc("power.1477")
#loc1414 = loc("reduce.1484")
#loc1415 = loc("multiply.1493")
#loc1416 = loc("reshape.1494")
#loc1417 = loc("add.1498")
#loc1418 = loc("rsqrt.1499")
#loc1419 = loc("reshape.1500")
#loc1420 = loc("broadcast.1501")
#loc1421 = loc("multiply.1502")
#loc1422 = loc("convert.1503")
#loc1423 = loc("multiply.1509")
#loc1424 = loc("transpose.1510")
#loc1425 = loc("multiply.1520")
#loc1426 = loc("slice.1512")
#loc1427 = loc("negate.1513")
#loc1428 = loc("slice.1511")
#loc1429 = loc("concatenate.1514")
#loc1430 = loc("multiply.1517")
#loc1431 = loc("add.1523")
#loc1432 = loc("convert.1524")
#loc1433 = loc("multiply.1526")
#loc1434 = loc("broadcast.1455")
#loc1435 = loc("reshape.1456")
#loc1436 = loc("convert.1457")
#loc1437 = loc("transpose.1458")
#loc1438 = loc("multiply.1460")
#loc1439 = loc("dot.1527")
#loc1440 = loc("add.1534")
#loc1441 = loc("convert.1560")
#loc1442 = loc("compare.1562")
#loc1443 = loc("not.1564")
#loc1445 = loc("or.1574")
#loc1446 = loc("select.1575")
#loc1447 = loc("reshape.1580")
#loc1448 = loc("not.1582")
#loc1449 = loc("reshape.1584")
#loc1450 = loc("broadcast.1585")
#loc1451 = loc("reduce.1540")
#loc1452 = loc("broadcast.1541")
#loc1453 = loc("subtract.1542")
#loc1454 = loc("exponential.1543")
#loc1455 = loc("reduce.1549")
#loc1456 = loc("broadcast.1550")
#loc1457 = loc("divide.1551")
#loc1458 = loc("select.1586")
#loc1459 = loc("broadcast.1433")
#loc1460 = loc("reshape.1434")
#loc1461 = loc("convert.1435")
#loc1462 = loc("dot.1587")
#loc1463 = loc("convert.1589")
#loc1464 = loc("transpose.1590")
#loc1465 = loc("reshape.1592")
#loc1466 = loc("reshape.1426")
#loc1467 = loc("reshape.1428")
#loc1468 = loc("transpose.1429")
#loc1470 = loc("reshape.1594")
#loc1471 = loc("add.1597")
#loc1472 = loc("reshape.1628")
#loc1473 = loc("reshape.1630")
#loc1474 = loc("broadcast.1631")
#loc1475 = loc("convert.1598")
#loc1476 = loc("power.1600")
#loc1477 = loc("reduce.1607")
#loc1478 = loc("multiply.1616")
#loc1479 = loc("reshape.1617")
#loc1480 = loc("add.1621")
#loc1481 = loc("rsqrt.1622")
#loc1482 = loc("reshape.1623")
#loc1483 = loc("broadcast.1624")
#loc1484 = loc("multiply.1625")
#loc1485 = loc("convert.1626")
#loc1486 = loc("multiply.1632")
#loc1487 = loc("reshape.1641")
#loc1488 = loc("reshape.1637")
#loc1489 = loc("reshape.1639")
#loc1490 = loc("transpose.1640")
#loc1491 = loc("dot.1642")
#loc1492 = loc("reshape.1643")
#loc1493 = loc("logistic.1644")
#loc1494 = loc("multiply.1645")
#loc1495 = loc("reshape.1417")
#loc1496 = loc("reshape.1419")
#loc1497 = loc("transpose.1420")
#loc1498 = loc("dot.1634")
#loc1499 = loc("reshape.1635")
#loc1500 = loc("multiply.1646")
#loc1501 = loc("reshape.1647")
#loc1502 = loc("reshape.1412")
#loc1503 = loc("reshape.1414")
#loc1504 = loc("transpose.1415")
#loc1506 = loc("reshape.1649")
#loc1507 = loc("add.1652")
#loc1508 = loc("convert.1653")
#loc1509 = loc("power.1655")
#loc1510 = loc("reduce.1662")
#loc1511 = loc("multiply.1671")
#loc1512 = loc("reshape.1672")
#loc1513 = loc("add.1676")
#loc1514 = loc("rsqrt.1677")
#loc1515 = loc("reshape.1678")
#loc1516 = loc("broadcast.1679")
#loc1517 = loc("multiply.1680")
#loc1518 = loc("convert.1681")
#loc1519 = loc("multiply.1687")
#loc1520 = loc("reshape.1688")
#loc1521 = loc("reshape.1403")
#loc1522 = loc("reshape.1405")
#loc1523 = loc("transpose.1406")
#loc1524 = loc("dot.1689")
#loc1525 = loc("reshape.1691")
#loc1526 = loc("convert.1692")
#loc1527 = loc("power.1694")
#loc1528 = loc("reduce.1701")
#loc1529 = loc("multiply.1710")
#loc1530 = loc("reshape.1711")
#loc1531 = loc("add.1715")
#loc1532 = loc("rsqrt.1716")
#loc1533 = loc("reshape.1717")
#loc1534 = loc("broadcast.1718")
#loc1535 = loc("multiply.1719")
#loc1536 = loc("convert.1720")
#loc1537 = loc("multiply.1726")
#loc1538 = loc("transpose.1727")
#loc1539 = loc("multiply.1738")
#loc1540 = loc("slice.1729")
#loc1541 = loc("negate.1730")
#loc1542 = loc("slice.1728")
#loc1543 = loc("concatenate.1731")
#loc1544 = loc("multiply.1734")
#loc1545 = loc("add.1741")
#loc1547 = loc("reshape.1763")
#loc1548 = loc("reshape.1765")
#loc1549 = loc("transpose.1766")
#loc1550 = loc("dot.1768")
#loc1551 = loc("reshape.1770")
#loc1552 = loc("transpose.1771")
#loc1554 = loc("reshape.2117")
#loc1555 = loc("reshape.2119")
#loc1556 = loc("broadcast.2120")
#loc1557 = loc("reshape.2078")
#loc1558 = loc("reshape.2080")
#loc1559 = loc("broadcast.2081")
#loc1560 = loc("reshape.1900")
#loc1561 = loc("reshape.1902")
#loc1562 = loc("broadcast.1903")
#loc1563 = loc("reshape.1862")
#loc1564 = loc("reshape.1864")
#loc1565 = loc("transpose.1865")
#loc1566 = loc("dot.1867")
#loc1567 = loc("reshape.1869")
#loc1568 = loc("convert.1870")
#loc1569 = loc("power.1872")
#loc1570 = loc("reduce.1879")
#loc1571 = loc("multiply.1888")
#loc1572 = loc("reshape.1889")
#loc1573 = loc("add.1893")
#loc1574 = loc("rsqrt.1894")
#loc1575 = loc("reshape.1895")
#loc1576 = loc("broadcast.1896")
#loc1577 = loc("multiply.1897")
#loc1578 = loc("convert.1898")
#loc1579 = loc("multiply.1904")
#loc1580 = loc("transpose.1905")
#loc1581 = loc("multiply.1915")
#loc1582 = loc("slice.1907")
#loc1583 = loc("negate.1908")
#loc1584 = loc("slice.1906")
#loc1585 = loc("concatenate.1909")
#loc1586 = loc("multiply.1912")
#loc1587 = loc("add.1918")
#loc1588 = loc("convert.1919")
#loc1589 = loc("multiply.1921")
#loc1590 = loc("broadcast.1850")
#loc1591 = loc("reshape.1851")
#loc1592 = loc("convert.1852")
#loc1593 = loc("transpose.1853")
#loc1594 = loc("multiply.1855")
#loc1595 = loc("dot.1922")
#loc1596 = loc("add.1929")
#loc1597 = loc("convert.1955")
#loc1598 = loc("compare.1957")
#loc1599 = loc("not.1959")
#loc1601 = loc("or.1969")
#loc1602 = loc("select.1970")
#loc1603 = loc("reshape.1975")
#loc1604 = loc("not.1977")
#loc1605 = loc("reshape.1979")
#loc1606 = loc("broadcast.1980")
#loc1607 = loc("reduce.1935")
#loc1608 = loc("broadcast.1936")
#loc1609 = loc("subtract.1937")
#loc1610 = loc("exponential.1938")
#loc1611 = loc("reduce.1944")
#loc1612 = loc("broadcast.1945")
#loc1613 = loc("divide.1946")
#loc1614 = loc("select.1981")
#loc1615 = loc("broadcast.1828")
#loc1616 = loc("reshape.1829")
#loc1617 = loc("convert.1830")
#loc1618 = loc("dot.1982")
#loc1619 = loc("convert.1984")
#loc1620 = loc("transpose.1985")
#loc1621 = loc("reshape.1987")
#loc1622 = loc("reshape.1821")
#loc1623 = loc("reshape.1823")
#loc1624 = loc("transpose.1824")
#loc1626 = loc("reshape.1989")
#loc1627 = loc("add.1992")
#loc1628 = loc("reshape.2023")
#loc1629 = loc("reshape.2025")
#loc1630 = loc("broadcast.2026")
#loc1631 = loc("convert.1993")
#loc1632 = loc("power.1995")
#loc1633 = loc("reduce.2002")
#loc1634 = loc("multiply.2011")
#loc1635 = loc("reshape.2012")
#loc1636 = loc("add.2016")
#loc1637 = loc("rsqrt.2017")
#loc1638 = loc("reshape.2018")
#loc1639 = loc("broadcast.2019")
#loc1640 = loc("multiply.2020")
#loc1641 = loc("convert.2021")
#loc1642 = loc("multiply.2027")
#loc1643 = loc("reshape.2036")
#loc1644 = loc("reshape.2032")
#loc1645 = loc("reshape.2034")
#loc1646 = loc("transpose.2035")
#loc1647 = loc("dot.2037")
#loc1648 = loc("reshape.2038")
#loc1649 = loc("logistic.2039")
#loc1650 = loc("multiply.2040")
#loc1651 = loc("reshape.1812")
#loc1652 = loc("reshape.1814")
#loc1653 = loc("transpose.1815")
#loc1654 = loc("dot.2029")
#loc1655 = loc("reshape.2030")
#loc1656 = loc("multiply.2041")
#loc1657 = loc("reshape.2042")
#loc1658 = loc("reshape.1807")
#loc1659 = loc("reshape.1809")
#loc1660 = loc("transpose.1810")
#loc1662 = loc("reshape.2044")
#loc1663 = loc("add.2047")
#loc1664 = loc("convert.2048")
#loc1665 = loc("power.2050")
#loc1666 = loc("reduce.2057")
#loc1667 = loc("multiply.2066")
#loc1668 = loc("reshape.2067")
#loc1669 = loc("add.2071")
#loc1670 = loc("rsqrt.2072")
#loc1671 = loc("reshape.2073")
#loc1672 = loc("broadcast.2074")
#loc1673 = loc("multiply.2075")
#loc1674 = loc("convert.2076")
#loc1675 = loc("multiply.2082")
#loc1676 = loc("reshape.2083")
#loc1677 = loc("reshape.1798")
#loc1678 = loc("reshape.1800")
#loc1679 = loc("transpose.1801")
#loc1680 = loc("dot.2084")
#loc1681 = loc("reshape.2086")
#loc1682 = loc("convert.2087")
#loc1683 = loc("power.2089")
#loc1684 = loc("reduce.2096")
#loc1685 = loc("multiply.2105")
#loc1686 = loc("reshape.2106")
#loc1687 = loc("add.2110")
#loc1688 = loc("rsqrt.2111")
#loc1689 = loc("reshape.2112")
#loc1690 = loc("broadcast.2113")
#loc1691 = loc("multiply.2114")
#loc1692 = loc("convert.2115")
#loc1693 = loc("multiply.2121")
#loc1694 = loc("transpose.2122")
#loc1695 = loc("multiply.2133")
#loc1696 = loc("slice.2124")
#loc1697 = loc("negate.2125")
#loc1698 = loc("slice.2123")
#loc1699 = loc("concatenate.2126")
#loc1700 = loc("multiply.2129")
#loc1701 = loc("add.2136")
#loc1703 = loc("reshape.2158")
#loc1704 = loc("reshape.2160")
#loc1705 = loc("transpose.2161")
#loc1706 = loc("dot.2163")
#loc1707 = loc("reshape.2165")
#loc1708 = loc("transpose.2166")
#loc1710 = loc("reshape.2512")
#loc1711 = loc("reshape.2514")
#loc1712 = loc("broadcast.2515")
#loc1713 = loc("reshape.2473")
#loc1714 = loc("reshape.2475")
#loc1715 = loc("broadcast.2476")
#loc1716 = loc("reshape.2295")
#loc1717 = loc("reshape.2297")
#loc1718 = loc("broadcast.2298")
#loc1719 = loc("reshape.2257")
#loc1720 = loc("reshape.2259")
#loc1721 = loc("transpose.2260")
#loc1722 = loc("dot.2262")
#loc1723 = loc("reshape.2264")
#loc1724 = loc("convert.2265")
#loc1725 = loc("power.2267")
#loc1726 = loc("reduce.2274")
#loc1727 = loc("multiply.2283")
#loc1728 = loc("reshape.2284")
#loc1729 = loc("add.2288")
#loc1730 = loc("rsqrt.2289")
#loc1731 = loc("reshape.2290")
#loc1732 = loc("broadcast.2291")
#loc1733 = loc("multiply.2292")
#loc1734 = loc("convert.2293")
#loc1735 = loc("multiply.2299")
#loc1736 = loc("transpose.2300")
#loc1737 = loc("multiply.2310")
#loc1738 = loc("slice.2302")
#loc1739 = loc("negate.2303")
#loc1740 = loc("slice.2301")
#loc1741 = loc("concatenate.2304")
#loc1742 = loc("multiply.2307")
#loc1743 = loc("add.2313")
#loc1744 = loc("convert.2314")
#loc1745 = loc("multiply.2316")
#loc1746 = loc("broadcast.2245")
#loc1747 = loc("reshape.2246")
#loc1748 = loc("convert.2247")
#loc1749 = loc("transpose.2248")
#loc1750 = loc("multiply.2250")
#loc1751 = loc("dot.2317")
#loc1752 = loc("add.2324")
#loc1753 = loc("convert.2350")
#loc1754 = loc("compare.2352")
#loc1755 = loc("not.2354")
#loc1757 = loc("or.2364")
#loc1758 = loc("select.2365")
#loc1759 = loc("reshape.2370")
#loc1760 = loc("not.2372")
#loc1761 = loc("reshape.2374")
#loc1762 = loc("broadcast.2375")
#loc1763 = loc("reduce.2330")
#loc1764 = loc("broadcast.2331")
#loc1765 = loc("subtract.2332")
#loc1766 = loc("exponential.2333")
#loc1767 = loc("reduce.2339")
#loc1768 = loc("broadcast.2340")
#loc1769 = loc("divide.2341")
#loc1770 = loc("select.2376")
#loc1771 = loc("broadcast.2223")
#loc1772 = loc("reshape.2224")
#loc1773 = loc("convert.2225")
#loc1774 = loc("dot.2377")
#loc1775 = loc("convert.2379")
#loc1776 = loc("transpose.2380")
#loc1777 = loc("reshape.2382")
#loc1778 = loc("reshape.2216")
#loc1779 = loc("reshape.2218")
#loc1780 = loc("transpose.2219")
#loc1782 = loc("reshape.2384")
#loc1783 = loc("add.2387")
#loc1784 = loc("reshape.2418")
#loc1785 = loc("reshape.2420")
#loc1786 = loc("broadcast.2421")
#loc1787 = loc("convert.2388")
#loc1788 = loc("power.2390")
#loc1789 = loc("reduce.2397")
#loc1790 = loc("multiply.2406")
#loc1791 = loc("reshape.2407")
#loc1792 = loc("add.2411")
#loc1793 = loc("rsqrt.2412")
#loc1794 = loc("reshape.2413")
#loc1795 = loc("broadcast.2414")
#loc1796 = loc("multiply.2415")
#loc1797 = loc("convert.2416")
#loc1798 = loc("multiply.2422")
#loc1799 = loc("reshape.2431")
#loc1800 = loc("reshape.2427")
#loc1801 = loc("reshape.2429")
#loc1802 = loc("transpose.2430")
#loc1803 = loc("dot.2432")
#loc1804 = loc("reshape.2433")
#loc1805 = loc("logistic.2434")
#loc1806 = loc("multiply.2435")
#loc1807 = loc("reshape.2207")
#loc1808 = loc("reshape.2209")
#loc1809 = loc("transpose.2210")
#loc1810 = loc("dot.2424")
#loc1811 = loc("reshape.2425")
#loc1812 = loc("multiply.2436")
#loc1813 = loc("reshape.2437")
#loc1814 = loc("reshape.2202")
#loc1815 = loc("reshape.2204")
#loc1816 = loc("transpose.2205")
#loc1818 = loc("reshape.2439")
#loc1819 = loc("add.2442")
#loc1820 = loc("convert.2443")
#loc1821 = loc("power.2445")
#loc1822 = loc("reduce.2452")
#loc1823 = loc("multiply.2461")
#loc1824 = loc("reshape.2462")
#loc1825 = loc("add.2466")
#loc1826 = loc("rsqrt.2467")
#loc1827 = loc("reshape.2468")
#loc1828 = loc("broadcast.2469")
#loc1829 = loc("multiply.2470")
#loc1830 = loc("convert.2471")
#loc1831 = loc("multiply.2477")
#loc1832 = loc("reshape.2478")
#loc1833 = loc("reshape.2193")
#loc1834 = loc("reshape.2195")
#loc1835 = loc("transpose.2196")
#loc1836 = loc("dot.2479")
#loc1837 = loc("reshape.2481")
#loc1838 = loc("convert.2482")
#loc1839 = loc("power.2484")
#loc1840 = loc("reduce.2491")
#loc1841 = loc("multiply.2500")
#loc1842 = loc("reshape.2501")
#loc1843 = loc("add.2505")
#loc1844 = loc("rsqrt.2506")
#loc1845 = loc("reshape.2507")
#loc1846 = loc("broadcast.2508")
#loc1847 = loc("multiply.2509")
#loc1848 = loc("convert.2510")
#loc1849 = loc("multiply.2516")
#loc1850 = loc("transpose.2517")
#loc1851 = loc("multiply.2528")
#loc1852 = loc("slice.2519")
#loc1853 = loc("negate.2520")
#loc1854 = loc("slice.2518")
#loc1855 = loc("concatenate.2521")
#loc1856 = loc("multiply.2524")
#loc1857 = loc("add.2531")
#loc1859 = loc("reshape.2553")
#loc1860 = loc("reshape.2555")
#loc1861 = loc("transpose.2556")
#loc1862 = loc("dot.2558")
#loc1863 = loc("reshape.2560")
#loc1864 = loc("transpose.2561")
#loc1866 = loc("reshape.2907")
#loc1867 = loc("reshape.2909")
#loc1868 = loc("broadcast.2910")
#loc1869 = loc("reshape.2868")
#loc1870 = loc("reshape.2870")
#loc1871 = loc("broadcast.2871")
#loc1872 = loc("reshape.2690")
#loc1873 = loc("reshape.2692")
#loc1874 = loc("broadcast.2693")
#loc1875 = loc("reshape.2652")
#loc1876 = loc("reshape.2654")
#loc1877 = loc("transpose.2655")
#loc1878 = loc("dot.2657")
#loc1879 = loc("reshape.2659")
#loc1880 = loc("convert.2660")
#loc1881 = loc("power.2662")
#loc1882 = loc("reduce.2669")
#loc1883 = loc("multiply.2678")
#loc1884 = loc("reshape.2679")
#loc1885 = loc("add.2683")
#loc1886 = loc("rsqrt.2684")
#loc1887 = loc("reshape.2685")
#loc1888 = loc("broadcast.2686")
#loc1889 = loc("multiply.2687")
#loc1890 = loc("convert.2688")
#loc1891 = loc("multiply.2694")
#loc1892 = loc("transpose.2695")
#loc1893 = loc("multiply.2705")
#loc1894 = loc("slice.2697")
#loc1895 = loc("negate.2698")
#loc1896 = loc("slice.2696")
#loc1897 = loc("concatenate.2699")
#loc1898 = loc("multiply.2702")
#loc1899 = loc("add.2708")
#loc1900 = loc("convert.2709")
#loc1901 = loc("multiply.2711")
#loc1902 = loc("broadcast.2640")
#loc1903 = loc("reshape.2641")
#loc1904 = loc("convert.2642")
#loc1905 = loc("transpose.2643")
#loc1906 = loc("multiply.2645")
#loc1907 = loc("dot.2712")
#loc1908 = loc("add.2719")
#loc1909 = loc("convert.2745")
#loc1910 = loc("compare.2747")
#loc1911 = loc("not.2749")
#loc1913 = loc("or.2759")
#loc1914 = loc("select.2760")
#loc1915 = loc("reshape.2765")
#loc1916 = loc("not.2767")
#loc1917 = loc("reshape.2769")
#loc1918 = loc("broadcast.2770")
#loc1919 = loc("reduce.2725")
#loc1920 = loc("broadcast.2726")
#loc1921 = loc("subtract.2727")
#loc1922 = loc("exponential.2728")
#loc1923 = loc("reduce.2734")
#loc1924 = loc("broadcast.2735")
#loc1925 = loc("divide.2736")
#loc1926 = loc("select.2771")
#loc1927 = loc("broadcast.2618")
#loc1928 = loc("reshape.2619")
#loc1929 = loc("convert.2620")
#loc1930 = loc("dot.2772")
#loc1931 = loc("convert.2774")
#loc1932 = loc("transpose.2775")
#loc1933 = loc("reshape.2777")
#loc1934 = loc("reshape.2611")
#loc1935 = loc("reshape.2613")
#loc1936 = loc("transpose.2614")
#loc1938 = loc("reshape.2779")
#loc1939 = loc("add.2782")
#loc1940 = loc("reshape.2813")
#loc1941 = loc("reshape.2815")
#loc1942 = loc("broadcast.2816")
#loc1943 = loc("convert.2783")
#loc1944 = loc("power.2785")
#loc1945 = loc("reduce.2792")
#loc1946 = loc("multiply.2801")
#loc1947 = loc("reshape.2802")
#loc1948 = loc("add.2806")
#loc1949 = loc("rsqrt.2807")
#loc1950 = loc("reshape.2808")
#loc1951 = loc("broadcast.2809")
#loc1952 = loc("multiply.2810")
#loc1953 = loc("convert.2811")
#loc1954 = loc("multiply.2817")
#loc1955 = loc("reshape.2826")
#loc1956 = loc("reshape.2822")
#loc1957 = loc("reshape.2824")
#loc1958 = loc("transpose.2825")
#loc1959 = loc("dot.2827")
#loc1960 = loc("reshape.2828")
#loc1961 = loc("logistic.2829")
#loc1962 = loc("multiply.2830")
#loc1963 = loc("reshape.2602")
#loc1964 = loc("reshape.2604")
#loc1965 = loc("transpose.2605")
#loc1966 = loc("dot.2819")
#loc1967 = loc("reshape.2820")
#loc1968 = loc("multiply.2831")
#loc1969 = loc("reshape.2832")
#loc1970 = loc("reshape.2597")
#loc1971 = loc("reshape.2599")
#loc1972 = loc("transpose.2600")
#loc1974 = loc("reshape.2834")
#loc1975 = loc("add.2837")
#loc1976 = loc("convert.2838")
#loc1977 = loc("power.2840")
#loc1978 = loc("reduce.2847")
#loc1979 = loc("multiply.2856")
#loc1980 = loc("reshape.2857")
#loc1981 = loc("add.2861")
#loc1982 = loc("rsqrt.2862")
#loc1983 = loc("reshape.2863")
#loc1984 = loc("broadcast.2864")
#loc1985 = loc("multiply.2865")
#loc1986 = loc("convert.2866")
#loc1987 = loc("multiply.2872")
#loc1988 = loc("reshape.2873")
#loc1989 = loc("reshape.2588")
#loc1990 = loc("reshape.2590")
#loc1991 = loc("transpose.2591")
#loc1992 = loc("dot.2874")
#loc1993 = loc("reshape.2876")
#loc1994 = loc("convert.2877")
#loc1995 = loc("power.2879")
#loc1996 = loc("reduce.2886")
#loc1997 = loc("multiply.2895")
#loc1998 = loc("reshape.2896")
#loc1999 = loc("add.2900")
#loc2000 = loc("rsqrt.2901")
#loc2001 = loc("reshape.2902")
#loc2002 = loc("broadcast.2903")
#loc2003 = loc("multiply.2904")
#loc2004 = loc("convert.2905")
#loc2005 = loc("multiply.2911")
#loc2006 = loc("transpose.2912")
#loc2007 = loc("multiply.2923")
#loc2008 = loc("slice.2914")
#loc2009 = loc("negate.2915")
#loc2010 = loc("slice.2913")
#loc2011 = loc("concatenate.2916")
#loc2012 = loc("multiply.2919")
#loc2013 = loc("add.2926")
#loc2015 = loc("reshape.2948")
#loc2016 = loc("reshape.2950")
#loc2017 = loc("transpose.2951")
#loc2018 = loc("dot.2953")
#loc2019 = loc("reshape.2955")
#loc2020 = loc("transpose.2956")
#loc2022 = loc("reshape.3302")
#loc2023 = loc("reshape.3304")
#loc2024 = loc("broadcast.3305")
#loc2025 = loc("reshape.3263")
#loc2026 = loc("reshape.3265")
#loc2027 = loc("broadcast.3266")
#loc2028 = loc("reshape.3085")
#loc2029 = loc("reshape.3087")
#loc2030 = loc("broadcast.3088")
#loc2031 = loc("reshape.3047")
#loc2032 = loc("reshape.3049")
#loc2033 = loc("transpose.3050")
#loc2034 = loc("dot.3052")
#loc2035 = loc("reshape.3054")
#loc2036 = loc("convert.3055")
#loc2037 = loc("power.3057")
#loc2038 = loc("reduce.3064")
#loc2039 = loc("multiply.3073")
#loc2040 = loc("reshape.3074")
#loc2041 = loc("add.3078")
#loc2042 = loc("rsqrt.3079")
#loc2043 = loc("reshape.3080")
#loc2044 = loc("broadcast.3081")
#loc2045 = loc("multiply.3082")
#loc2046 = loc("convert.3083")
#loc2047 = loc("multiply.3089")
#loc2048 = loc("transpose.3090")
#loc2049 = loc("multiply.3100")
#loc2050 = loc("slice.3092")
#loc2051 = loc("negate.3093")
#loc2052 = loc("slice.3091")
#loc2053 = loc("concatenate.3094")
#loc2054 = loc("multiply.3097")
#loc2055 = loc("add.3103")
#loc2056 = loc("convert.3104")
#loc2057 = loc("multiply.3106")
#loc2058 = loc("broadcast.3035")
#loc2059 = loc("reshape.3036")
#loc2060 = loc("convert.3037")
#loc2061 = loc("transpose.3038")
#loc2062 = loc("multiply.3040")
#loc2063 = loc("dot.3107")
#loc2064 = loc("add.3114")
#loc2065 = loc("convert.3140")
#loc2066 = loc("compare.3142")
#loc2067 = loc("not.3144")
#loc2069 = loc("or.3154")
#loc2070 = loc("select.3155")
#loc2071 = loc("reshape.3160")
#loc2072 = loc("not.3162")
#loc2073 = loc("reshape.3164")
#loc2074 = loc("broadcast.3165")
#loc2075 = loc("reduce.3120")
#loc2076 = loc("broadcast.3121")
#loc2077 = loc("subtract.3122")
#loc2078 = loc("exponential.3123")
#loc2079 = loc("reduce.3129")
#loc2080 = loc("broadcast.3130")
#loc2081 = loc("divide.3131")
#loc2082 = loc("select.3166")
#loc2083 = loc("broadcast.3013")
#loc2084 = loc("reshape.3014")
#loc2085 = loc("convert.3015")
#loc2086 = loc("dot.3167")
#loc2087 = loc("convert.3169")
#loc2088 = loc("transpose.3170")
#loc2089 = loc("reshape.3172")
#loc2090 = loc("reshape.3006")
#loc2091 = loc("reshape.3008")
#loc2092 = loc("transpose.3009")
#loc2094 = loc("reshape.3174")
#loc2095 = loc("add.3177")
#loc2096 = loc("reshape.3208")
#loc2097 = loc("reshape.3210")
#loc2098 = loc("broadcast.3211")
#loc2099 = loc("convert.3178")
#loc2100 = loc("power.3180")
#loc2101 = loc("reduce.3187")
#loc2102 = loc("multiply.3196")
#loc2103 = loc("reshape.3197")
#loc2104 = loc("add.3201")
#loc2105 = loc("rsqrt.3202")
#loc2106 = loc("reshape.3203")
#loc2107 = loc("broadcast.3204")
#loc2108 = loc("multiply.3205")
#loc2109 = loc("convert.3206")
#loc2110 = loc("multiply.3212")
#loc2111 = loc("reshape.3221")
#loc2112 = loc("reshape.3217")
#loc2113 = loc("reshape.3219")
#loc2114 = loc("transpose.3220")
#loc2115 = loc("dot.3222")
#loc2116 = loc("reshape.3223")
#loc2117 = loc("logistic.3224")
#loc2118 = loc("multiply.3225")
#loc2119 = loc("reshape.2997")
#loc2120 = loc("reshape.2999")
#loc2121 = loc("transpose.3000")
#loc2122 = loc("dot.3214")
#loc2123 = loc("reshape.3215")
#loc2124 = loc("multiply.3226")
#loc2125 = loc("reshape.3227")
#loc2126 = loc("reshape.2992")
#loc2127 = loc("reshape.2994")
#loc2128 = loc("transpose.2995")
#loc2130 = loc("reshape.3229")
#loc2131 = loc("add.3232")
#loc2132 = loc("convert.3233")
#loc2133 = loc("power.3235")
#loc2134 = loc("reduce.3242")
#loc2135 = loc("multiply.3251")
#loc2136 = loc("reshape.3252")
#loc2137 = loc("add.3256")
#loc2138 = loc("rsqrt.3257")
#loc2139 = loc("reshape.3258")
#loc2140 = loc("broadcast.3259")
#loc2141 = loc("multiply.3260")
#loc2142 = loc("convert.3261")
#loc2143 = loc("multiply.3267")
#loc2144 = loc("reshape.3268")
#loc2145 = loc("reshape.2983")
#loc2146 = loc("reshape.2985")
#loc2147 = loc("transpose.2986")
#loc2148 = loc("dot.3269")
#loc2149 = loc("reshape.3271")
#loc2150 = loc("convert.3272")
#loc2151 = loc("power.3274")
#loc2152 = loc("reduce.3281")
#loc2153 = loc("multiply.3290")
#loc2154 = loc("reshape.3291")
#loc2155 = loc("add.3295")
#loc2156 = loc("rsqrt.3296")
#loc2157 = loc("reshape.3297")
#loc2158 = loc("broadcast.3298")
#loc2159 = loc("multiply.3299")
#loc2160 = loc("convert.3300")
#loc2161 = loc("multiply.3306")
#loc2162 = loc("transpose.3307")
#loc2163 = loc("multiply.3318")
#loc2164 = loc("slice.3309")
#loc2165 = loc("negate.3310")
#loc2166 = loc("slice.3308")
#loc2167 = loc("concatenate.3311")
#loc2168 = loc("multiply.3314")
#loc2169 = loc("add.3321")
#loc2171 = loc("reshape.3343")
#loc2172 = loc("reshape.3345")
#loc2173 = loc("transpose.3346")
#loc2174 = loc("dot.3348")
#loc2175 = loc("reshape.3350")
#loc2176 = loc("transpose.3351")
#loc2178 = loc("reshape.3697")
#loc2179 = loc("reshape.3699")
#loc2180 = loc("broadcast.3700")
#loc2181 = loc("reshape.3658")
#loc2182 = loc("reshape.3660")
#loc2183 = loc("broadcast.3661")
#loc2184 = loc("reshape.3480")
#loc2185 = loc("reshape.3482")
#loc2186 = loc("broadcast.3483")
#loc2187 = loc("reshape.3442")
#loc2188 = loc("reshape.3444")
#loc2189 = loc("transpose.3445")
#loc2190 = loc("dot.3447")
#loc2191 = loc("reshape.3449")
#loc2192 = loc("convert.3450")
#loc2193 = loc("power.3452")
#loc2194 = loc("reduce.3459")
#loc2195 = loc("multiply.3468")
#loc2196 = loc("reshape.3469")
#loc2197 = loc("add.3473")
#loc2198 = loc("rsqrt.3474")
#loc2199 = loc("reshape.3475")
#loc2200 = loc("broadcast.3476")
#loc2201 = loc("multiply.3477")
#loc2202 = loc("convert.3478")
#loc2203 = loc("multiply.3484")
#loc2204 = loc("transpose.3485")
#loc2205 = loc("multiply.3495")
#loc2206 = loc("slice.3487")
#loc2207 = loc("negate.3488")
#loc2208 = loc("slice.3486")
#loc2209 = loc("concatenate.3489")
#loc2210 = loc("multiply.3492")
#loc2211 = loc("add.3498")
#loc2212 = loc("convert.3499")
#loc2213 = loc("multiply.3501")
#loc2214 = loc("broadcast.3430")
#loc2215 = loc("reshape.3431")
#loc2216 = loc("convert.3432")
#loc2217 = loc("transpose.3433")
#loc2218 = loc("multiply.3435")
#loc2219 = loc("dot.3502")
#loc2220 = loc("add.3509")
#loc2221 = loc("convert.3535")
#loc2222 = loc("compare.3537")
#loc2223 = loc("not.3539")
#loc2225 = loc("or.3549")
#loc2226 = loc("select.3550")
#loc2227 = loc("reshape.3555")
#loc2228 = loc("not.3557")
#loc2229 = loc("reshape.3559")
#loc2230 = loc("broadcast.3560")
#loc2231 = loc("reduce.3515")
#loc2232 = loc("broadcast.3516")
#loc2233 = loc("subtract.3517")
#loc2234 = loc("exponential.3518")
#loc2235 = loc("reduce.3524")
#loc2236 = loc("broadcast.3525")
#loc2237 = loc("divide.3526")
#loc2238 = loc("select.3561")
#loc2239 = loc("broadcast.3408")
#loc2240 = loc("reshape.3409")
#loc2241 = loc("convert.3410")
#loc2242 = loc("dot.3562")
#loc2243 = loc("convert.3564")
#loc2244 = loc("transpose.3565")
#loc2245 = loc("reshape.3567")
#loc2246 = loc("reshape.3401")
#loc2247 = loc("reshape.3403")
#loc2248 = loc("transpose.3404")
#loc2250 = loc("reshape.3569")
#loc2251 = loc("add.3572")
#loc2252 = loc("reshape.3603")
#loc2253 = loc("reshape.3605")
#loc2254 = loc("broadcast.3606")
#loc2255 = loc("convert.3573")
#loc2256 = loc("power.3575")
#loc2257 = loc("reduce.3582")
#loc2258 = loc("multiply.3591")
#loc2259 = loc("reshape.3592")
#loc2260 = loc("add.3596")
#loc2261 = loc("rsqrt.3597")
#loc2262 = loc("reshape.3598")
#loc2263 = loc("broadcast.3599")
#loc2264 = loc("multiply.3600")
#loc2265 = loc("convert.3601")
#loc2266 = loc("multiply.3607")
#loc2267 = loc("reshape.3616")
#loc2268 = loc("reshape.3612")
#loc2269 = loc("reshape.3614")
#loc2270 = loc("transpose.3615")
#loc2271 = loc("dot.3617")
#loc2272 = loc("reshape.3618")
#loc2273 = loc("logistic.3619")
#loc2274 = loc("multiply.3620")
#loc2275 = loc("reshape.3392")
#loc2276 = loc("reshape.3394")
#loc2277 = loc("transpose.3395")
#loc2278 = loc("dot.3609")
#loc2279 = loc("reshape.3610")
#loc2280 = loc("multiply.3621")
#loc2281 = loc("reshape.3622")
#loc2282 = loc("reshape.3387")
#loc2283 = loc("reshape.3389")
#loc2284 = loc("transpose.3390")
#loc2286 = loc("reshape.3624")
#loc2287 = loc("add.3627")
#loc2288 = loc("convert.3628")
#loc2289 = loc("power.3630")
#loc2290 = loc("reduce.3637")
#loc2291 = loc("multiply.3646")
#loc2292 = loc("reshape.3647")
#loc2293 = loc("add.3651")
#loc2294 = loc("rsqrt.3652")
#loc2295 = loc("reshape.3653")
#loc2296 = loc("broadcast.3654")
#loc2297 = loc("multiply.3655")
#loc2298 = loc("convert.3656")
#loc2299 = loc("multiply.3662")
#loc2300 = loc("reshape.3663")
#loc2301 = loc("reshape.3378")
#loc2302 = loc("reshape.3380")
#loc2303 = loc("transpose.3381")
#loc2304 = loc("dot.3664")
#loc2305 = loc("reshape.3666")
#loc2306 = loc("convert.3667")
#loc2307 = loc("power.3669")
#loc2308 = loc("reduce.3676")
#loc2309 = loc("multiply.3685")
#loc2310 = loc("reshape.3686")
#loc2311 = loc("add.3690")
#loc2312 = loc("rsqrt.3691")
#loc2313 = loc("reshape.3692")
#loc2314 = loc("broadcast.3693")
#loc2315 = loc("multiply.3694")
#loc2316 = loc("convert.3695")
#loc2317 = loc("multiply.3701")
#loc2318 = loc("transpose.3702")
#loc2319 = loc("multiply.3713")
#loc2320 = loc("slice.3704")
#loc2321 = loc("negate.3705")
#loc2322 = loc("slice.3703")
#loc2323 = loc("concatenate.3706")
#loc2324 = loc("multiply.3709")
#loc2325 = loc("add.3716")
#loc2327 = loc("reshape.3738")
#loc2328 = loc("reshape.3740")
#loc2329 = loc("transpose.3741")
#loc2330 = loc("dot.3743")
#loc2331 = loc("reshape.3745")
#loc2332 = loc("transpose.3746")
#loc2334 = loc("reshape.4092")
#loc2335 = loc("reshape.4094")
#loc2336 = loc("broadcast.4095")
#loc2337 = loc("reshape.4053")
#loc2338 = loc("reshape.4055")
#loc2339 = loc("broadcast.4056")
#loc2340 = loc("reshape.3875")
#loc2341 = loc("reshape.3877")
#loc2342 = loc("broadcast.3878")
#loc2343 = loc("reshape.3837")
#loc2344 = loc("reshape.3839")
#loc2345 = loc("transpose.3840")
#loc2346 = loc("dot.3842")
#loc2347 = loc("reshape.3844")
#loc2348 = loc("convert.3845")
#loc2349 = loc("power.3847")
#loc2350 = loc("reduce.3854")
#loc2351 = loc("multiply.3863")
#loc2352 = loc("reshape.3864")
#loc2353 = loc("add.3868")
#loc2354 = loc("rsqrt.3869")
#loc2355 = loc("reshape.3870")
#loc2356 = loc("broadcast.3871")
#loc2357 = loc("multiply.3872")
#loc2358 = loc("convert.3873")
#loc2359 = loc("multiply.3879")
#loc2360 = loc("transpose.3880")
#loc2361 = loc("multiply.3890")
#loc2362 = loc("slice.3882")
#loc2363 = loc("negate.3883")
#loc2364 = loc("slice.3881")
#loc2365 = loc("concatenate.3884")
#loc2366 = loc("multiply.3887")
#loc2367 = loc("add.3893")
#loc2368 = loc("convert.3894")
#loc2369 = loc("multiply.3896")
#loc2370 = loc("broadcast.3825")
#loc2371 = loc("reshape.3826")
#loc2372 = loc("convert.3827")
#loc2373 = loc("transpose.3828")
#loc2374 = loc("multiply.3830")
#loc2375 = loc("dot.3897")
#loc2376 = loc("add.3904")
#loc2377 = loc("convert.3930")
#loc2378 = loc("compare.3932")
#loc2379 = loc("not.3934")
#loc2381 = loc("or.3944")
#loc2382 = loc("select.3945")
#loc2383 = loc("reshape.3950")
#loc2384 = loc("not.3952")
#loc2385 = loc("reshape.3954")
#loc2386 = loc("broadcast.3955")
#loc2387 = loc("reduce.3910")
#loc2388 = loc("broadcast.3911")
#loc2389 = loc("subtract.3912")
#loc2390 = loc("exponential.3913")
#loc2391 = loc("reduce.3919")
#loc2392 = loc("broadcast.3920")
#loc2393 = loc("divide.3921")
#loc2394 = loc("select.3956")
#loc2395 = loc("broadcast.3803")
#loc2396 = loc("reshape.3804")
#loc2397 = loc("convert.3805")
#loc2398 = loc("dot.3957")
#loc2399 = loc("convert.3959")
#loc2400 = loc("transpose.3960")
#loc2401 = loc("reshape.3962")
#loc2402 = loc("reshape.3796")
#loc2403 = loc("reshape.3798")
#loc2404 = loc("transpose.3799")
#loc2406 = loc("reshape.3964")
#loc2407 = loc("add.3967")
#loc2408 = loc("reshape.3998")
#loc2409 = loc("reshape.4000")
#loc2410 = loc("broadcast.4001")
#loc2411 = loc("convert.3968")
#loc2412 = loc("power.3970")
#loc2413 = loc("reduce.3977")
#loc2414 = loc("multiply.3986")
#loc2415 = loc("reshape.3987")
#loc2416 = loc("add.3991")
#loc2417 = loc("rsqrt.3992")
#loc2418 = loc("reshape.3993")
#loc2419 = loc("broadcast.3994")
#loc2420 = loc("multiply.3995")
#loc2421 = loc("convert.3996")
#loc2422 = loc("multiply.4002")
#loc2423 = loc("reshape.4011")
#loc2424 = loc("reshape.4007")
#loc2425 = loc("reshape.4009")
#loc2426 = loc("transpose.4010")
#loc2427 = loc("dot.4012")
#loc2428 = loc("reshape.4013")
#loc2429 = loc("logistic.4014")
#loc2430 = loc("multiply.4015")
#loc2431 = loc("reshape.3787")
#loc2432 = loc("reshape.3789")
#loc2433 = loc("transpose.3790")
#loc2434 = loc("dot.4004")
#loc2435 = loc("reshape.4005")
#loc2436 = loc("multiply.4016")
#loc2437 = loc("reshape.4017")
#loc2438 = loc("reshape.3782")
#loc2439 = loc("reshape.3784")
#loc2440 = loc("transpose.3785")
#loc2442 = loc("reshape.4019")
#loc2443 = loc("add.4022")
#loc2444 = loc("convert.4023")
#loc2445 = loc("power.4025")
#loc2446 = loc("reduce.4032")
#loc2447 = loc("multiply.4041")
#loc2448 = loc("reshape.4042")
#loc2449 = loc("add.4046")
#loc2450 = loc("rsqrt.4047")
#loc2451 = loc("reshape.4048")
#loc2452 = loc("broadcast.4049")
#loc2453 = loc("multiply.4050")
#loc2454 = loc("convert.4051")
#loc2455 = loc("multiply.4057")
#loc2456 = loc("reshape.4058")
#loc2457 = loc("reshape.3773")
#loc2458 = loc("reshape.3775")
#loc2459 = loc("transpose.3776")
#loc2460 = loc("dot.4059")
#loc2461 = loc("reshape.4061")
#loc2462 = loc("convert.4062")
#loc2463 = loc("power.4064")
#loc2464 = loc("reduce.4071")
#loc2465 = loc("multiply.4080")
#loc2466 = loc("reshape.4081")
#loc2467 = loc("add.4085")
#loc2468 = loc("rsqrt.4086")
#loc2469 = loc("reshape.4087")
#loc2470 = loc("broadcast.4088")
#loc2471 = loc("multiply.4089")
#loc2472 = loc("convert.4090")
#loc2473 = loc("multiply.4096")
#loc2474 = loc("transpose.4097")
#loc2475 = loc("multiply.4108")
#loc2476 = loc("slice.4099")
#loc2477 = loc("negate.4100")
#loc2478 = loc("slice.4098")
#loc2479 = loc("concatenate.4101")
#loc2480 = loc("multiply.4104")
#loc2481 = loc("add.4111")
#loc2483 = loc("reshape.4133")
#loc2484 = loc("reshape.4135")
#loc2485 = loc("transpose.4136")
#loc2486 = loc("dot.4138")
#loc2487 = loc("reshape.4140")
#loc2488 = loc("transpose.4141")
#loc2490 = loc("reshape.4487")
#loc2491 = loc("reshape.4489")
#loc2492 = loc("broadcast.4490")
#loc2493 = loc("reshape.4448")
#loc2494 = loc("reshape.4450")
#loc2495 = loc("broadcast.4451")
#loc2496 = loc("reshape.4270")
#loc2497 = loc("reshape.4272")
#loc2498 = loc("broadcast.4273")
#loc2499 = loc("reshape.4232")
#loc2500 = loc("reshape.4234")
#loc2501 = loc("transpose.4235")
#loc2502 = loc("dot.4237")
#loc2503 = loc("reshape.4239")
#loc2504 = loc("convert.4240")
#loc2505 = loc("power.4242")
#loc2506 = loc("reduce.4249")
#loc2507 = loc("multiply.4258")
#loc2508 = loc("reshape.4259")
#loc2509 = loc("add.4263")
#loc2510 = loc("rsqrt.4264")
#loc2511 = loc("reshape.4265")
#loc2512 = loc("broadcast.4266")
#loc2513 = loc("multiply.4267")
#loc2514 = loc("convert.4268")
#loc2515 = loc("multiply.4274")
#loc2516 = loc("transpose.4275")
#loc2517 = loc("multiply.4285")
#loc2518 = loc("slice.4277")
#loc2519 = loc("negate.4278")
#loc2520 = loc("slice.4276")
#loc2521 = loc("concatenate.4279")
#loc2522 = loc("multiply.4282")
#loc2523 = loc("add.4288")
#loc2524 = loc("convert.4289")
#loc2525 = loc("multiply.4291")
#loc2526 = loc("broadcast.4220")
#loc2527 = loc("reshape.4221")
#loc2528 = loc("convert.4222")
#loc2529 = loc("transpose.4223")
#loc2530 = loc("multiply.4225")
#loc2531 = loc("dot.4292")
#loc2532 = loc("add.4299")
#loc2533 = loc("convert.4325")
#loc2534 = loc("compare.4327")
#loc2535 = loc("not.4329")
#loc2537 = loc("or.4339")
#loc2538 = loc("select.4340")
#loc2539 = loc("reshape.4345")
#loc2540 = loc("not.4347")
#loc2541 = loc("reshape.4349")
#loc2542 = loc("broadcast.4350")
#loc2543 = loc("reduce.4305")
#loc2544 = loc("broadcast.4306")
#loc2545 = loc("subtract.4307")
#loc2546 = loc("exponential.4308")
#loc2547 = loc("reduce.4314")
#loc2548 = loc("broadcast.4315")
#loc2549 = loc("divide.4316")
#loc2550 = loc("select.4351")
#loc2551 = loc("broadcast.4198")
#loc2552 = loc("reshape.4199")
#loc2553 = loc("convert.4200")
#loc2554 = loc("dot.4352")
#loc2555 = loc("convert.4354")
#loc2556 = loc("transpose.4355")
#loc2557 = loc("reshape.4357")
#loc2558 = loc("reshape.4191")
#loc2559 = loc("reshape.4193")
#loc2560 = loc("transpose.4194")
#loc2562 = loc("reshape.4359")
#loc2563 = loc("add.4362")
#loc2564 = loc("reshape.4393")
#loc2565 = loc("reshape.4395")
#loc2566 = loc("broadcast.4396")
#loc2567 = loc("convert.4363")
#loc2568 = loc("power.4365")
#loc2569 = loc("reduce.4372")
#loc2570 = loc("multiply.4381")
#loc2571 = loc("reshape.4382")
#loc2572 = loc("add.4386")
#loc2573 = loc("rsqrt.4387")
#loc2574 = loc("reshape.4388")
#loc2575 = loc("broadcast.4389")
#loc2576 = loc("multiply.4390")
#loc2577 = loc("convert.4391")
#loc2578 = loc("multiply.4397")
#loc2579 = loc("reshape.4406")
#loc2580 = loc("reshape.4402")
#loc2581 = loc("reshape.4404")
#loc2582 = loc("transpose.4405")
#loc2583 = loc("dot.4407")
#loc2584 = loc("reshape.4408")
#loc2585 = loc("logistic.4409")
#loc2586 = loc("multiply.4410")
#loc2587 = loc("reshape.4182")
#loc2588 = loc("reshape.4184")
#loc2589 = loc("transpose.4185")
#loc2590 = loc("dot.4399")
#loc2591 = loc("reshape.4400")
#loc2592 = loc("multiply.4411")
#loc2593 = loc("reshape.4412")
#loc2594 = loc("reshape.4177")
#loc2595 = loc("reshape.4179")
#loc2596 = loc("transpose.4180")
#loc2598 = loc("reshape.4414")
#loc2599 = loc("add.4417")
#loc2600 = loc("convert.4418")
#loc2601 = loc("power.4420")
#loc2602 = loc("reduce.4427")
#loc2603 = loc("multiply.4436")
#loc2604 = loc("reshape.4437")
#loc2605 = loc("add.4441")
#loc2606 = loc("rsqrt.4442")
#loc2607 = loc("reshape.4443")
#loc2608 = loc("broadcast.4444")
#loc2609 = loc("multiply.4445")
#loc2610 = loc("convert.4446")
#loc2611 = loc("multiply.4452")
#loc2612 = loc("reshape.4453")
#loc2613 = loc("reshape.4168")
#loc2614 = loc("reshape.4170")
#loc2615 = loc("transpose.4171")
#loc2616 = loc("dot.4454")
#loc2617 = loc("reshape.4456")
#loc2618 = loc("convert.4457")
#loc2619 = loc("power.4459")
#loc2620 = loc("reduce.4466")
#loc2621 = loc("multiply.4475")
#loc2622 = loc("reshape.4476")
#loc2623 = loc("add.4480")
#loc2624 = loc("rsqrt.4481")
#loc2625 = loc("reshape.4482")
#loc2626 = loc("broadcast.4483")
#loc2627 = loc("multiply.4484")
#loc2628 = loc("convert.4485")
#loc2629 = loc("multiply.4491")
#loc2630 = loc("transpose.4492")
#loc2631 = loc("multiply.4503")
#loc2632 = loc("slice.4494")
#loc2633 = loc("negate.4495")
#loc2634 = loc("slice.4493")
#loc2635 = loc("concatenate.4496")
#loc2636 = loc("multiply.4499")
#loc2637 = loc("add.4506")
#loc2639 = loc("reshape.4528")
#loc2640 = loc("reshape.4530")
#loc2641 = loc("transpose.4531")
#loc2642 = loc("dot.4533")
#loc2643 = loc("reshape.4535")
#loc2644 = loc("transpose.4536")
#loc2646 = loc("reshape.4882")
#loc2647 = loc("reshape.4884")
#loc2648 = loc("broadcast.4885")
#loc2649 = loc("reshape.4843")
#loc2650 = loc("reshape.4845")
#loc2651 = loc("broadcast.4846")
#loc2652 = loc("reshape.4665")
#loc2653 = loc("reshape.4667")
#loc2654 = loc("broadcast.4668")
#loc2655 = loc("reshape.4627")
#loc2656 = loc("reshape.4629")
#loc2657 = loc("transpose.4630")
#loc2658 = loc("dot.4632")
#loc2659 = loc("reshape.4634")
#loc2660 = loc("convert.4635")
#loc2661 = loc("power.4637")
#loc2662 = loc("reduce.4644")
#loc2663 = loc("multiply.4653")
#loc2664 = loc("reshape.4654")
#loc2665 = loc("add.4658")
#loc2666 = loc("rsqrt.4659")
#loc2667 = loc("reshape.4660")
#loc2668 = loc("broadcast.4661")
#loc2669 = loc("multiply.4662")
#loc2670 = loc("convert.4663")
#loc2671 = loc("multiply.4669")
#loc2672 = loc("transpose.4670")
#loc2673 = loc("multiply.4680")
#loc2674 = loc("slice.4672")
#loc2675 = loc("negate.4673")
#loc2676 = loc("slice.4671")
#loc2677 = loc("concatenate.4674")
#loc2678 = loc("multiply.4677")
#loc2679 = loc("add.4683")
#loc2680 = loc("convert.4684")
#loc2681 = loc("multiply.4686")
#loc2682 = loc("broadcast.4615")
#loc2683 = loc("reshape.4616")
#loc2684 = loc("convert.4617")
#loc2685 = loc("transpose.4618")
#loc2686 = loc("multiply.4620")
#loc2687 = loc("dot.4687")
#loc2688 = loc("add.4694")
#loc2689 = loc("convert.4720")
#loc2690 = loc("compare.4722")
#loc2691 = loc("not.4724")
#loc2693 = loc("or.4734")
#loc2694 = loc("select.4735")
#loc2695 = loc("reshape.4740")
#loc2696 = loc("not.4742")
#loc2697 = loc("reshape.4744")
#loc2698 = loc("broadcast.4745")
#loc2699 = loc("reduce.4700")
#loc2700 = loc("broadcast.4701")
#loc2701 = loc("subtract.4702")
#loc2702 = loc("exponential.4703")
#loc2703 = loc("reduce.4709")
#loc2704 = loc("broadcast.4710")
#loc2705 = loc("divide.4711")
#loc2706 = loc("select.4746")
#loc2707 = loc("broadcast.4593")
#loc2708 = loc("reshape.4594")
#loc2709 = loc("convert.4595")
#loc2710 = loc("dot.4747")
#loc2711 = loc("convert.4749")
#loc2712 = loc("transpose.4750")
#loc2713 = loc("reshape.4752")
#loc2714 = loc("reshape.4586")
#loc2715 = loc("reshape.4588")
#loc2716 = loc("transpose.4589")
#loc2718 = loc("reshape.4754")
#loc2719 = loc("add.4757")
#loc2720 = loc("reshape.4788")
#loc2721 = loc("reshape.4790")
#loc2722 = loc("broadcast.4791")
#loc2723 = loc("convert.4758")
#loc2724 = loc("power.4760")
#loc2725 = loc("reduce.4767")
#loc2726 = loc("multiply.4776")
#loc2727 = loc("reshape.4777")
#loc2728 = loc("add.4781")
#loc2729 = loc("rsqrt.4782")
#loc2730 = loc("reshape.4783")
#loc2731 = loc("broadcast.4784")
#loc2732 = loc("multiply.4785")
#loc2733 = loc("convert.4786")
#loc2734 = loc("multiply.4792")
#loc2735 = loc("reshape.4801")
#loc2736 = loc("reshape.4797")
#loc2737 = loc("reshape.4799")
#loc2738 = loc("transpose.4800")
#loc2739 = loc("dot.4802")
#loc2740 = loc("reshape.4803")
#loc2741 = loc("logistic.4804")
#loc2742 = loc("multiply.4805")
#loc2743 = loc("reshape.4577")
#loc2744 = loc("reshape.4579")
#loc2745 = loc("transpose.4580")
#loc2746 = loc("dot.4794")
#loc2747 = loc("reshape.4795")
#loc2748 = loc("multiply.4806")
#loc2749 = loc("reshape.4807")
#loc2750 = loc("reshape.4572")
#loc2751 = loc("reshape.4574")
#loc2752 = loc("transpose.4575")
#loc2754 = loc("reshape.4809")
#loc2755 = loc("add.4812")
#loc2756 = loc("convert.4813")
#loc2757 = loc("power.4815")
#loc2758 = loc("reduce.4822")
#loc2759 = loc("multiply.4831")
#loc2760 = loc("reshape.4832")
#loc2761 = loc("add.4836")
#loc2762 = loc("rsqrt.4837")
#loc2763 = loc("reshape.4838")
#loc2764 = loc("broadcast.4839")
#loc2765 = loc("multiply.4840")
#loc2766 = loc("convert.4841")
#loc2767 = loc("multiply.4847")
#loc2768 = loc("reshape.4848")
#loc2769 = loc("reshape.4563")
#loc2770 = loc("reshape.4565")
#loc2771 = loc("transpose.4566")
#loc2772 = loc("dot.4849")
#loc2773 = loc("reshape.4851")
#loc2774 = loc("convert.4852")
#loc2775 = loc("power.4854")
#loc2776 = loc("reduce.4861")
#loc2777 = loc("multiply.4870")
#loc2778 = loc("reshape.4871")
#loc2779 = loc("add.4875")
#loc2780 = loc("rsqrt.4876")
#loc2781 = loc("reshape.4877")
#loc2782 = loc("broadcast.4878")
#loc2783 = loc("multiply.4879")
#loc2784 = loc("convert.4880")
#loc2785 = loc("multiply.4886")
#loc2786 = loc("transpose.4887")
#loc2787 = loc("multiply.4898")
#loc2788 = loc("slice.4889")
#loc2789 = loc("negate.4890")
#loc2790 = loc("slice.4888")
#loc2791 = loc("concatenate.4891")
#loc2792 = loc("multiply.4894")
#loc2793 = loc("add.4901")
#loc2795 = loc("reshape.4923")
#loc2796 = loc("reshape.4925")
#loc2797 = loc("transpose.4926")
#loc2798 = loc("dot.4928")
#loc2799 = loc("reshape.4930")
#loc2800 = loc("transpose.4931")
#loc2802 = loc("reshape.5277")
#loc2803 = loc("reshape.5279")
#loc2804 = loc("broadcast.5280")
#loc2805 = loc("reshape.5238")
#loc2806 = loc("reshape.5240")
#loc2807 = loc("broadcast.5241")
#loc2808 = loc("reshape.5060")
#loc2809 = loc("reshape.5062")
#loc2810 = loc("broadcast.5063")
#loc2811 = loc("reshape.5022")
#loc2812 = loc("reshape.5024")
#loc2813 = loc("transpose.5025")
#loc2814 = loc("dot.5027")
#loc2815 = loc("reshape.5029")
#loc2816 = loc("convert.5030")
#loc2817 = loc("power.5032")
#loc2818 = loc("reduce.5039")
#loc2819 = loc("multiply.5048")
#loc2820 = loc("reshape.5049")
#loc2821 = loc("add.5053")
#loc2822 = loc("rsqrt.5054")
#loc2823 = loc("reshape.5055")
#loc2824 = loc("broadcast.5056")
#loc2825 = loc("multiply.5057")
#loc2826 = loc("convert.5058")
#loc2827 = loc("multiply.5064")
#loc2828 = loc("transpose.5065")
#loc2829 = loc("multiply.5075")
#loc2830 = loc("slice.5067")
#loc2831 = loc("negate.5068")
#loc2832 = loc("slice.5066")
#loc2833 = loc("concatenate.5069")
#loc2834 = loc("multiply.5072")
#loc2835 = loc("add.5078")
#loc2836 = loc("convert.5079")
#loc2837 = loc("multiply.5081")
#loc2838 = loc("broadcast.5010")
#loc2839 = loc("reshape.5011")
#loc2840 = loc("convert.5012")
#loc2841 = loc("transpose.5013")
#loc2842 = loc("multiply.5015")
#loc2843 = loc("dot.5082")
#loc2844 = loc("add.5089")
#loc2845 = loc("convert.5115")
#loc2846 = loc("compare.5117")
#loc2847 = loc("not.5119")
#loc2849 = loc("or.5129")
#loc2850 = loc("select.5130")
#loc2851 = loc("reshape.5135")
#loc2852 = loc("not.5137")
#loc2853 = loc("reshape.5139")
#loc2854 = loc("broadcast.5140")
#loc2855 = loc("reduce.5095")
#loc2856 = loc("broadcast.5096")
#loc2857 = loc("subtract.5097")
#loc2858 = loc("exponential.5098")
#loc2859 = loc("reduce.5104")
#loc2860 = loc("broadcast.5105")
#loc2861 = loc("divide.5106")
#loc2862 = loc("select.5141")
#loc2863 = loc("broadcast.4988")
#loc2864 = loc("reshape.4989")
#loc2865 = loc("convert.4990")
#loc2866 = loc("dot.5142")
#loc2867 = loc("convert.5144")
#loc2868 = loc("transpose.5145")
#loc2869 = loc("reshape.5147")
#loc2870 = loc("reshape.4981")
#loc2871 = loc("reshape.4983")
#loc2872 = loc("transpose.4984")
#loc2874 = loc("reshape.5149")
#loc2875 = loc("add.5152")
#loc2876 = loc("reshape.5183")
#loc2877 = loc("reshape.5185")
#loc2878 = loc("broadcast.5186")
#loc2879 = loc("convert.5153")
#loc2880 = loc("power.5155")
#loc2881 = loc("reduce.5162")
#loc2882 = loc("multiply.5171")
#loc2883 = loc("reshape.5172")
#loc2884 = loc("add.5176")
#loc2885 = loc("rsqrt.5177")
#loc2886 = loc("reshape.5178")
#loc2887 = loc("broadcast.5179")
#loc2888 = loc("multiply.5180")
#loc2889 = loc("convert.5181")
#loc2890 = loc("multiply.5187")
#loc2891 = loc("reshape.5196")
#loc2892 = loc("reshape.5192")
#loc2893 = loc("reshape.5194")
#loc2894 = loc("transpose.5195")
#loc2895 = loc("dot.5197")
#loc2896 = loc("reshape.5198")
#loc2897 = loc("logistic.5199")
#loc2898 = loc("multiply.5200")
#loc2899 = loc("reshape.4972")
#loc2900 = loc("reshape.4974")
#loc2901 = loc("transpose.4975")
#loc2902 = loc("dot.5189")
#loc2903 = loc("reshape.5190")
#loc2904 = loc("multiply.5201")
#loc2905 = loc("reshape.5202")
#loc2906 = loc("reshape.4967")
#loc2907 = loc("reshape.4969")
#loc2908 = loc("transpose.4970")
#loc2910 = loc("reshape.5204")
#loc2911 = loc("add.5207")
#loc2912 = loc("convert.5208")
#loc2913 = loc("power.5210")
#loc2914 = loc("reduce.5217")
#loc2915 = loc("multiply.5226")
#loc2916 = loc("reshape.5227")
#loc2917 = loc("add.5231")
#loc2918 = loc("rsqrt.5232")
#loc2919 = loc("reshape.5233")
#loc2920 = loc("broadcast.5234")
#loc2921 = loc("multiply.5235")
#loc2922 = loc("convert.5236")
#loc2923 = loc("multiply.5242")
#loc2924 = loc("reshape.5243")
#loc2925 = loc("reshape.4958")
#loc2926 = loc("reshape.4960")
#loc2927 = loc("transpose.4961")
#loc2928 = loc("dot.5244")
#loc2929 = loc("reshape.5246")
#loc2930 = loc("convert.5247")
#loc2931 = loc("power.5249")
#loc2932 = loc("reduce.5256")
#loc2933 = loc("multiply.5265")
#loc2934 = loc("reshape.5266")
#loc2935 = loc("add.5270")
#loc2936 = loc("rsqrt.5271")
#loc2937 = loc("reshape.5272")
#loc2938 = loc("broadcast.5273")
#loc2939 = loc("multiply.5274")
#loc2940 = loc("convert.5275")
#loc2941 = loc("multiply.5281")
#loc2942 = loc("transpose.5282")
#loc2943 = loc("multiply.5293")
#loc2944 = loc("slice.5284")
#loc2945 = loc("negate.5285")
#loc2946 = loc("slice.5283")
#loc2947 = loc("concatenate.5286")
#loc2948 = loc("multiply.5289")
#loc2949 = loc("add.5296")
#loc2951 = loc("reshape.5318")
#loc2952 = loc("reshape.5320")
#loc2953 = loc("transpose.5321")
#loc2954 = loc("dot.5323")
#loc2955 = loc("reshape.5325")
#loc2956 = loc("transpose.5326")
#loc2958 = loc("reshape.5672")
#loc2959 = loc("reshape.5674")
#loc2960 = loc("broadcast.5675")
#loc2961 = loc("reshape.5633")
#loc2962 = loc("reshape.5635")
#loc2963 = loc("broadcast.5636")
#loc2964 = loc("reshape.5455")
#loc2965 = loc("reshape.5457")
#loc2966 = loc("broadcast.5458")
#loc2967 = loc("reshape.5417")
#loc2968 = loc("reshape.5419")
#loc2969 = loc("transpose.5420")
#loc2970 = loc("dot.5422")
#loc2971 = loc("reshape.5424")
#loc2972 = loc("convert.5425")
#loc2973 = loc("power.5427")
#loc2974 = loc("reduce.5434")
#loc2975 = loc("multiply.5443")
#loc2976 = loc("reshape.5444")
#loc2977 = loc("add.5448")
#loc2978 = loc("rsqrt.5449")
#loc2979 = loc("reshape.5450")
#loc2980 = loc("broadcast.5451")
#loc2981 = loc("multiply.5452")
#loc2982 = loc("convert.5453")
#loc2983 = loc("multiply.5459")
#loc2984 = loc("transpose.5460")
#loc2985 = loc("multiply.5470")
#loc2986 = loc("slice.5462")
#loc2987 = loc("negate.5463")
#loc2988 = loc("slice.5461")
#loc2989 = loc("concatenate.5464")
#loc2990 = loc("multiply.5467")
#loc2991 = loc("add.5473")
#loc2992 = loc("convert.5474")
#loc2993 = loc("multiply.5476")
#loc2994 = loc("broadcast.5405")
#loc2995 = loc("reshape.5406")
#loc2996 = loc("convert.5407")
#loc2997 = loc("transpose.5408")
#loc2998 = loc("multiply.5410")
#loc2999 = loc("dot.5477")
#loc3000 = loc("add.5484")
#loc3001 = loc("convert.5510")
#loc3002 = loc("compare.5512")
#loc3003 = loc("not.5514")
#loc3005 = loc("or.5524")
#loc3006 = loc("select.5525")
#loc3007 = loc("reshape.5530")
#loc3008 = loc("not.5532")
#loc3009 = loc("reshape.5534")
#loc3010 = loc("broadcast.5535")
#loc3011 = loc("reduce.5490")
#loc3012 = loc("broadcast.5491")
#loc3013 = loc("subtract.5492")
#loc3014 = loc("exponential.5493")
#loc3015 = loc("reduce.5499")
#loc3016 = loc("broadcast.5500")
#loc3017 = loc("divide.5501")
#loc3018 = loc("select.5536")
#loc3019 = loc("broadcast.5383")
#loc3020 = loc("reshape.5384")
#loc3021 = loc("convert.5385")
#loc3022 = loc("dot.5537")
#loc3023 = loc("convert.5539")
#loc3024 = loc("transpose.5540")
#loc3025 = loc("reshape.5542")
#loc3026 = loc("reshape.5376")
#loc3027 = loc("reshape.5378")
#loc3028 = loc("transpose.5379")
#loc3030 = loc("reshape.5544")
#loc3031 = loc("add.5547")
#loc3032 = loc("reshape.5578")
#loc3033 = loc("reshape.5580")
#loc3034 = loc("broadcast.5581")
#loc3035 = loc("convert.5548")
#loc3036 = loc("power.5550")
#loc3037 = loc("reduce.5557")
#loc3038 = loc("multiply.5566")
#loc3039 = loc("reshape.5567")
#loc3040 = loc("add.5571")
#loc3041 = loc("rsqrt.5572")
#loc3042 = loc("reshape.5573")
#loc3043 = loc("broadcast.5574")
#loc3044 = loc("multiply.5575")
#loc3045 = loc("convert.5576")
#loc3046 = loc("multiply.5582")
#loc3047 = loc("reshape.5591")
#loc3048 = loc("reshape.5587")
#loc3049 = loc("reshape.5589")
#loc3050 = loc("transpose.5590")
#loc3051 = loc("dot.5592")
#loc3052 = loc("reshape.5593")
#loc3053 = loc("logistic.5594")
#loc3054 = loc("multiply.5595")
#loc3055 = loc("reshape.5367")
#loc3056 = loc("reshape.5369")
#loc3057 = loc("transpose.5370")
#loc3058 = loc("dot.5584")
#loc3059 = loc("reshape.5585")
#loc3060 = loc("multiply.5596")
#loc3061 = loc("reshape.5597")
#loc3062 = loc("reshape.5362")
#loc3063 = loc("reshape.5364")
#loc3064 = loc("transpose.5365")
#loc3066 = loc("reshape.5599")
#loc3067 = loc("add.5602")
#loc3068 = loc("convert.5603")
#loc3069 = loc("power.5605")
#loc3070 = loc("reduce.5612")
#loc3071 = loc("multiply.5621")
#loc3072 = loc("reshape.5622")
#loc3073 = loc("add.5626")
#loc3074 = loc("rsqrt.5627")
#loc3075 = loc("reshape.5628")
#loc3076 = loc("broadcast.5629")
#loc3077 = loc("multiply.5630")
#loc3078 = loc("convert.5631")
#loc3079 = loc("multiply.5637")
#loc3080 = loc("reshape.5638")
#loc3081 = loc("reshape.5353")
#loc3082 = loc("reshape.5355")
#loc3083 = loc("transpose.5356")
#loc3084 = loc("dot.5639")
#loc3085 = loc("reshape.5641")
#loc3086 = loc("convert.5642")
#loc3087 = loc("power.5644")
#loc3088 = loc("reduce.5651")
#loc3089 = loc("multiply.5660")
#loc3090 = loc("reshape.5661")
#loc3091 = loc("add.5665")
#loc3092 = loc("rsqrt.5666")
#loc3093 = loc("reshape.5667")
#loc3094 = loc("broadcast.5668")
#loc3095 = loc("multiply.5669")
#loc3096 = loc("convert.5670")
#loc3097 = loc("multiply.5676")
#loc3098 = loc("transpose.5677")
#loc3099 = loc("multiply.5688")
#loc3100 = loc("slice.5679")
#loc3101 = loc("negate.5680")
#loc3102 = loc("slice.5678")
#loc3103 = loc("concatenate.5681")
#loc3104 = loc("multiply.5684")
#loc3105 = loc("add.5691")
#loc3107 = loc("reshape.5713")
#loc3108 = loc("reshape.5715")
#loc3109 = loc("transpose.5716")
#loc3110 = loc("dot.5718")
#loc3111 = loc("reshape.5720")
#loc3112 = loc("transpose.5721")
#loc3114 = loc("reshape.6067")
#loc3115 = loc("reshape.6069")
#loc3116 = loc("broadcast.6070")
#loc3117 = loc("reshape.6028")
#loc3118 = loc("reshape.6030")
#loc3119 = loc("broadcast.6031")
#loc3120 = loc("reshape.5850")
#loc3121 = loc("reshape.5852")
#loc3122 = loc("broadcast.5853")
#loc3123 = loc("reshape.5812")
#loc3124 = loc("reshape.5814")
#loc3125 = loc("transpose.5815")
#loc3126 = loc("dot.5817")
#loc3127 = loc("reshape.5819")
#loc3128 = loc("convert.5820")
#loc3129 = loc("power.5822")
#loc3130 = loc("reduce.5829")
#loc3131 = loc("multiply.5838")
#loc3132 = loc("reshape.5839")
#loc3133 = loc("add.5843")
#loc3134 = loc("rsqrt.5844")
#loc3135 = loc("reshape.5845")
#loc3136 = loc("broadcast.5846")
#loc3137 = loc("multiply.5847")
#loc3138 = loc("convert.5848")
#loc3139 = loc("multiply.5854")
#loc3140 = loc("transpose.5855")
#loc3141 = loc("multiply.5865")
#loc3142 = loc("slice.5857")
#loc3143 = loc("negate.5858")
#loc3144 = loc("slice.5856")
#loc3145 = loc("concatenate.5859")
#loc3146 = loc("multiply.5862")
#loc3147 = loc("add.5868")
#loc3148 = loc("convert.5869")
#loc3149 = loc("multiply.5871")
#loc3150 = loc("broadcast.5800")
#loc3151 = loc("reshape.5801")
#loc3152 = loc("convert.5802")
#loc3153 = loc("transpose.5803")
#loc3154 = loc("multiply.5805")
#loc3155 = loc("dot.5872")
#loc3156 = loc("add.5879")
#loc3157 = loc("convert.5905")
#loc3158 = loc("compare.5907")
#loc3159 = loc("not.5909")
#loc3161 = loc("or.5919")
#loc3162 = loc("select.5920")
#loc3163 = loc("reshape.5925")
#loc3164 = loc("not.5927")
#loc3165 = loc("reshape.5929")
#loc3166 = loc("broadcast.5930")
#loc3167 = loc("reduce.5885")
#loc3168 = loc("broadcast.5886")
#loc3169 = loc("subtract.5887")
#loc3170 = loc("exponential.5888")
#loc3171 = loc("reduce.5894")
#loc3172 = loc("broadcast.5895")
#loc3173 = loc("divide.5896")
#loc3174 = loc("select.5931")
#loc3175 = loc("broadcast.5778")
#loc3176 = loc("reshape.5779")
#loc3177 = loc("convert.5780")
#loc3178 = loc("dot.5932")
#loc3179 = loc("convert.5934")
#loc3180 = loc("transpose.5935")
#loc3181 = loc("reshape.5937")
#loc3182 = loc("reshape.5771")
#loc3183 = loc("reshape.5773")
#loc3184 = loc("transpose.5774")
#loc3186 = loc("reshape.5939")
#loc3187 = loc("add.5942")
#loc3188 = loc("reshape.5973")
#loc3189 = loc("reshape.5975")
#loc3190 = loc("broadcast.5976")
#loc3191 = loc("convert.5943")
#loc3192 = loc("power.5945")
#loc3193 = loc("reduce.5952")
#loc3194 = loc("multiply.5961")
#loc3195 = loc("reshape.5962")
#loc3196 = loc("add.5966")
#loc3197 = loc("rsqrt.5967")
#loc3198 = loc("reshape.5968")
#loc3199 = loc("broadcast.5969")
#loc3200 = loc("multiply.5970")
#loc3201 = loc("convert.5971")
#loc3202 = loc("multiply.5977")
#loc3203 = loc("reshape.5986")
#loc3204 = loc("reshape.5982")
#loc3205 = loc("reshape.5984")
#loc3206 = loc("transpose.5985")
#loc3207 = loc("dot.5987")
#loc3208 = loc("reshape.5988")
#loc3209 = loc("logistic.5989")
#loc3210 = loc("multiply.5990")
#loc3211 = loc("reshape.5762")
#loc3212 = loc("reshape.5764")
#loc3213 = loc("transpose.5765")
#loc3214 = loc("dot.5979")
#loc3215 = loc("reshape.5980")
#loc3216 = loc("multiply.5991")
#loc3217 = loc("reshape.5992")
#loc3218 = loc("reshape.5757")
#loc3219 = loc("reshape.5759")
#loc3220 = loc("transpose.5760")
#loc3222 = loc("reshape.5994")
#loc3223 = loc("add.5997")
#loc3224 = loc("convert.5998")
#loc3225 = loc("power.6000")
#loc3226 = loc("reduce.6007")
#loc3227 = loc("multiply.6016")
#loc3228 = loc("reshape.6017")
#loc3229 = loc("add.6021")
#loc3230 = loc("rsqrt.6022")
#loc3231 = loc("reshape.6023")
#loc3232 = loc("broadcast.6024")
#loc3233 = loc("multiply.6025")
#loc3234 = loc("convert.6026")
#loc3235 = loc("multiply.6032")
#loc3236 = loc("reshape.6033")
#loc3237 = loc("reshape.5748")
#loc3238 = loc("reshape.5750")
#loc3239 = loc("transpose.5751")
#loc3240 = loc("dot.6034")
#loc3241 = loc("reshape.6036")
#loc3242 = loc("convert.6037")
#loc3243 = loc("power.6039")
#loc3244 = loc("reduce.6046")
#loc3245 = loc("multiply.6055")
#loc3246 = loc("reshape.6056")
#loc3247 = loc("add.6060")
#loc3248 = loc("rsqrt.6061")
#loc3249 = loc("reshape.6062")
#loc3250 = loc("broadcast.6063")
#loc3251 = loc("multiply.6064")
#loc3252 = loc("convert.6065")
#loc3253 = loc("multiply.6071")
#loc3254 = loc("transpose.6072")
#loc3255 = loc("multiply.6083")
#loc3256 = loc("slice.6074")
#loc3257 = loc("negate.6075")
#loc3258 = loc("slice.6073")
#loc3259 = loc("concatenate.6076")
#loc3260 = loc("multiply.6079")
#loc3261 = loc("add.6086")
#loc3263 = loc("reshape.6108")
#loc3264 = loc("reshape.6110")
#loc3265 = loc("transpose.6111")
#loc3266 = loc("dot.6113")
#loc3267 = loc("reshape.6115")
#loc3268 = loc("transpose.6116")
#loc3270 = loc("reshape.6462")
#loc3271 = loc("reshape.6464")
#loc3272 = loc("broadcast.6465")
#loc3273 = loc("reshape.6423")
#loc3274 = loc("reshape.6425")
#loc3275 = loc("broadcast.6426")
#loc3276 = loc("reshape.6245")
#loc3277 = loc("reshape.6247")
#loc3278 = loc("broadcast.6248")
#loc3279 = loc("reshape.6207")
#loc3280 = loc("reshape.6209")
#loc3281 = loc("transpose.6210")
#loc3282 = loc("dot.6212")
#loc3283 = loc("reshape.6214")
#loc3284 = loc("convert.6215")
#loc3285 = loc("power.6217")
#loc3286 = loc("reduce.6224")
#loc3287 = loc("multiply.6233")
#loc3288 = loc("reshape.6234")
#loc3289 = loc("add.6238")
#loc3290 = loc("rsqrt.6239")
#loc3291 = loc("reshape.6240")
#loc3292 = loc("broadcast.6241")
#loc3293 = loc("multiply.6242")
#loc3294 = loc("convert.6243")
#loc3295 = loc("multiply.6249")
#loc3296 = loc("transpose.6250")
#loc3297 = loc("multiply.6260")
#loc3298 = loc("slice.6252")
#loc3299 = loc("negate.6253")
#loc3300 = loc("slice.6251")
#loc3301 = loc("concatenate.6254")
#loc3302 = loc("multiply.6257")
#loc3303 = loc("add.6263")
#loc3304 = loc("convert.6264")
#loc3305 = loc("multiply.6266")
#loc3306 = loc("broadcast.6195")
#loc3307 = loc("reshape.6196")
#loc3308 = loc("convert.6197")
#loc3309 = loc("transpose.6198")
#loc3310 = loc("multiply.6200")
#loc3311 = loc("dot.6267")
#loc3312 = loc("add.6274")
#loc3313 = loc("convert.6300")
#loc3314 = loc("compare.6302")
#loc3315 = loc("not.6304")
#loc3317 = loc("or.6314")
#loc3318 = loc("select.6315")
#loc3319 = loc("reshape.6320")
#loc3320 = loc("not.6322")
#loc3321 = loc("reshape.6324")
#loc3322 = loc("broadcast.6325")
#loc3323 = loc("reduce.6280")
#loc3324 = loc("broadcast.6281")
#loc3325 = loc("subtract.6282")
#loc3326 = loc("exponential.6283")
#loc3327 = loc("reduce.6289")
#loc3328 = loc("broadcast.6290")
#loc3329 = loc("divide.6291")
#loc3330 = loc("select.6326")
#loc3331 = loc("broadcast.6173")
#loc3332 = loc("reshape.6174")
#loc3333 = loc("convert.6175")
#loc3334 = loc("dot.6327")
#loc3335 = loc("convert.6329")
#loc3336 = loc("transpose.6330")
#loc3337 = loc("reshape.6332")
#loc3338 = loc("reshape.6166")
#loc3339 = loc("reshape.6168")
#loc3340 = loc("transpose.6169")
#loc3342 = loc("reshape.6334")
#loc3343 = loc("add.6337")
#loc3344 = loc("reshape.6368")
#loc3345 = loc("reshape.6370")
#loc3346 = loc("broadcast.6371")
#loc3347 = loc("convert.6338")
#loc3348 = loc("power.6340")
#loc3349 = loc("reduce.6347")
#loc3350 = loc("multiply.6356")
#loc3351 = loc("reshape.6357")
#loc3352 = loc("add.6361")
#loc3353 = loc("rsqrt.6362")
#loc3354 = loc("reshape.6363")
#loc3355 = loc("broadcast.6364")
#loc3356 = loc("multiply.6365")
#loc3357 = loc("convert.6366")
#loc3358 = loc("multiply.6372")
#loc3359 = loc("reshape.6381")
#loc3360 = loc("reshape.6377")
#loc3361 = loc("reshape.6379")
#loc3362 = loc("transpose.6380")
#loc3363 = loc("dot.6382")
#loc3364 = loc("reshape.6383")
#loc3365 = loc("logistic.6384")
#loc3366 = loc("multiply.6385")
#loc3367 = loc("reshape.6157")
#loc3368 = loc("reshape.6159")
#loc3369 = loc("transpose.6160")
#loc3370 = loc("dot.6374")
#loc3371 = loc("reshape.6375")
#loc3372 = loc("multiply.6386")
#loc3373 = loc("reshape.6387")
#loc3374 = loc("reshape.6152")
#loc3375 = loc("reshape.6154")
#loc3376 = loc("transpose.6155")
#loc3378 = loc("reshape.6389")
#loc3379 = loc("add.6392")
#loc3380 = loc("convert.6393")
#loc3381 = loc("power.6395")
#loc3382 = loc("reduce.6402")
#loc3383 = loc("multiply.6411")
#loc3384 = loc("reshape.6412")
#loc3385 = loc("add.6416")
#loc3386 = loc("rsqrt.6417")
#loc3387 = loc("reshape.6418")
#loc3388 = loc("broadcast.6419")
#loc3389 = loc("multiply.6420")
#loc3390 = loc("convert.6421")
#loc3391 = loc("multiply.6427")
#loc3392 = loc("reshape.6428")
#loc3393 = loc("reshape.6143")
#loc3394 = loc("reshape.6145")
#loc3395 = loc("transpose.6146")
#loc3396 = loc("dot.6429")
#loc3397 = loc("reshape.6431")
#loc3398 = loc("convert.6432")
#loc3399 = loc("power.6434")
#loc3400 = loc("reduce.6441")
#loc3401 = loc("multiply.6450")
#loc3402 = loc("reshape.6451")
#loc3403 = loc("add.6455")
#loc3404 = loc("rsqrt.6456")
#loc3405 = loc("reshape.6457")
#loc3406 = loc("broadcast.6458")
#loc3407 = loc("multiply.6459")
#loc3408 = loc("convert.6460")
#loc3409 = loc("multiply.6466")
#loc3410 = loc("transpose.6467")
#loc3411 = loc("multiply.6478")
#loc3412 = loc("slice.6469")
#loc3413 = loc("negate.6470")
#loc3414 = loc("slice.6468")
#loc3415 = loc("concatenate.6471")
#loc3416 = loc("multiply.6474")
#loc3417 = loc("add.6481")
#loc3419 = loc("reshape.6503")
#loc3420 = loc("reshape.6505")
#loc3421 = loc("transpose.6506")
#loc3422 = loc("dot.6508")
#loc3423 = loc("reshape.6510")
#loc3424 = loc("transpose.6511")
#loc3426 = loc("reshape.6857")
#loc3427 = loc("reshape.6859")
#loc3428 = loc("broadcast.6860")
#loc3429 = loc("reshape.6818")
#loc3430 = loc("reshape.6820")
#loc3431 = loc("broadcast.6821")
#loc3432 = loc("reshape.6640")
#loc3433 = loc("reshape.6642")
#loc3434 = loc("broadcast.6643")
#loc3435 = loc("reshape.6602")
#loc3436 = loc("reshape.6604")
#loc3437 = loc("transpose.6605")
#loc3438 = loc("dot.6607")
#loc3439 = loc("reshape.6609")
#loc3440 = loc("convert.6610")
#loc3441 = loc("power.6612")
#loc3442 = loc("reduce.6619")
#loc3443 = loc("multiply.6628")
#loc3444 = loc("reshape.6629")
#loc3445 = loc("add.6633")
#loc3446 = loc("rsqrt.6634")
#loc3447 = loc("reshape.6635")
#loc3448 = loc("broadcast.6636")
#loc3449 = loc("multiply.6637")
#loc3450 = loc("convert.6638")
#loc3451 = loc("multiply.6644")
#loc3452 = loc("transpose.6645")
#loc3453 = loc("multiply.6655")
#loc3454 = loc("slice.6647")
#loc3455 = loc("negate.6648")
#loc3456 = loc("slice.6646")
#loc3457 = loc("concatenate.6649")
#loc3458 = loc("multiply.6652")
#loc3459 = loc("add.6658")
#loc3460 = loc("convert.6659")
#loc3461 = loc("multiply.6661")
#loc3462 = loc("broadcast.6590")
#loc3463 = loc("reshape.6591")
#loc3464 = loc("convert.6592")
#loc3465 = loc("transpose.6593")
#loc3466 = loc("multiply.6595")
#loc3467 = loc("dot.6662")
#loc3468 = loc("add.6669")
#loc3469 = loc("convert.6695")
#loc3470 = loc("compare.6697")
#loc3471 = loc("not.6699")
#loc3473 = loc("or.6709")
#loc3474 = loc("select.6710")
#loc3475 = loc("reshape.6715")
#loc3476 = loc("not.6717")
#loc3477 = loc("reshape.6719")
#loc3478 = loc("broadcast.6720")
#loc3479 = loc("reduce.6675")
#loc3480 = loc("broadcast.6676")
#loc3481 = loc("subtract.6677")
#loc3482 = loc("exponential.6678")
#loc3483 = loc("reduce.6684")
#loc3484 = loc("broadcast.6685")
#loc3485 = loc("divide.6686")
#loc3486 = loc("select.6721")
#loc3487 = loc("broadcast.6568")
#loc3488 = loc("reshape.6569")
#loc3489 = loc("convert.6570")
#loc3490 = loc("dot.6722")
#loc3491 = loc("convert.6724")
#loc3492 = loc("transpose.6725")
#loc3493 = loc("reshape.6727")
#loc3494 = loc("reshape.6561")
#loc3495 = loc("reshape.6563")
#loc3496 = loc("transpose.6564")
#loc3498 = loc("reshape.6729")
#loc3499 = loc("add.6732")
#loc3500 = loc("reshape.6763")
#loc3501 = loc("reshape.6765")
#loc3502 = loc("broadcast.6766")
#loc3503 = loc("convert.6733")
#loc3504 = loc("power.6735")
#loc3505 = loc("reduce.6742")
#loc3506 = loc("multiply.6751")
#loc3507 = loc("reshape.6752")
#loc3508 = loc("add.6756")
#loc3509 = loc("rsqrt.6757")
#loc3510 = loc("reshape.6758")
#loc3511 = loc("broadcast.6759")
#loc3512 = loc("multiply.6760")
#loc3513 = loc("convert.6761")
#loc3514 = loc("multiply.6767")
#loc3515 = loc("reshape.6776")
#loc3516 = loc("reshape.6772")
#loc3517 = loc("reshape.6774")
#loc3518 = loc("transpose.6775")
#loc3519 = loc("dot.6777")
#loc3520 = loc("reshape.6778")
#loc3521 = loc("logistic.6779")
#loc3522 = loc("multiply.6780")
#loc3523 = loc("reshape.6552")
#loc3524 = loc("reshape.6554")
#loc3525 = loc("transpose.6555")
#loc3526 = loc("dot.6769")
#loc3527 = loc("reshape.6770")
#loc3528 = loc("multiply.6781")
#loc3529 = loc("reshape.6782")
#loc3530 = loc("reshape.6547")
#loc3531 = loc("reshape.6549")
#loc3532 = loc("transpose.6550")
#loc3534 = loc("reshape.6784")
#loc3535 = loc("add.6787")
#loc3536 = loc("convert.6788")
#loc3537 = loc("power.6790")
#loc3538 = loc("reduce.6797")
#loc3539 = loc("multiply.6806")
#loc3540 = loc("reshape.6807")
#loc3541 = loc("add.6811")
#loc3542 = loc("rsqrt.6812")
#loc3543 = loc("reshape.6813")
#loc3544 = loc("broadcast.6814")
#loc3545 = loc("multiply.6815")
#loc3546 = loc("convert.6816")
#loc3547 = loc("multiply.6822")
#loc3548 = loc("reshape.6823")
#loc3549 = loc("reshape.6538")
#loc3550 = loc("reshape.6540")
#loc3551 = loc("transpose.6541")
#loc3552 = loc("dot.6824")
#loc3553 = loc("reshape.6826")
#loc3554 = loc("convert.6827")
#loc3555 = loc("power.6829")
#loc3556 = loc("reduce.6836")
#loc3557 = loc("multiply.6845")
#loc3558 = loc("reshape.6846")
#loc3559 = loc("add.6850")
#loc3560 = loc("rsqrt.6851")
#loc3561 = loc("reshape.6852")
#loc3562 = loc("broadcast.6853")
#loc3563 = loc("multiply.6854")
#loc3564 = loc("convert.6855")
#loc3565 = loc("multiply.6861")
#loc3566 = loc("transpose.6862")
#loc3567 = loc("multiply.6873")
#loc3568 = loc("slice.6864")
#loc3569 = loc("negate.6865")
#loc3570 = loc("slice.6863")
#loc3571 = loc("concatenate.6866")
#loc3572 = loc("multiply.6869")
#loc3573 = loc("add.6876")
#loc3575 = loc("reshape.6898")
#loc3576 = loc("reshape.6900")
#loc3577 = loc("transpose.6901")
#loc3578 = loc("dot.6903")
#loc3579 = loc("reshape.6905")
#loc3580 = loc("transpose.6906")
#loc3582 = loc("reshape.7252")
#loc3583 = loc("reshape.7254")
#loc3584 = loc("broadcast.7255")
#loc3585 = loc("reshape.7213")
#loc3586 = loc("reshape.7215")
#loc3587 = loc("broadcast.7216")
#loc3588 = loc("reshape.7035")
#loc3589 = loc("reshape.7037")
#loc3590 = loc("broadcast.7038")
#loc3591 = loc("reshape.6997")
#loc3592 = loc("reshape.6999")
#loc3593 = loc("transpose.7000")
#loc3594 = loc("dot.7002")
#loc3595 = loc("reshape.7004")
#loc3596 = loc("convert.7005")
#loc3597 = loc("power.7007")
#loc3598 = loc("reduce.7014")
#loc3599 = loc("multiply.7023")
#loc3600 = loc("reshape.7024")
#loc3601 = loc("add.7028")
#loc3602 = loc("rsqrt.7029")
#loc3603 = loc("reshape.7030")
#loc3604 = loc("broadcast.7031")
#loc3605 = loc("multiply.7032")
#loc3606 = loc("convert.7033")
#loc3607 = loc("multiply.7039")
#loc3608 = loc("transpose.7040")
#loc3609 = loc("multiply.7050")
#loc3610 = loc("slice.7042")
#loc3611 = loc("negate.7043")
#loc3612 = loc("slice.7041")
#loc3613 = loc("concatenate.7044")
#loc3614 = loc("multiply.7047")
#loc3615 = loc("add.7053")
#loc3616 = loc("convert.7054")
#loc3617 = loc("multiply.7056")
#loc3618 = loc("broadcast.6985")
#loc3619 = loc("reshape.6986")
#loc3620 = loc("convert.6987")
#loc3621 = loc("transpose.6988")
#loc3622 = loc("multiply.6990")
#loc3623 = loc("dot.7057")
#loc3624 = loc("add.7064")
#loc3625 = loc("convert.7090")
#loc3626 = loc("compare.7092")
#loc3627 = loc("not.7094")
#loc3629 = loc("or.7104")
#loc3630 = loc("select.7105")
#loc3631 = loc("reshape.7110")
#loc3632 = loc("not.7112")
#loc3633 = loc("reshape.7114")
#loc3634 = loc("broadcast.7115")
#loc3635 = loc("reduce.7070")
#loc3636 = loc("broadcast.7071")
#loc3637 = loc("subtract.7072")
#loc3638 = loc("exponential.7073")
#loc3639 = loc("reduce.7079")
#loc3640 = loc("broadcast.7080")
#loc3641 = loc("divide.7081")
#loc3642 = loc("select.7116")
#loc3643 = loc("broadcast.6963")
#loc3644 = loc("reshape.6964")
#loc3645 = loc("convert.6965")
#loc3646 = loc("dot.7117")
#loc3647 = loc("convert.7119")
#loc3648 = loc("transpose.7120")
#loc3649 = loc("reshape.7122")
#loc3650 = loc("reshape.6956")
#loc3651 = loc("reshape.6958")
#loc3652 = loc("transpose.6959")
#loc3654 = loc("reshape.7124")
#loc3655 = loc("add.7127")
#loc3656 = loc("reshape.7158")
#loc3657 = loc("reshape.7160")
#loc3658 = loc("broadcast.7161")
#loc3659 = loc("convert.7128")
#loc3660 = loc("power.7130")
#loc3661 = loc("reduce.7137")
#loc3662 = loc("multiply.7146")
#loc3663 = loc("reshape.7147")
#loc3664 = loc("add.7151")
#loc3665 = loc("rsqrt.7152")
#loc3666 = loc("reshape.7153")
#loc3667 = loc("broadcast.7154")
#loc3668 = loc("multiply.7155")
#loc3669 = loc("convert.7156")
#loc3670 = loc("multiply.7162")
#loc3671 = loc("reshape.7171")
#loc3672 = loc("reshape.7167")
#loc3673 = loc("reshape.7169")
#loc3674 = loc("transpose.7170")
#loc3675 = loc("dot.7172")
#loc3676 = loc("reshape.7173")
#loc3677 = loc("logistic.7174")
#loc3678 = loc("multiply.7175")
#loc3679 = loc("reshape.6947")
#loc3680 = loc("reshape.6949")
#loc3681 = loc("transpose.6950")
#loc3682 = loc("dot.7164")
#loc3683 = loc("reshape.7165")
#loc3684 = loc("multiply.7176")
#loc3685 = loc("reshape.7177")
#loc3686 = loc("reshape.6942")
#loc3687 = loc("reshape.6944")
#loc3688 = loc("transpose.6945")
#loc3690 = loc("reshape.7179")
#loc3691 = loc("add.7182")
#loc3692 = loc("convert.7183")
#loc3693 = loc("power.7185")
#loc3694 = loc("reduce.7192")
#loc3695 = loc("multiply.7201")
#loc3696 = loc("reshape.7202")
#loc3697 = loc("add.7206")
#loc3698 = loc("rsqrt.7207")
#loc3699 = loc("reshape.7208")
#loc3700 = loc("broadcast.7209")
#loc3701 = loc("multiply.7210")
#loc3702 = loc("convert.7211")
#loc3703 = loc("multiply.7217")
#loc3704 = loc("reshape.7218")
#loc3705 = loc("reshape.6933")
#loc3706 = loc("reshape.6935")
#loc3707 = loc("transpose.6936")
#loc3708 = loc("dot.7219")
#loc3709 = loc("reshape.7221")
#loc3710 = loc("convert.7222")
#loc3711 = loc("power.7224")
#loc3712 = loc("reduce.7231")
#loc3713 = loc("multiply.7240")
#loc3714 = loc("reshape.7241")
#loc3715 = loc("add.7245")
#loc3716 = loc("rsqrt.7246")
#loc3717 = loc("reshape.7247")
#loc3718 = loc("broadcast.7248")
#loc3719 = loc("multiply.7249")
#loc3720 = loc("convert.7250")
#loc3721 = loc("multiply.7256")
#loc3722 = loc("transpose.7257")
#loc3723 = loc("multiply.7268")
#loc3724 = loc("slice.7259")
#loc3725 = loc("negate.7260")
#loc3726 = loc("slice.7258")
#loc3727 = loc("concatenate.7261")
#loc3728 = loc("multiply.7264")
#loc3729 = loc("add.7271")
#loc3731 = loc("reshape.7293")
#loc3732 = loc("reshape.7295")
#loc3733 = loc("transpose.7296")
#loc3734 = loc("dot.7298")
#loc3735 = loc("reshape.7300")
#loc3736 = loc("transpose.7301")
#loc3738 = loc("reshape.7647")
#loc3739 = loc("reshape.7649")
#loc3740 = loc("broadcast.7650")
#loc3741 = loc("reshape.7608")
#loc3742 = loc("reshape.7610")
#loc3743 = loc("broadcast.7611")
#loc3744 = loc("reshape.7430")
#loc3745 = loc("reshape.7432")
#loc3746 = loc("broadcast.7433")
#loc3747 = loc("reshape.7392")
#loc3748 = loc("reshape.7394")
#loc3749 = loc("transpose.7395")
#loc3750 = loc("dot.7397")
#loc3751 = loc("reshape.7399")
#loc3752 = loc("convert.7400")
#loc3753 = loc("power.7402")
#loc3754 = loc("reduce.7409")
#loc3755 = loc("multiply.7418")
#loc3756 = loc("reshape.7419")
#loc3757 = loc("add.7423")
#loc3758 = loc("rsqrt.7424")
#loc3759 = loc("reshape.7425")
#loc3760 = loc("broadcast.7426")
#loc3761 = loc("multiply.7427")
#loc3762 = loc("convert.7428")
#loc3763 = loc("multiply.7434")
#loc3764 = loc("transpose.7435")
#loc3765 = loc("multiply.7445")
#loc3766 = loc("slice.7437")
#loc3767 = loc("negate.7438")
#loc3768 = loc("slice.7436")
#loc3769 = loc("concatenate.7439")
#loc3770 = loc("multiply.7442")
#loc3771 = loc("add.7448")
#loc3772 = loc("convert.7449")
#loc3773 = loc("multiply.7451")
#loc3774 = loc("broadcast.7380")
#loc3775 = loc("reshape.7381")
#loc3776 = loc("convert.7382")
#loc3777 = loc("transpose.7383")
#loc3778 = loc("multiply.7385")
#loc3779 = loc("dot.7452")
#loc3780 = loc("add.7459")
#loc3781 = loc("convert.7485")
#loc3782 = loc("compare.7487")
#loc3783 = loc("not.7489")
#loc3785 = loc("or.7499")
#loc3786 = loc("select.7500")
#loc3787 = loc("reshape.7505")
#loc3788 = loc("not.7507")
#loc3789 = loc("reshape.7509")
#loc3790 = loc("broadcast.7510")
#loc3791 = loc("reduce.7465")
#loc3792 = loc("broadcast.7466")
#loc3793 = loc("subtract.7467")
#loc3794 = loc("exponential.7468")
#loc3795 = loc("reduce.7474")
#loc3796 = loc("broadcast.7475")
#loc3797 = loc("divide.7476")
#loc3798 = loc("select.7511")
#loc3799 = loc("broadcast.7358")
#loc3800 = loc("reshape.7359")
#loc3801 = loc("convert.7360")
#loc3802 = loc("dot.7512")
#loc3803 = loc("convert.7514")
#loc3804 = loc("transpose.7515")
#loc3805 = loc("reshape.7517")
#loc3806 = loc("reshape.7351")
#loc3807 = loc("reshape.7353")
#loc3808 = loc("transpose.7354")
#loc3810 = loc("reshape.7519")
#loc3811 = loc("add.7522")
#loc3812 = loc("reshape.7553")
#loc3813 = loc("reshape.7555")
#loc3814 = loc("broadcast.7556")
#loc3815 = loc("convert.7523")
#loc3816 = loc("power.7525")
#loc3817 = loc("reduce.7532")
#loc3818 = loc("multiply.7541")
#loc3819 = loc("reshape.7542")
#loc3820 = loc("add.7546")
#loc3821 = loc("rsqrt.7547")
#loc3822 = loc("reshape.7548")
#loc3823 = loc("broadcast.7549")
#loc3824 = loc("multiply.7550")
#loc3825 = loc("convert.7551")
#loc3826 = loc("multiply.7557")
#loc3827 = loc("reshape.7566")
#loc3828 = loc("reshape.7562")
#loc3829 = loc("reshape.7564")
#loc3830 = loc("transpose.7565")
#loc3831 = loc("dot.7567")
#loc3832 = loc("reshape.7568")
#loc3833 = loc("logistic.7569")
#loc3834 = loc("multiply.7570")
#loc3835 = loc("reshape.7342")
#loc3836 = loc("reshape.7344")
#loc3837 = loc("transpose.7345")
#loc3838 = loc("dot.7559")
#loc3839 = loc("reshape.7560")
#loc3840 = loc("multiply.7571")
#loc3841 = loc("reshape.7572")
#loc3842 = loc("reshape.7337")
#loc3843 = loc("reshape.7339")
#loc3844 = loc("transpose.7340")
#loc3846 = loc("reshape.7574")
#loc3847 = loc("add.7577")
#loc3848 = loc("convert.7578")
#loc3849 = loc("power.7580")
#loc3850 = loc("reduce.7587")
#loc3851 = loc("multiply.7596")
#loc3852 = loc("reshape.7597")
#loc3853 = loc("add.7601")
#loc3854 = loc("rsqrt.7602")
#loc3855 = loc("reshape.7603")
#loc3856 = loc("broadcast.7604")
#loc3857 = loc("multiply.7605")
#loc3858 = loc("convert.7606")
#loc3859 = loc("multiply.7612")
#loc3860 = loc("reshape.7613")
#loc3861 = loc("reshape.7328")
#loc3862 = loc("reshape.7330")
#loc3863 = loc("transpose.7331")
#loc3864 = loc("dot.7614")
#loc3865 = loc("reshape.7616")
#loc3866 = loc("convert.7617")
#loc3867 = loc("power.7619")
#loc3868 = loc("reduce.7626")
#loc3869 = loc("multiply.7635")
#loc3870 = loc("reshape.7636")
#loc3871 = loc("add.7640")
#loc3872 = loc("rsqrt.7641")
#loc3873 = loc("reshape.7642")
#loc3874 = loc("broadcast.7643")
#loc3875 = loc("multiply.7644")
#loc3876 = loc("convert.7645")
#loc3877 = loc("multiply.7651")
#loc3878 = loc("transpose.7652")
#loc3879 = loc("multiply.7663")
#loc3880 = loc("slice.7654")
#loc3881 = loc("negate.7655")
#loc3882 = loc("slice.7653")
#loc3883 = loc("concatenate.7656")
#loc3884 = loc("multiply.7659")
#loc3885 = loc("add.7666")
#loc3887 = loc("reshape.7688")
#loc3888 = loc("reshape.7690")
#loc3889 = loc("transpose.7691")
#loc3890 = loc("dot.7693")
#loc3891 = loc("reshape.7695")
#loc3892 = loc("transpose.7696")
#loc3894 = loc("reshape.8042")
#loc3895 = loc("reshape.8044")
#loc3896 = loc("broadcast.8045")
#loc3897 = loc("reshape.8003")
#loc3898 = loc("reshape.8005")
#loc3899 = loc("broadcast.8006")
#loc3900 = loc("reshape.7825")
#loc3901 = loc("reshape.7827")
#loc3902 = loc("broadcast.7828")
#loc3903 = loc("reshape.7787")
#loc3904 = loc("reshape.7789")
#loc3905 = loc("transpose.7790")
#loc3906 = loc("dot.7792")
#loc3907 = loc("reshape.7794")
#loc3908 = loc("convert.7795")
#loc3909 = loc("power.7797")
#loc3910 = loc("reduce.7804")
#loc3911 = loc("multiply.7813")
#loc3912 = loc("reshape.7814")
#loc3913 = loc("add.7818")
#loc3914 = loc("rsqrt.7819")
#loc3915 = loc("reshape.7820")
#loc3916 = loc("broadcast.7821")
#loc3917 = loc("multiply.7822")
#loc3918 = loc("convert.7823")
#loc3919 = loc("multiply.7829")
#loc3920 = loc("transpose.7830")
#loc3921 = loc("multiply.7840")
#loc3922 = loc("slice.7832")
#loc3923 = loc("negate.7833")
#loc3924 = loc("slice.7831")
#loc3925 = loc("concatenate.7834")
#loc3926 = loc("multiply.7837")
#loc3927 = loc("add.7843")
#loc3928 = loc("convert.7844")
#loc3929 = loc("multiply.7846")
#loc3930 = loc("broadcast.7775")
#loc3931 = loc("reshape.7776")
#loc3932 = loc("convert.7777")
#loc3933 = loc("transpose.7778")
#loc3934 = loc("multiply.7780")
#loc3935 = loc("dot.7847")
#loc3936 = loc("add.7854")
#loc3937 = loc("convert.7880")
#loc3938 = loc("compare.7882")
#loc3939 = loc("not.7884")
#loc3941 = loc("or.7894")
#loc3942 = loc("select.7895")
#loc3943 = loc("reshape.7900")
#loc3944 = loc("not.7902")
#loc3945 = loc("reshape.7904")
#loc3946 = loc("broadcast.7905")
#loc3947 = loc("reduce.7860")
#loc3948 = loc("broadcast.7861")
#loc3949 = loc("subtract.7862")
#loc3950 = loc("exponential.7863")
#loc3951 = loc("reduce.7869")
#loc3952 = loc("broadcast.7870")
#loc3953 = loc("divide.7871")
#loc3954 = loc("select.7906")
#loc3955 = loc("broadcast.7753")
#loc3956 = loc("reshape.7754")
#loc3957 = loc("convert.7755")
#loc3958 = loc("dot.7907")
#loc3959 = loc("convert.7909")
#loc3960 = loc("transpose.7910")
#loc3961 = loc("reshape.7912")
#loc3962 = loc("reshape.7746")
#loc3963 = loc("reshape.7748")
#loc3964 = loc("transpose.7749")
#loc3966 = loc("reshape.7914")
#loc3967 = loc("add.7917")
#loc3968 = loc("reshape.7948")
#loc3969 = loc("reshape.7950")
#loc3970 = loc("broadcast.7951")
#loc3971 = loc("convert.7918")
#loc3972 = loc("power.7920")
#loc3973 = loc("reduce.7927")
#loc3974 = loc("multiply.7936")
#loc3975 = loc("reshape.7937")
#loc3976 = loc("add.7941")
#loc3977 = loc("rsqrt.7942")
#loc3978 = loc("reshape.7943")
#loc3979 = loc("broadcast.7944")
#loc3980 = loc("multiply.7945")
#loc3981 = loc("convert.7946")
#loc3982 = loc("multiply.7952")
#loc3983 = loc("reshape.7961")
#loc3984 = loc("reshape.7957")
#loc3985 = loc("reshape.7959")
#loc3986 = loc("transpose.7960")
#loc3987 = loc("dot.7962")
#loc3988 = loc("reshape.7963")
#loc3989 = loc("logistic.7964")
#loc3990 = loc("multiply.7965")
#loc3991 = loc("reshape.7737")
#loc3992 = loc("reshape.7739")
#loc3993 = loc("transpose.7740")
#loc3994 = loc("dot.7954")
#loc3995 = loc("reshape.7955")
#loc3996 = loc("multiply.7966")
#loc3997 = loc("reshape.7967")
#loc3998 = loc("reshape.7732")
#loc3999 = loc("reshape.7734")
#loc4000 = loc("transpose.7735")
#loc4002 = loc("reshape.7969")
#loc4003 = loc("add.7972")
#loc4004 = loc("convert.7973")
#loc4005 = loc("power.7975")
#loc4006 = loc("reduce.7982")
#loc4007 = loc("multiply.7991")
#loc4008 = loc("reshape.7992")
#loc4009 = loc("add.7996")
#loc4010 = loc("rsqrt.7997")
#loc4011 = loc("reshape.7998")
#loc4012 = loc("broadcast.7999")
#loc4013 = loc("multiply.8000")
#loc4014 = loc("convert.8001")
#loc4015 = loc("multiply.8007")
#loc4016 = loc("reshape.8008")
#loc4017 = loc("reshape.7723")
#loc4018 = loc("reshape.7725")
#loc4019 = loc("transpose.7726")
#loc4020 = loc("dot.8009")
#loc4021 = loc("reshape.8011")
#loc4022 = loc("convert.8012")
#loc4023 = loc("power.8014")
#loc4024 = loc("reduce.8021")
#loc4025 = loc("multiply.8030")
#loc4026 = loc("reshape.8031")
#loc4027 = loc("add.8035")
#loc4028 = loc("rsqrt.8036")
#loc4029 = loc("reshape.8037")
#loc4030 = loc("broadcast.8038")
#loc4031 = loc("multiply.8039")
#loc4032 = loc("convert.8040")
#loc4033 = loc("multiply.8046")
#loc4034 = loc("transpose.8047")
#loc4035 = loc("multiply.8058")
#loc4036 = loc("slice.8049")
#loc4037 = loc("negate.8050")
#loc4038 = loc("slice.8048")
#loc4039 = loc("concatenate.8051")
#loc4040 = loc("multiply.8054")
#loc4041 = loc("add.8061")
#loc4043 = loc("reshape.8083")
#loc4044 = loc("reshape.8085")
#loc4045 = loc("transpose.8086")
#loc4046 = loc("dot.8088")
#loc4047 = loc("reshape.8090")
#loc4048 = loc("transpose.8091")
#loc4050 = loc("reshape.8437")
#loc4051 = loc("reshape.8439")
#loc4052 = loc("broadcast.8440")
#loc4053 = loc("reshape.8398")
#loc4054 = loc("reshape.8400")
#loc4055 = loc("broadcast.8401")
#loc4056 = loc("reshape.8220")
#loc4057 = loc("reshape.8222")
#loc4058 = loc("broadcast.8223")
#loc4059 = loc("reshape.8182")
#loc4060 = loc("reshape.8184")
#loc4061 = loc("transpose.8185")
#loc4062 = loc("dot.8187")
#loc4063 = loc("reshape.8189")
#loc4064 = loc("convert.8190")
#loc4065 = loc("power.8192")
#loc4066 = loc("reduce.8199")
#loc4067 = loc("multiply.8208")
#loc4068 = loc("reshape.8209")
#loc4069 = loc("add.8213")
#loc4070 = loc("rsqrt.8214")
#loc4071 = loc("reshape.8215")
#loc4072 = loc("broadcast.8216")
#loc4073 = loc("multiply.8217")
#loc4074 = loc("convert.8218")
#loc4075 = loc("multiply.8224")
#loc4076 = loc("transpose.8225")
#loc4077 = loc("multiply.8235")
#loc4078 = loc("slice.8227")
#loc4079 = loc("negate.8228")
#loc4080 = loc("slice.8226")
#loc4081 = loc("concatenate.8229")
#loc4082 = loc("multiply.8232")
#loc4083 = loc("add.8238")
#loc4084 = loc("convert.8239")
#loc4085 = loc("multiply.8241")
#loc4086 = loc("broadcast.8170")
#loc4087 = loc("reshape.8171")
#loc4088 = loc("convert.8172")
#loc4089 = loc("transpose.8173")
#loc4090 = loc("multiply.8175")
#loc4091 = loc("dot.8242")
#loc4092 = loc("add.8249")
#loc4093 = loc("convert.8275")
#loc4094 = loc("compare.8277")
#loc4095 = loc("not.8279")
#loc4097 = loc("or.8289")
#loc4098 = loc("select.8290")
#loc4099 = loc("reshape.8295")
#loc4100 = loc("not.8297")
#loc4101 = loc("reshape.8299")
#loc4102 = loc("broadcast.8300")
#loc4103 = loc("reduce.8255")
#loc4104 = loc("broadcast.8256")
#loc4105 = loc("subtract.8257")
#loc4106 = loc("exponential.8258")
#loc4107 = loc("reduce.8264")
#loc4108 = loc("broadcast.8265")
#loc4109 = loc("divide.8266")
#loc4110 = loc("select.8301")
#loc4111 = loc("broadcast.8148")
#loc4112 = loc("reshape.8149")
#loc4113 = loc("convert.8150")
#loc4114 = loc("dot.8302")
#loc4115 = loc("convert.8304")
#loc4116 = loc("transpose.8305")
#loc4117 = loc("reshape.8307")
#loc4118 = loc("reshape.8141")
#loc4119 = loc("reshape.8143")
#loc4120 = loc("transpose.8144")
#loc4122 = loc("reshape.8309")
#loc4123 = loc("add.8312")
#loc4124 = loc("reshape.8343")
#loc4125 = loc("reshape.8345")
#loc4126 = loc("broadcast.8346")
#loc4127 = loc("convert.8313")
#loc4128 = loc("power.8315")
#loc4129 = loc("reduce.8322")
#loc4130 = loc("multiply.8331")
#loc4131 = loc("reshape.8332")
#loc4132 = loc("add.8336")
#loc4133 = loc("rsqrt.8337")
#loc4134 = loc("reshape.8338")
#loc4135 = loc("broadcast.8339")
#loc4136 = loc("multiply.8340")
#loc4137 = loc("convert.8341")
#loc4138 = loc("multiply.8347")
#loc4139 = loc("reshape.8356")
#loc4140 = loc("reshape.8352")
#loc4141 = loc("reshape.8354")
#loc4142 = loc("transpose.8355")
#loc4143 = loc("dot.8357")
#loc4144 = loc("reshape.8358")
#loc4145 = loc("logistic.8359")
#loc4146 = loc("multiply.8360")
#loc4147 = loc("reshape.8132")
#loc4148 = loc("reshape.8134")
#loc4149 = loc("transpose.8135")
#loc4150 = loc("dot.8349")
#loc4151 = loc("reshape.8350")
#loc4152 = loc("multiply.8361")
#loc4153 = loc("reshape.8362")
#loc4154 = loc("reshape.8127")
#loc4155 = loc("reshape.8129")
#loc4156 = loc("transpose.8130")
#loc4158 = loc("reshape.8364")
#loc4159 = loc("add.8367")
#loc4160 = loc("convert.8368")
#loc4161 = loc("power.8370")
#loc4162 = loc("reduce.8377")
#loc4163 = loc("multiply.8386")
#loc4164 = loc("reshape.8387")
#loc4165 = loc("add.8391")
#loc4166 = loc("rsqrt.8392")
#loc4167 = loc("reshape.8393")
#loc4168 = loc("broadcast.8394")
#loc4169 = loc("multiply.8395")
#loc4170 = loc("convert.8396")
#loc4171 = loc("multiply.8402")
#loc4172 = loc("reshape.8403")
#loc4173 = loc("reshape.8118")
#loc4174 = loc("reshape.8120")
#loc4175 = loc("transpose.8121")
#loc4176 = loc("dot.8404")
#loc4177 = loc("reshape.8406")
#loc4178 = loc("convert.8407")
#loc4179 = loc("power.8409")
#loc4180 = loc("reduce.8416")
#loc4181 = loc("multiply.8425")
#loc4182 = loc("reshape.8426")
#loc4183 = loc("add.8430")
#loc4184 = loc("rsqrt.8431")
#loc4185 = loc("reshape.8432")
#loc4186 = loc("broadcast.8433")
#loc4187 = loc("multiply.8434")
#loc4188 = loc("convert.8435")
#loc4189 = loc("multiply.8441")
#loc4190 = loc("transpose.8442")
#loc4191 = loc("multiply.8453")
#loc4192 = loc("slice.8444")
#loc4193 = loc("negate.8445")
#loc4194 = loc("slice.8443")
#loc4195 = loc("concatenate.8446")
#loc4196 = loc("multiply.8449")
#loc4197 = loc("add.8456")
#loc4199 = loc("reshape.8478")
#loc4200 = loc("reshape.8480")
#loc4201 = loc("transpose.8481")
#loc4202 = loc("dot.8483")
#loc4203 = loc("reshape.8485")
#loc4204 = loc("transpose.8486")
#loc4206 = loc("reshape.8832")
#loc4207 = loc("reshape.8834")
#loc4208 = loc("broadcast.8835")
#loc4209 = loc("reshape.8793")
#loc4210 = loc("reshape.8795")
#loc4211 = loc("broadcast.8796")
#loc4212 = loc("reshape.8615")
#loc4213 = loc("reshape.8617")
#loc4214 = loc("broadcast.8618")
#loc4215 = loc("reshape.8577")
#loc4216 = loc("reshape.8579")
#loc4217 = loc("transpose.8580")
#loc4218 = loc("dot.8582")
#loc4219 = loc("reshape.8584")
#loc4220 = loc("convert.8585")
#loc4221 = loc("power.8587")
#loc4222 = loc("reduce.8594")
#loc4223 = loc("multiply.8603")
#loc4224 = loc("reshape.8604")
#loc4225 = loc("add.8608")
#loc4226 = loc("rsqrt.8609")
#loc4227 = loc("reshape.8610")
#loc4228 = loc("broadcast.8611")
#loc4229 = loc("multiply.8612")
#loc4230 = loc("convert.8613")
#loc4231 = loc("multiply.8619")
#loc4232 = loc("transpose.8620")
#loc4233 = loc("multiply.8630")
#loc4234 = loc("slice.8622")
#loc4235 = loc("negate.8623")
#loc4236 = loc("slice.8621")
#loc4237 = loc("concatenate.8624")
#loc4238 = loc("multiply.8627")
#loc4239 = loc("add.8633")
#loc4240 = loc("convert.8634")
#loc4241 = loc("multiply.8636")
#loc4242 = loc("broadcast.8565")
#loc4243 = loc("reshape.8566")
#loc4244 = loc("convert.8567")
#loc4245 = loc("transpose.8568")
#loc4246 = loc("multiply.8570")
#loc4247 = loc("dot.8637")
#loc4248 = loc("add.8644")
#loc4249 = loc("convert.8670")
#loc4250 = loc("compare.8672")
#loc4251 = loc("not.8674")
#loc4253 = loc("or.8684")
#loc4254 = loc("select.8685")
#loc4255 = loc("reshape.8690")
#loc4256 = loc("not.8692")
#loc4257 = loc("reshape.8694")
#loc4258 = loc("broadcast.8695")
#loc4259 = loc("reduce.8650")
#loc4260 = loc("broadcast.8651")
#loc4261 = loc("subtract.8652")
#loc4262 = loc("exponential.8653")
#loc4263 = loc("reduce.8659")
#loc4264 = loc("broadcast.8660")
#loc4265 = loc("divide.8661")
#loc4266 = loc("select.8696")
#loc4267 = loc("broadcast.8543")
#loc4268 = loc("reshape.8544")
#loc4269 = loc("convert.8545")
#loc4270 = loc("dot.8697")
#loc4271 = loc("convert.8699")
#loc4272 = loc("transpose.8700")
#loc4273 = loc("reshape.8702")
#loc4274 = loc("reshape.8536")
#loc4275 = loc("reshape.8538")
#loc4276 = loc("transpose.8539")
#loc4278 = loc("reshape.8704")
#loc4279 = loc("add.8707")
#loc4280 = loc("reshape.8738")
#loc4281 = loc("reshape.8740")
#loc4282 = loc("broadcast.8741")
#loc4283 = loc("convert.8708")
#loc4284 = loc("power.8710")
#loc4285 = loc("reduce.8717")
#loc4286 = loc("multiply.8726")
#loc4287 = loc("reshape.8727")
#loc4288 = loc("add.8731")
#loc4289 = loc("rsqrt.8732")
#loc4290 = loc("reshape.8733")
#loc4291 = loc("broadcast.8734")
#loc4292 = loc("multiply.8735")
#loc4293 = loc("convert.8736")
#loc4294 = loc("multiply.8742")
#loc4295 = loc("reshape.8751")
#loc4296 = loc("reshape.8747")
#loc4297 = loc("reshape.8749")
#loc4298 = loc("transpose.8750")
#loc4299 = loc("dot.8752")
#loc4300 = loc("reshape.8753")
#loc4301 = loc("logistic.8754")
#loc4302 = loc("multiply.8755")
#loc4303 = loc("reshape.8527")
#loc4304 = loc("reshape.8529")
#loc4305 = loc("transpose.8530")
#loc4306 = loc("dot.8744")
#loc4307 = loc("reshape.8745")
#loc4308 = loc("multiply.8756")
#loc4309 = loc("reshape.8757")
#loc4310 = loc("reshape.8522")
#loc4311 = loc("reshape.8524")
#loc4312 = loc("transpose.8525")
#loc4314 = loc("reshape.8759")
#loc4315 = loc("add.8762")
#loc4316 = loc("convert.8763")
#loc4317 = loc("power.8765")
#loc4318 = loc("reduce.8772")
#loc4319 = loc("multiply.8781")
#loc4320 = loc("reshape.8782")
#loc4321 = loc("add.8786")
#loc4322 = loc("rsqrt.8787")
#loc4323 = loc("reshape.8788")
#loc4324 = loc("broadcast.8789")
#loc4325 = loc("multiply.8790")
#loc4326 = loc("convert.8791")
#loc4327 = loc("multiply.8797")
#loc4328 = loc("reshape.8798")
#loc4329 = loc("reshape.8513")
#loc4330 = loc("reshape.8515")
#loc4331 = loc("transpose.8516")
#loc4332 = loc("dot.8799")
#loc4333 = loc("reshape.8801")
#loc4334 = loc("convert.8802")
#loc4335 = loc("power.8804")
#loc4336 = loc("reduce.8811")
#loc4337 = loc("multiply.8820")
#loc4338 = loc("reshape.8821")
#loc4339 = loc("add.8825")
#loc4340 = loc("rsqrt.8826")
#loc4341 = loc("reshape.8827")
#loc4342 = loc("broadcast.8828")
#loc4343 = loc("multiply.8829")
#loc4344 = loc("convert.8830")
#loc4345 = loc("multiply.8836")
#loc4346 = loc("transpose.8837")
#loc4347 = loc("multiply.8848")
#loc4348 = loc("slice.8839")
#loc4349 = loc("negate.8840")
#loc4350 = loc("slice.8838")
#loc4351 = loc("concatenate.8841")
#loc4352 = loc("multiply.8844")
#loc4353 = loc("add.8851")
#loc4355 = loc("reshape.8873")
#loc4356 = loc("reshape.8875")
#loc4357 = loc("transpose.8876")
#loc4358 = loc("dot.8878")
#loc4359 = loc("reshape.8880")
#loc4360 = loc("transpose.8881")
#loc4362 = loc("reshape.9227")
#loc4363 = loc("reshape.9229")
#loc4364 = loc("broadcast.9230")
#loc4365 = loc("reshape.9188")
#loc4366 = loc("reshape.9190")
#loc4367 = loc("broadcast.9191")
#loc4368 = loc("reshape.9010")
#loc4369 = loc("reshape.9012")
#loc4370 = loc("broadcast.9013")
#loc4371 = loc("reshape.8972")
#loc4372 = loc("reshape.8974")
#loc4373 = loc("transpose.8975")
#loc4374 = loc("dot.8977")
#loc4375 = loc("reshape.8979")
#loc4376 = loc("convert.8980")
#loc4377 = loc("power.8982")
#loc4378 = loc("reduce.8989")
#loc4379 = loc("multiply.8998")
#loc4380 = loc("reshape.8999")
#loc4381 = loc("add.9003")
#loc4382 = loc("rsqrt.9004")
#loc4383 = loc("reshape.9005")
#loc4384 = loc("broadcast.9006")
#loc4385 = loc("multiply.9007")
#loc4386 = loc("convert.9008")
#loc4387 = loc("multiply.9014")
#loc4388 = loc("transpose.9015")
#loc4389 = loc("multiply.9025")
#loc4390 = loc("slice.9017")
#loc4391 = loc("negate.9018")
#loc4392 = loc("slice.9016")
#loc4393 = loc("concatenate.9019")
#loc4394 = loc("multiply.9022")
#loc4395 = loc("add.9028")
#loc4396 = loc("convert.9029")
#loc4397 = loc("multiply.9031")
#loc4398 = loc("broadcast.8960")
#loc4399 = loc("reshape.8961")
#loc4400 = loc("convert.8962")
#loc4401 = loc("transpose.8963")
#loc4402 = loc("multiply.8965")
#loc4403 = loc("dot.9032")
#loc4404 = loc("add.9039")
#loc4405 = loc("convert.9065")
#loc4406 = loc("compare.9067")
#loc4407 = loc("not.9069")
#loc4409 = loc("or.9079")
#loc4410 = loc("select.9080")
#loc4411 = loc("reshape.9085")
#loc4412 = loc("not.9087")
#loc4413 = loc("reshape.9089")
#loc4414 = loc("broadcast.9090")
#loc4415 = loc("reduce.9045")
#loc4416 = loc("broadcast.9046")
#loc4417 = loc("subtract.9047")
#loc4418 = loc("exponential.9048")
#loc4419 = loc("reduce.9054")
#loc4420 = loc("broadcast.9055")
#loc4421 = loc("divide.9056")
#loc4422 = loc("select.9091")
#loc4423 = loc("broadcast.8938")
#loc4424 = loc("reshape.8939")
#loc4425 = loc("convert.8940")
#loc4426 = loc("dot.9092")
#loc4427 = loc("convert.9094")
#loc4428 = loc("transpose.9095")
#loc4429 = loc("reshape.9097")
#loc4430 = loc("reshape.8931")
#loc4431 = loc("reshape.8933")
#loc4432 = loc("transpose.8934")
#loc4434 = loc("reshape.9099")
#loc4435 = loc("add.9102")
#loc4436 = loc("reshape.9133")
#loc4437 = loc("reshape.9135")
#loc4438 = loc("broadcast.9136")
#loc4439 = loc("convert.9103")
#loc4440 = loc("power.9105")
#loc4441 = loc("reduce.9112")
#loc4442 = loc("multiply.9121")
#loc4443 = loc("reshape.9122")
#loc4444 = loc("add.9126")
#loc4445 = loc("rsqrt.9127")
#loc4446 = loc("reshape.9128")
#loc4447 = loc("broadcast.9129")
#loc4448 = loc("multiply.9130")
#loc4449 = loc("convert.9131")
#loc4450 = loc("multiply.9137")
#loc4451 = loc("reshape.9146")
#loc4452 = loc("reshape.9142")
#loc4453 = loc("reshape.9144")
#loc4454 = loc("transpose.9145")
#loc4455 = loc("dot.9147")
#loc4456 = loc("reshape.9148")
#loc4457 = loc("logistic.9149")
#loc4458 = loc("multiply.9150")
#loc4459 = loc("reshape.8922")
#loc4460 = loc("reshape.8924")
#loc4461 = loc("transpose.8925")
#loc4462 = loc("dot.9139")
#loc4463 = loc("reshape.9140")
#loc4464 = loc("multiply.9151")
#loc4465 = loc("reshape.9152")
#loc4466 = loc("reshape.8917")
#loc4467 = loc("reshape.8919")
#loc4468 = loc("transpose.8920")
#loc4470 = loc("reshape.9154")
#loc4471 = loc("add.9157")
#loc4472 = loc("convert.9158")
#loc4473 = loc("power.9160")
#loc4474 = loc("reduce.9167")
#loc4475 = loc("multiply.9176")
#loc4476 = loc("reshape.9177")
#loc4477 = loc("add.9181")
#loc4478 = loc("rsqrt.9182")
#loc4479 = loc("reshape.9183")
#loc4480 = loc("broadcast.9184")
#loc4481 = loc("multiply.9185")
#loc4482 = loc("convert.9186")
#loc4483 = loc("multiply.9192")
#loc4484 = loc("reshape.9193")
#loc4485 = loc("reshape.8908")
#loc4486 = loc("reshape.8910")
#loc4487 = loc("transpose.8911")
#loc4488 = loc("dot.9194")
#loc4489 = loc("reshape.9196")
#loc4490 = loc("convert.9197")
#loc4491 = loc("power.9199")
#loc4492 = loc("reduce.9206")
#loc4493 = loc("multiply.9215")
#loc4494 = loc("reshape.9216")
#loc4495 = loc("add.9220")
#loc4496 = loc("rsqrt.9221")
#loc4497 = loc("reshape.9222")
#loc4498 = loc("broadcast.9223")
#loc4499 = loc("multiply.9224")
#loc4500 = loc("convert.9225")
#loc4501 = loc("multiply.9231")
#loc4502 = loc("transpose.9232")
#loc4503 = loc("multiply.9243")
#loc4504 = loc("slice.9234")
#loc4505 = loc("negate.9235")
#loc4506 = loc("slice.9233")
#loc4507 = loc("concatenate.9236")
#loc4508 = loc("multiply.9239")
#loc4509 = loc("add.9246")
#loc4511 = loc("reshape.9268")
#loc4512 = loc("reshape.9270")
#loc4513 = loc("transpose.9271")
#loc4514 = loc("dot.9273")
#loc4515 = loc("reshape.9275")
#loc4516 = loc("transpose.9276")
#loc4518 = loc("reshape.9622")
#loc4519 = loc("reshape.9624")
#loc4520 = loc("broadcast.9625")
#loc4521 = loc("reshape.9583")
#loc4522 = loc("reshape.9585")
#loc4523 = loc("broadcast.9586")
#loc4524 = loc("reshape.9405")
#loc4525 = loc("reshape.9407")
#loc4526 = loc("broadcast.9408")
#loc4527 = loc("reshape.9367")
#loc4528 = loc("reshape.9369")
#loc4529 = loc("transpose.9370")
#loc4530 = loc("dot.9372")
#loc4531 = loc("reshape.9374")
#loc4532 = loc("convert.9375")
#loc4533 = loc("power.9377")
#loc4534 = loc("reduce.9384")
#loc4535 = loc("multiply.9393")
#loc4536 = loc("reshape.9394")
#loc4537 = loc("add.9398")
#loc4538 = loc("rsqrt.9399")
#loc4539 = loc("reshape.9400")
#loc4540 = loc("broadcast.9401")
#loc4541 = loc("multiply.9402")
#loc4542 = loc("convert.9403")
#loc4543 = loc("multiply.9409")
#loc4544 = loc("transpose.9410")
#loc4545 = loc("multiply.9420")
#loc4546 = loc("slice.9412")
#loc4547 = loc("negate.9413")
#loc4548 = loc("slice.9411")
#loc4549 = loc("concatenate.9414")
#loc4550 = loc("multiply.9417")
#loc4551 = loc("add.9423")
#loc4552 = loc("convert.9424")
#loc4553 = loc("multiply.9426")
#loc4554 = loc("broadcast.9355")
#loc4555 = loc("reshape.9356")
#loc4556 = loc("convert.9357")
#loc4557 = loc("transpose.9358")
#loc4558 = loc("multiply.9360")
#loc4559 = loc("dot.9427")
#loc4560 = loc("add.9434")
#loc4561 = loc("convert.9460")
#loc4562 = loc("compare.9462")
#loc4563 = loc("not.9464")
#loc4565 = loc("or.9474")
#loc4566 = loc("select.9475")
#loc4567 = loc("reshape.9480")
#loc4568 = loc("not.9482")
#loc4569 = loc("reshape.9484")
#loc4570 = loc("broadcast.9485")
#loc4571 = loc("reduce.9440")
#loc4572 = loc("broadcast.9441")
#loc4573 = loc("subtract.9442")
#loc4574 = loc("exponential.9443")
#loc4575 = loc("reduce.9449")
#loc4576 = loc("broadcast.9450")
#loc4577 = loc("divide.9451")
#loc4578 = loc("select.9486")
#loc4579 = loc("broadcast.9333")
#loc4580 = loc("reshape.9334")
#loc4581 = loc("convert.9335")
#loc4582 = loc("dot.9487")
#loc4583 = loc("convert.9489")
#loc4584 = loc("transpose.9490")
#loc4585 = loc("reshape.9492")
#loc4586 = loc("reshape.9326")
#loc4587 = loc("reshape.9328")
#loc4588 = loc("transpose.9329")
#loc4590 = loc("reshape.9494")
#loc4591 = loc("add.9497")
#loc4592 = loc("reshape.9528")
#loc4593 = loc("reshape.9530")
#loc4594 = loc("broadcast.9531")
#loc4595 = loc("convert.9498")
#loc4596 = loc("power.9500")
#loc4597 = loc("reduce.9507")
#loc4598 = loc("multiply.9516")
#loc4599 = loc("reshape.9517")
#loc4600 = loc("add.9521")
#loc4601 = loc("rsqrt.9522")
#loc4602 = loc("reshape.9523")
#loc4603 = loc("broadcast.9524")
#loc4604 = loc("multiply.9525")
#loc4605 = loc("convert.9526")
#loc4606 = loc("multiply.9532")
#loc4607 = loc("reshape.9541")
#loc4608 = loc("reshape.9537")
#loc4609 = loc("reshape.9539")
#loc4610 = loc("transpose.9540")
#loc4611 = loc("dot.9542")
#loc4612 = loc("reshape.9543")
#loc4613 = loc("logistic.9544")
#loc4614 = loc("multiply.9545")
#loc4615 = loc("reshape.9317")
#loc4616 = loc("reshape.9319")
#loc4617 = loc("transpose.9320")
#loc4618 = loc("dot.9534")
#loc4619 = loc("reshape.9535")
#loc4620 = loc("multiply.9546")
#loc4621 = loc("reshape.9547")
#loc4622 = loc("reshape.9312")
#loc4623 = loc("reshape.9314")
#loc4624 = loc("transpose.9315")
#loc4626 = loc("reshape.9549")
#loc4627 = loc("add.9552")
#loc4628 = loc("convert.9553")
#loc4629 = loc("power.9555")
#loc4630 = loc("reduce.9562")
#loc4631 = loc("multiply.9571")
#loc4632 = loc("reshape.9572")
#loc4633 = loc("add.9576")
#loc4634 = loc("rsqrt.9577")
#loc4635 = loc("reshape.9578")
#loc4636 = loc("broadcast.9579")
#loc4637 = loc("multiply.9580")
#loc4638 = loc("convert.9581")
#loc4639 = loc("multiply.9587")
#loc4640 = loc("reshape.9588")
#loc4641 = loc("reshape.9303")
#loc4642 = loc("reshape.9305")
#loc4643 = loc("transpose.9306")
#loc4644 = loc("dot.9589")
#loc4645 = loc("reshape.9591")
#loc4646 = loc("convert.9592")
#loc4647 = loc("power.9594")
#loc4648 = loc("reduce.9601")
#loc4649 = loc("multiply.9610")
#loc4650 = loc("reshape.9611")
#loc4651 = loc("add.9615")
#loc4652 = loc("rsqrt.9616")
#loc4653 = loc("reshape.9617")
#loc4654 = loc("broadcast.9618")
#loc4655 = loc("multiply.9619")
#loc4656 = loc("convert.9620")
#loc4657 = loc("multiply.9626")
#loc4658 = loc("transpose.9627")
#loc4659 = loc("multiply.9638")
#loc4660 = loc("slice.9629")
#loc4661 = loc("negate.9630")
#loc4662 = loc("slice.9628")
#loc4663 = loc("concatenate.9631")
#loc4664 = loc("multiply.9634")
#loc4665 = loc("add.9641")
#loc4667 = loc("reshape.9663")
#loc4668 = loc("reshape.9665")
#loc4669 = loc("transpose.9666")
#loc4670 = loc("dot.9668")
#loc4671 = loc("reshape.9670")
#loc4672 = loc("transpose.9671")
#loc4674 = loc("reshape.10017")
#loc4675 = loc("reshape.10019")
#loc4676 = loc("broadcast.10020")
#loc4677 = loc("reshape.9978")
#loc4678 = loc("reshape.9980")
#loc4679 = loc("broadcast.9981")
#loc4680 = loc("reshape.9800")
#loc4681 = loc("reshape.9802")
#loc4682 = loc("broadcast.9803")
#loc4683 = loc("reshape.9762")
#loc4684 = loc("reshape.9764")
#loc4685 = loc("transpose.9765")
#loc4686 = loc("dot.9767")
#loc4687 = loc("reshape.9769")
#loc4688 = loc("convert.9770")
#loc4689 = loc("power.9772")
#loc4690 = loc("reduce.9779")
#loc4691 = loc("multiply.9788")
#loc4692 = loc("reshape.9789")
#loc4693 = loc("add.9793")
#loc4694 = loc("rsqrt.9794")
#loc4695 = loc("reshape.9795")
#loc4696 = loc("broadcast.9796")
#loc4697 = loc("multiply.9797")
#loc4698 = loc("convert.9798")
#loc4699 = loc("multiply.9804")
#loc4700 = loc("transpose.9805")
#loc4701 = loc("multiply.9815")
#loc4702 = loc("slice.9807")
#loc4703 = loc("negate.9808")
#loc4704 = loc("slice.9806")
#loc4705 = loc("concatenate.9809")
#loc4706 = loc("multiply.9812")
#loc4707 = loc("add.9818")
#loc4708 = loc("convert.9819")
#loc4709 = loc("multiply.9821")
#loc4710 = loc("broadcast.9750")
#loc4711 = loc("reshape.9751")
#loc4712 = loc("convert.9752")
#loc4713 = loc("transpose.9753")
#loc4714 = loc("multiply.9755")
#loc4715 = loc("dot.9822")
#loc4716 = loc("add.9829")
#loc4717 = loc("convert.9855")
#loc4718 = loc("compare.9857")
#loc4719 = loc("not.9859")
#loc4721 = loc("or.9869")
#loc4722 = loc("select.9870")
#loc4723 = loc("reshape.9875")
#loc4724 = loc("not.9877")
#loc4725 = loc("reshape.9879")
#loc4726 = loc("broadcast.9880")
#loc4727 = loc("reduce.9835")
#loc4728 = loc("broadcast.9836")
#loc4729 = loc("subtract.9837")
#loc4730 = loc("exponential.9838")
#loc4731 = loc("reduce.9844")
#loc4732 = loc("broadcast.9845")
#loc4733 = loc("divide.9846")
#loc4734 = loc("select.9881")
#loc4735 = loc("broadcast.9728")
#loc4736 = loc("reshape.9729")
#loc4737 = loc("convert.9730")
#loc4738 = loc("dot.9882")
#loc4739 = loc("convert.9884")
#loc4740 = loc("transpose.9885")
#loc4741 = loc("reshape.9887")
#loc4742 = loc("reshape.9721")
#loc4743 = loc("reshape.9723")
#loc4744 = loc("transpose.9724")
#loc4746 = loc("reshape.9889")
#loc4747 = loc("add.9892")
#loc4748 = loc("reshape.9923")
#loc4749 = loc("reshape.9925")
#loc4750 = loc("broadcast.9926")
#loc4751 = loc("convert.9893")
#loc4752 = loc("power.9895")
#loc4753 = loc("reduce.9902")
#loc4754 = loc("multiply.9911")
#loc4755 = loc("reshape.9912")
#loc4756 = loc("add.9916")
#loc4757 = loc("rsqrt.9917")
#loc4758 = loc("reshape.9918")
#loc4759 = loc("broadcast.9919")
#loc4760 = loc("multiply.9920")
#loc4761 = loc("convert.9921")
#loc4762 = loc("multiply.9927")
#loc4763 = loc("reshape.9936")
#loc4764 = loc("reshape.9932")
#loc4765 = loc("reshape.9934")
#loc4766 = loc("transpose.9935")
#loc4767 = loc("dot.9937")
#loc4768 = loc("reshape.9938")
#loc4769 = loc("logistic.9939")
#loc4770 = loc("multiply.9940")
#loc4771 = loc("reshape.9712")
#loc4772 = loc("reshape.9714")
#loc4773 = loc("transpose.9715")
#loc4774 = loc("dot.9929")
#loc4775 = loc("reshape.9930")
#loc4776 = loc("multiply.9941")
#loc4777 = loc("reshape.9942")
#loc4778 = loc("reshape.9707")
#loc4779 = loc("reshape.9709")
#loc4780 = loc("transpose.9710")
#loc4782 = loc("reshape.9944")
#loc4783 = loc("add.9947")
#loc4784 = loc("convert.9948")
#loc4785 = loc("power.9950")
#loc4786 = loc("reduce.9957")
#loc4787 = loc("multiply.9966")
#loc4788 = loc("reshape.9967")
#loc4789 = loc("add.9971")
#loc4790 = loc("rsqrt.9972")
#loc4791 = loc("reshape.9973")
#loc4792 = loc("broadcast.9974")
#loc4793 = loc("multiply.9975")
#loc4794 = loc("convert.9976")
#loc4795 = loc("multiply.9982")
#loc4796 = loc("reshape.9983")
#loc4797 = loc("reshape.9698")
#loc4798 = loc("reshape.9700")
#loc4799 = loc("transpose.9701")
#loc4800 = loc("dot.9984")
#loc4801 = loc("reshape.9986")
#loc4802 = loc("convert.9987")
#loc4803 = loc("power.9989")
#loc4804 = loc("reduce.9996")
#loc4805 = loc("multiply.10005")
#loc4806 = loc("reshape.10006")
#loc4807 = loc("add.10010")
#loc4808 = loc("rsqrt.10011")
#loc4809 = loc("reshape.10012")
#loc4810 = loc("broadcast.10013")
#loc4811 = loc("multiply.10014")
#loc4812 = loc("convert.10015")
#loc4813 = loc("multiply.10021")
#loc4814 = loc("transpose.10022")
#loc4815 = loc("multiply.10033")
#loc4816 = loc("slice.10024")
#loc4817 = loc("negate.10025")
#loc4818 = loc("slice.10023")
#loc4819 = loc("concatenate.10026")
#loc4820 = loc("multiply.10029")
#loc4821 = loc("add.10036")
#loc4823 = loc("reshape.10058")
#loc4824 = loc("reshape.10060")
#loc4825 = loc("transpose.10061")
#loc4826 = loc("dot.10063")
#loc4827 = loc("reshape.10065")
#loc4828 = loc("transpose.10066")
#loc4830 = loc("reshape.10412")
#loc4831 = loc("reshape.10414")
#loc4832 = loc("broadcast.10415")
#loc4833 = loc("reshape.10373")
#loc4834 = loc("reshape.10375")
#loc4835 = loc("broadcast.10376")
#loc4836 = loc("reshape.10195")
#loc4837 = loc("reshape.10197")
#loc4838 = loc("broadcast.10198")
#loc4839 = loc("reshape.10157")
#loc4840 = loc("reshape.10159")
#loc4841 = loc("transpose.10160")
#loc4842 = loc("dot.10162")
#loc4843 = loc("reshape.10164")
#loc4844 = loc("convert.10165")
#loc4845 = loc("power.10167")
#loc4846 = loc("reduce.10174")
#loc4847 = loc("multiply.10183")
#loc4848 = loc("reshape.10184")
#loc4849 = loc("add.10188")
#loc4850 = loc("rsqrt.10189")
#loc4851 = loc("reshape.10190")
#loc4852 = loc("broadcast.10191")
#loc4853 = loc("multiply.10192")
#loc4854 = loc("convert.10193")
#loc4855 = loc("multiply.10199")
#loc4856 = loc("transpose.10200")
#loc4857 = loc("multiply.10210")
#loc4858 = loc("slice.10202")
#loc4859 = loc("negate.10203")
#loc4860 = loc("slice.10201")
#loc4861 = loc("concatenate.10204")
#loc4862 = loc("multiply.10207")
#loc4863 = loc("add.10213")
#loc4864 = loc("convert.10214")
#loc4865 = loc("multiply.10216")
#loc4866 = loc("broadcast.10145")
#loc4867 = loc("reshape.10146")
#loc4868 = loc("convert.10147")
#loc4869 = loc("transpose.10148")
#loc4870 = loc("multiply.10150")
#loc4871 = loc("dot.10217")
#loc4872 = loc("add.10224")
#loc4873 = loc("convert.10250")
#loc4874 = loc("compare.10252")
#loc4875 = loc("not.10254")
#loc4877 = loc("or.10264")
#loc4878 = loc("select.10265")
#loc4879 = loc("reshape.10270")
#loc4880 = loc("not.10272")
#loc4881 = loc("reshape.10274")
#loc4882 = loc("broadcast.10275")
#loc4883 = loc("reduce.10230")
#loc4884 = loc("broadcast.10231")
#loc4885 = loc("subtract.10232")
#loc4886 = loc("exponential.10233")
#loc4887 = loc("reduce.10239")
#loc4888 = loc("broadcast.10240")
#loc4889 = loc("divide.10241")
#loc4890 = loc("select.10276")
#loc4891 = loc("broadcast.10123")
#loc4892 = loc("reshape.10124")
#loc4893 = loc("convert.10125")
#loc4894 = loc("dot.10277")
#loc4895 = loc("convert.10279")
#loc4896 = loc("transpose.10280")
#loc4897 = loc("reshape.10282")
#loc4898 = loc("reshape.10116")
#loc4899 = loc("reshape.10118")
#loc4900 = loc("transpose.10119")
#loc4902 = loc("reshape.10284")
#loc4903 = loc("add.10287")
#loc4904 = loc("reshape.10318")
#loc4905 = loc("reshape.10320")
#loc4906 = loc("broadcast.10321")
#loc4907 = loc("convert.10288")
#loc4908 = loc("power.10290")
#loc4909 = loc("reduce.10297")
#loc4910 = loc("multiply.10306")
#loc4911 = loc("reshape.10307")
#loc4912 = loc("add.10311")
#loc4913 = loc("rsqrt.10312")
#loc4914 = loc("reshape.10313")
#loc4915 = loc("broadcast.10314")
#loc4916 = loc("multiply.10315")
#loc4917 = loc("convert.10316")
#loc4918 = loc("multiply.10322")
#loc4919 = loc("reshape.10331")
#loc4920 = loc("reshape.10327")
#loc4921 = loc("reshape.10329")
#loc4922 = loc("transpose.10330")
#loc4923 = loc("dot.10332")
#loc4924 = loc("reshape.10333")
#loc4925 = loc("logistic.10334")
#loc4926 = loc("multiply.10335")
#loc4927 = loc("reshape.10107")
#loc4928 = loc("reshape.10109")
#loc4929 = loc("transpose.10110")
#loc4930 = loc("dot.10324")
#loc4931 = loc("reshape.10325")
#loc4932 = loc("multiply.10336")
#loc4933 = loc("reshape.10337")
#loc4934 = loc("reshape.10102")
#loc4935 = loc("reshape.10104")
#loc4936 = loc("transpose.10105")
#loc4938 = loc("reshape.10339")
#loc4939 = loc("add.10342")
#loc4940 = loc("convert.10343")
#loc4941 = loc("power.10345")
#loc4942 = loc("reduce.10352")
#loc4943 = loc("multiply.10361")
#loc4944 = loc("reshape.10362")
#loc4945 = loc("add.10366")
#loc4946 = loc("rsqrt.10367")
#loc4947 = loc("reshape.10368")
#loc4948 = loc("broadcast.10369")
#loc4949 = loc("multiply.10370")
#loc4950 = loc("convert.10371")
#loc4951 = loc("multiply.10377")
#loc4952 = loc("reshape.10378")
#loc4953 = loc("reshape.10093")
#loc4954 = loc("reshape.10095")
#loc4955 = loc("transpose.10096")
#loc4956 = loc("dot.10379")
#loc4957 = loc("reshape.10381")
#loc4958 = loc("convert.10382")
#loc4959 = loc("power.10384")
#loc4960 = loc("reduce.10391")
#loc4961 = loc("multiply.10400")
#loc4962 = loc("reshape.10401")
#loc4963 = loc("add.10405")
#loc4964 = loc("rsqrt.10406")
#loc4965 = loc("reshape.10407")
#loc4966 = loc("broadcast.10408")
#loc4967 = loc("multiply.10409")
#loc4968 = loc("convert.10410")
#loc4969 = loc("multiply.10416")
#loc4970 = loc("transpose.10417")
#loc4971 = loc("multiply.10428")
#loc4972 = loc("slice.10419")
#loc4973 = loc("negate.10420")
#loc4974 = loc("slice.10418")
#loc4975 = loc("concatenate.10421")
#loc4976 = loc("multiply.10424")
#loc4977 = loc("add.10431")
#loc4979 = loc("reshape.10453")
#loc4980 = loc("reshape.10455")
#loc4981 = loc("transpose.10456")
#loc4982 = loc("dot.10458")
#loc4983 = loc("reshape.10460")
#loc4984 = loc("transpose.10461")
#loc4986 = loc("reshape.10807")
#loc4987 = loc("reshape.10809")
#loc4988 = loc("broadcast.10810")
#loc4989 = loc("reshape.10768")
#loc4990 = loc("reshape.10770")
#loc4991 = loc("broadcast.10771")
#loc4992 = loc("reshape.10590")
#loc4993 = loc("reshape.10592")
#loc4994 = loc("broadcast.10593")
#loc4995 = loc("reshape.10552")
#loc4996 = loc("reshape.10554")
#loc4997 = loc("transpose.10555")
#loc4998 = loc("dot.10557")
#loc4999 = loc("reshape.10559")
#loc5000 = loc("convert.10560")
#loc5001 = loc("power.10562")
#loc5002 = loc("reduce.10569")
#loc5003 = loc("multiply.10578")
#loc5004 = loc("reshape.10579")
#loc5005 = loc("add.10583")
#loc5006 = loc("rsqrt.10584")
#loc5007 = loc("reshape.10585")
#loc5008 = loc("broadcast.10586")
#loc5009 = loc("multiply.10587")
#loc5010 = loc("convert.10588")
#loc5011 = loc("multiply.10594")
#loc5012 = loc("transpose.10595")
#loc5013 = loc("multiply.10605")
#loc5014 = loc("slice.10597")
#loc5015 = loc("negate.10598")
#loc5016 = loc("slice.10596")
#loc5017 = loc("concatenate.10599")
#loc5018 = loc("multiply.10602")
#loc5019 = loc("add.10608")
#loc5020 = loc("convert.10609")
#loc5021 = loc("multiply.10611")
#loc5022 = loc("broadcast.10540")
#loc5023 = loc("reshape.10541")
#loc5024 = loc("convert.10542")
#loc5025 = loc("transpose.10543")
#loc5026 = loc("multiply.10545")
#loc5027 = loc("dot.10612")
#loc5028 = loc("add.10619")
#loc5029 = loc("convert.10645")
#loc5030 = loc("compare.10647")
#loc5031 = loc("not.10649")
#loc5033 = loc("or.10659")
#loc5034 = loc("select.10660")
#loc5035 = loc("reshape.10665")
#loc5036 = loc("not.10667")
#loc5037 = loc("reshape.10669")
#loc5038 = loc("broadcast.10670")
#loc5039 = loc("reduce.10625")
#loc5040 = loc("broadcast.10626")
#loc5041 = loc("subtract.10627")
#loc5042 = loc("exponential.10628")
#loc5043 = loc("reduce.10634")
#loc5044 = loc("broadcast.10635")
#loc5045 = loc("divide.10636")
#loc5046 = loc("select.10671")
#loc5047 = loc("broadcast.10518")
#loc5048 = loc("reshape.10519")
#loc5049 = loc("convert.10520")
#loc5050 = loc("dot.10672")
#loc5051 = loc("convert.10674")
#loc5052 = loc("transpose.10675")
#loc5053 = loc("reshape.10677")
#loc5054 = loc("reshape.10511")
#loc5055 = loc("reshape.10513")
#loc5056 = loc("transpose.10514")
#loc5058 = loc("reshape.10679")
#loc5059 = loc("add.10682")
#loc5060 = loc("reshape.10713")
#loc5061 = loc("reshape.10715")
#loc5062 = loc("broadcast.10716")
#loc5063 = loc("convert.10683")
#loc5064 = loc("power.10685")
#loc5065 = loc("reduce.10692")
#loc5066 = loc("multiply.10701")
#loc5067 = loc("reshape.10702")
#loc5068 = loc("add.10706")
#loc5069 = loc("rsqrt.10707")
#loc5070 = loc("reshape.10708")
#loc5071 = loc("broadcast.10709")
#loc5072 = loc("multiply.10710")
#loc5073 = loc("convert.10711")
#loc5074 = loc("multiply.10717")
#loc5075 = loc("reshape.10726")
#loc5076 = loc("reshape.10722")
#loc5077 = loc("reshape.10724")
#loc5078 = loc("transpose.10725")
#loc5079 = loc("dot.10727")
#loc5080 = loc("reshape.10728")
#loc5081 = loc("logistic.10729")
#loc5082 = loc("multiply.10730")
#loc5083 = loc("reshape.10502")
#loc5084 = loc("reshape.10504")
#loc5085 = loc("transpose.10505")
#loc5086 = loc("dot.10719")
#loc5087 = loc("reshape.10720")
#loc5088 = loc("multiply.10731")
#loc5089 = loc("reshape.10732")
#loc5090 = loc("reshape.10497")
#loc5091 = loc("reshape.10499")
#loc5092 = loc("transpose.10500")
#loc5094 = loc("reshape.10734")
#loc5095 = loc("add.10737")
#loc5096 = loc("convert.10738")
#loc5097 = loc("power.10740")
#loc5098 = loc("reduce.10747")
#loc5099 = loc("multiply.10756")
#loc5100 = loc("reshape.10757")
#loc5101 = loc("add.10761")
#loc5102 = loc("rsqrt.10762")
#loc5103 = loc("reshape.10763")
#loc5104 = loc("broadcast.10764")
#loc5105 = loc("multiply.10765")
#loc5106 = loc("convert.10766")
#loc5107 = loc("multiply.10772")
#loc5108 = loc("reshape.10773")
#loc5109 = loc("reshape.10488")
#loc5110 = loc("reshape.10490")
#loc5111 = loc("transpose.10491")
#loc5112 = loc("dot.10774")
#loc5113 = loc("reshape.10776")
#loc5114 = loc("convert.10777")
#loc5115 = loc("power.10779")
#loc5116 = loc("reduce.10786")
#loc5117 = loc("multiply.10795")
#loc5118 = loc("reshape.10796")
#loc5119 = loc("add.10800")
#loc5120 = loc("rsqrt.10801")
#loc5121 = loc("reshape.10802")
#loc5122 = loc("broadcast.10803")
#loc5123 = loc("multiply.10804")
#loc5124 = loc("convert.10805")
#loc5125 = loc("multiply.10811")
#loc5126 = loc("transpose.10812")
#loc5127 = loc("multiply.10823")
#loc5128 = loc("slice.10814")
#loc5129 = loc("negate.10815")
#loc5130 = loc("slice.10813")
#loc5131 = loc("concatenate.10816")
#loc5132 = loc("multiply.10819")
#loc5133 = loc("add.10826")
#loc5135 = loc("reshape.10848")
#loc5136 = loc("reshape.10850")
#loc5137 = loc("transpose.10851")
#loc5138 = loc("dot.10853")
#loc5139 = loc("reshape.10855")
#loc5140 = loc("transpose.10856")
#loc5142 = loc("reshape.11202")
#loc5143 = loc("reshape.11204")
#loc5144 = loc("broadcast.11205")
#loc5145 = loc("reshape.11163")
#loc5146 = loc("reshape.11165")
#loc5147 = loc("broadcast.11166")
#loc5148 = loc("reshape.10985")
#loc5149 = loc("reshape.10987")
#loc5150 = loc("broadcast.10988")
#loc5151 = loc("reshape.10947")
#loc5152 = loc("reshape.10949")
#loc5153 = loc("transpose.10950")
#loc5154 = loc("dot.10952")
#loc5155 = loc("reshape.10954")
#loc5156 = loc("convert.10955")
#loc5157 = loc("power.10957")
#loc5158 = loc("reduce.10964")
#loc5159 = loc("multiply.10973")
#loc5160 = loc("reshape.10974")
#loc5161 = loc("add.10978")
#loc5162 = loc("rsqrt.10979")
#loc5163 = loc("reshape.10980")
#loc5164 = loc("broadcast.10981")
#loc5165 = loc("multiply.10982")
#loc5166 = loc("convert.10983")
#loc5167 = loc("multiply.10989")
#loc5168 = loc("transpose.10990")
#loc5169 = loc("multiply.11000")
#loc5170 = loc("slice.10992")
#loc5171 = loc("negate.10993")
#loc5172 = loc("slice.10991")
#loc5173 = loc("concatenate.10994")
#loc5174 = loc("multiply.10997")
#loc5175 = loc("add.11003")
#loc5176 = loc("convert.11004")
#loc5177 = loc("multiply.11006")
#loc5178 = loc("broadcast.10935")
#loc5179 = loc("reshape.10936")
#loc5180 = loc("convert.10937")
#loc5181 = loc("transpose.10938")
#loc5182 = loc("multiply.10940")
#loc5183 = loc("dot.11007")
#loc5184 = loc("add.11014")
#loc5185 = loc("convert.11040")
#loc5186 = loc("compare.11042")
#loc5187 = loc("not.11044")
#loc5189 = loc("or.11054")
#loc5190 = loc("select.11055")
#loc5191 = loc("reshape.11060")
#loc5192 = loc("not.11062")
#loc5193 = loc("reshape.11064")
#loc5194 = loc("broadcast.11065")
#loc5195 = loc("reduce.11020")
#loc5196 = loc("broadcast.11021")
#loc5197 = loc("subtract.11022")
#loc5198 = loc("exponential.11023")
#loc5199 = loc("reduce.11029")
#loc5200 = loc("broadcast.11030")
#loc5201 = loc("divide.11031")
#loc5202 = loc("select.11066")
#loc5203 = loc("broadcast.10913")
#loc5204 = loc("reshape.10914")
#loc5205 = loc("convert.10915")
#loc5206 = loc("dot.11067")
#loc5207 = loc("convert.11069")
#loc5208 = loc("transpose.11070")
#loc5209 = loc("reshape.11072")
#loc5210 = loc("reshape.10906")
#loc5211 = loc("reshape.10908")
#loc5212 = loc("transpose.10909")
#loc5214 = loc("reshape.11074")
#loc5215 = loc("add.11077")
#loc5216 = loc("reshape.11108")
#loc5217 = loc("reshape.11110")
#loc5218 = loc("broadcast.11111")
#loc5219 = loc("convert.11078")
#loc5220 = loc("power.11080")
#loc5221 = loc("reduce.11087")
#loc5222 = loc("multiply.11096")
#loc5223 = loc("reshape.11097")
#loc5224 = loc("add.11101")
#loc5225 = loc("rsqrt.11102")
#loc5226 = loc("reshape.11103")
#loc5227 = loc("broadcast.11104")
#loc5228 = loc("multiply.11105")
#loc5229 = loc("convert.11106")
#loc5230 = loc("multiply.11112")
#loc5231 = loc("reshape.11121")
#loc5232 = loc("reshape.11117")
#loc5233 = loc("reshape.11119")
#loc5234 = loc("transpose.11120")
#loc5235 = loc("dot.11122")
#loc5236 = loc("reshape.11123")
#loc5237 = loc("logistic.11124")
#loc5238 = loc("multiply.11125")
#loc5239 = loc("reshape.10897")
#loc5240 = loc("reshape.10899")
#loc5241 = loc("transpose.10900")
#loc5242 = loc("dot.11114")
#loc5243 = loc("reshape.11115")
#loc5244 = loc("multiply.11126")
#loc5245 = loc("reshape.11127")
#loc5246 = loc("reshape.10892")
#loc5247 = loc("reshape.10894")
#loc5248 = loc("transpose.10895")
#loc5250 = loc("reshape.11129")
#loc5251 = loc("add.11132")
#loc5252 = loc("convert.11133")
#loc5253 = loc("power.11135")
#loc5254 = loc("reduce.11142")
#loc5255 = loc("multiply.11151")
#loc5256 = loc("reshape.11152")
#loc5257 = loc("add.11156")
#loc5258 = loc("rsqrt.11157")
#loc5259 = loc("reshape.11158")
#loc5260 = loc("broadcast.11159")
#loc5261 = loc("multiply.11160")
#loc5262 = loc("convert.11161")
#loc5263 = loc("multiply.11167")
#loc5264 = loc("reshape.11168")
#loc5265 = loc("reshape.10883")
#loc5266 = loc("reshape.10885")
#loc5267 = loc("transpose.10886")
#loc5268 = loc("dot.11169")
#loc5269 = loc("reshape.11171")
#loc5270 = loc("convert.11172")
#loc5271 = loc("power.11174")
#loc5272 = loc("reduce.11181")
#loc5273 = loc("multiply.11190")
#loc5274 = loc("reshape.11191")
#loc5275 = loc("add.11195")
#loc5276 = loc("rsqrt.11196")
#loc5277 = loc("reshape.11197")
#loc5278 = loc("broadcast.11198")
#loc5279 = loc("multiply.11199")
#loc5280 = loc("convert.11200")
#loc5281 = loc("multiply.11206")
#loc5282 = loc("transpose.11207")
#loc5283 = loc("multiply.11218")
#loc5284 = loc("slice.11209")
#loc5285 = loc("negate.11210")
#loc5286 = loc("slice.11208")
#loc5287 = loc("concatenate.11211")
#loc5288 = loc("multiply.11214")
#loc5289 = loc("add.11221")
#loc5291 = loc("reshape.11243")
#loc5292 = loc("reshape.11245")
#loc5293 = loc("transpose.11246")
#loc5294 = loc("dot.11248")
#loc5295 = loc("reshape.11250")
#loc5296 = loc("transpose.11251")
#loc5298 = loc("reshape.11597")
#loc5299 = loc("reshape.11599")
#loc5300 = loc("broadcast.11600")
#loc5301 = loc("reshape.11558")
#loc5302 = loc("reshape.11560")
#loc5303 = loc("broadcast.11561")
#loc5304 = loc("reshape.11380")
#loc5305 = loc("reshape.11382")
#loc5306 = loc("broadcast.11383")
#loc5307 = loc("reshape.11342")
#loc5308 = loc("reshape.11344")
#loc5309 = loc("transpose.11345")
#loc5310 = loc("dot.11347")
#loc5311 = loc("reshape.11349")
#loc5312 = loc("convert.11350")
#loc5313 = loc("power.11352")
#loc5314 = loc("reduce.11359")
#loc5315 = loc("multiply.11368")
#loc5316 = loc("reshape.11369")
#loc5317 = loc("add.11373")
#loc5318 = loc("rsqrt.11374")
#loc5319 = loc("reshape.11375")
#loc5320 = loc("broadcast.11376")
#loc5321 = loc("multiply.11377")
#loc5322 = loc("convert.11378")
#loc5323 = loc("multiply.11384")
#loc5324 = loc("transpose.11385")
#loc5325 = loc("multiply.11395")
#loc5326 = loc("slice.11387")
#loc5327 = loc("negate.11388")
#loc5328 = loc("slice.11386")
#loc5329 = loc("concatenate.11389")
#loc5330 = loc("multiply.11392")
#loc5331 = loc("add.11398")
#loc5332 = loc("convert.11399")
#loc5333 = loc("multiply.11401")
#loc5334 = loc("broadcast.11330")
#loc5335 = loc("reshape.11331")
#loc5336 = loc("convert.11332")
#loc5337 = loc("transpose.11333")
#loc5338 = loc("multiply.11335")
#loc5339 = loc("dot.11402")
#loc5340 = loc("add.11409")
#loc5341 = loc("convert.11435")
#loc5342 = loc("compare.11437")
#loc5343 = loc("not.11439")
#loc5345 = loc("or.11449")
#loc5346 = loc("select.11450")
#loc5347 = loc("reshape.11455")
#loc5348 = loc("not.11457")
#loc5349 = loc("reshape.11459")
#loc5350 = loc("broadcast.11460")
#loc5351 = loc("reduce.11415")
#loc5352 = loc("broadcast.11416")
#loc5353 = loc("subtract.11417")
#loc5354 = loc("exponential.11418")
#loc5355 = loc("reduce.11424")
#loc5356 = loc("broadcast.11425")
#loc5357 = loc("divide.11426")
#loc5358 = loc("select.11461")
#loc5359 = loc("broadcast.11308")
#loc5360 = loc("reshape.11309")
#loc5361 = loc("convert.11310")
#loc5362 = loc("dot.11462")
#loc5363 = loc("convert.11464")
#loc5364 = loc("transpose.11465")
#loc5365 = loc("reshape.11467")
#loc5366 = loc("reshape.11301")
#loc5367 = loc("reshape.11303")
#loc5368 = loc("transpose.11304")
#loc5370 = loc("reshape.11469")
#loc5371 = loc("add.11472")
#loc5372 = loc("reshape.11503")
#loc5373 = loc("reshape.11505")
#loc5374 = loc("broadcast.11506")
#loc5375 = loc("convert.11473")
#loc5376 = loc("power.11475")
#loc5377 = loc("reduce.11482")
#loc5378 = loc("multiply.11491")
#loc5379 = loc("reshape.11492")
#loc5380 = loc("add.11496")
#loc5381 = loc("rsqrt.11497")
#loc5382 = loc("reshape.11498")
#loc5383 = loc("broadcast.11499")
#loc5384 = loc("multiply.11500")
#loc5385 = loc("convert.11501")
#loc5386 = loc("multiply.11507")
#loc5387 = loc("reshape.11516")
#loc5388 = loc("reshape.11512")
#loc5389 = loc("reshape.11514")
#loc5390 = loc("transpose.11515")
#loc5391 = loc("dot.11517")
#loc5392 = loc("reshape.11518")
#loc5393 = loc("logistic.11519")
#loc5394 = loc("multiply.11520")
#loc5395 = loc("reshape.11292")
#loc5396 = loc("reshape.11294")
#loc5397 = loc("transpose.11295")
#loc5398 = loc("dot.11509")
#loc5399 = loc("reshape.11510")
#loc5400 = loc("multiply.11521")
#loc5401 = loc("reshape.11522")
#loc5402 = loc("reshape.11287")
#loc5403 = loc("reshape.11289")
#loc5404 = loc("transpose.11290")
#loc5406 = loc("reshape.11524")
#loc5407 = loc("add.11527")
#loc5408 = loc("convert.11528")
#loc5409 = loc("power.11530")
#loc5410 = loc("reduce.11537")
#loc5411 = loc("multiply.11546")
#loc5412 = loc("reshape.11547")
#loc5413 = loc("add.11551")
#loc5414 = loc("rsqrt.11552")
#loc5415 = loc("reshape.11553")
#loc5416 = loc("broadcast.11554")
#loc5417 = loc("multiply.11555")
#loc5418 = loc("convert.11556")
#loc5419 = loc("multiply.11562")
#loc5420 = loc("reshape.11563")
#loc5421 = loc("reshape.11278")
#loc5422 = loc("reshape.11280")
#loc5423 = loc("transpose.11281")
#loc5424 = loc("dot.11564")
#loc5425 = loc("reshape.11566")
#loc5426 = loc("convert.11567")
#loc5427 = loc("power.11569")
#loc5428 = loc("reduce.11576")
#loc5429 = loc("multiply.11585")
#loc5430 = loc("reshape.11586")
#loc5431 = loc("add.11590")
#loc5432 = loc("rsqrt.11591")
#loc5433 = loc("reshape.11592")
#loc5434 = loc("broadcast.11593")
#loc5435 = loc("multiply.11594")
#loc5436 = loc("convert.11595")
#loc5437 = loc("multiply.11601")
#loc5438 = loc("transpose.11602")
#loc5439 = loc("multiply.11613")
#loc5440 = loc("slice.11604")
#loc5441 = loc("negate.11605")
#loc5442 = loc("slice.11603")
#loc5443 = loc("concatenate.11606")
#loc5444 = loc("multiply.11609")
#loc5445 = loc("add.11616")
#loc5447 = loc("reshape.11638")
#loc5448 = loc("reshape.11640")
#loc5449 = loc("transpose.11641")
#loc5450 = loc("dot.11643")
#loc5451 = loc("reshape.11645")
#loc5452 = loc("transpose.11646")
#loc5454 = loc("reshape.11992")
#loc5455 = loc("reshape.11994")
#loc5456 = loc("broadcast.11995")
#loc5457 = loc("reshape.11953")
#loc5458 = loc("reshape.11955")
#loc5459 = loc("broadcast.11956")
#loc5460 = loc("reshape.11775")
#loc5461 = loc("reshape.11777")
#loc5462 = loc("broadcast.11778")
#loc5463 = loc("reshape.11737")
#loc5464 = loc("reshape.11739")
#loc5465 = loc("transpose.11740")
#loc5466 = loc("dot.11742")
#loc5467 = loc("reshape.11744")
#loc5468 = loc("convert.11745")
#loc5469 = loc("power.11747")
#loc5470 = loc("reduce.11754")
#loc5471 = loc("multiply.11763")
#loc5472 = loc("reshape.11764")
#loc5473 = loc("add.11768")
#loc5474 = loc("rsqrt.11769")
#loc5475 = loc("reshape.11770")
#loc5476 = loc("broadcast.11771")
#loc5477 = loc("multiply.11772")
#loc5478 = loc("convert.11773")
#loc5479 = loc("multiply.11779")
#loc5480 = loc("transpose.11780")
#loc5481 = loc("multiply.11790")
#loc5482 = loc("slice.11782")
#loc5483 = loc("negate.11783")
#loc5484 = loc("slice.11781")
#loc5485 = loc("concatenate.11784")
#loc5486 = loc("multiply.11787")
#loc5487 = loc("add.11793")
#loc5488 = loc("convert.11794")
#loc5489 = loc("multiply.11796")
#loc5490 = loc("broadcast.11725")
#loc5491 = loc("reshape.11726")
#loc5492 = loc("convert.11727")
#loc5493 = loc("transpose.11728")
#loc5494 = loc("multiply.11730")
#loc5495 = loc("dot.11797")
#loc5496 = loc("add.11804")
#loc5497 = loc("convert.11830")
#loc5498 = loc("compare.11832")
#loc5499 = loc("not.11834")
#loc5501 = loc("or.11844")
#loc5502 = loc("select.11845")
#loc5503 = loc("reshape.11850")
#loc5504 = loc("not.11852")
#loc5505 = loc("reshape.11854")
#loc5506 = loc("broadcast.11855")
#loc5507 = loc("reduce.11810")
#loc5508 = loc("broadcast.11811")
#loc5509 = loc("subtract.11812")
#loc5510 = loc("exponential.11813")
#loc5511 = loc("reduce.11819")
#loc5512 = loc("broadcast.11820")
#loc5513 = loc("divide.11821")
#loc5514 = loc("select.11856")
#loc5515 = loc("broadcast.11703")
#loc5516 = loc("reshape.11704")
#loc5517 = loc("convert.11705")
#loc5518 = loc("dot.11857")
#loc5519 = loc("convert.11859")
#loc5520 = loc("transpose.11860")
#loc5521 = loc("reshape.11862")
#loc5522 = loc("reshape.11696")
#loc5523 = loc("reshape.11698")
#loc5524 = loc("transpose.11699")
#loc5526 = loc("reshape.11864")
#loc5527 = loc("add.11867")
#loc5528 = loc("reshape.11898")
#loc5529 = loc("reshape.11900")
#loc5530 = loc("broadcast.11901")
#loc5531 = loc("convert.11868")
#loc5532 = loc("power.11870")
#loc5533 = loc("reduce.11877")
#loc5534 = loc("multiply.11886")
#loc5535 = loc("reshape.11887")
#loc5536 = loc("add.11891")
#loc5537 = loc("rsqrt.11892")
#loc5538 = loc("reshape.11893")
#loc5539 = loc("broadcast.11894")
#loc5540 = loc("multiply.11895")
#loc5541 = loc("convert.11896")
#loc5542 = loc("multiply.11902")
#loc5543 = loc("reshape.11911")
#loc5544 = loc("reshape.11907")
#loc5545 = loc("reshape.11909")
#loc5546 = loc("transpose.11910")
#loc5547 = loc("dot.11912")
#loc5548 = loc("reshape.11913")
#loc5549 = loc("logistic.11914")
#loc5550 = loc("multiply.11915")
#loc5551 = loc("reshape.11687")
#loc5552 = loc("reshape.11689")
#loc5553 = loc("transpose.11690")
#loc5554 = loc("dot.11904")
#loc5555 = loc("reshape.11905")
#loc5556 = loc("multiply.11916")
#loc5557 = loc("reshape.11917")
#loc5558 = loc("reshape.11682")
#loc5559 = loc("reshape.11684")
#loc5560 = loc("transpose.11685")
#loc5562 = loc("reshape.11919")
#loc5563 = loc("add.11922")
#loc5564 = loc("convert.11923")
#loc5565 = loc("power.11925")
#loc5566 = loc("reduce.11932")
#loc5567 = loc("multiply.11941")
#loc5568 = loc("reshape.11942")
#loc5569 = loc("add.11946")
#loc5570 = loc("rsqrt.11947")
#loc5571 = loc("reshape.11948")
#loc5572 = loc("broadcast.11949")
#loc5573 = loc("multiply.11950")
#loc5574 = loc("convert.11951")
#loc5575 = loc("multiply.11957")
#loc5576 = loc("reshape.11958")
#loc5577 = loc("reshape.11673")
#loc5578 = loc("reshape.11675")
#loc5579 = loc("transpose.11676")
#loc5580 = loc("dot.11959")
#loc5581 = loc("reshape.11961")
#loc5582 = loc("convert.11962")
#loc5583 = loc("power.11964")
#loc5584 = loc("reduce.11971")
#loc5585 = loc("multiply.11980")
#loc5586 = loc("reshape.11981")
#loc5587 = loc("add.11985")
#loc5588 = loc("rsqrt.11986")
#loc5589 = loc("reshape.11987")
#loc5590 = loc("broadcast.11988")
#loc5591 = loc("multiply.11989")
#loc5592 = loc("convert.11990")
#loc5593 = loc("multiply.11996")
#loc5594 = loc("transpose.11997")
#loc5595 = loc("multiply.12008")
#loc5596 = loc("slice.11999")
#loc5597 = loc("negate.12000")
#loc5598 = loc("slice.11998")
#loc5599 = loc("concatenate.12001")
#loc5600 = loc("multiply.12004")
#loc5601 = loc("add.12011")
#loc5603 = loc("reshape.12033")
#loc5604 = loc("reshape.12035")
#loc5605 = loc("transpose.12036")
#loc5606 = loc("dot.12038")
#loc5607 = loc("reshape.12040")
#loc5608 = loc("transpose.12041")
#loc5610 = loc("reshape.12387")
#loc5611 = loc("reshape.12389")
#loc5612 = loc("broadcast.12390")
#loc5613 = loc("reshape.12348")
#loc5614 = loc("reshape.12350")
#loc5615 = loc("broadcast.12351")
#loc5616 = loc("reshape.12170")
#loc5617 = loc("reshape.12172")
#loc5618 = loc("broadcast.12173")
#loc5619 = loc("reshape.12132")
#loc5620 = loc("reshape.12134")
#loc5621 = loc("transpose.12135")
#loc5622 = loc("dot.12137")
#loc5623 = loc("reshape.12139")
#loc5624 = loc("convert.12140")
#loc5625 = loc("power.12142")
#loc5626 = loc("reduce.12149")
#loc5627 = loc("multiply.12158")
#loc5628 = loc("reshape.12159")
#loc5629 = loc("add.12163")
#loc5630 = loc("rsqrt.12164")
#loc5631 = loc("reshape.12165")
#loc5632 = loc("broadcast.12166")
#loc5633 = loc("multiply.12167")
#loc5634 = loc("convert.12168")
#loc5635 = loc("multiply.12174")
#loc5636 = loc("transpose.12175")
#loc5637 = loc("multiply.12185")
#loc5638 = loc("slice.12177")
#loc5639 = loc("negate.12178")
#loc5640 = loc("slice.12176")
#loc5641 = loc("concatenate.12179")
#loc5642 = loc("multiply.12182")
#loc5643 = loc("add.12188")
#loc5644 = loc("convert.12189")
#loc5645 = loc("multiply.12191")
#loc5646 = loc("broadcast.12120")
#loc5647 = loc("reshape.12121")
#loc5648 = loc("convert.12122")
#loc5649 = loc("transpose.12123")
#loc5650 = loc("multiply.12125")
#loc5651 = loc("dot.12192")
#loc5652 = loc("add.12199")
#loc5653 = loc("convert.12225")
#loc5654 = loc("compare.12227")
#loc5655 = loc("not.12229")
#loc5657 = loc("or.12239")
#loc5658 = loc("select.12240")
#loc5659 = loc("reshape.12245")
#loc5660 = loc("not.12247")
#loc5661 = loc("reshape.12249")
#loc5662 = loc("broadcast.12250")
#loc5663 = loc("reduce.12205")
#loc5664 = loc("broadcast.12206")
#loc5665 = loc("subtract.12207")
#loc5666 = loc("exponential.12208")
#loc5667 = loc("reduce.12214")
#loc5668 = loc("broadcast.12215")
#loc5669 = loc("divide.12216")
#loc5670 = loc("select.12251")
#loc5671 = loc("broadcast.12098")
#loc5672 = loc("reshape.12099")
#loc5673 = loc("convert.12100")
#loc5674 = loc("dot.12252")
#loc5675 = loc("convert.12254")
#loc5676 = loc("transpose.12255")
#loc5677 = loc("reshape.12257")
#loc5678 = loc("reshape.12091")
#loc5679 = loc("reshape.12093")
#loc5680 = loc("transpose.12094")
#loc5682 = loc("reshape.12259")
#loc5683 = loc("add.12262")
#loc5684 = loc("reshape.12293")
#loc5685 = loc("reshape.12295")
#loc5686 = loc("broadcast.12296")
#loc5687 = loc("convert.12263")
#loc5688 = loc("power.12265")
#loc5689 = loc("reduce.12272")
#loc5690 = loc("multiply.12281")
#loc5691 = loc("reshape.12282")
#loc5692 = loc("add.12286")
#loc5693 = loc("rsqrt.12287")
#loc5694 = loc("reshape.12288")
#loc5695 = loc("broadcast.12289")
#loc5696 = loc("multiply.12290")
#loc5697 = loc("convert.12291")
#loc5698 = loc("multiply.12297")
#loc5699 = loc("reshape.12306")
#loc5700 = loc("reshape.12302")
#loc5701 = loc("reshape.12304")
#loc5702 = loc("transpose.12305")
#loc5703 = loc("dot.12307")
#loc5704 = loc("reshape.12308")
#loc5705 = loc("logistic.12309")
#loc5706 = loc("multiply.12310")
#loc5707 = loc("reshape.12082")
#loc5708 = loc("reshape.12084")
#loc5709 = loc("transpose.12085")
#loc5710 = loc("dot.12299")
#loc5711 = loc("reshape.12300")
#loc5712 = loc("multiply.12311")
#loc5713 = loc("reshape.12312")
#loc5714 = loc("reshape.12077")
#loc5715 = loc("reshape.12079")
#loc5716 = loc("transpose.12080")
#loc5718 = loc("reshape.12314")
#loc5719 = loc("add.12317")
#loc5720 = loc("convert.12318")
#loc5721 = loc("power.12320")
#loc5722 = loc("reduce.12327")
#loc5723 = loc("multiply.12336")
#loc5724 = loc("reshape.12337")
#loc5725 = loc("add.12341")
#loc5726 = loc("rsqrt.12342")
#loc5727 = loc("reshape.12343")
#loc5728 = loc("broadcast.12344")
#loc5729 = loc("multiply.12345")
#loc5730 = loc("convert.12346")
#loc5731 = loc("multiply.12352")
#loc5732 = loc("reshape.12353")
#loc5733 = loc("reshape.12068")
#loc5734 = loc("reshape.12070")
#loc5735 = loc("transpose.12071")
#loc5736 = loc("dot.12354")
#loc5737 = loc("reshape.12356")
#loc5738 = loc("convert.12357")
#loc5739 = loc("power.12359")
#loc5740 = loc("reduce.12366")
#loc5741 = loc("multiply.12375")
#loc5742 = loc("reshape.12376")
#loc5743 = loc("add.12380")
#loc5744 = loc("rsqrt.12381")
#loc5745 = loc("reshape.12382")
#loc5746 = loc("broadcast.12383")
#loc5747 = loc("multiply.12384")
#loc5748 = loc("convert.12385")
#loc5749 = loc("multiply.12391")
#loc5750 = loc("transpose.12392")
#loc5751 = loc("multiply.12403")
#loc5752 = loc("slice.12394")
#loc5753 = loc("negate.12395")
#loc5754 = loc("slice.12393")
#loc5755 = loc("concatenate.12396")
#loc5756 = loc("multiply.12399")
#loc5757 = loc("add.12406")
#loc5759 = loc("reshape.12428")
#loc5760 = loc("reshape.12430")
#loc5761 = loc("transpose.12431")
#loc5762 = loc("dot.12433")
#loc5763 = loc("reshape.12435")
#loc5764 = loc("transpose.12436")
#loc5766 = loc("reshape.12782")
#loc5767 = loc("reshape.12784")
#loc5768 = loc("broadcast.12785")
#loc5769 = loc("reshape.12743")
#loc5770 = loc("reshape.12745")
#loc5771 = loc("broadcast.12746")
#loc5772 = loc("reshape.12565")
#loc5773 = loc("reshape.12567")
#loc5774 = loc("broadcast.12568")
#loc5775 = loc("reshape.12527")
#loc5776 = loc("reshape.12529")
#loc5777 = loc("transpose.12530")
#loc5778 = loc("dot.12532")
#loc5779 = loc("reshape.12534")
#loc5780 = loc("convert.12535")
#loc5781 = loc("power.12537")
#loc5782 = loc("reduce.12544")
#loc5783 = loc("multiply.12553")
#loc5784 = loc("reshape.12554")
#loc5785 = loc("add.12558")
#loc5786 = loc("rsqrt.12559")
#loc5787 = loc("reshape.12560")
#loc5788 = loc("broadcast.12561")
#loc5789 = loc("multiply.12562")
#loc5790 = loc("convert.12563")
#loc5791 = loc("multiply.12569")
#loc5792 = loc("transpose.12570")
#loc5793 = loc("multiply.12580")
#loc5794 = loc("slice.12572")
#loc5795 = loc("negate.12573")
#loc5796 = loc("slice.12571")
#loc5797 = loc("concatenate.12574")
#loc5798 = loc("multiply.12577")
#loc5799 = loc("add.12583")
#loc5800 = loc("convert.12584")
#loc5801 = loc("multiply.12586")
#loc5802 = loc("broadcast.12515")
#loc5803 = loc("reshape.12516")
#loc5804 = loc("convert.12517")
#loc5805 = loc("transpose.12518")
#loc5806 = loc("multiply.12520")
#loc5807 = loc("dot.12587")
#loc5808 = loc("add.12594")
#loc5809 = loc("convert.12620")
#loc5810 = loc("compare.12622")
#loc5811 = loc("not.12624")
#loc5813 = loc("or.12634")
#loc5814 = loc("select.12635")
#loc5815 = loc("reshape.12640")
#loc5816 = loc("not.12642")
#loc5817 = loc("reshape.12644")
#loc5818 = loc("broadcast.12645")
#loc5819 = loc("reduce.12600")
#loc5820 = loc("broadcast.12601")
#loc5821 = loc("subtract.12602")
#loc5822 = loc("exponential.12603")
#loc5823 = loc("reduce.12609")
#loc5824 = loc("broadcast.12610")
#loc5825 = loc("divide.12611")
#loc5826 = loc("select.12646")
#loc5827 = loc("broadcast.12493")
#loc5828 = loc("reshape.12494")
#loc5829 = loc("convert.12495")
#loc5830 = loc("dot.12647")
#loc5831 = loc("convert.12649")
#loc5832 = loc("transpose.12650")
#loc5833 = loc("reshape.12652")
#loc5834 = loc("reshape.12486")
#loc5835 = loc("reshape.12488")
#loc5836 = loc("transpose.12489")
#loc5838 = loc("reshape.12654")
#loc5839 = loc("add.12657")
#loc5840 = loc("reshape.12688")
#loc5841 = loc("reshape.12690")
#loc5842 = loc("broadcast.12691")
#loc5843 = loc("convert.12658")
#loc5844 = loc("power.12660")
#loc5845 = loc("reduce.12667")
#loc5846 = loc("multiply.12676")
#loc5847 = loc("reshape.12677")
#loc5848 = loc("add.12681")
#loc5849 = loc("rsqrt.12682")
#loc5850 = loc("reshape.12683")
#loc5851 = loc("broadcast.12684")
#loc5852 = loc("multiply.12685")
#loc5853 = loc("convert.12686")
#loc5854 = loc("multiply.12692")
#loc5855 = loc("reshape.12701")
#loc5856 = loc("reshape.12697")
#loc5857 = loc("reshape.12699")
#loc5858 = loc("transpose.12700")
#loc5859 = loc("dot.12702")
#loc5860 = loc("reshape.12703")
#loc5861 = loc("logistic.12704")
#loc5862 = loc("multiply.12705")
#loc5863 = loc("reshape.12477")
#loc5864 = loc("reshape.12479")
#loc5865 = loc("transpose.12480")
#loc5866 = loc("dot.12694")
#loc5867 = loc("reshape.12695")
#loc5868 = loc("multiply.12706")
#loc5869 = loc("reshape.12707")
#loc5870 = loc("reshape.12472")
#loc5871 = loc("reshape.12474")
#loc5872 = loc("transpose.12475")
#loc5874 = loc("reshape.12709")
#loc5875 = loc("add.12712")
#loc5876 = loc("convert.12713")
#loc5877 = loc("power.12715")
#loc5878 = loc("reduce.12722")
#loc5879 = loc("multiply.12731")
#loc5880 = loc("reshape.12732")
#loc5881 = loc("add.12736")
#loc5882 = loc("rsqrt.12737")
#loc5883 = loc("reshape.12738")
#loc5884 = loc("broadcast.12739")
#loc5885 = loc("multiply.12740")
#loc5886 = loc("convert.12741")
#loc5887 = loc("multiply.12747")
#loc5888 = loc("reshape.12748")
#loc5889 = loc("reshape.12463")
#loc5890 = loc("reshape.12465")
#loc5891 = loc("transpose.12466")
#loc5892 = loc("dot.12749")
#loc5893 = loc("reshape.12751")
#loc5894 = loc("convert.12752")
#loc5895 = loc("power.12754")
#loc5896 = loc("reduce.12761")
#loc5897 = loc("multiply.12770")
#loc5898 = loc("reshape.12771")
#loc5899 = loc("add.12775")
#loc5900 = loc("rsqrt.12776")
#loc5901 = loc("reshape.12777")
#loc5902 = loc("broadcast.12778")
#loc5903 = loc("multiply.12779")
#loc5904 = loc("convert.12780")
#loc5905 = loc("multiply.12786")
#loc5906 = loc("transpose.12787")
#loc5907 = loc("multiply.12798")
#loc5908 = loc("slice.12789")
#loc5909 = loc("negate.12790")
#loc5910 = loc("slice.12788")
#loc5911 = loc("concatenate.12791")
#loc5912 = loc("multiply.12794")
#loc5913 = loc("add.12801")
#loc5915 = loc("reshape.12823")
#loc5916 = loc("reshape.12825")
#loc5917 = loc("transpose.12826")
#loc5918 = loc("dot.12828")
#loc5919 = loc("reshape.12830")
#loc5920 = loc("transpose.12831")
#loc5922 = loc("reshape.13177")
#loc5923 = loc("reshape.13179")
#loc5924 = loc("broadcast.13180")
#loc5925 = loc("reshape.13138")
#loc5926 = loc("reshape.13140")
#loc5927 = loc("broadcast.13141")
#loc5928 = loc("reshape.12960")
#loc5929 = loc("reshape.12962")
#loc5930 = loc("broadcast.12963")
#loc5931 = loc("reshape.12922")
#loc5932 = loc("reshape.12924")
#loc5933 = loc("transpose.12925")
#loc5934 = loc("dot.12927")
#loc5935 = loc("reshape.12929")
#loc5936 = loc("convert.12930")
#loc5937 = loc("power.12932")
#loc5938 = loc("reduce.12939")
#loc5939 = loc("multiply.12948")
#loc5940 = loc("reshape.12949")
#loc5941 = loc("add.12953")
#loc5942 = loc("rsqrt.12954")
#loc5943 = loc("reshape.12955")
#loc5944 = loc("broadcast.12956")
#loc5945 = loc("multiply.12957")
#loc5946 = loc("convert.12958")
#loc5947 = loc("multiply.12964")
#loc5948 = loc("transpose.12965")
#loc5949 = loc("multiply.12975")
#loc5950 = loc("slice.12967")
#loc5951 = loc("negate.12968")
#loc5952 = loc("slice.12966")
#loc5953 = loc("concatenate.12969")
#loc5954 = loc("multiply.12972")
#loc5955 = loc("add.12978")
#loc5956 = loc("convert.12979")
#loc5957 = loc("multiply.12981")
#loc5958 = loc("broadcast.12910")
#loc5959 = loc("reshape.12911")
#loc5960 = loc("convert.12912")
#loc5961 = loc("transpose.12913")
#loc5962 = loc("multiply.12915")
#loc5963 = loc("dot.12982")
#loc5964 = loc("add.12989")
#loc5965 = loc("convert.13015")
#loc5966 = loc("compare.13017")
#loc5967 = loc("not.13019")
#loc5969 = loc("or.13029")
#loc5970 = loc("select.13030")
#loc5971 = loc("reshape.13035")
#loc5972 = loc("not.13037")
#loc5973 = loc("reshape.13039")
#loc5974 = loc("broadcast.13040")
#loc5975 = loc("reduce.12995")
#loc5976 = loc("broadcast.12996")
#loc5977 = loc("subtract.12997")
#loc5978 = loc("exponential.12998")
#loc5979 = loc("reduce.13004")
#loc5980 = loc("broadcast.13005")
#loc5981 = loc("divide.13006")
#loc5982 = loc("select.13041")
#loc5983 = loc("broadcast.12888")
#loc5984 = loc("reshape.12889")
#loc5985 = loc("convert.12890")
#loc5986 = loc("dot.13042")
#loc5987 = loc("convert.13044")
#loc5988 = loc("transpose.13045")
#loc5989 = loc("reshape.13047")
#loc5990 = loc("reshape.12881")
#loc5991 = loc("reshape.12883")
#loc5992 = loc("transpose.12884")
#loc5994 = loc("reshape.13049")
#loc5995 = loc("add.13052")
#loc5996 = loc("reshape.13083")
#loc5997 = loc("reshape.13085")
#loc5998 = loc("broadcast.13086")
#loc5999 = loc("convert.13053")
#loc6000 = loc("power.13055")
#loc6001 = loc("reduce.13062")
#loc6002 = loc("multiply.13071")
#loc6003 = loc("reshape.13072")
#loc6004 = loc("add.13076")
#loc6005 = loc("rsqrt.13077")
#loc6006 = loc("reshape.13078")
#loc6007 = loc("broadcast.13079")
#loc6008 = loc("multiply.13080")
#loc6009 = loc("convert.13081")
#loc6010 = loc("multiply.13087")
#loc6011 = loc("reshape.13096")
#loc6012 = loc("reshape.13092")
#loc6013 = loc("reshape.13094")
#loc6014 = loc("transpose.13095")
#loc6015 = loc("dot.13097")
#loc6016 = loc("reshape.13098")
#loc6017 = loc("logistic.13099")
#loc6018 = loc("multiply.13100")
#loc6019 = loc("reshape.12872")
#loc6020 = loc("reshape.12874")
#loc6021 = loc("transpose.12875")
#loc6022 = loc("dot.13089")
#loc6023 = loc("reshape.13090")
#loc6024 = loc("multiply.13101")
#loc6025 = loc("reshape.13102")
#loc6026 = loc("reshape.12867")
#loc6027 = loc("reshape.12869")
#loc6028 = loc("transpose.12870")
#loc6030 = loc("reshape.13104")
#loc6031 = loc("add.13107")
#loc6032 = loc("convert.13108")
#loc6033 = loc("power.13110")
#loc6034 = loc("reduce.13117")
#loc6035 = loc("multiply.13126")
#loc6036 = loc("reshape.13127")
#loc6037 = loc("add.13131")
#loc6038 = loc("rsqrt.13132")
#loc6039 = loc("reshape.13133")
#loc6040 = loc("broadcast.13134")
#loc6041 = loc("multiply.13135")
#loc6042 = loc("convert.13136")
#loc6043 = loc("multiply.13142")
#loc6044 = loc("reshape.13143")
#loc6045 = loc("reshape.12858")
#loc6046 = loc("reshape.12860")
#loc6047 = loc("transpose.12861")
#loc6048 = loc("dot.13144")
#loc6049 = loc("reshape.13146")
#loc6050 = loc("convert.13147")
#loc6051 = loc("power.13149")
#loc6052 = loc("reduce.13156")
#loc6053 = loc("multiply.13165")
#loc6054 = loc("reshape.13166")
#loc6055 = loc("add.13170")
#loc6056 = loc("rsqrt.13171")
#loc6057 = loc("reshape.13172")
#loc6058 = loc("broadcast.13173")
#loc6059 = loc("multiply.13174")
#loc6060 = loc("convert.13175")
#loc6061 = loc("multiply.13181")
#loc6062 = loc("transpose.13182")
#loc6063 = loc("multiply.13193")
#loc6064 = loc("slice.13184")
#loc6065 = loc("negate.13185")
#loc6066 = loc("slice.13183")
#loc6067 = loc("concatenate.13186")
#loc6068 = loc("multiply.13189")
#loc6069 = loc("add.13196")
#loc6071 = loc("reshape.13218")
#loc6072 = loc("reshape.13220")
#loc6073 = loc("transpose.13221")
#loc6074 = loc("dot.13223")
#loc6075 = loc("reshape.13225")
#loc6076 = loc("transpose.13226")
#loc6078 = loc("reshape.13572")
#loc6079 = loc("reshape.13574")
#loc6080 = loc("broadcast.13575")
#loc6081 = loc("reshape.13533")
#loc6082 = loc("reshape.13535")
#loc6083 = loc("broadcast.13536")
#loc6084 = loc("reshape.13355")
#loc6085 = loc("reshape.13357")
#loc6086 = loc("broadcast.13358")
#loc6087 = loc("reshape.13317")
#loc6088 = loc("reshape.13319")
#loc6089 = loc("transpose.13320")
#loc6090 = loc("dot.13322")
#loc6091 = loc("reshape.13324")
#loc6092 = loc("convert.13325")
#loc6093 = loc("power.13327")
#loc6094 = loc("reduce.13334")
#loc6095 = loc("multiply.13343")
#loc6096 = loc("reshape.13344")
#loc6097 = loc("add.13348")
#loc6098 = loc("rsqrt.13349")
#loc6099 = loc("reshape.13350")
#loc6100 = loc("broadcast.13351")
#loc6101 = loc("multiply.13352")
#loc6102 = loc("convert.13353")
#loc6103 = loc("multiply.13359")
#loc6104 = loc("transpose.13360")
#loc6105 = loc("multiply.13370")
#loc6106 = loc("slice.13362")
#loc6107 = loc("negate.13363")
#loc6108 = loc("slice.13361")
#loc6109 = loc("concatenate.13364")
#loc6110 = loc("multiply.13367")
#loc6111 = loc("add.13373")
#loc6112 = loc("convert.13374")
#loc6113 = loc("multiply.13376")
#loc6114 = loc("broadcast.13305")
#loc6115 = loc("reshape.13306")
#loc6116 = loc("convert.13307")
#loc6117 = loc("transpose.13308")
#loc6118 = loc("multiply.13310")
#loc6119 = loc("dot.13377")
#loc6120 = loc("add.13384")
#loc6121 = loc("convert.13410")
#loc6122 = loc("compare.13412")
#loc6123 = loc("not.13414")
#loc6125 = loc("or.13424")
#loc6126 = loc("select.13425")
#loc6127 = loc("reshape.13430")
#loc6128 = loc("not.13432")
#loc6129 = loc("reshape.13434")
#loc6130 = loc("broadcast.13435")
#loc6131 = loc("reduce.13390")
#loc6132 = loc("broadcast.13391")
#loc6133 = loc("subtract.13392")
#loc6134 = loc("exponential.13393")
#loc6135 = loc("reduce.13399")
#loc6136 = loc("broadcast.13400")
#loc6137 = loc("divide.13401")
#loc6138 = loc("select.13436")
#loc6139 = loc("broadcast.13283")
#loc6140 = loc("reshape.13284")
#loc6141 = loc("convert.13285")
#loc6142 = loc("dot.13437")
#loc6143 = loc("convert.13439")
#loc6144 = loc("transpose.13440")
#loc6145 = loc("reshape.13442")
#loc6146 = loc("reshape.13276")
#loc6147 = loc("reshape.13278")
#loc6148 = loc("transpose.13279")
#loc6150 = loc("reshape.13444")
#loc6151 = loc("add.13447")
#loc6152 = loc("reshape.13478")
#loc6153 = loc("reshape.13480")
#loc6154 = loc("broadcast.13481")
#loc6155 = loc("convert.13448")
#loc6156 = loc("power.13450")
#loc6157 = loc("reduce.13457")
#loc6158 = loc("multiply.13466")
#loc6159 = loc("reshape.13467")
#loc6160 = loc("add.13471")
#loc6161 = loc("rsqrt.13472")
#loc6162 = loc("reshape.13473")
#loc6163 = loc("broadcast.13474")
#loc6164 = loc("multiply.13475")
#loc6165 = loc("convert.13476")
#loc6166 = loc("multiply.13482")
#loc6167 = loc("reshape.13491")
#loc6168 = loc("reshape.13487")
#loc6169 = loc("reshape.13489")
#loc6170 = loc("transpose.13490")
#loc6171 = loc("dot.13492")
#loc6172 = loc("reshape.13493")
#loc6173 = loc("logistic.13494")
#loc6174 = loc("multiply.13495")
#loc6175 = loc("reshape.13267")
#loc6176 = loc("reshape.13269")
#loc6177 = loc("transpose.13270")
#loc6178 = loc("dot.13484")
#loc6179 = loc("reshape.13485")
#loc6180 = loc("multiply.13496")
#loc6181 = loc("reshape.13497")
#loc6182 = loc("reshape.13262")
#loc6183 = loc("reshape.13264")
#loc6184 = loc("transpose.13265")
#loc6186 = loc("reshape.13499")
#loc6187 = loc("add.13502")
#loc6188 = loc("convert.13503")
#loc6189 = loc("power.13505")
#loc6190 = loc("reduce.13512")
#loc6191 = loc("multiply.13521")
#loc6192 = loc("reshape.13522")
#loc6193 = loc("add.13526")
#loc6194 = loc("rsqrt.13527")
#loc6195 = loc("reshape.13528")
#loc6196 = loc("broadcast.13529")
#loc6197 = loc("multiply.13530")
#loc6198 = loc("convert.13531")
#loc6199 = loc("multiply.13537")
#loc6200 = loc("reshape.13538")
#loc6201 = loc("reshape.13253")
#loc6202 = loc("reshape.13255")
#loc6203 = loc("transpose.13256")
#loc6204 = loc("dot.13539")
#loc6205 = loc("reshape.13541")
#loc6206 = loc("convert.13542")
#loc6207 = loc("power.13544")
#loc6208 = loc("reduce.13551")
#loc6209 = loc("multiply.13560")
#loc6210 = loc("reshape.13561")
#loc6211 = loc("add.13565")
#loc6212 = loc("rsqrt.13566")
#loc6213 = loc("reshape.13567")
#loc6214 = loc("broadcast.13568")
#loc6215 = loc("multiply.13569")
#loc6216 = loc("convert.13570")
#loc6217 = loc("multiply.13576")
#loc6218 = loc("transpose.13577")
#loc6219 = loc("multiply.13588")
#loc6220 = loc("slice.13579")
#loc6221 = loc("negate.13580")
#loc6222 = loc("slice.13578")
#loc6223 = loc("concatenate.13581")
#loc6224 = loc("multiply.13584")
#loc6225 = loc("add.13591")
#loc6227 = loc("reshape.13613")
#loc6228 = loc("reshape.13615")
#loc6229 = loc("transpose.13616")
#loc6230 = loc("dot.13618")
#loc6231 = loc("reshape.13620")
#loc6232 = loc("transpose.13621")
#loc6234 = loc("reshape.13967")
#loc6235 = loc("reshape.13969")
#loc6236 = loc("broadcast.13970")
#loc6237 = loc("reshape.13928")
#loc6238 = loc("reshape.13930")
#loc6239 = loc("broadcast.13931")
#loc6240 = loc("reshape.13750")
#loc6241 = loc("reshape.13752")
#loc6242 = loc("broadcast.13753")
#loc6243 = loc("reshape.13712")
#loc6244 = loc("reshape.13714")
#loc6245 = loc("transpose.13715")
#loc6246 = loc("dot.13717")
#loc6247 = loc("reshape.13719")
#loc6248 = loc("convert.13720")
#loc6249 = loc("power.13722")
#loc6250 = loc("reduce.13729")
#loc6251 = loc("multiply.13738")
#loc6252 = loc("reshape.13739")
#loc6253 = loc("add.13743")
#loc6254 = loc("rsqrt.13744")
#loc6255 = loc("reshape.13745")
#loc6256 = loc("broadcast.13746")
#loc6257 = loc("multiply.13747")
#loc6258 = loc("convert.13748")
#loc6259 = loc("multiply.13754")
#loc6260 = loc("transpose.13755")
#loc6261 = loc("multiply.13765")
#loc6262 = loc("slice.13757")
#loc6263 = loc("negate.13758")
#loc6264 = loc("slice.13756")
#loc6265 = loc("concatenate.13759")
#loc6266 = loc("multiply.13762")
#loc6267 = loc("add.13768")
#loc6268 = loc("convert.13769")
#loc6269 = loc("multiply.13771")
#loc6270 = loc("broadcast.13700")
#loc6271 = loc("reshape.13701")
#loc6272 = loc("convert.13702")
#loc6273 = loc("transpose.13703")
#loc6274 = loc("multiply.13705")
#loc6275 = loc("dot.13772")
#loc6276 = loc("add.13779")
#loc6277 = loc("convert.13805")
#loc6278 = loc("compare.13807")
#loc6279 = loc("not.13809")
#loc6281 = loc("or.13819")
#loc6282 = loc("select.13820")
#loc6283 = loc("reshape.13825")
#loc6284 = loc("not.13827")
#loc6285 = loc("reshape.13829")
#loc6286 = loc("broadcast.13830")
#loc6287 = loc("reduce.13785")
#loc6288 = loc("broadcast.13786")
#loc6289 = loc("subtract.13787")
#loc6290 = loc("exponential.13788")
#loc6291 = loc("reduce.13794")
#loc6292 = loc("broadcast.13795")
#loc6293 = loc("divide.13796")
#loc6294 = loc("select.13831")
#loc6295 = loc("broadcast.13678")
#loc6296 = loc("reshape.13679")
#loc6297 = loc("convert.13680")
#loc6298 = loc("dot.13832")
#loc6299 = loc("convert.13834")
#loc6300 = loc("transpose.13835")
#loc6301 = loc("reshape.13837")
#loc6302 = loc("reshape.13671")
#loc6303 = loc("reshape.13673")
#loc6304 = loc("transpose.13674")
#loc6306 = loc("reshape.13839")
#loc6307 = loc("add.13842")
#loc6308 = loc("reshape.13873")
#loc6309 = loc("reshape.13875")
#loc6310 = loc("broadcast.13876")
#loc6311 = loc("convert.13843")
#loc6312 = loc("power.13845")
#loc6313 = loc("reduce.13852")
#loc6314 = loc("multiply.13861")
#loc6315 = loc("reshape.13862")
#loc6316 = loc("add.13866")
#loc6317 = loc("rsqrt.13867")
#loc6318 = loc("reshape.13868")
#loc6319 = loc("broadcast.13869")
#loc6320 = loc("multiply.13870")
#loc6321 = loc("convert.13871")
#loc6322 = loc("multiply.13877")
#loc6323 = loc("reshape.13886")
#loc6324 = loc("reshape.13882")
#loc6325 = loc("reshape.13884")
#loc6326 = loc("transpose.13885")
#loc6327 = loc("dot.13887")
#loc6328 = loc("reshape.13888")
#loc6329 = loc("logistic.13889")
#loc6330 = loc("multiply.13890")
#loc6331 = loc("reshape.13662")
#loc6332 = loc("reshape.13664")
#loc6333 = loc("transpose.13665")
#loc6334 = loc("dot.13879")
#loc6335 = loc("reshape.13880")
#loc6336 = loc("multiply.13891")
#loc6337 = loc("reshape.13892")
#loc6338 = loc("reshape.13657")
#loc6339 = loc("reshape.13659")
#loc6340 = loc("transpose.13660")
#loc6342 = loc("reshape.13894")
#loc6343 = loc("add.13897")
#loc6344 = loc("convert.13898")
#loc6345 = loc("power.13900")
#loc6346 = loc("reduce.13907")
#loc6347 = loc("multiply.13916")
#loc6348 = loc("reshape.13917")
#loc6349 = loc("add.13921")
#loc6350 = loc("rsqrt.13922")
#loc6351 = loc("reshape.13923")
#loc6352 = loc("broadcast.13924")
#loc6353 = loc("multiply.13925")
#loc6354 = loc("convert.13926")
#loc6355 = loc("multiply.13932")
#loc6356 = loc("reshape.13933")
#loc6357 = loc("reshape.13648")
#loc6358 = loc("reshape.13650")
#loc6359 = loc("transpose.13651")
#loc6360 = loc("dot.13934")
#loc6361 = loc("reshape.13936")
#loc6362 = loc("convert.13937")
#loc6363 = loc("power.13939")
#loc6364 = loc("reduce.13946")
#loc6365 = loc("multiply.13955")
#loc6366 = loc("reshape.13956")
#loc6367 = loc("add.13960")
#loc6368 = loc("rsqrt.13961")
#loc6369 = loc("reshape.13962")
#loc6370 = loc("broadcast.13963")
#loc6371 = loc("multiply.13964")
#loc6372 = loc("convert.13965")
#loc6373 = loc("multiply.13971")
#loc6374 = loc("transpose.13972")
#loc6375 = loc("multiply.13983")
#loc6376 = loc("slice.13974")
#loc6377 = loc("negate.13975")
#loc6378 = loc("slice.13973")
#loc6379 = loc("concatenate.13976")
#loc6380 = loc("multiply.13979")
#loc6381 = loc("add.13986")
#loc6383 = loc("reshape.14008")
#loc6384 = loc("reshape.14010")
#loc6385 = loc("transpose.14011")
#loc6386 = loc("dot.14013")
#loc6387 = loc("reshape.14015")
#loc6388 = loc("transpose.14016")
#loc6390 = loc("reshape.14362")
#loc6391 = loc("reshape.14364")
#loc6392 = loc("broadcast.14365")
#loc6393 = loc("reshape.14323")
#loc6394 = loc("reshape.14325")
#loc6395 = loc("broadcast.14326")
#loc6396 = loc("reshape.14145")
#loc6397 = loc("reshape.14147")
#loc6398 = loc("broadcast.14148")
#loc6399 = loc("reshape.14107")
#loc6400 = loc("reshape.14109")
#loc6401 = loc("transpose.14110")
#loc6402 = loc("dot.14112")
#loc6403 = loc("reshape.14114")
#loc6404 = loc("convert.14115")
#loc6405 = loc("power.14117")
#loc6406 = loc("reduce.14124")
#loc6407 = loc("multiply.14133")
#loc6408 = loc("reshape.14134")
#loc6409 = loc("add.14138")
#loc6410 = loc("rsqrt.14139")
#loc6411 = loc("reshape.14140")
#loc6412 = loc("broadcast.14141")
#loc6413 = loc("multiply.14142")
#loc6414 = loc("convert.14143")
#loc6415 = loc("multiply.14149")
#loc6416 = loc("transpose.14150")
#loc6417 = loc("multiply.14160")
#loc6418 = loc("slice.14152")
#loc6419 = loc("negate.14153")
#loc6420 = loc("slice.14151")
#loc6421 = loc("concatenate.14154")
#loc6422 = loc("multiply.14157")
#loc6423 = loc("add.14163")
#loc6424 = loc("convert.14164")
#loc6425 = loc("multiply.14166")
#loc6426 = loc("broadcast.14095")
#loc6427 = loc("reshape.14096")
#loc6428 = loc("convert.14097")
#loc6429 = loc("transpose.14098")
#loc6430 = loc("multiply.14100")
#loc6431 = loc("dot.14167")
#loc6432 = loc("add.14174")
#loc6433 = loc("convert.14200")
#loc6434 = loc("compare.14202")
#loc6435 = loc("not.14204")
#loc6437 = loc("or.14214")
#loc6438 = loc("select.14215")
#loc6439 = loc("reshape.14220")
#loc6440 = loc("not.14222")
#loc6441 = loc("reshape.14224")
#loc6442 = loc("broadcast.14225")
#loc6443 = loc("reduce.14180")
#loc6444 = loc("broadcast.14181")
#loc6445 = loc("subtract.14182")
#loc6446 = loc("exponential.14183")
#loc6447 = loc("reduce.14189")
#loc6448 = loc("broadcast.14190")
#loc6449 = loc("divide.14191")
#loc6450 = loc("select.14226")
#loc6451 = loc("broadcast.14073")
#loc6452 = loc("reshape.14074")
#loc6453 = loc("convert.14075")
#loc6454 = loc("dot.14227")
#loc6455 = loc("convert.14229")
#loc6456 = loc("transpose.14230")
#loc6457 = loc("reshape.14232")
#loc6458 = loc("reshape.14066")
#loc6459 = loc("reshape.14068")
#loc6460 = loc("transpose.14069")
#loc6462 = loc("reshape.14234")
#loc6463 = loc("add.14237")
#loc6464 = loc("reshape.14268")
#loc6465 = loc("reshape.14270")
#loc6466 = loc("broadcast.14271")
#loc6467 = loc("convert.14238")
#loc6468 = loc("power.14240")
#loc6469 = loc("reduce.14247")
#loc6470 = loc("multiply.14256")
#loc6471 = loc("reshape.14257")
#loc6472 = loc("add.14261")
#loc6473 = loc("rsqrt.14262")
#loc6474 = loc("reshape.14263")
#loc6475 = loc("broadcast.14264")
#loc6476 = loc("multiply.14265")
#loc6477 = loc("convert.14266")
#loc6478 = loc("multiply.14272")
#loc6479 = loc("reshape.14281")
#loc6480 = loc("reshape.14277")
#loc6481 = loc("reshape.14279")
#loc6482 = loc("transpose.14280")
#loc6483 = loc("dot.14282")
#loc6484 = loc("reshape.14283")
#loc6485 = loc("logistic.14284")
#loc6486 = loc("multiply.14285")
#loc6487 = loc("reshape.14057")
#loc6488 = loc("reshape.14059")
#loc6489 = loc("transpose.14060")
#loc6490 = loc("dot.14274")
#loc6491 = loc("reshape.14275")
#loc6492 = loc("multiply.14286")
#loc6493 = loc("reshape.14287")
#loc6494 = loc("reshape.14052")
#loc6495 = loc("reshape.14054")
#loc6496 = loc("transpose.14055")
#loc6498 = loc("reshape.14289")
#loc6499 = loc("add.14292")
#loc6500 = loc("convert.14293")
#loc6501 = loc("power.14295")
#loc6502 = loc("reduce.14302")
#loc6503 = loc("multiply.14311")
#loc6504 = loc("reshape.14312")
#loc6505 = loc("add.14316")
#loc6506 = loc("rsqrt.14317")
#loc6507 = loc("reshape.14318")
#loc6508 = loc("broadcast.14319")
#loc6509 = loc("multiply.14320")
#loc6510 = loc("convert.14321")
#loc6511 = loc("multiply.14327")
#loc6512 = loc("reshape.14328")
#loc6513 = loc("reshape.14043")
#loc6514 = loc("reshape.14045")
#loc6515 = loc("transpose.14046")
#loc6516 = loc("dot.14329")
#loc6517 = loc("reshape.14331")
#loc6518 = loc("convert.14332")
#loc6519 = loc("power.14334")
#loc6520 = loc("reduce.14341")
#loc6521 = loc("multiply.14350")
#loc6522 = loc("reshape.14351")
#loc6523 = loc("add.14355")
#loc6524 = loc("rsqrt.14356")
#loc6525 = loc("reshape.14357")
#loc6526 = loc("broadcast.14358")
#loc6527 = loc("multiply.14359")
#loc6528 = loc("convert.14360")
#loc6529 = loc("multiply.14366")
#loc6530 = loc("transpose.14367")
#loc6531 = loc("multiply.14378")
#loc6532 = loc("slice.14369")
#loc6533 = loc("negate.14370")
#loc6534 = loc("slice.14368")
#loc6535 = loc("concatenate.14371")
#loc6536 = loc("multiply.14374")
#loc6537 = loc("add.14381")
#loc6539 = loc("reshape.14403")
#loc6540 = loc("reshape.14405")
#loc6541 = loc("transpose.14406")
#loc6542 = loc("dot.14408")
#loc6543 = loc("reshape.14410")
#loc6544 = loc("transpose.14411")
#loc6546 = loc("reshape.14757")
#loc6547 = loc("reshape.14759")
#loc6548 = loc("broadcast.14760")
#loc6549 = loc("reshape.14718")
#loc6550 = loc("reshape.14720")
#loc6551 = loc("broadcast.14721")
#loc6552 = loc("reshape.14540")
#loc6553 = loc("reshape.14542")
#loc6554 = loc("broadcast.14543")
#loc6555 = loc("reshape.14502")
#loc6556 = loc("reshape.14504")
#loc6557 = loc("transpose.14505")
#loc6558 = loc("dot.14507")
#loc6559 = loc("reshape.14509")
#loc6560 = loc("convert.14510")
#loc6561 = loc("power.14512")
#loc6562 = loc("reduce.14519")
#loc6563 = loc("multiply.14528")
#loc6564 = loc("reshape.14529")
#loc6565 = loc("add.14533")
#loc6566 = loc("rsqrt.14534")
#loc6567 = loc("reshape.14535")
#loc6568 = loc("broadcast.14536")
#loc6569 = loc("multiply.14537")
#loc6570 = loc("convert.14538")
#loc6571 = loc("multiply.14544")
#loc6572 = loc("transpose.14545")
#loc6573 = loc("multiply.14555")
#loc6574 = loc("slice.14547")
#loc6575 = loc("negate.14548")
#loc6576 = loc("slice.14546")
#loc6577 = loc("concatenate.14549")
#loc6578 = loc("multiply.14552")
#loc6579 = loc("add.14558")
#loc6580 = loc("convert.14559")
#loc6581 = loc("multiply.14561")
#loc6582 = loc("broadcast.14490")
#loc6583 = loc("reshape.14491")
#loc6584 = loc("convert.14492")
#loc6585 = loc("transpose.14493")
#loc6586 = loc("multiply.14495")
#loc6587 = loc("dot.14562")
#loc6588 = loc("add.14569")
#loc6589 = loc("convert.14595")
#loc6590 = loc("compare.14597")
#loc6591 = loc("not.14599")
#loc6593 = loc("or.14609")
#loc6594 = loc("select.14610")
#loc6595 = loc("reshape.14615")
#loc6596 = loc("not.14617")
#loc6597 = loc("reshape.14619")
#loc6598 = loc("broadcast.14620")
#loc6599 = loc("reduce.14575")
#loc6600 = loc("broadcast.14576")
#loc6601 = loc("subtract.14577")
#loc6602 = loc("exponential.14578")
#loc6603 = loc("reduce.14584")
#loc6604 = loc("broadcast.14585")
#loc6605 = loc("divide.14586")
#loc6606 = loc("select.14621")
#loc6607 = loc("broadcast.14468")
#loc6608 = loc("reshape.14469")
#loc6609 = loc("convert.14470")
#loc6610 = loc("dot.14622")
#loc6611 = loc("convert.14624")
#loc6612 = loc("transpose.14625")
#loc6613 = loc("reshape.14627")
#loc6614 = loc("reshape.14461")
#loc6615 = loc("reshape.14463")
#loc6616 = loc("transpose.14464")
#loc6618 = loc("reshape.14629")
#loc6619 = loc("add.14632")
#loc6620 = loc("reshape.14663")
#loc6621 = loc("reshape.14665")
#loc6622 = loc("broadcast.14666")
#loc6623 = loc("convert.14633")
#loc6624 = loc("power.14635")
#loc6625 = loc("reduce.14642")
#loc6626 = loc("multiply.14651")
#loc6627 = loc("reshape.14652")
#loc6628 = loc("add.14656")
#loc6629 = loc("rsqrt.14657")
#loc6630 = loc("reshape.14658")
#loc6631 = loc("broadcast.14659")
#loc6632 = loc("multiply.14660")
#loc6633 = loc("convert.14661")
#loc6634 = loc("multiply.14667")
#loc6635 = loc("reshape.14676")
#loc6636 = loc("reshape.14672")
#loc6637 = loc("reshape.14674")
#loc6638 = loc("transpose.14675")
#loc6639 = loc("dot.14677")
#loc6640 = loc("reshape.14678")
#loc6641 = loc("logistic.14679")
#loc6642 = loc("multiply.14680")
#loc6643 = loc("reshape.14452")
#loc6644 = loc("reshape.14454")
#loc6645 = loc("transpose.14455")
#loc6646 = loc("dot.14669")
#loc6647 = loc("reshape.14670")
#loc6648 = loc("multiply.14681")
#loc6649 = loc("reshape.14682")
#loc6650 = loc("reshape.14447")
#loc6651 = loc("reshape.14449")
#loc6652 = loc("transpose.14450")
#loc6654 = loc("reshape.14684")
#loc6655 = loc("add.14687")
#loc6656 = loc("convert.14688")
#loc6657 = loc("power.14690")
#loc6658 = loc("reduce.14697")
#loc6659 = loc("multiply.14706")
#loc6660 = loc("reshape.14707")
#loc6661 = loc("add.14711")
#loc6662 = loc("rsqrt.14712")
#loc6663 = loc("reshape.14713")
#loc6664 = loc("broadcast.14714")
#loc6665 = loc("multiply.14715")
#loc6666 = loc("convert.14716")
#loc6667 = loc("multiply.14722")
#loc6668 = loc("reshape.14723")
#loc6669 = loc("reshape.14438")
#loc6670 = loc("reshape.14440")
#loc6671 = loc("transpose.14441")
#loc6672 = loc("dot.14724")
#loc6673 = loc("reshape.14726")
#loc6674 = loc("convert.14727")
#loc6675 = loc("power.14729")
#loc6676 = loc("reduce.14736")
#loc6677 = loc("multiply.14745")
#loc6678 = loc("reshape.14746")
#loc6679 = loc("add.14750")
#loc6680 = loc("rsqrt.14751")
#loc6681 = loc("reshape.14752")
#loc6682 = loc("broadcast.14753")
#loc6683 = loc("multiply.14754")
#loc6684 = loc("convert.14755")
#loc6685 = loc("multiply.14761")
#loc6686 = loc("transpose.14762")
#loc6687 = loc("multiply.14773")
#loc6688 = loc("slice.14764")
#loc6689 = loc("negate.14765")
#loc6690 = loc("slice.14763")
#loc6691 = loc("concatenate.14766")
#loc6692 = loc("multiply.14769")
#loc6693 = loc("add.14776")
#loc6695 = loc("reshape.14798")
#loc6696 = loc("reshape.14800")
#loc6697 = loc("transpose.14801")
#loc6698 = loc("dot.14803")
#loc6699 = loc("reshape.14805")
#loc6700 = loc("transpose.14806")
#loc6702 = loc("reshape.15152")
#loc6703 = loc("reshape.15154")
#loc6704 = loc("broadcast.15155")
#loc6705 = loc("reshape.15113")
#loc6706 = loc("reshape.15115")
#loc6707 = loc("broadcast.15116")
#loc6708 = loc("reshape.14935")
#loc6709 = loc("reshape.14937")
#loc6710 = loc("broadcast.14938")
#loc6711 = loc("reshape.14897")
#loc6712 = loc("reshape.14899")
#loc6713 = loc("transpose.14900")
#loc6714 = loc("dot.14902")
#loc6715 = loc("reshape.14904")
#loc6716 = loc("convert.14905")
#loc6717 = loc("power.14907")
#loc6718 = loc("reduce.14914")
#loc6719 = loc("multiply.14923")
#loc6720 = loc("reshape.14924")
#loc6721 = loc("add.14928")
#loc6722 = loc("rsqrt.14929")
#loc6723 = loc("reshape.14930")
#loc6724 = loc("broadcast.14931")
#loc6725 = loc("multiply.14932")
#loc6726 = loc("convert.14933")
#loc6727 = loc("multiply.14939")
#loc6728 = loc("transpose.14940")
#loc6729 = loc("multiply.14950")
#loc6730 = loc("slice.14942")
#loc6731 = loc("negate.14943")
#loc6732 = loc("slice.14941")
#loc6733 = loc("concatenate.14944")
#loc6734 = loc("multiply.14947")
#loc6735 = loc("add.14953")
#loc6736 = loc("convert.14954")
#loc6737 = loc("multiply.14956")
#loc6738 = loc("broadcast.14885")
#loc6739 = loc("reshape.14886")
#loc6740 = loc("convert.14887")
#loc6741 = loc("transpose.14888")
#loc6742 = loc("multiply.14890")
#loc6743 = loc("dot.14957")
#loc6744 = loc("add.14964")
#loc6745 = loc("convert.14990")
#loc6746 = loc("compare.14992")
#loc6747 = loc("not.14994")
#loc6749 = loc("or.15004")
#loc6750 = loc("select.15005")
#loc6751 = loc("reshape.15010")
#loc6752 = loc("not.15012")
#loc6753 = loc("reshape.15014")
#loc6754 = loc("broadcast.15015")
#loc6755 = loc("reduce.14970")
#loc6756 = loc("broadcast.14971")
#loc6757 = loc("subtract.14972")
#loc6758 = loc("exponential.14973")
#loc6759 = loc("reduce.14979")
#loc6760 = loc("broadcast.14980")
#loc6761 = loc("divide.14981")
#loc6762 = loc("select.15016")
#loc6763 = loc("broadcast.14863")
#loc6764 = loc("reshape.14864")
#loc6765 = loc("convert.14865")
#loc6766 = loc("dot.15017")
#loc6767 = loc("convert.15019")
#loc6768 = loc("transpose.15020")
#loc6769 = loc("reshape.15022")
#loc6770 = loc("reshape.14856")
#loc6771 = loc("reshape.14858")
#loc6772 = loc("transpose.14859")
#loc6774 = loc("reshape.15024")
#loc6775 = loc("add.15027")
#loc6776 = loc("reshape.15058")
#loc6777 = loc("reshape.15060")
#loc6778 = loc("broadcast.15061")
#loc6779 = loc("convert.15028")
#loc6780 = loc("power.15030")
#loc6781 = loc("reduce.15037")
#loc6782 = loc("multiply.15046")
#loc6783 = loc("reshape.15047")
#loc6784 = loc("add.15051")
#loc6785 = loc("rsqrt.15052")
#loc6786 = loc("reshape.15053")
#loc6787 = loc("broadcast.15054")
#loc6788 = loc("multiply.15055")
#loc6789 = loc("convert.15056")
#loc6790 = loc("multiply.15062")
#loc6791 = loc("reshape.15071")
#loc6792 = loc("reshape.15067")
#loc6793 = loc("reshape.15069")
#loc6794 = loc("transpose.15070")
#loc6795 = loc("dot.15072")
#loc6796 = loc("reshape.15073")
#loc6797 = loc("logistic.15074")
#loc6798 = loc("multiply.15075")
#loc6799 = loc("reshape.14847")
#loc6800 = loc("reshape.14849")
#loc6801 = loc("transpose.14850")
#loc6802 = loc("dot.15064")
#loc6803 = loc("reshape.15065")
#loc6804 = loc("multiply.15076")
#loc6805 = loc("reshape.15077")
#loc6806 = loc("reshape.14842")
#loc6807 = loc("reshape.14844")
#loc6808 = loc("transpose.14845")
#loc6810 = loc("reshape.15079")
#loc6811 = loc("add.15082")
#loc6812 = loc("convert.15083")
#loc6813 = loc("power.15085")
#loc6814 = loc("reduce.15092")
#loc6815 = loc("multiply.15101")
#loc6816 = loc("reshape.15102")
#loc6817 = loc("add.15106")
#loc6818 = loc("rsqrt.15107")
#loc6819 = loc("reshape.15108")
#loc6820 = loc("broadcast.15109")
#loc6821 = loc("multiply.15110")
#loc6822 = loc("convert.15111")
#loc6823 = loc("multiply.15117")
#loc6824 = loc("reshape.15118")
#loc6825 = loc("reshape.14833")
#loc6826 = loc("reshape.14835")
#loc6827 = loc("transpose.14836")
#loc6828 = loc("dot.15119")
#loc6829 = loc("reshape.15121")
#loc6830 = loc("convert.15122")
#loc6831 = loc("power.15124")
#loc6832 = loc("reduce.15131")
#loc6833 = loc("multiply.15140")
#loc6834 = loc("reshape.15141")
#loc6835 = loc("add.15145")
#loc6836 = loc("rsqrt.15146")
#loc6837 = loc("reshape.15147")
#loc6838 = loc("broadcast.15148")
#loc6839 = loc("multiply.15149")
#loc6840 = loc("convert.15150")
#loc6841 = loc("multiply.15156")
#loc6842 = loc("transpose.15157")
#loc6843 = loc("multiply.15168")
#loc6844 = loc("slice.15159")
#loc6845 = loc("negate.15160")
#loc6846 = loc("slice.15158")
#loc6847 = loc("concatenate.15161")
#loc6848 = loc("multiply.15164")
#loc6849 = loc("add.15171")
#loc6851 = loc("reshape.15193")
#loc6852 = loc("reshape.15195")
#loc6853 = loc("transpose.15196")
#loc6854 = loc("dot.15198")
#loc6855 = loc("reshape.15200")
#loc6856 = loc("transpose.15201")
#loc6858 = loc("reshape.15547")
#loc6859 = loc("reshape.15549")
#loc6860 = loc("broadcast.15550")
#loc6861 = loc("reshape.15508")
#loc6862 = loc("reshape.15510")
#loc6863 = loc("broadcast.15511")
#loc6864 = loc("reshape.15330")
#loc6865 = loc("reshape.15332")
#loc6866 = loc("broadcast.15333")
#loc6867 = loc("reshape.15292")
#loc6868 = loc("reshape.15294")
#loc6869 = loc("transpose.15295")
#loc6870 = loc("dot.15297")
#loc6871 = loc("reshape.15299")
#loc6872 = loc("convert.15300")
#loc6873 = loc("power.15302")
#loc6874 = loc("reduce.15309")
#loc6875 = loc("multiply.15318")
#loc6876 = loc("reshape.15319")
#loc6877 = loc("add.15323")
#loc6878 = loc("rsqrt.15324")
#loc6879 = loc("reshape.15325")
#loc6880 = loc("broadcast.15326")
#loc6881 = loc("multiply.15327")
#loc6882 = loc("convert.15328")
#loc6883 = loc("multiply.15334")
#loc6884 = loc("transpose.15335")
#loc6885 = loc("multiply.15345")
#loc6886 = loc("slice.15337")
#loc6887 = loc("negate.15338")
#loc6888 = loc("slice.15336")
#loc6889 = loc("concatenate.15339")
#loc6890 = loc("multiply.15342")
#loc6891 = loc("add.15348")
#loc6892 = loc("convert.15349")
#loc6893 = loc("multiply.15351")
#loc6894 = loc("broadcast.15280")
#loc6895 = loc("reshape.15281")
#loc6896 = loc("convert.15282")
#loc6897 = loc("transpose.15283")
#loc6898 = loc("multiply.15285")
#loc6899 = loc("dot.15352")
#loc6900 = loc("add.15359")
#loc6901 = loc("convert.15385")
#loc6902 = loc("compare.15387")
#loc6903 = loc("not.15389")
#loc6905 = loc("or.15399")
#loc6906 = loc("select.15400")
#loc6907 = loc("reshape.15405")
#loc6908 = loc("not.15407")
#loc6909 = loc("reshape.15409")
#loc6910 = loc("broadcast.15410")
#loc6911 = loc("reduce.15365")
#loc6912 = loc("broadcast.15366")
#loc6913 = loc("subtract.15367")
#loc6914 = loc("exponential.15368")
#loc6915 = loc("reduce.15374")
#loc6916 = loc("broadcast.15375")
#loc6917 = loc("divide.15376")
#loc6918 = loc("select.15411")
#loc6919 = loc("broadcast.15258")
#loc6920 = loc("reshape.15259")
#loc6921 = loc("convert.15260")
#loc6922 = loc("dot.15412")
#loc6923 = loc("convert.15414")
#loc6924 = loc("transpose.15415")
#loc6925 = loc("reshape.15417")
#loc6926 = loc("reshape.15251")
#loc6927 = loc("reshape.15253")
#loc6928 = loc("transpose.15254")
#loc6930 = loc("reshape.15419")
#loc6931 = loc("add.15422")
#loc6932 = loc("reshape.15453")
#loc6933 = loc("reshape.15455")
#loc6934 = loc("broadcast.15456")
#loc6935 = loc("convert.15423")
#loc6936 = loc("power.15425")
#loc6937 = loc("reduce.15432")
#loc6938 = loc("multiply.15441")
#loc6939 = loc("reshape.15442")
#loc6940 = loc("add.15446")
#loc6941 = loc("rsqrt.15447")
#loc6942 = loc("reshape.15448")
#loc6943 = loc("broadcast.15449")
#loc6944 = loc("multiply.15450")
#loc6945 = loc("convert.15451")
#loc6946 = loc("multiply.15457")
#loc6947 = loc("reshape.15466")
#loc6948 = loc("reshape.15462")
#loc6949 = loc("reshape.15464")
#loc6950 = loc("transpose.15465")
#loc6951 = loc("dot.15467")
#loc6952 = loc("reshape.15468")
#loc6953 = loc("logistic.15469")
#loc6954 = loc("multiply.15470")
#loc6955 = loc("reshape.15242")
#loc6956 = loc("reshape.15244")
#loc6957 = loc("transpose.15245")
#loc6958 = loc("dot.15459")
#loc6959 = loc("reshape.15460")
#loc6960 = loc("multiply.15471")
#loc6961 = loc("reshape.15472")
#loc6962 = loc("reshape.15237")
#loc6963 = loc("reshape.15239")
#loc6964 = loc("transpose.15240")
#loc6966 = loc("reshape.15474")
#loc6967 = loc("add.15477")
#loc6968 = loc("convert.15478")
#loc6969 = loc("power.15480")
#loc6970 = loc("reduce.15487")
#loc6971 = loc("multiply.15496")
#loc6972 = loc("reshape.15497")
#loc6973 = loc("add.15501")
#loc6974 = loc("rsqrt.15502")
#loc6975 = loc("reshape.15503")
#loc6976 = loc("broadcast.15504")
#loc6977 = loc("multiply.15505")
#loc6978 = loc("convert.15506")
#loc6979 = loc("multiply.15512")
#loc6980 = loc("reshape.15513")
#loc6981 = loc("reshape.15228")
#loc6982 = loc("reshape.15230")
#loc6983 = loc("transpose.15231")
#loc6984 = loc("dot.15514")
#loc6985 = loc("reshape.15516")
#loc6986 = loc("convert.15517")
#loc6987 = loc("power.15519")
#loc6988 = loc("reduce.15526")
#loc6989 = loc("multiply.15535")
#loc6990 = loc("reshape.15536")
#loc6991 = loc("add.15540")
#loc6992 = loc("rsqrt.15541")
#loc6993 = loc("reshape.15542")
#loc6994 = loc("broadcast.15543")
#loc6995 = loc("multiply.15544")
#loc6996 = loc("convert.15545")
#loc6997 = loc("multiply.15551")
#loc6998 = loc("transpose.15552")
#loc6999 = loc("multiply.15563")
#loc7000 = loc("slice.15554")
#loc7001 = loc("negate.15555")
#loc7002 = loc("slice.15553")
#loc7003 = loc("concatenate.15556")
#loc7004 = loc("multiply.15559")
#loc7005 = loc("add.15566")
#loc7007 = loc("reshape.15588")
#loc7008 = loc("reshape.15590")
#loc7009 = loc("transpose.15591")
#loc7010 = loc("dot.15593")
#loc7011 = loc("reshape.15595")
#loc7012 = loc("transpose.15596")
#loc7014 = loc("reshape.15942")
#loc7015 = loc("reshape.15944")
#loc7016 = loc("broadcast.15945")
#loc7017 = loc("reshape.15903")
#loc7018 = loc("reshape.15905")
#loc7019 = loc("broadcast.15906")
#loc7020 = loc("reshape.15725")
#loc7021 = loc("reshape.15727")
#loc7022 = loc("broadcast.15728")
#loc7023 = loc("reshape.15687")
#loc7024 = loc("reshape.15689")
#loc7025 = loc("transpose.15690")
#loc7026 = loc("dot.15692")
#loc7027 = loc("reshape.15694")
#loc7028 = loc("convert.15695")
#loc7029 = loc("power.15697")
#loc7030 = loc("reduce.15704")
#loc7031 = loc("multiply.15713")
#loc7032 = loc("reshape.15714")
#loc7033 = loc("add.15718")
#loc7034 = loc("rsqrt.15719")
#loc7035 = loc("reshape.15720")
#loc7036 = loc("broadcast.15721")
#loc7037 = loc("multiply.15722")
#loc7038 = loc("convert.15723")
#loc7039 = loc("multiply.15729")
#loc7040 = loc("transpose.15730")
#loc7041 = loc("multiply.15740")
#loc7042 = loc("slice.15732")
#loc7043 = loc("negate.15733")
#loc7044 = loc("slice.15731")
#loc7045 = loc("concatenate.15734")
#loc7046 = loc("multiply.15737")
#loc7047 = loc("add.15743")
#loc7048 = loc("convert.15744")
#loc7049 = loc("multiply.15746")
#loc7050 = loc("broadcast.15675")
#loc7051 = loc("reshape.15676")
#loc7052 = loc("convert.15677")
#loc7053 = loc("transpose.15678")
#loc7054 = loc("multiply.15680")
#loc7055 = loc("dot.15747")
#loc7056 = loc("add.15754")
#loc7057 = loc("convert.15780")
#loc7058 = loc("compare.15782")
#loc7059 = loc("not.15784")
#loc7061 = loc("or.15794")
#loc7062 = loc("select.15795")
#loc7063 = loc("reshape.15800")
#loc7064 = loc("not.15802")
#loc7065 = loc("reshape.15804")
#loc7066 = loc("broadcast.15805")
#loc7067 = loc("reduce.15760")
#loc7068 = loc("broadcast.15761")
#loc7069 = loc("subtract.15762")
#loc7070 = loc("exponential.15763")
#loc7071 = loc("reduce.15769")
#loc7072 = loc("broadcast.15770")
#loc7073 = loc("divide.15771")
#loc7074 = loc("select.15806")
#loc7075 = loc("broadcast.15653")
#loc7076 = loc("reshape.15654")
#loc7077 = loc("convert.15655")
#loc7078 = loc("dot.15807")
#loc7079 = loc("convert.15809")
#loc7080 = loc("transpose.15810")
#loc7081 = loc("reshape.15812")
#loc7082 = loc("reshape.15646")
#loc7083 = loc("reshape.15648")
#loc7084 = loc("transpose.15649")
#loc7086 = loc("reshape.15814")
#loc7087 = loc("add.15817")
#loc7088 = loc("reshape.15848")
#loc7089 = loc("reshape.15850")
#loc7090 = loc("broadcast.15851")
#loc7091 = loc("convert.15818")
#loc7092 = loc("power.15820")
#loc7093 = loc("reduce.15827")
#loc7094 = loc("multiply.15836")
#loc7095 = loc("reshape.15837")
#loc7096 = loc("add.15841")
#loc7097 = loc("rsqrt.15842")
#loc7098 = loc("reshape.15843")
#loc7099 = loc("broadcast.15844")
#loc7100 = loc("multiply.15845")
#loc7101 = loc("convert.15846")
#loc7102 = loc("multiply.15852")
#loc7103 = loc("reshape.15861")
#loc7104 = loc("reshape.15857")
#loc7105 = loc("reshape.15859")
#loc7106 = loc("transpose.15860")
#loc7107 = loc("dot.15862")
#loc7108 = loc("reshape.15863")
#loc7109 = loc("logistic.15864")
#loc7110 = loc("multiply.15865")
#loc7111 = loc("reshape.15637")
#loc7112 = loc("reshape.15639")
#loc7113 = loc("transpose.15640")
#loc7114 = loc("dot.15854")
#loc7115 = loc("reshape.15855")
#loc7116 = loc("multiply.15866")
#loc7117 = loc("reshape.15867")
#loc7118 = loc("reshape.15632")
#loc7119 = loc("reshape.15634")
#loc7120 = loc("transpose.15635")
#loc7122 = loc("reshape.15869")
#loc7123 = loc("add.15872")
#loc7124 = loc("convert.15873")
#loc7125 = loc("power.15875")
#loc7126 = loc("reduce.15882")
#loc7127 = loc("multiply.15891")
#loc7128 = loc("reshape.15892")
#loc7129 = loc("add.15896")
#loc7130 = loc("rsqrt.15897")
#loc7131 = loc("reshape.15898")
#loc7132 = loc("broadcast.15899")
#loc7133 = loc("multiply.15900")
#loc7134 = loc("convert.15901")
#loc7135 = loc("multiply.15907")
#loc7136 = loc("reshape.15908")
#loc7137 = loc("reshape.15623")
#loc7138 = loc("reshape.15625")
#loc7139 = loc("transpose.15626")
#loc7140 = loc("dot.15909")
#loc7141 = loc("reshape.15911")
#loc7142 = loc("convert.15912")
#loc7143 = loc("power.15914")
#loc7144 = loc("reduce.15921")
#loc7145 = loc("multiply.15930")
#loc7146 = loc("reshape.15931")
#loc7147 = loc("add.15935")
#loc7148 = loc("rsqrt.15936")
#loc7149 = loc("reshape.15937")
#loc7150 = loc("broadcast.15938")
#loc7151 = loc("multiply.15939")
#loc7152 = loc("convert.15940")
#loc7153 = loc("multiply.15946")
#loc7154 = loc("transpose.15947")
#loc7155 = loc("multiply.15958")
#loc7156 = loc("slice.15949")
#loc7157 = loc("negate.15950")
#loc7158 = loc("slice.15948")
#loc7159 = loc("concatenate.15951")
#loc7160 = loc("multiply.15954")
#loc7161 = loc("add.15961")
#loc7163 = loc("reshape.15983")
#loc7164 = loc("reshape.15985")
#loc7165 = loc("transpose.15986")
#loc7166 = loc("dot.15988")
#loc7167 = loc("reshape.15990")
#loc7168 = loc("transpose.15991")
#loc7170 = loc("reshape.16337")
#loc7171 = loc("reshape.16339")
#loc7172 = loc("broadcast.16340")
#loc7173 = loc("reshape.16298")
#loc7174 = loc("reshape.16300")
#loc7175 = loc("broadcast.16301")
#loc7176 = loc("reshape.16120")
#loc7177 = loc("reshape.16122")
#loc7178 = loc("broadcast.16123")
#loc7179 = loc("reshape.16082")
#loc7180 = loc("reshape.16084")
#loc7181 = loc("transpose.16085")
#loc7182 = loc("dot.16087")
#loc7183 = loc("reshape.16089")
#loc7184 = loc("convert.16090")
#loc7185 = loc("power.16092")
#loc7186 = loc("reduce.16099")
#loc7187 = loc("multiply.16108")
#loc7188 = loc("reshape.16109")
#loc7189 = loc("add.16113")
#loc7190 = loc("rsqrt.16114")
#loc7191 = loc("reshape.16115")
#loc7192 = loc("broadcast.16116")
#loc7193 = loc("multiply.16117")
#loc7194 = loc("convert.16118")
#loc7195 = loc("multiply.16124")
#loc7196 = loc("transpose.16125")
#loc7197 = loc("multiply.16135")
#loc7198 = loc("slice.16127")
#loc7199 = loc("negate.16128")
#loc7200 = loc("slice.16126")
#loc7201 = loc("concatenate.16129")
#loc7202 = loc("multiply.16132")
#loc7203 = loc("add.16138")
#loc7204 = loc("convert.16139")
#loc7205 = loc("multiply.16141")
#loc7206 = loc("broadcast.16070")
#loc7207 = loc("reshape.16071")
#loc7208 = loc("convert.16072")
#loc7209 = loc("transpose.16073")
#loc7210 = loc("multiply.16075")
#loc7211 = loc("dot.16142")
#loc7212 = loc("add.16149")
#loc7213 = loc("convert.16175")
#loc7214 = loc("compare.16177")
#loc7215 = loc("not.16179")
#loc7217 = loc("or.16189")
#loc7218 = loc("select.16190")
#loc7219 = loc("reshape.16195")
#loc7220 = loc("not.16197")
#loc7221 = loc("reshape.16199")
#loc7222 = loc("broadcast.16200")
#loc7223 = loc("reduce.16155")
#loc7224 = loc("broadcast.16156")
#loc7225 = loc("subtract.16157")
#loc7226 = loc("exponential.16158")
#loc7227 = loc("reduce.16164")
#loc7228 = loc("broadcast.16165")
#loc7229 = loc("divide.16166")
#loc7230 = loc("select.16201")
#loc7231 = loc("broadcast.16048")
#loc7232 = loc("reshape.16049")
#loc7233 = loc("convert.16050")
#loc7234 = loc("dot.16202")
#loc7235 = loc("convert.16204")
#loc7236 = loc("transpose.16205")
#loc7237 = loc("reshape.16207")
#loc7238 = loc("reshape.16041")
#loc7239 = loc("reshape.16043")
#loc7240 = loc("transpose.16044")
#loc7242 = loc("reshape.16209")
#loc7243 = loc("add.16212")
#loc7244 = loc("reshape.16243")
#loc7245 = loc("reshape.16245")
#loc7246 = loc("broadcast.16246")
#loc7247 = loc("convert.16213")
#loc7248 = loc("power.16215")
#loc7249 = loc("reduce.16222")
#loc7250 = loc("multiply.16231")
#loc7251 = loc("reshape.16232")
#loc7252 = loc("add.16236")
#loc7253 = loc("rsqrt.16237")
#loc7254 = loc("reshape.16238")
#loc7255 = loc("broadcast.16239")
#loc7256 = loc("multiply.16240")
#loc7257 = loc("convert.16241")
#loc7258 = loc("multiply.16247")
#loc7259 = loc("reshape.16256")
#loc7260 = loc("reshape.16252")
#loc7261 = loc("reshape.16254")
#loc7262 = loc("transpose.16255")
#loc7263 = loc("dot.16257")
#loc7264 = loc("reshape.16258")
#loc7265 = loc("logistic.16259")
#loc7266 = loc("multiply.16260")
#loc7267 = loc("reshape.16032")
#loc7268 = loc("reshape.16034")
#loc7269 = loc("transpose.16035")
#loc7270 = loc("dot.16249")
#loc7271 = loc("reshape.16250")
#loc7272 = loc("multiply.16261")
#loc7273 = loc("reshape.16262")
#loc7274 = loc("reshape.16027")
#loc7275 = loc("reshape.16029")
#loc7276 = loc("transpose.16030")
#loc7278 = loc("reshape.16264")
#loc7279 = loc("add.16267")
#loc7280 = loc("convert.16268")
#loc7281 = loc("power.16270")
#loc7282 = loc("reduce.16277")
#loc7283 = loc("multiply.16286")
#loc7284 = loc("reshape.16287")
#loc7285 = loc("add.16291")
#loc7286 = loc("rsqrt.16292")
#loc7287 = loc("reshape.16293")
#loc7288 = loc("broadcast.16294")
#loc7289 = loc("multiply.16295")
#loc7290 = loc("convert.16296")
#loc7291 = loc("multiply.16302")
#loc7292 = loc("reshape.16303")
#loc7293 = loc("reshape.16018")
#loc7294 = loc("reshape.16020")
#loc7295 = loc("transpose.16021")
#loc7296 = loc("dot.16304")
#loc7297 = loc("reshape.16306")
#loc7298 = loc("convert.16307")
#loc7299 = loc("power.16309")
#loc7300 = loc("reduce.16316")
#loc7301 = loc("multiply.16325")
#loc7302 = loc("reshape.16326")
#loc7303 = loc("add.16330")
#loc7304 = loc("rsqrt.16331")
#loc7305 = loc("reshape.16332")
#loc7306 = loc("broadcast.16333")
#loc7307 = loc("multiply.16334")
#loc7308 = loc("convert.16335")
#loc7309 = loc("multiply.16341")
#loc7310 = loc("transpose.16342")
#loc7311 = loc("multiply.16353")
#loc7312 = loc("slice.16344")
#loc7313 = loc("negate.16345")
#loc7314 = loc("slice.16343")
#loc7315 = loc("concatenate.16346")
#loc7316 = loc("multiply.16349")
#loc7317 = loc("add.16356")
#loc7319 = loc("reshape.16378")
#loc7320 = loc("reshape.16380")
#loc7321 = loc("transpose.16381")
#loc7322 = loc("dot.16383")
#loc7323 = loc("reshape.16385")
#loc7324 = loc("transpose.16386")
#loc7326 = loc("reshape.16732")
#loc7327 = loc("reshape.16734")
#loc7328 = loc("broadcast.16735")
#loc7329 = loc("reshape.16693")
#loc7330 = loc("reshape.16695")
#loc7331 = loc("broadcast.16696")
#loc7332 = loc("reshape.16515")
#loc7333 = loc("reshape.16517")
#loc7334 = loc("broadcast.16518")
#loc7335 = loc("reshape.16477")
#loc7336 = loc("reshape.16479")
#loc7337 = loc("transpose.16480")
#loc7338 = loc("dot.16482")
#loc7339 = loc("reshape.16484")
#loc7340 = loc("convert.16485")
#loc7341 = loc("power.16487")
#loc7342 = loc("reduce.16494")
#loc7343 = loc("multiply.16503")
#loc7344 = loc("reshape.16504")
#loc7345 = loc("add.16508")
#loc7346 = loc("rsqrt.16509")
#loc7347 = loc("reshape.16510")
#loc7348 = loc("broadcast.16511")
#loc7349 = loc("multiply.16512")
#loc7350 = loc("convert.16513")
#loc7351 = loc("multiply.16519")
#loc7352 = loc("transpose.16520")
#loc7353 = loc("multiply.16530")
#loc7354 = loc("slice.16522")
#loc7355 = loc("negate.16523")
#loc7356 = loc("slice.16521")
#loc7357 = loc("concatenate.16524")
#loc7358 = loc("multiply.16527")
#loc7359 = loc("add.16533")
#loc7360 = loc("convert.16534")
#loc7361 = loc("multiply.16536")
#loc7362 = loc("broadcast.16465")
#loc7363 = loc("reshape.16466")
#loc7364 = loc("convert.16467")
#loc7365 = loc("transpose.16468")
#loc7366 = loc("multiply.16470")
#loc7367 = loc("dot.16537")
#loc7368 = loc("add.16544")
#loc7369 = loc("convert.16570")
#loc7370 = loc("compare.16572")
#loc7371 = loc("not.16574")
#loc7373 = loc("or.16584")
#loc7374 = loc("select.16585")
#loc7375 = loc("reshape.16590")
#loc7376 = loc("not.16592")
#loc7377 = loc("reshape.16594")
#loc7378 = loc("broadcast.16595")
#loc7379 = loc("reduce.16550")
#loc7380 = loc("broadcast.16551")
#loc7381 = loc("subtract.16552")
#loc7382 = loc("exponential.16553")
#loc7383 = loc("reduce.16559")
#loc7384 = loc("broadcast.16560")
#loc7385 = loc("divide.16561")
#loc7386 = loc("select.16596")
#loc7387 = loc("broadcast.16443")
#loc7388 = loc("reshape.16444")
#loc7389 = loc("convert.16445")
#loc7390 = loc("dot.16597")
#loc7391 = loc("convert.16599")
#loc7392 = loc("transpose.16600")
#loc7393 = loc("reshape.16602")
#loc7394 = loc("reshape.16436")
#loc7395 = loc("reshape.16438")
#loc7396 = loc("transpose.16439")
#loc7398 = loc("reshape.16604")
#loc7399 = loc("add.16607")
#loc7400 = loc("reshape.16638")
#loc7401 = loc("reshape.16640")
#loc7402 = loc("broadcast.16641")
#loc7403 = loc("convert.16608")
#loc7404 = loc("power.16610")
#loc7405 = loc("reduce.16617")
#loc7406 = loc("multiply.16626")
#loc7407 = loc("reshape.16627")
#loc7408 = loc("add.16631")
#loc7409 = loc("rsqrt.16632")
#loc7410 = loc("reshape.16633")
#loc7411 = loc("broadcast.16634")
#loc7412 = loc("multiply.16635")
#loc7413 = loc("convert.16636")
#loc7414 = loc("multiply.16642")
#loc7415 = loc("reshape.16651")
#loc7416 = loc("reshape.16647")
#loc7417 = loc("reshape.16649")
#loc7418 = loc("transpose.16650")
#loc7419 = loc("dot.16652")
#loc7420 = loc("reshape.16653")
#loc7421 = loc("logistic.16654")
#loc7422 = loc("multiply.16655")
#loc7423 = loc("reshape.16427")
#loc7424 = loc("reshape.16429")
#loc7425 = loc("transpose.16430")
#loc7426 = loc("dot.16644")
#loc7427 = loc("reshape.16645")
#loc7428 = loc("multiply.16656")
#loc7429 = loc("reshape.16657")
#loc7430 = loc("reshape.16422")
#loc7431 = loc("reshape.16424")
#loc7432 = loc("transpose.16425")
#loc7434 = loc("reshape.16659")
#loc7435 = loc("add.16662")
#loc7436 = loc("convert.16663")
#loc7437 = loc("power.16665")
#loc7438 = loc("reduce.16672")
#loc7439 = loc("multiply.16681")
#loc7440 = loc("reshape.16682")
#loc7441 = loc("add.16686")
#loc7442 = loc("rsqrt.16687")
#loc7443 = loc("reshape.16688")
#loc7444 = loc("broadcast.16689")
#loc7445 = loc("multiply.16690")
#loc7446 = loc("convert.16691")
#loc7447 = loc("multiply.16697")
#loc7448 = loc("reshape.16698")
#loc7449 = loc("reshape.16413")
#loc7450 = loc("reshape.16415")
#loc7451 = loc("transpose.16416")
#loc7452 = loc("dot.16699")
#loc7453 = loc("reshape.16701")
#loc7454 = loc("convert.16702")
#loc7455 = loc("power.16704")
#loc7456 = loc("reduce.16711")
#loc7457 = loc("multiply.16720")
#loc7458 = loc("reshape.16721")
#loc7459 = loc("add.16725")
#loc7460 = loc("rsqrt.16726")
#loc7461 = loc("reshape.16727")
#loc7462 = loc("broadcast.16728")
#loc7463 = loc("multiply.16729")
#loc7464 = loc("convert.16730")
#loc7465 = loc("multiply.16736")
#loc7466 = loc("transpose.16737")
#loc7467 = loc("multiply.16748")
#loc7468 = loc("slice.16739")
#loc7469 = loc("negate.16740")
#loc7470 = loc("slice.16738")
#loc7471 = loc("concatenate.16741")
#loc7472 = loc("multiply.16744")
#loc7473 = loc("add.16751")
#loc7475 = loc("reshape.16773")
#loc7476 = loc("reshape.16775")
#loc7477 = loc("transpose.16776")
#loc7478 = loc("dot.16778")
#loc7479 = loc("reshape.16780")
#loc7480 = loc("transpose.16781")
#loc7482 = loc("reshape.17127")
#loc7483 = loc("reshape.17129")
#loc7484 = loc("broadcast.17130")
#loc7485 = loc("reshape.17088")
#loc7486 = loc("reshape.17090")
#loc7487 = loc("broadcast.17091")
#loc7488 = loc("reshape.16910")
#loc7489 = loc("reshape.16912")
#loc7490 = loc("broadcast.16913")
#loc7491 = loc("reshape.16872")
#loc7492 = loc("reshape.16874")
#loc7493 = loc("transpose.16875")
#loc7494 = loc("dot.16877")
#loc7495 = loc("reshape.16879")
#loc7496 = loc("convert.16880")
#loc7497 = loc("power.16882")
#loc7498 = loc("reduce.16889")
#loc7499 = loc("multiply.16898")
#loc7500 = loc("reshape.16899")
#loc7501 = loc("add.16903")
#loc7502 = loc("rsqrt.16904")
#loc7503 = loc("reshape.16905")
#loc7504 = loc("broadcast.16906")
#loc7505 = loc("multiply.16907")
#loc7506 = loc("convert.16908")
#loc7507 = loc("multiply.16914")
#loc7508 = loc("transpose.16915")
#loc7509 = loc("multiply.16925")
#loc7510 = loc("slice.16917")
#loc7511 = loc("negate.16918")
#loc7512 = loc("slice.16916")
#loc7513 = loc("concatenate.16919")
#loc7514 = loc("multiply.16922")
#loc7515 = loc("add.16928")
#loc7516 = loc("convert.16929")
#loc7517 = loc("multiply.16931")
#loc7518 = loc("broadcast.16860")
#loc7519 = loc("reshape.16861")
#loc7520 = loc("convert.16862")
#loc7521 = loc("transpose.16863")
#loc7522 = loc("multiply.16865")
#loc7523 = loc("dot.16932")
#loc7524 = loc("add.16939")
#loc7525 = loc("convert.16965")
#loc7526 = loc("compare.16967")
#loc7527 = loc("not.16969")
#loc7529 = loc("or.16979")
#loc7530 = loc("select.16980")
#loc7531 = loc("reshape.16985")
#loc7532 = loc("not.16987")
#loc7533 = loc("reshape.16989")
#loc7534 = loc("broadcast.16990")
#loc7535 = loc("reduce.16945")
#loc7536 = loc("broadcast.16946")
#loc7537 = loc("subtract.16947")
#loc7538 = loc("exponential.16948")
#loc7539 = loc("reduce.16954")
#loc7540 = loc("broadcast.16955")
#loc7541 = loc("divide.16956")
#loc7542 = loc("select.16991")
#loc7543 = loc("broadcast.16838")
#loc7544 = loc("reshape.16839")
#loc7545 = loc("convert.16840")
#loc7546 = loc("dot.16992")
#loc7547 = loc("convert.16994")
#loc7548 = loc("transpose.16995")
#loc7549 = loc("reshape.16997")
#loc7550 = loc("reshape.16831")
#loc7551 = loc("reshape.16833")
#loc7552 = loc("transpose.16834")
#loc7554 = loc("reshape.16999")
#loc7555 = loc("add.17002")
#loc7556 = loc("reshape.17033")
#loc7557 = loc("reshape.17035")
#loc7558 = loc("broadcast.17036")
#loc7559 = loc("convert.17003")
#loc7560 = loc("power.17005")
#loc7561 = loc("reduce.17012")
#loc7562 = loc("multiply.17021")
#loc7563 = loc("reshape.17022")
#loc7564 = loc("add.17026")
#loc7565 = loc("rsqrt.17027")
#loc7566 = loc("reshape.17028")
#loc7567 = loc("broadcast.17029")
#loc7568 = loc("multiply.17030")
#loc7569 = loc("convert.17031")
#loc7570 = loc("multiply.17037")
#loc7571 = loc("reshape.17046")
#loc7572 = loc("reshape.17042")
#loc7573 = loc("reshape.17044")
#loc7574 = loc("transpose.17045")
#loc7575 = loc("dot.17047")
#loc7576 = loc("reshape.17048")
#loc7577 = loc("logistic.17049")
#loc7578 = loc("multiply.17050")
#loc7579 = loc("reshape.16822")
#loc7580 = loc("reshape.16824")
#loc7581 = loc("transpose.16825")
#loc7582 = loc("dot.17039")
#loc7583 = loc("reshape.17040")
#loc7584 = loc("multiply.17051")
#loc7585 = loc("reshape.17052")
#loc7586 = loc("reshape.16817")
#loc7587 = loc("reshape.16819")
#loc7588 = loc("transpose.16820")
#loc7590 = loc("reshape.17054")
#loc7591 = loc("add.17057")
#loc7592 = loc("convert.17058")
#loc7593 = loc("power.17060")
#loc7594 = loc("reduce.17067")
#loc7595 = loc("multiply.17076")
#loc7596 = loc("reshape.17077")
#loc7597 = loc("add.17081")
#loc7598 = loc("rsqrt.17082")
#loc7599 = loc("reshape.17083")
#loc7600 = loc("broadcast.17084")
#loc7601 = loc("multiply.17085")
#loc7602 = loc("convert.17086")
#loc7603 = loc("multiply.17092")
#loc7604 = loc("reshape.17093")
#loc7605 = loc("reshape.16808")
#loc7606 = loc("reshape.16810")
#loc7607 = loc("transpose.16811")
#loc7608 = loc("dot.17094")
#loc7609 = loc("reshape.17096")
#loc7610 = loc("convert.17097")
#loc7611 = loc("power.17099")
#loc7612 = loc("reduce.17106")
#loc7613 = loc("multiply.17115")
#loc7614 = loc("reshape.17116")
#loc7615 = loc("add.17120")
#loc7616 = loc("rsqrt.17121")
#loc7617 = loc("reshape.17122")
#loc7618 = loc("broadcast.17123")
#loc7619 = loc("multiply.17124")
#loc7620 = loc("convert.17125")
#loc7621 = loc("multiply.17131")
#loc7622 = loc("transpose.17132")
#loc7623 = loc("multiply.17143")
#loc7624 = loc("slice.17134")
#loc7625 = loc("negate.17135")
#loc7626 = loc("slice.17133")
#loc7627 = loc("concatenate.17136")
#loc7628 = loc("multiply.17139")
#loc7629 = loc("add.17146")
#loc7631 = loc("reshape.17168")
#loc7632 = loc("reshape.17170")
#loc7633 = loc("transpose.17171")
#loc7634 = loc("dot.17173")
#loc7635 = loc("reshape.17175")
#loc7636 = loc("transpose.17176")
#loc7638 = loc("reshape.17522")
#loc7639 = loc("reshape.17524")
#loc7640 = loc("broadcast.17525")
#loc7641 = loc("reshape.17483")
#loc7642 = loc("reshape.17485")
#loc7643 = loc("broadcast.17486")
#loc7644 = loc("reshape.17305")
#loc7645 = loc("reshape.17307")
#loc7646 = loc("broadcast.17308")
#loc7647 = loc("reshape.17267")
#loc7648 = loc("reshape.17269")
#loc7649 = loc("transpose.17270")
#loc7650 = loc("dot.17272")
#loc7651 = loc("reshape.17274")
#loc7652 = loc("convert.17275")
#loc7653 = loc("power.17277")
#loc7654 = loc("reduce.17284")
#loc7655 = loc("multiply.17293")
#loc7656 = loc("reshape.17294")
#loc7657 = loc("add.17298")
#loc7658 = loc("rsqrt.17299")
#loc7659 = loc("reshape.17300")
#loc7660 = loc("broadcast.17301")
#loc7661 = loc("multiply.17302")
#loc7662 = loc("convert.17303")
#loc7663 = loc("multiply.17309")
#loc7664 = loc("transpose.17310")
#loc7665 = loc("multiply.17320")
#loc7666 = loc("slice.17312")
#loc7667 = loc("negate.17313")
#loc7668 = loc("slice.17311")
#loc7669 = loc("concatenate.17314")
#loc7670 = loc("multiply.17317")
#loc7671 = loc("add.17323")
#loc7672 = loc("convert.17324")
#loc7673 = loc("multiply.17326")
#loc7674 = loc("broadcast.17255")
#loc7675 = loc("reshape.17256")
#loc7676 = loc("convert.17257")
#loc7677 = loc("transpose.17258")
#loc7678 = loc("multiply.17260")
#loc7679 = loc("dot.17327")
#loc7680 = loc("add.17334")
#loc7681 = loc("convert.17360")
#loc7682 = loc("compare.17362")
#loc7683 = loc("not.17364")
#loc7685 = loc("or.17374")
#loc7686 = loc("select.17375")
#loc7687 = loc("reshape.17380")
#loc7688 = loc("not.17382")
#loc7689 = loc("reshape.17384")
#loc7690 = loc("broadcast.17385")
#loc7691 = loc("reduce.17340")
#loc7692 = loc("broadcast.17341")
#loc7693 = loc("subtract.17342")
#loc7694 = loc("exponential.17343")
#loc7695 = loc("reduce.17349")
#loc7696 = loc("broadcast.17350")
#loc7697 = loc("divide.17351")
#loc7698 = loc("select.17386")
#loc7699 = loc("broadcast.17233")
#loc7700 = loc("reshape.17234")
#loc7701 = loc("convert.17235")
#loc7702 = loc("dot.17387")
#loc7703 = loc("convert.17389")
#loc7704 = loc("transpose.17390")
#loc7705 = loc("reshape.17392")
#loc7706 = loc("reshape.17226")
#loc7707 = loc("reshape.17228")
#loc7708 = loc("transpose.17229")
#loc7710 = loc("reshape.17394")
#loc7711 = loc("add.17397")
#loc7712 = loc("reshape.17428")
#loc7713 = loc("reshape.17430")
#loc7714 = loc("broadcast.17431")
#loc7715 = loc("convert.17398")
#loc7716 = loc("power.17400")
#loc7717 = loc("reduce.17407")
#loc7718 = loc("multiply.17416")
#loc7719 = loc("reshape.17417")
#loc7720 = loc("add.17421")
#loc7721 = loc("rsqrt.17422")
#loc7722 = loc("reshape.17423")
#loc7723 = loc("broadcast.17424")
#loc7724 = loc("multiply.17425")
#loc7725 = loc("convert.17426")
#loc7726 = loc("multiply.17432")
#loc7727 = loc("reshape.17441")
#loc7728 = loc("reshape.17437")
#loc7729 = loc("reshape.17439")
#loc7730 = loc("transpose.17440")
#loc7731 = loc("dot.17442")
#loc7732 = loc("reshape.17443")
#loc7733 = loc("logistic.17444")
#loc7734 = loc("multiply.17445")
#loc7735 = loc("reshape.17217")
#loc7736 = loc("reshape.17219")
#loc7737 = loc("transpose.17220")
#loc7738 = loc("dot.17434")
#loc7739 = loc("reshape.17435")
#loc7740 = loc("multiply.17446")
#loc7741 = loc("reshape.17447")
#loc7742 = loc("reshape.17212")
#loc7743 = loc("reshape.17214")
#loc7744 = loc("transpose.17215")
#loc7746 = loc("reshape.17449")
#loc7747 = loc("add.17452")
#loc7748 = loc("convert.17453")
#loc7749 = loc("power.17455")
#loc7750 = loc("reduce.17462")
#loc7751 = loc("multiply.17471")
#loc7752 = loc("reshape.17472")
#loc7753 = loc("add.17476")
#loc7754 = loc("rsqrt.17477")
#loc7755 = loc("reshape.17478")
#loc7756 = loc("broadcast.17479")
#loc7757 = loc("multiply.17480")
#loc7758 = loc("convert.17481")
#loc7759 = loc("multiply.17487")
#loc7760 = loc("reshape.17488")
#loc7761 = loc("reshape.17203")
#loc7762 = loc("reshape.17205")
#loc7763 = loc("transpose.17206")
#loc7764 = loc("dot.17489")
#loc7765 = loc("reshape.17491")
#loc7766 = loc("convert.17492")
#loc7767 = loc("power.17494")
#loc7768 = loc("reduce.17501")
#loc7769 = loc("multiply.17510")
#loc7770 = loc("reshape.17511")
#loc7771 = loc("add.17515")
#loc7772 = loc("rsqrt.17516")
#loc7773 = loc("reshape.17517")
#loc7774 = loc("broadcast.17518")
#loc7775 = loc("multiply.17519")
#loc7776 = loc("convert.17520")
#loc7777 = loc("multiply.17526")
#loc7778 = loc("transpose.17527")
#loc7779 = loc("multiply.17538")
#loc7780 = loc("slice.17529")
#loc7781 = loc("negate.17530")
#loc7782 = loc("slice.17528")
#loc7783 = loc("concatenate.17531")
#loc7784 = loc("multiply.17534")
#loc7785 = loc("add.17541")
#loc7787 = loc("reshape.17563")
#loc7788 = loc("reshape.17565")
#loc7789 = loc("transpose.17566")
#loc7790 = loc("dot.17568")
#loc7791 = loc("reshape.17570")
#loc7792 = loc("transpose.17571")
#loc7794 = loc("reshape.17917")
#loc7795 = loc("reshape.17919")
#loc7796 = loc("broadcast.17920")
#loc7797 = loc("reshape.17878")
#loc7798 = loc("reshape.17880")
#loc7799 = loc("broadcast.17881")
#loc7800 = loc("reshape.17700")
#loc7801 = loc("reshape.17702")
#loc7802 = loc("broadcast.17703")
#loc7803 = loc("reshape.17662")
#loc7804 = loc("reshape.17664")
#loc7805 = loc("transpose.17665")
#loc7806 = loc("dot.17667")
#loc7807 = loc("reshape.17669")
#loc7808 = loc("convert.17670")
#loc7809 = loc("power.17672")
#loc7810 = loc("reduce.17679")
#loc7811 = loc("multiply.17688")
#loc7812 = loc("reshape.17689")
#loc7813 = loc("add.17693")
#loc7814 = loc("rsqrt.17694")
#loc7815 = loc("reshape.17695")
#loc7816 = loc("broadcast.17696")
#loc7817 = loc("multiply.17697")
#loc7818 = loc("convert.17698")
#loc7819 = loc("multiply.17704")
#loc7820 = loc("transpose.17705")
#loc7821 = loc("multiply.17715")
#loc7822 = loc("slice.17707")
#loc7823 = loc("negate.17708")
#loc7824 = loc("slice.17706")
#loc7825 = loc("concatenate.17709")
#loc7826 = loc("multiply.17712")
#loc7827 = loc("add.17718")
#loc7828 = loc("convert.17719")
#loc7829 = loc("multiply.17721")
#loc7830 = loc("broadcast.17650")
#loc7831 = loc("reshape.17651")
#loc7832 = loc("convert.17652")
#loc7833 = loc("transpose.17653")
#loc7834 = loc("multiply.17655")
#loc7835 = loc("dot.17722")
#loc7836 = loc("add.17729")
#loc7837 = loc("convert.17755")
#loc7838 = loc("compare.17757")
#loc7839 = loc("not.17759")
#loc7841 = loc("or.17769")
#loc7842 = loc("select.17770")
#loc7843 = loc("reshape.17775")
#loc7844 = loc("not.17777")
#loc7845 = loc("reshape.17779")
#loc7846 = loc("broadcast.17780")
#loc7847 = loc("reduce.17735")
#loc7848 = loc("broadcast.17736")
#loc7849 = loc("subtract.17737")
#loc7850 = loc("exponential.17738")
#loc7851 = loc("reduce.17744")
#loc7852 = loc("broadcast.17745")
#loc7853 = loc("divide.17746")
#loc7854 = loc("select.17781")
#loc7855 = loc("broadcast.17628")
#loc7856 = loc("reshape.17629")
#loc7857 = loc("convert.17630")
#loc7858 = loc("dot.17782")
#loc7859 = loc("convert.17784")
#loc7860 = loc("transpose.17785")
#loc7861 = loc("reshape.17787")
#loc7862 = loc("reshape.17621")
#loc7863 = loc("reshape.17623")
#loc7864 = loc("transpose.17624")
#loc7866 = loc("reshape.17789")
#loc7867 = loc("add.17792")
#loc7868 = loc("reshape.17823")
#loc7869 = loc("reshape.17825")
#loc7870 = loc("broadcast.17826")
#loc7871 = loc("convert.17793")
#loc7872 = loc("power.17795")
#loc7873 = loc("reduce.17802")
#loc7874 = loc("multiply.17811")
#loc7875 = loc("reshape.17812")
#loc7876 = loc("add.17816")
#loc7877 = loc("rsqrt.17817")
#loc7878 = loc("reshape.17818")
#loc7879 = loc("broadcast.17819")
#loc7880 = loc("multiply.17820")
#loc7881 = loc("convert.17821")
#loc7882 = loc("multiply.17827")
#loc7883 = loc("reshape.17836")
#loc7884 = loc("reshape.17832")
#loc7885 = loc("reshape.17834")
#loc7886 = loc("transpose.17835")
#loc7887 = loc("dot.17837")
#loc7888 = loc("reshape.17838")
#loc7889 = loc("logistic.17839")
#loc7890 = loc("multiply.17840")
#loc7891 = loc("reshape.17612")
#loc7892 = loc("reshape.17614")
#loc7893 = loc("transpose.17615")
#loc7894 = loc("dot.17829")
#loc7895 = loc("reshape.17830")
#loc7896 = loc("multiply.17841")
#loc7897 = loc("reshape.17842")
#loc7898 = loc("reshape.17607")
#loc7899 = loc("reshape.17609")
#loc7900 = loc("transpose.17610")
#loc7902 = loc("reshape.17844")
#loc7903 = loc("add.17847")
#loc7904 = loc("convert.17848")
#loc7905 = loc("power.17850")
#loc7906 = loc("reduce.17857")
#loc7907 = loc("multiply.17866")
#loc7908 = loc("reshape.17867")
#loc7909 = loc("add.17871")
#loc7910 = loc("rsqrt.17872")
#loc7911 = loc("reshape.17873")
#loc7912 = loc("broadcast.17874")
#loc7913 = loc("multiply.17875")
#loc7914 = loc("convert.17876")
#loc7915 = loc("multiply.17882")
#loc7916 = loc("reshape.17883")
#loc7917 = loc("reshape.17598")
#loc7918 = loc("reshape.17600")
#loc7919 = loc("transpose.17601")
#loc7920 = loc("dot.17884")
#loc7921 = loc("reshape.17886")
#loc7922 = loc("convert.17887")
#loc7923 = loc("power.17889")
#loc7924 = loc("reduce.17896")
#loc7925 = loc("multiply.17905")
#loc7926 = loc("reshape.17906")
#loc7927 = loc("add.17910")
#loc7928 = loc("rsqrt.17911")
#loc7929 = loc("reshape.17912")
#loc7930 = loc("broadcast.17913")
#loc7931 = loc("multiply.17914")
#loc7932 = loc("convert.17915")
#loc7933 = loc("multiply.17921")
#loc7934 = loc("transpose.17922")
#loc7935 = loc("multiply.17933")
#loc7936 = loc("slice.17924")
#loc7937 = loc("negate.17925")
#loc7938 = loc("slice.17923")
#loc7939 = loc("concatenate.17926")
#loc7940 = loc("multiply.17929")
#loc7941 = loc("add.17936")
#loc7943 = loc("reshape.17958")
#loc7944 = loc("reshape.17960")
#loc7945 = loc("transpose.17961")
#loc7946 = loc("dot.17963")
#loc7947 = loc("reshape.17965")
#loc7948 = loc("transpose.17966")
#loc7950 = loc("reshape.18312")
#loc7951 = loc("reshape.18314")
#loc7952 = loc("broadcast.18315")
#loc7953 = loc("reshape.18273")
#loc7954 = loc("reshape.18275")
#loc7955 = loc("broadcast.18276")
#loc7956 = loc("reshape.18095")
#loc7957 = loc("reshape.18097")
#loc7958 = loc("broadcast.18098")
#loc7959 = loc("reshape.18057")
#loc7960 = loc("reshape.18059")
#loc7961 = loc("transpose.18060")
#loc7962 = loc("dot.18062")
#loc7963 = loc("reshape.18064")
#loc7964 = loc("convert.18065")
#loc7965 = loc("power.18067")
#loc7966 = loc("reduce.18074")
#loc7967 = loc("multiply.18083")
#loc7968 = loc("reshape.18084")
#loc7969 = loc("add.18088")
#loc7970 = loc("rsqrt.18089")
#loc7971 = loc("reshape.18090")
#loc7972 = loc("broadcast.18091")
#loc7973 = loc("multiply.18092")
#loc7974 = loc("convert.18093")
#loc7975 = loc("multiply.18099")
#loc7976 = loc("transpose.18100")
#loc7977 = loc("multiply.18110")
#loc7978 = loc("slice.18102")
#loc7979 = loc("negate.18103")
#loc7980 = loc("slice.18101")
#loc7981 = loc("concatenate.18104")
#loc7982 = loc("multiply.18107")
#loc7983 = loc("add.18113")
#loc7984 = loc("convert.18114")
#loc7985 = loc("multiply.18116")
#loc7986 = loc("broadcast.18045")
#loc7987 = loc("reshape.18046")
#loc7988 = loc("convert.18047")
#loc7989 = loc("transpose.18048")
#loc7990 = loc("multiply.18050")
#loc7991 = loc("dot.18117")
#loc7992 = loc("add.18124")
#loc7993 = loc("convert.18150")
#loc7994 = loc("compare.18152")
#loc7995 = loc("not.18154")
#loc7997 = loc("or.18164")
#loc7998 = loc("select.18165")
#loc7999 = loc("reshape.18170")
#loc8000 = loc("not.18172")
#loc8001 = loc("reshape.18174")
#loc8002 = loc("broadcast.18175")
#loc8003 = loc("reduce.18130")
#loc8004 = loc("broadcast.18131")
#loc8005 = loc("subtract.18132")
#loc8006 = loc("exponential.18133")
#loc8007 = loc("reduce.18139")
#loc8008 = loc("broadcast.18140")
#loc8009 = loc("divide.18141")
#loc8010 = loc("select.18176")
#loc8011 = loc("broadcast.18023")
#loc8012 = loc("reshape.18024")
#loc8013 = loc("convert.18025")
#loc8014 = loc("dot.18177")
#loc8015 = loc("convert.18179")
#loc8016 = loc("transpose.18180")
#loc8017 = loc("reshape.18182")
#loc8018 = loc("reshape.18016")
#loc8019 = loc("reshape.18018")
#loc8020 = loc("transpose.18019")
#loc8022 = loc("reshape.18184")
#loc8023 = loc("add.18187")
#loc8024 = loc("reshape.18218")
#loc8025 = loc("reshape.18220")
#loc8026 = loc("broadcast.18221")
#loc8027 = loc("convert.18188")
#loc8028 = loc("power.18190")
#loc8029 = loc("reduce.18197")
#loc8030 = loc("multiply.18206")
#loc8031 = loc("reshape.18207")
#loc8032 = loc("add.18211")
#loc8033 = loc("rsqrt.18212")
#loc8034 = loc("reshape.18213")
#loc8035 = loc("broadcast.18214")
#loc8036 = loc("multiply.18215")
#loc8037 = loc("convert.18216")
#loc8038 = loc("multiply.18222")
#loc8039 = loc("reshape.18231")
#loc8040 = loc("reshape.18227")
#loc8041 = loc("reshape.18229")
#loc8042 = loc("transpose.18230")
#loc8043 = loc("dot.18232")
#loc8044 = loc("reshape.18233")
#loc8045 = loc("logistic.18234")
#loc8046 = loc("multiply.18235")
#loc8047 = loc("reshape.18007")
#loc8048 = loc("reshape.18009")
#loc8049 = loc("transpose.18010")
#loc8050 = loc("dot.18224")
#loc8051 = loc("reshape.18225")
#loc8052 = loc("multiply.18236")
#loc8053 = loc("reshape.18237")
#loc8054 = loc("reshape.18002")
#loc8055 = loc("reshape.18004")
#loc8056 = loc("transpose.18005")
#loc8058 = loc("reshape.18239")
#loc8059 = loc("add.18242")
#loc8060 = loc("convert.18243")
#loc8061 = loc("power.18245")
#loc8062 = loc("reduce.18252")
#loc8063 = loc("multiply.18261")
#loc8064 = loc("reshape.18262")
#loc8065 = loc("add.18266")
#loc8066 = loc("rsqrt.18267")
#loc8067 = loc("reshape.18268")
#loc8068 = loc("broadcast.18269")
#loc8069 = loc("multiply.18270")
#loc8070 = loc("convert.18271")
#loc8071 = loc("multiply.18277")
#loc8072 = loc("reshape.18278")
#loc8073 = loc("reshape.17993")
#loc8074 = loc("reshape.17995")
#loc8075 = loc("transpose.17996")
#loc8076 = loc("dot.18279")
#loc8077 = loc("reshape.18281")
#loc8078 = loc("convert.18282")
#loc8079 = loc("power.18284")
#loc8080 = loc("reduce.18291")
#loc8081 = loc("multiply.18300")
#loc8082 = loc("reshape.18301")
#loc8083 = loc("add.18305")
#loc8084 = loc("rsqrt.18306")
#loc8085 = loc("reshape.18307")
#loc8086 = loc("broadcast.18308")
#loc8087 = loc("multiply.18309")
#loc8088 = loc("convert.18310")
#loc8089 = loc("multiply.18316")
#loc8090 = loc("transpose.18317")
#loc8091 = loc("multiply.18328")
#loc8092 = loc("slice.18319")
#loc8093 = loc("negate.18320")
#loc8094 = loc("slice.18318")
#loc8095 = loc("concatenate.18321")
#loc8096 = loc("multiply.18324")
#loc8097 = loc("add.18331")
#loc8099 = loc("reshape.18353")
#loc8100 = loc("reshape.18355")
#loc8101 = loc("transpose.18356")
#loc8102 = loc("dot.18358")
#loc8103 = loc("reshape.18360")
#loc8104 = loc("transpose.18361")
#loc8106 = loc("reshape.18707")
#loc8107 = loc("reshape.18709")
#loc8108 = loc("broadcast.18710")
#loc8109 = loc("reshape.18668")
#loc8110 = loc("reshape.18670")
#loc8111 = loc("broadcast.18671")
#loc8112 = loc("reshape.18490")
#loc8113 = loc("reshape.18492")
#loc8114 = loc("broadcast.18493")
#loc8115 = loc("reshape.18452")
#loc8116 = loc("reshape.18454")
#loc8117 = loc("transpose.18455")
#loc8118 = loc("dot.18457")
#loc8119 = loc("reshape.18459")
#loc8120 = loc("convert.18460")
#loc8121 = loc("power.18462")
#loc8122 = loc("reduce.18469")
#loc8123 = loc("multiply.18478")
#loc8124 = loc("reshape.18479")
#loc8125 = loc("add.18483")
#loc8126 = loc("rsqrt.18484")
#loc8127 = loc("reshape.18485")
#loc8128 = loc("broadcast.18486")
#loc8129 = loc("multiply.18487")
#loc8130 = loc("convert.18488")
#loc8131 = loc("multiply.18494")
#loc8132 = loc("transpose.18495")
#loc8133 = loc("multiply.18505")
#loc8134 = loc("slice.18497")
#loc8135 = loc("negate.18498")
#loc8136 = loc("slice.18496")
#loc8137 = loc("concatenate.18499")
#loc8138 = loc("multiply.18502")
#loc8139 = loc("add.18508")
#loc8140 = loc("convert.18509")
#loc8141 = loc("multiply.18511")
#loc8142 = loc("broadcast.18440")
#loc8143 = loc("reshape.18441")
#loc8144 = loc("convert.18442")
#loc8145 = loc("transpose.18443")
#loc8146 = loc("multiply.18445")
#loc8147 = loc("dot.18512")
#loc8148 = loc("add.18519")
#loc8149 = loc("convert.18545")
#loc8150 = loc("compare.18547")
#loc8151 = loc("not.18549")
#loc8153 = loc("or.18559")
#loc8154 = loc("select.18560")
#loc8155 = loc("reshape.18565")
#loc8156 = loc("not.18567")
#loc8157 = loc("reshape.18569")
#loc8158 = loc("broadcast.18570")
#loc8159 = loc("reduce.18525")
#loc8160 = loc("broadcast.18526")
#loc8161 = loc("subtract.18527")
#loc8162 = loc("exponential.18528")
#loc8163 = loc("reduce.18534")
#loc8164 = loc("broadcast.18535")
#loc8165 = loc("divide.18536")
#loc8166 = loc("select.18571")
#loc8167 = loc("broadcast.18418")
#loc8168 = loc("reshape.18419")
#loc8169 = loc("convert.18420")
#loc8170 = loc("dot.18572")
#loc8171 = loc("convert.18574")
#loc8172 = loc("transpose.18575")
#loc8173 = loc("reshape.18577")
#loc8174 = loc("reshape.18411")
#loc8175 = loc("reshape.18413")
#loc8176 = loc("transpose.18414")
#loc8178 = loc("reshape.18579")
#loc8179 = loc("add.18582")
#loc8180 = loc("reshape.18613")
#loc8181 = loc("reshape.18615")
#loc8182 = loc("broadcast.18616")
#loc8183 = loc("convert.18583")
#loc8184 = loc("power.18585")
#loc8185 = loc("reduce.18592")
#loc8186 = loc("multiply.18601")
#loc8187 = loc("reshape.18602")
#loc8188 = loc("add.18606")
#loc8189 = loc("rsqrt.18607")
#loc8190 = loc("reshape.18608")
#loc8191 = loc("broadcast.18609")
#loc8192 = loc("multiply.18610")
#loc8193 = loc("convert.18611")
#loc8194 = loc("multiply.18617")
#loc8195 = loc("reshape.18626")
#loc8196 = loc("reshape.18622")
#loc8197 = loc("reshape.18624")
#loc8198 = loc("transpose.18625")
#loc8199 = loc("dot.18627")
#loc8200 = loc("reshape.18628")
#loc8201 = loc("logistic.18629")
#loc8202 = loc("multiply.18630")
#loc8203 = loc("reshape.18402")
#loc8204 = loc("reshape.18404")
#loc8205 = loc("transpose.18405")
#loc8206 = loc("dot.18619")
#loc8207 = loc("reshape.18620")
#loc8208 = loc("multiply.18631")
#loc8209 = loc("reshape.18632")
#loc8210 = loc("reshape.18397")
#loc8211 = loc("reshape.18399")
#loc8212 = loc("transpose.18400")
#loc8214 = loc("reshape.18634")
#loc8215 = loc("add.18637")
#loc8216 = loc("convert.18638")
#loc8217 = loc("power.18640")
#loc8218 = loc("reduce.18647")
#loc8219 = loc("multiply.18656")
#loc8220 = loc("reshape.18657")
#loc8221 = loc("add.18661")
#loc8222 = loc("rsqrt.18662")
#loc8223 = loc("reshape.18663")
#loc8224 = loc("broadcast.18664")
#loc8225 = loc("multiply.18665")
#loc8226 = loc("convert.18666")
#loc8227 = loc("multiply.18672")
#loc8228 = loc("reshape.18673")
#loc8229 = loc("reshape.18388")
#loc8230 = loc("reshape.18390")
#loc8231 = loc("transpose.18391")
#loc8232 = loc("dot.18674")
#loc8233 = loc("reshape.18676")
#loc8234 = loc("convert.18677")
#loc8235 = loc("power.18679")
#loc8236 = loc("reduce.18686")
#loc8237 = loc("multiply.18695")
#loc8238 = loc("reshape.18696")
#loc8239 = loc("add.18700")
#loc8240 = loc("rsqrt.18701")
#loc8241 = loc("reshape.18702")
#loc8242 = loc("broadcast.18703")
#loc8243 = loc("multiply.18704")
#loc8244 = loc("convert.18705")
#loc8245 = loc("multiply.18711")
#loc8246 = loc("transpose.18712")
#loc8247 = loc("multiply.18723")
#loc8248 = loc("slice.18714")
#loc8249 = loc("negate.18715")
#loc8250 = loc("slice.18713")
#loc8251 = loc("concatenate.18716")
#loc8252 = loc("multiply.18719")
#loc8253 = loc("add.18726")
#loc8255 = loc("reshape.18748")
#loc8256 = loc("reshape.18750")
#loc8257 = loc("transpose.18751")
#loc8258 = loc("dot.18753")
#loc8259 = loc("reshape.18755")
#loc8260 = loc("transpose.18756")
#loc8262 = loc("reshape.19102")
#loc8263 = loc("reshape.19104")
#loc8264 = loc("broadcast.19105")
#loc8265 = loc("reshape.19063")
#loc8266 = loc("reshape.19065")
#loc8267 = loc("broadcast.19066")
#loc8268 = loc("reshape.18885")
#loc8269 = loc("reshape.18887")
#loc8270 = loc("broadcast.18888")
#loc8271 = loc("reshape.18847")
#loc8272 = loc("reshape.18849")
#loc8273 = loc("transpose.18850")
#loc8274 = loc("dot.18852")
#loc8275 = loc("reshape.18854")
#loc8276 = loc("convert.18855")
#loc8277 = loc("power.18857")
#loc8278 = loc("reduce.18864")
#loc8279 = loc("multiply.18873")
#loc8280 = loc("reshape.18874")
#loc8281 = loc("add.18878")
#loc8282 = loc("rsqrt.18879")
#loc8283 = loc("reshape.18880")
#loc8284 = loc("broadcast.18881")
#loc8285 = loc("multiply.18882")
#loc8286 = loc("convert.18883")
#loc8287 = loc("multiply.18889")
#loc8288 = loc("transpose.18890")
#loc8289 = loc("multiply.18900")
#loc8290 = loc("slice.18892")
#loc8291 = loc("negate.18893")
#loc8292 = loc("slice.18891")
#loc8293 = loc("concatenate.18894")
#loc8294 = loc("multiply.18897")
#loc8295 = loc("add.18903")
#loc8296 = loc("convert.18904")
#loc8297 = loc("multiply.18906")
#loc8298 = loc("broadcast.18835")
#loc8299 = loc("reshape.18836")
#loc8300 = loc("convert.18837")
#loc8301 = loc("transpose.18838")
#loc8302 = loc("multiply.18840")
#loc8303 = loc("dot.18907")
#loc8304 = loc("add.18914")
#loc8305 = loc("convert.18940")
#loc8306 = loc("compare.18942")
#loc8307 = loc("not.18944")
#loc8309 = loc("or.18954")
#loc8310 = loc("select.18955")
#loc8311 = loc("reshape.18960")
#loc8312 = loc("not.18962")
#loc8313 = loc("reshape.18964")
#loc8314 = loc("broadcast.18965")
#loc8315 = loc("reduce.18920")
#loc8316 = loc("broadcast.18921")
#loc8317 = loc("subtract.18922")
#loc8318 = loc("exponential.18923")
#loc8319 = loc("reduce.18929")
#loc8320 = loc("broadcast.18930")
#loc8321 = loc("divide.18931")
#loc8322 = loc("select.18966")
#loc8323 = loc("broadcast.18813")
#loc8324 = loc("reshape.18814")
#loc8325 = loc("convert.18815")
#loc8326 = loc("dot.18967")
#loc8327 = loc("convert.18969")
#loc8328 = loc("transpose.18970")
#loc8329 = loc("reshape.18972")
#loc8330 = loc("reshape.18806")
#loc8331 = loc("reshape.18808")
#loc8332 = loc("transpose.18809")
#loc8334 = loc("reshape.18974")
#loc8335 = loc("add.18977")
#loc8336 = loc("reshape.19008")
#loc8337 = loc("reshape.19010")
#loc8338 = loc("broadcast.19011")
#loc8339 = loc("convert.18978")
#loc8340 = loc("power.18980")
#loc8341 = loc("reduce.18987")
#loc8342 = loc("multiply.18996")
#loc8343 = loc("reshape.18997")
#loc8344 = loc("add.19001")
#loc8345 = loc("rsqrt.19002")
#loc8346 = loc("reshape.19003")
#loc8347 = loc("broadcast.19004")
#loc8348 = loc("multiply.19005")
#loc8349 = loc("convert.19006")
#loc8350 = loc("multiply.19012")
#loc8351 = loc("reshape.19021")
#loc8352 = loc("reshape.19017")
#loc8353 = loc("reshape.19019")
#loc8354 = loc("transpose.19020")
#loc8355 = loc("dot.19022")
#loc8356 = loc("reshape.19023")
#loc8357 = loc("logistic.19024")
#loc8358 = loc("multiply.19025")
#loc8359 = loc("reshape.18797")
#loc8360 = loc("reshape.18799")
#loc8361 = loc("transpose.18800")
#loc8362 = loc("dot.19014")
#loc8363 = loc("reshape.19015")
#loc8364 = loc("multiply.19026")
#loc8365 = loc("reshape.19027")
#loc8366 = loc("reshape.18792")
#loc8367 = loc("reshape.18794")
#loc8368 = loc("transpose.18795")
#loc8370 = loc("reshape.19029")
#loc8371 = loc("add.19032")
#loc8372 = loc("convert.19033")
#loc8373 = loc("power.19035")
#loc8374 = loc("reduce.19042")
#loc8375 = loc("multiply.19051")
#loc8376 = loc("reshape.19052")
#loc8377 = loc("add.19056")
#loc8378 = loc("rsqrt.19057")
#loc8379 = loc("reshape.19058")
#loc8380 = loc("broadcast.19059")
#loc8381 = loc("multiply.19060")
#loc8382 = loc("convert.19061")
#loc8383 = loc("multiply.19067")
#loc8384 = loc("reshape.19068")
#loc8385 = loc("reshape.18783")
#loc8386 = loc("reshape.18785")
#loc8387 = loc("transpose.18786")
#loc8388 = loc("dot.19069")
#loc8389 = loc("reshape.19071")
#loc8390 = loc("convert.19072")
#loc8391 = loc("power.19074")
#loc8392 = loc("reduce.19081")
#loc8393 = loc("multiply.19090")
#loc8394 = loc("reshape.19091")
#loc8395 = loc("add.19095")
#loc8396 = loc("rsqrt.19096")
#loc8397 = loc("reshape.19097")
#loc8398 = loc("broadcast.19098")
#loc8399 = loc("multiply.19099")
#loc8400 = loc("convert.19100")
#loc8401 = loc("multiply.19106")
#loc8402 = loc("transpose.19107")
#loc8403 = loc("multiply.19118")
#loc8404 = loc("slice.19109")
#loc8405 = loc("negate.19110")
#loc8406 = loc("slice.19108")
#loc8407 = loc("concatenate.19111")
#loc8408 = loc("multiply.19114")
#loc8409 = loc("add.19121")
#loc8411 = loc("reshape.19143")
#loc8412 = loc("reshape.19145")
#loc8413 = loc("transpose.19146")
#loc8414 = loc("dot.19148")
#loc8415 = loc("reshape.19150")
#loc8416 = loc("transpose.19151")
#loc8418 = loc("reshape.19497")
#loc8419 = loc("reshape.19499")
#loc8420 = loc("broadcast.19500")
#loc8421 = loc("reshape.19458")
#loc8422 = loc("reshape.19460")
#loc8423 = loc("broadcast.19461")
#loc8424 = loc("reshape.19280")
#loc8425 = loc("reshape.19282")
#loc8426 = loc("broadcast.19283")
#loc8427 = loc("reshape.19242")
#loc8428 = loc("reshape.19244")
#loc8429 = loc("transpose.19245")
#loc8430 = loc("dot.19247")
#loc8431 = loc("reshape.19249")
#loc8432 = loc("convert.19250")
#loc8433 = loc("power.19252")
#loc8434 = loc("reduce.19259")
#loc8435 = loc("multiply.19268")
#loc8436 = loc("reshape.19269")
#loc8437 = loc("add.19273")
#loc8438 = loc("rsqrt.19274")
#loc8439 = loc("reshape.19275")
#loc8440 = loc("broadcast.19276")
#loc8441 = loc("multiply.19277")
#loc8442 = loc("convert.19278")
#loc8443 = loc("multiply.19284")
#loc8444 = loc("transpose.19285")
#loc8445 = loc("multiply.19295")
#loc8446 = loc("slice.19287")
#loc8447 = loc("negate.19288")
#loc8448 = loc("slice.19286")
#loc8449 = loc("concatenate.19289")
#loc8450 = loc("multiply.19292")
#loc8451 = loc("add.19298")
#loc8452 = loc("convert.19299")
#loc8453 = loc("multiply.19301")
#loc8454 = loc("broadcast.19230")
#loc8455 = loc("reshape.19231")
#loc8456 = loc("convert.19232")
#loc8457 = loc("transpose.19233")
#loc8458 = loc("multiply.19235")
#loc8459 = loc("dot.19302")
#loc8460 = loc("add.19309")
#loc8461 = loc("convert.19335")
#loc8462 = loc("compare.19337")
#loc8463 = loc("not.19339")
#loc8465 = loc("or.19349")
#loc8466 = loc("select.19350")
#loc8467 = loc("reshape.19355")
#loc8468 = loc("not.19357")
#loc8469 = loc("reshape.19359")
#loc8470 = loc("broadcast.19360")
#loc8471 = loc("reduce.19315")
#loc8472 = loc("broadcast.19316")
#loc8473 = loc("subtract.19317")
#loc8474 = loc("exponential.19318")
#loc8475 = loc("reduce.19324")
#loc8476 = loc("broadcast.19325")
#loc8477 = loc("divide.19326")
#loc8478 = loc("select.19361")
#loc8479 = loc("broadcast.19208")
#loc8480 = loc("reshape.19209")
#loc8481 = loc("convert.19210")
#loc8482 = loc("dot.19362")
#loc8483 = loc("convert.19364")
#loc8484 = loc("transpose.19365")
#loc8485 = loc("reshape.19367")
#loc8486 = loc("reshape.19201")
#loc8487 = loc("reshape.19203")
#loc8488 = loc("transpose.19204")
#loc8490 = loc("reshape.19369")
#loc8491 = loc("add.19372")
#loc8492 = loc("reshape.19403")
#loc8493 = loc("reshape.19405")
#loc8494 = loc("broadcast.19406")
#loc8495 = loc("convert.19373")
#loc8496 = loc("power.19375")
#loc8497 = loc("reduce.19382")
#loc8498 = loc("multiply.19391")
#loc8499 = loc("reshape.19392")
#loc8500 = loc("add.19396")
#loc8501 = loc("rsqrt.19397")
#loc8502 = loc("reshape.19398")
#loc8503 = loc("broadcast.19399")
#loc8504 = loc("multiply.19400")
#loc8505 = loc("convert.19401")
#loc8506 = loc("multiply.19407")
#loc8507 = loc("reshape.19416")
#loc8508 = loc("reshape.19412")
#loc8509 = loc("reshape.19414")
#loc8510 = loc("transpose.19415")
#loc8511 = loc("dot.19417")
#loc8512 = loc("reshape.19418")
#loc8513 = loc("logistic.19419")
#loc8514 = loc("multiply.19420")
#loc8515 = loc("reshape.19192")
#loc8516 = loc("reshape.19194")
#loc8517 = loc("transpose.19195")
#loc8518 = loc("dot.19409")
#loc8519 = loc("reshape.19410")
#loc8520 = loc("multiply.19421")
#loc8521 = loc("reshape.19422")
#loc8522 = loc("reshape.19187")
#loc8523 = loc("reshape.19189")
#loc8524 = loc("transpose.19190")
#loc8526 = loc("reshape.19424")
#loc8527 = loc("add.19427")
#loc8528 = loc("convert.19428")
#loc8529 = loc("power.19430")
#loc8530 = loc("reduce.19437")
#loc8531 = loc("multiply.19446")
#loc8532 = loc("reshape.19447")
#loc8533 = loc("add.19451")
#loc8534 = loc("rsqrt.19452")
#loc8535 = loc("reshape.19453")
#loc8536 = loc("broadcast.19454")
#loc8537 = loc("multiply.19455")
#loc8538 = loc("convert.19456")
#loc8539 = loc("multiply.19462")
#loc8540 = loc("reshape.19463")
#loc8541 = loc("reshape.19178")
#loc8542 = loc("reshape.19180")
#loc8543 = loc("transpose.19181")
#loc8544 = loc("dot.19464")
#loc8545 = loc("reshape.19466")
#loc8546 = loc("convert.19467")
#loc8547 = loc("power.19469")
#loc8548 = loc("reduce.19476")
#loc8549 = loc("multiply.19485")
#loc8550 = loc("reshape.19486")
#loc8551 = loc("add.19490")
#loc8552 = loc("rsqrt.19491")
#loc8553 = loc("reshape.19492")
#loc8554 = loc("broadcast.19493")
#loc8555 = loc("multiply.19494")
#loc8556 = loc("convert.19495")
#loc8557 = loc("multiply.19501")
#loc8558 = loc("transpose.19502")
#loc8559 = loc("multiply.19513")
#loc8560 = loc("slice.19504")
#loc8561 = loc("negate.19505")
#loc8562 = loc("slice.19503")
#loc8563 = loc("concatenate.19506")
#loc8564 = loc("multiply.19509")
#loc8565 = loc("add.19516")
#loc8567 = loc("reshape.19538")
#loc8568 = loc("reshape.19540")
#loc8569 = loc("transpose.19541")
#loc8570 = loc("dot.19543")
#loc8571 = loc("reshape.19545")
#loc8572 = loc("transpose.19546")
#loc8574 = loc("reshape.19892")
#loc8575 = loc("reshape.19894")
#loc8576 = loc("broadcast.19895")
#loc8577 = loc("reshape.19853")
#loc8578 = loc("reshape.19855")
#loc8579 = loc("broadcast.19856")
#loc8580 = loc("reshape.19675")
#loc8581 = loc("reshape.19677")
#loc8582 = loc("broadcast.19678")
#loc8583 = loc("reshape.19637")
#loc8584 = loc("reshape.19639")
#loc8585 = loc("transpose.19640")
#loc8586 = loc("dot.19642")
#loc8587 = loc("reshape.19644")
#loc8588 = loc("convert.19645")
#loc8589 = loc("power.19647")
#loc8590 = loc("reduce.19654")
#loc8591 = loc("multiply.19663")
#loc8592 = loc("reshape.19664")
#loc8593 = loc("add.19668")
#loc8594 = loc("rsqrt.19669")
#loc8595 = loc("reshape.19670")
#loc8596 = loc("broadcast.19671")
#loc8597 = loc("multiply.19672")
#loc8598 = loc("convert.19673")
#loc8599 = loc("multiply.19679")
#loc8600 = loc("transpose.19680")
#loc8601 = loc("multiply.19690")
#loc8602 = loc("slice.19682")
#loc8603 = loc("negate.19683")
#loc8604 = loc("slice.19681")
#loc8605 = loc("concatenate.19684")
#loc8606 = loc("multiply.19687")
#loc8607 = loc("add.19693")
#loc8608 = loc("convert.19694")
#loc8609 = loc("multiply.19696")
#loc8610 = loc("broadcast.19625")
#loc8611 = loc("reshape.19626")
#loc8612 = loc("convert.19627")
#loc8613 = loc("transpose.19628")
#loc8614 = loc("multiply.19630")
#loc8615 = loc("dot.19697")
#loc8616 = loc("add.19704")
#loc8617 = loc("convert.19730")
#loc8618 = loc("compare.19732")
#loc8619 = loc("not.19734")
#loc8621 = loc("or.19744")
#loc8622 = loc("select.19745")
#loc8623 = loc("reshape.19750")
#loc8624 = loc("not.19752")
#loc8625 = loc("reshape.19754")
#loc8626 = loc("broadcast.19755")
#loc8627 = loc("reduce.19710")
#loc8628 = loc("broadcast.19711")
#loc8629 = loc("subtract.19712")
#loc8630 = loc("exponential.19713")
#loc8631 = loc("reduce.19719")
#loc8632 = loc("broadcast.19720")
#loc8633 = loc("divide.19721")
#loc8634 = loc("select.19756")
#loc8635 = loc("broadcast.19603")
#loc8636 = loc("reshape.19604")
#loc8637 = loc("convert.19605")
#loc8638 = loc("dot.19757")
#loc8639 = loc("convert.19759")
#loc8640 = loc("transpose.19760")
#loc8641 = loc("reshape.19762")
#loc8642 = loc("reshape.19596")
#loc8643 = loc("reshape.19598")
#loc8644 = loc("transpose.19599")
#loc8646 = loc("reshape.19764")
#loc8647 = loc("add.19767")
#loc8648 = loc("reshape.19798")
#loc8649 = loc("reshape.19800")
#loc8650 = loc("broadcast.19801")
#loc8651 = loc("convert.19768")
#loc8652 = loc("power.19770")
#loc8653 = loc("reduce.19777")
#loc8654 = loc("multiply.19786")
#loc8655 = loc("reshape.19787")
#loc8656 = loc("add.19791")
#loc8657 = loc("rsqrt.19792")
#loc8658 = loc("reshape.19793")
#loc8659 = loc("broadcast.19794")
#loc8660 = loc("multiply.19795")
#loc8661 = loc("convert.19796")
#loc8662 = loc("multiply.19802")
#loc8663 = loc("reshape.19811")
#loc8664 = loc("reshape.19807")
#loc8665 = loc("reshape.19809")
#loc8666 = loc("transpose.19810")
#loc8667 = loc("dot.19812")
#loc8668 = loc("reshape.19813")
#loc8669 = loc("logistic.19814")
#loc8670 = loc("multiply.19815")
#loc8671 = loc("reshape.19587")
#loc8672 = loc("reshape.19589")
#loc8673 = loc("transpose.19590")
#loc8674 = loc("dot.19804")
#loc8675 = loc("reshape.19805")
#loc8676 = loc("multiply.19816")
#loc8677 = loc("reshape.19817")
#loc8678 = loc("reshape.19582")
#loc8679 = loc("reshape.19584")
#loc8680 = loc("transpose.19585")
#loc8682 = loc("reshape.19819")
#loc8683 = loc("add.19822")
#loc8684 = loc("convert.19823")
#loc8685 = loc("power.19825")
#loc8686 = loc("reduce.19832")
#loc8687 = loc("multiply.19841")
#loc8688 = loc("reshape.19842")
#loc8689 = loc("add.19846")
#loc8690 = loc("rsqrt.19847")
#loc8691 = loc("reshape.19848")
#loc8692 = loc("broadcast.19849")
#loc8693 = loc("multiply.19850")
#loc8694 = loc("convert.19851")
#loc8695 = loc("multiply.19857")
#loc8696 = loc("reshape.19858")
#loc8697 = loc("reshape.19573")
#loc8698 = loc("reshape.19575")
#loc8699 = loc("transpose.19576")
#loc8700 = loc("dot.19859")
#loc8701 = loc("reshape.19861")
#loc8702 = loc("convert.19862")
#loc8703 = loc("power.19864")
#loc8704 = loc("reduce.19871")
#loc8705 = loc("multiply.19880")
#loc8706 = loc("reshape.19881")
#loc8707 = loc("add.19885")
#loc8708 = loc("rsqrt.19886")
#loc8709 = loc("reshape.19887")
#loc8710 = loc("broadcast.19888")
#loc8711 = loc("multiply.19889")
#loc8712 = loc("convert.19890")
#loc8713 = loc("multiply.19896")
#loc8714 = loc("transpose.19897")
#loc8715 = loc("multiply.19908")
#loc8716 = loc("slice.19899")
#loc8717 = loc("negate.19900")
#loc8718 = loc("slice.19898")
#loc8719 = loc("concatenate.19901")
#loc8720 = loc("multiply.19904")
#loc8721 = loc("add.19911")
#loc8723 = loc("reshape.19933")
#loc8724 = loc("reshape.19935")
#loc8725 = loc("transpose.19936")
#loc8726 = loc("dot.19938")
#loc8727 = loc("reshape.19940")
#loc8728 = loc("transpose.19941")
#loc8730 = loc("reshape.20287")
#loc8731 = loc("reshape.20289")
#loc8732 = loc("broadcast.20290")
#loc8733 = loc("reshape.20248")
#loc8734 = loc("reshape.20250")
#loc8735 = loc("broadcast.20251")
#loc8736 = loc("reshape.20070")
#loc8737 = loc("reshape.20072")
#loc8738 = loc("broadcast.20073")
#loc8739 = loc("reshape.20032")
#loc8740 = loc("reshape.20034")
#loc8741 = loc("transpose.20035")
#loc8742 = loc("dot.20037")
#loc8743 = loc("reshape.20039")
#loc8744 = loc("convert.20040")
#loc8745 = loc("power.20042")
#loc8746 = loc("reduce.20049")
#loc8747 = loc("multiply.20058")
#loc8748 = loc("reshape.20059")
#loc8749 = loc("add.20063")
#loc8750 = loc("rsqrt.20064")
#loc8751 = loc("reshape.20065")
#loc8752 = loc("broadcast.20066")
#loc8753 = loc("multiply.20067")
#loc8754 = loc("convert.20068")
#loc8755 = loc("multiply.20074")
#loc8756 = loc("transpose.20075")
#loc8757 = loc("multiply.20085")
#loc8758 = loc("slice.20077")
#loc8759 = loc("negate.20078")
#loc8760 = loc("slice.20076")
#loc8761 = loc("concatenate.20079")
#loc8762 = loc("multiply.20082")
#loc8763 = loc("add.20088")
#loc8764 = loc("convert.20089")
#loc8765 = loc("multiply.20091")
#loc8766 = loc("broadcast.20020")
#loc8767 = loc("reshape.20021")
#loc8768 = loc("convert.20022")
#loc8769 = loc("transpose.20023")
#loc8770 = loc("multiply.20025")
#loc8771 = loc("dot.20092")
#loc8772 = loc("add.20099")
#loc8773 = loc("convert.20125")
#loc8774 = loc("compare.20127")
#loc8775 = loc("not.20129")
#loc8777 = loc("or.20139")
#loc8778 = loc("select.20140")
#loc8779 = loc("reshape.20145")
#loc8780 = loc("not.20147")
#loc8781 = loc("reshape.20149")
#loc8782 = loc("broadcast.20150")
#loc8783 = loc("reduce.20105")
#loc8784 = loc("broadcast.20106")
#loc8785 = loc("subtract.20107")
#loc8786 = loc("exponential.20108")
#loc8787 = loc("reduce.20114")
#loc8788 = loc("broadcast.20115")
#loc8789 = loc("divide.20116")
#loc8790 = loc("select.20151")
#loc8791 = loc("broadcast.19998")
#loc8792 = loc("reshape.19999")
#loc8793 = loc("convert.20000")
#loc8794 = loc("dot.20152")
#loc8795 = loc("convert.20154")
#loc8796 = loc("transpose.20155")
#loc8797 = loc("reshape.20157")
#loc8798 = loc("reshape.19991")
#loc8799 = loc("reshape.19993")
#loc8800 = loc("transpose.19994")
#loc8802 = loc("reshape.20159")
#loc8803 = loc("add.20162")
#loc8804 = loc("reshape.20193")
#loc8805 = loc("reshape.20195")
#loc8806 = loc("broadcast.20196")
#loc8807 = loc("convert.20163")
#loc8808 = loc("power.20165")
#loc8809 = loc("reduce.20172")
#loc8810 = loc("multiply.20181")
#loc8811 = loc("reshape.20182")
#loc8812 = loc("add.20186")
#loc8813 = loc("rsqrt.20187")
#loc8814 = loc("reshape.20188")
#loc8815 = loc("broadcast.20189")
#loc8816 = loc("multiply.20190")
#loc8817 = loc("convert.20191")
#loc8818 = loc("multiply.20197")
#loc8819 = loc("reshape.20206")
#loc8820 = loc("reshape.20202")
#loc8821 = loc("reshape.20204")
#loc8822 = loc("transpose.20205")
#loc8823 = loc("dot.20207")
#loc8824 = loc("reshape.20208")
#loc8825 = loc("logistic.20209")
#loc8826 = loc("multiply.20210")
#loc8827 = loc("reshape.19982")
#loc8828 = loc("reshape.19984")
#loc8829 = loc("transpose.19985")
#loc8830 = loc("dot.20199")
#loc8831 = loc("reshape.20200")
#loc8832 = loc("multiply.20211")
#loc8833 = loc("reshape.20212")
#loc8834 = loc("reshape.19977")
#loc8835 = loc("reshape.19979")
#loc8836 = loc("transpose.19980")
#loc8838 = loc("reshape.20214")
#loc8839 = loc("add.20217")
#loc8840 = loc("convert.20218")
#loc8841 = loc("power.20220")
#loc8842 = loc("reduce.20227")
#loc8843 = loc("multiply.20236")
#loc8844 = loc("reshape.20237")
#loc8845 = loc("add.20241")
#loc8846 = loc("rsqrt.20242")
#loc8847 = loc("reshape.20243")
#loc8848 = loc("broadcast.20244")
#loc8849 = loc("multiply.20245")
#loc8850 = loc("convert.20246")
#loc8851 = loc("multiply.20252")
#loc8852 = loc("reshape.20253")
#loc8853 = loc("reshape.19968")
#loc8854 = loc("reshape.19970")
#loc8855 = loc("transpose.19971")
#loc8856 = loc("dot.20254")
#loc8857 = loc("reshape.20256")
#loc8858 = loc("convert.20257")
#loc8859 = loc("power.20259")
#loc8860 = loc("reduce.20266")
#loc8861 = loc("multiply.20275")
#loc8862 = loc("reshape.20276")
#loc8863 = loc("add.20280")
#loc8864 = loc("rsqrt.20281")
#loc8865 = loc("reshape.20282")
#loc8866 = loc("broadcast.20283")
#loc8867 = loc("multiply.20284")
#loc8868 = loc("convert.20285")
#loc8869 = loc("multiply.20291")
#loc8870 = loc("transpose.20292")
#loc8871 = loc("multiply.20303")
#loc8872 = loc("slice.20294")
#loc8873 = loc("negate.20295")
#loc8874 = loc("slice.20293")
#loc8875 = loc("concatenate.20296")
#loc8876 = loc("multiply.20299")
#loc8877 = loc("add.20306")
#loc8879 = loc("reshape.20328")
#loc8880 = loc("reshape.20330")
#loc8881 = loc("transpose.20331")
#loc8882 = loc("dot.20333")
#loc8883 = loc("reshape.20335")
#loc8884 = loc("transpose.20336")
#loc8886 = loc("reshape.20682")
#loc8887 = loc("reshape.20684")
#loc8888 = loc("broadcast.20685")
#loc8889 = loc("reshape.20643")
#loc8890 = loc("reshape.20645")
#loc8891 = loc("broadcast.20646")
#loc8892 = loc("reshape.20465")
#loc8893 = loc("reshape.20467")
#loc8894 = loc("broadcast.20468")
#loc8895 = loc("reshape.20427")
#loc8896 = loc("reshape.20429")
#loc8897 = loc("transpose.20430")
#loc8898 = loc("dot.20432")
#loc8899 = loc("reshape.20434")
#loc8900 = loc("convert.20435")
#loc8901 = loc("power.20437")
#loc8902 = loc("reduce.20444")
#loc8903 = loc("multiply.20453")
#loc8904 = loc("reshape.20454")
#loc8905 = loc("add.20458")
#loc8906 = loc("rsqrt.20459")
#loc8907 = loc("reshape.20460")
#loc8908 = loc("broadcast.20461")
#loc8909 = loc("multiply.20462")
#loc8910 = loc("convert.20463")
#loc8911 = loc("multiply.20469")
#loc8912 = loc("transpose.20470")
#loc8913 = loc("multiply.20480")
#loc8914 = loc("slice.20472")
#loc8915 = loc("negate.20473")
#loc8916 = loc("slice.20471")
#loc8917 = loc("concatenate.20474")
#loc8918 = loc("multiply.20477")
#loc8919 = loc("add.20483")
#loc8920 = loc("convert.20484")
#loc8921 = loc("multiply.20486")
#loc8922 = loc("broadcast.20415")
#loc8923 = loc("reshape.20416")
#loc8924 = loc("convert.20417")
#loc8925 = loc("transpose.20418")
#loc8926 = loc("multiply.20420")
#loc8927 = loc("dot.20487")
#loc8928 = loc("add.20494")
#loc8929 = loc("convert.20520")
#loc8930 = loc("compare.20522")
#loc8931 = loc("not.20524")
#loc8933 = loc("or.20534")
#loc8934 = loc("select.20535")
#loc8935 = loc("reshape.20540")
#loc8936 = loc("not.20542")
#loc8937 = loc("reshape.20544")
#loc8938 = loc("broadcast.20545")
#loc8939 = loc("reduce.20500")
#loc8940 = loc("broadcast.20501")
#loc8941 = loc("subtract.20502")
#loc8942 = loc("exponential.20503")
#loc8943 = loc("reduce.20509")
#loc8944 = loc("broadcast.20510")
#loc8945 = loc("divide.20511")
#loc8946 = loc("select.20546")
#loc8947 = loc("broadcast.20393")
#loc8948 = loc("reshape.20394")
#loc8949 = loc("convert.20395")
#loc8950 = loc("dot.20547")
#loc8951 = loc("convert.20549")
#loc8952 = loc("transpose.20550")
#loc8953 = loc("reshape.20552")
#loc8954 = loc("reshape.20386")
#loc8955 = loc("reshape.20388")
#loc8956 = loc("transpose.20389")
#loc8958 = loc("reshape.20554")
#loc8959 = loc("add.20557")
#loc8960 = loc("reshape.20588")
#loc8961 = loc("reshape.20590")
#loc8962 = loc("broadcast.20591")
#loc8963 = loc("convert.20558")
#loc8964 = loc("power.20560")
#loc8965 = loc("reduce.20567")
#loc8966 = loc("multiply.20576")
#loc8967 = loc("reshape.20577")
#loc8968 = loc("add.20581")
#loc8969 = loc("rsqrt.20582")
#loc8970 = loc("reshape.20583")
#loc8971 = loc("broadcast.20584")
#loc8972 = loc("multiply.20585")
#loc8973 = loc("convert.20586")
#loc8974 = loc("multiply.20592")
#loc8975 = loc("reshape.20601")
#loc8976 = loc("reshape.20597")
#loc8977 = loc("reshape.20599")
#loc8978 = loc("transpose.20600")
#loc8979 = loc("dot.20602")
#loc8980 = loc("reshape.20603")
#loc8981 = loc("logistic.20604")
#loc8982 = loc("multiply.20605")
#loc8983 = loc("reshape.20377")
#loc8984 = loc("reshape.20379")
#loc8985 = loc("transpose.20380")
#loc8986 = loc("dot.20594")
#loc8987 = loc("reshape.20595")
#loc8988 = loc("multiply.20606")
#loc8989 = loc("reshape.20607")
#loc8990 = loc("reshape.20372")
#loc8991 = loc("reshape.20374")
#loc8992 = loc("transpose.20375")
#loc8994 = loc("reshape.20609")
#loc8995 = loc("add.20612")
#loc8996 = loc("convert.20613")
#loc8997 = loc("power.20615")
#loc8998 = loc("reduce.20622")
#loc8999 = loc("multiply.20631")
#loc9000 = loc("reshape.20632")
#loc9001 = loc("add.20636")
#loc9002 = loc("rsqrt.20637")
#loc9003 = loc("reshape.20638")
#loc9004 = loc("broadcast.20639")
#loc9005 = loc("multiply.20640")
#loc9006 = loc("convert.20641")
#loc9007 = loc("multiply.20647")
#loc9008 = loc("reshape.20648")
#loc9009 = loc("reshape.20363")
#loc9010 = loc("reshape.20365")
#loc9011 = loc("transpose.20366")
#loc9012 = loc("dot.20649")
#loc9013 = loc("reshape.20651")
#loc9014 = loc("convert.20652")
#loc9015 = loc("power.20654")
#loc9016 = loc("reduce.20661")
#loc9017 = loc("multiply.20670")
#loc9018 = loc("reshape.20671")
#loc9019 = loc("add.20675")
#loc9020 = loc("rsqrt.20676")
#loc9021 = loc("reshape.20677")
#loc9022 = loc("broadcast.20678")
#loc9023 = loc("multiply.20679")
#loc9024 = loc("convert.20680")
#loc9025 = loc("multiply.20686")
#loc9026 = loc("transpose.20687")
#loc9027 = loc("multiply.20698")
#loc9028 = loc("slice.20689")
#loc9029 = loc("negate.20690")
#loc9030 = loc("slice.20688")
#loc9031 = loc("concatenate.20691")
#loc9032 = loc("multiply.20694")
#loc9033 = loc("add.20701")
#loc9035 = loc("reshape.20723")
#loc9036 = loc("reshape.20725")
#loc9037 = loc("transpose.20726")
#loc9038 = loc("dot.20728")
#loc9039 = loc("reshape.20730")
#loc9040 = loc("transpose.20731")
#loc9042 = loc("reshape.21077")
#loc9043 = loc("reshape.21079")
#loc9044 = loc("broadcast.21080")
#loc9045 = loc("reshape.21038")
#loc9046 = loc("reshape.21040")
#loc9047 = loc("broadcast.21041")
#loc9048 = loc("reshape.20860")
#loc9049 = loc("reshape.20862")
#loc9050 = loc("broadcast.20863")
#loc9051 = loc("reshape.20822")
#loc9052 = loc("reshape.20824")
#loc9053 = loc("transpose.20825")
#loc9054 = loc("dot.20827")
#loc9055 = loc("reshape.20829")
#loc9056 = loc("convert.20830")
#loc9057 = loc("power.20832")
#loc9058 = loc("reduce.20839")
#loc9059 = loc("multiply.20848")
#loc9060 = loc("reshape.20849")
#loc9061 = loc("add.20853")
#loc9062 = loc("rsqrt.20854")
#loc9063 = loc("reshape.20855")
#loc9064 = loc("broadcast.20856")
#loc9065 = loc("multiply.20857")
#loc9066 = loc("convert.20858")
#loc9067 = loc("multiply.20864")
#loc9068 = loc("transpose.20865")
#loc9069 = loc("multiply.20875")
#loc9070 = loc("slice.20867")
#loc9071 = loc("negate.20868")
#loc9072 = loc("slice.20866")
#loc9073 = loc("concatenate.20869")
#loc9074 = loc("multiply.20872")
#loc9075 = loc("add.20878")
#loc9076 = loc("convert.20879")
#loc9077 = loc("multiply.20881")
#loc9078 = loc("broadcast.20810")
#loc9079 = loc("reshape.20811")
#loc9080 = loc("convert.20812")
#loc9081 = loc("transpose.20813")
#loc9082 = loc("multiply.20815")
#loc9083 = loc("dot.20882")
#loc9084 = loc("add.20889")
#loc9085 = loc("convert.20915")
#loc9086 = loc("compare.20917")
#loc9087 = loc("not.20919")
#loc9089 = loc("or.20929")
#loc9090 = loc("select.20930")
#loc9091 = loc("reshape.20935")
#loc9092 = loc("not.20937")
#loc9093 = loc("reshape.20939")
#loc9094 = loc("broadcast.20940")
#loc9095 = loc("reduce.20895")
#loc9096 = loc("broadcast.20896")
#loc9097 = loc("subtract.20897")
#loc9098 = loc("exponential.20898")
#loc9099 = loc("reduce.20904")
#loc9100 = loc("broadcast.20905")
#loc9101 = loc("divide.20906")
#loc9102 = loc("select.20941")
#loc9103 = loc("broadcast.20788")
#loc9104 = loc("reshape.20789")
#loc9105 = loc("convert.20790")
#loc9106 = loc("dot.20942")
#loc9107 = loc("convert.20944")
#loc9108 = loc("transpose.20945")
#loc9109 = loc("reshape.20947")
#loc9110 = loc("reshape.20781")
#loc9111 = loc("reshape.20783")
#loc9112 = loc("transpose.20784")
#loc9114 = loc("reshape.20949")
#loc9115 = loc("add.20952")
#loc9116 = loc("reshape.20983")
#loc9117 = loc("reshape.20985")
#loc9118 = loc("broadcast.20986")
#loc9119 = loc("convert.20953")
#loc9120 = loc("power.20955")
#loc9121 = loc("reduce.20962")
#loc9122 = loc("multiply.20971")
#loc9123 = loc("reshape.20972")
#loc9124 = loc("add.20976")
#loc9125 = loc("rsqrt.20977")
#loc9126 = loc("reshape.20978")
#loc9127 = loc("broadcast.20979")
#loc9128 = loc("multiply.20980")
#loc9129 = loc("convert.20981")
#loc9130 = loc("multiply.20987")
#loc9131 = loc("reshape.20996")
#loc9132 = loc("reshape.20992")
#loc9133 = loc("reshape.20994")
#loc9134 = loc("transpose.20995")
#loc9135 = loc("dot.20997")
#loc9136 = loc("reshape.20998")
#loc9137 = loc("logistic.20999")
#loc9138 = loc("multiply.21000")
#loc9139 = loc("reshape.20772")
#loc9140 = loc("reshape.20774")
#loc9141 = loc("transpose.20775")
#loc9142 = loc("dot.20989")
#loc9143 = loc("reshape.20990")
#loc9144 = loc("multiply.21001")
#loc9145 = loc("reshape.21002")
#loc9146 = loc("reshape.20767")
#loc9147 = loc("reshape.20769")
#loc9148 = loc("transpose.20770")
#loc9150 = loc("reshape.21004")
#loc9151 = loc("add.21007")
#loc9152 = loc("convert.21008")
#loc9153 = loc("power.21010")
#loc9154 = loc("reduce.21017")
#loc9155 = loc("multiply.21026")
#loc9156 = loc("reshape.21027")
#loc9157 = loc("add.21031")
#loc9158 = loc("rsqrt.21032")
#loc9159 = loc("reshape.21033")
#loc9160 = loc("broadcast.21034")
#loc9161 = loc("multiply.21035")
#loc9162 = loc("convert.21036")
#loc9163 = loc("multiply.21042")
#loc9164 = loc("reshape.21043")
#loc9165 = loc("reshape.20758")
#loc9166 = loc("reshape.20760")
#loc9167 = loc("transpose.20761")
#loc9168 = loc("dot.21044")
#loc9169 = loc("reshape.21046")
#loc9170 = loc("convert.21047")
#loc9171 = loc("power.21049")
#loc9172 = loc("reduce.21056")
#loc9173 = loc("multiply.21065")
#loc9174 = loc("reshape.21066")
#loc9175 = loc("add.21070")
#loc9176 = loc("rsqrt.21071")
#loc9177 = loc("reshape.21072")
#loc9178 = loc("broadcast.21073")
#loc9179 = loc("multiply.21074")
#loc9180 = loc("convert.21075")
#loc9181 = loc("multiply.21081")
#loc9182 = loc("transpose.21082")
#loc9183 = loc("multiply.21093")
#loc9184 = loc("slice.21084")
#loc9185 = loc("negate.21085")
#loc9186 = loc("slice.21083")
#loc9187 = loc("concatenate.21086")
#loc9188 = loc("multiply.21089")
#loc9189 = loc("add.21096")
#loc9191 = loc("reshape.21118")
#loc9192 = loc("reshape.21120")
#loc9193 = loc("transpose.21121")
#loc9194 = loc("dot.21123")
#loc9195 = loc("reshape.21125")
#loc9196 = loc("transpose.21126")
#loc9198 = loc("reshape.21472")
#loc9199 = loc("reshape.21474")
#loc9200 = loc("broadcast.21475")
#loc9201 = loc("reshape.21433")
#loc9202 = loc("reshape.21435")
#loc9203 = loc("broadcast.21436")
#loc9204 = loc("reshape.21255")
#loc9205 = loc("reshape.21257")
#loc9206 = loc("broadcast.21258")
#loc9207 = loc("reshape.21217")
#loc9208 = loc("reshape.21219")
#loc9209 = loc("transpose.21220")
#loc9210 = loc("dot.21222")
#loc9211 = loc("reshape.21224")
#loc9212 = loc("convert.21225")
#loc9213 = loc("power.21227")
#loc9214 = loc("reduce.21234")
#loc9215 = loc("multiply.21243")
#loc9216 = loc("reshape.21244")
#loc9217 = loc("add.21248")
#loc9218 = loc("rsqrt.21249")
#loc9219 = loc("reshape.21250")
#loc9220 = loc("broadcast.21251")
#loc9221 = loc("multiply.21252")
#loc9222 = loc("convert.21253")
#loc9223 = loc("multiply.21259")
#loc9224 = loc("transpose.21260")
#loc9225 = loc("multiply.21270")
#loc9226 = loc("slice.21262")
#loc9227 = loc("negate.21263")
#loc9228 = loc("slice.21261")
#loc9229 = loc("concatenate.21264")
#loc9230 = loc("multiply.21267")
#loc9231 = loc("add.21273")
#loc9232 = loc("convert.21274")
#loc9233 = loc("multiply.21276")
#loc9234 = loc("broadcast.21205")
#loc9235 = loc("reshape.21206")
#loc9236 = loc("convert.21207")
#loc9237 = loc("transpose.21208")
#loc9238 = loc("multiply.21210")
#loc9239 = loc("dot.21277")
#loc9240 = loc("add.21284")
#loc9241 = loc("convert.21310")
#loc9242 = loc("compare.21312")
#loc9243 = loc("not.21314")
#loc9245 = loc("or.21324")
#loc9246 = loc("select.21325")
#loc9247 = loc("reshape.21330")
#loc9248 = loc("not.21332")
#loc9249 = loc("reshape.21334")
#loc9250 = loc("broadcast.21335")
#loc9251 = loc("reduce.21290")
#loc9252 = loc("broadcast.21291")
#loc9253 = loc("subtract.21292")
#loc9254 = loc("exponential.21293")
#loc9255 = loc("reduce.21299")
#loc9256 = loc("broadcast.21300")
#loc9257 = loc("divide.21301")
#loc9258 = loc("select.21336")
#loc9259 = loc("broadcast.21183")
#loc9260 = loc("reshape.21184")
#loc9261 = loc("convert.21185")
#loc9262 = loc("dot.21337")
#loc9263 = loc("convert.21339")
#loc9264 = loc("transpose.21340")
#loc9265 = loc("reshape.21342")
#loc9266 = loc("reshape.21176")
#loc9267 = loc("reshape.21178")
#loc9268 = loc("transpose.21179")
#loc9270 = loc("reshape.21344")
#loc9271 = loc("add.21347")
#loc9272 = loc("reshape.21378")
#loc9273 = loc("reshape.21380")
#loc9274 = loc("broadcast.21381")
#loc9275 = loc("convert.21348")
#loc9276 = loc("power.21350")
#loc9277 = loc("reduce.21357")
#loc9278 = loc("multiply.21366")
#loc9279 = loc("reshape.21367")
#loc9280 = loc("add.21371")
#loc9281 = loc("rsqrt.21372")
#loc9282 = loc("reshape.21373")
#loc9283 = loc("broadcast.21374")
#loc9284 = loc("multiply.21375")
#loc9285 = loc("convert.21376")
#loc9286 = loc("multiply.21382")
#loc9287 = loc("reshape.21391")
#loc9288 = loc("reshape.21387")
#loc9289 = loc("reshape.21389")
#loc9290 = loc("transpose.21390")
#loc9291 = loc("dot.21392")
#loc9292 = loc("reshape.21393")
#loc9293 = loc("logistic.21394")
#loc9294 = loc("multiply.21395")
#loc9295 = loc("reshape.21167")
#loc9296 = loc("reshape.21169")
#loc9297 = loc("transpose.21170")
#loc9298 = loc("dot.21384")
#loc9299 = loc("reshape.21385")
#loc9300 = loc("multiply.21396")
#loc9301 = loc("reshape.21397")
#loc9302 = loc("reshape.21162")
#loc9303 = loc("reshape.21164")
#loc9304 = loc("transpose.21165")
#loc9306 = loc("reshape.21399")
#loc9307 = loc("add.21402")
#loc9308 = loc("convert.21403")
#loc9309 = loc("power.21405")
#loc9310 = loc("reduce.21412")
#loc9311 = loc("multiply.21421")
#loc9312 = loc("reshape.21422")
#loc9313 = loc("add.21426")
#loc9314 = loc("rsqrt.21427")
#loc9315 = loc("reshape.21428")
#loc9316 = loc("broadcast.21429")
#loc9317 = loc("multiply.21430")
#loc9318 = loc("convert.21431")
#loc9319 = loc("multiply.21437")
#loc9320 = loc("reshape.21438")
#loc9321 = loc("reshape.21153")
#loc9322 = loc("reshape.21155")
#loc9323 = loc("transpose.21156")
#loc9324 = loc("dot.21439")
#loc9325 = loc("reshape.21441")
#loc9326 = loc("convert.21442")
#loc9327 = loc("power.21444")
#loc9328 = loc("reduce.21451")
#loc9329 = loc("multiply.21460")
#loc9330 = loc("reshape.21461")
#loc9331 = loc("add.21465")
#loc9332 = loc("rsqrt.21466")
#loc9333 = loc("reshape.21467")
#loc9334 = loc("broadcast.21468")
#loc9335 = loc("multiply.21469")
#loc9336 = loc("convert.21470")
#loc9337 = loc("multiply.21476")
#loc9338 = loc("transpose.21477")
#loc9339 = loc("multiply.21488")
#loc9340 = loc("slice.21479")
#loc9341 = loc("negate.21480")
#loc9342 = loc("slice.21478")
#loc9343 = loc("concatenate.21481")
#loc9344 = loc("multiply.21484")
#loc9345 = loc("add.21491")
#loc9347 = loc("reshape.21513")
#loc9348 = loc("reshape.21515")
#loc9349 = loc("transpose.21516")
#loc9350 = loc("dot.21518")
#loc9351 = loc("reshape.21520")
#loc9352 = loc("transpose.21521")
#loc9354 = loc("reshape.21867")
#loc9355 = loc("reshape.21869")
#loc9356 = loc("broadcast.21870")
#loc9357 = loc("reshape.21828")
#loc9358 = loc("reshape.21830")
#loc9359 = loc("broadcast.21831")
#loc9360 = loc("reshape.21650")
#loc9361 = loc("reshape.21652")
#loc9362 = loc("broadcast.21653")
#loc9363 = loc("reshape.21612")
#loc9364 = loc("reshape.21614")
#loc9365 = loc("transpose.21615")
#loc9366 = loc("dot.21617")
#loc9367 = loc("reshape.21619")
#loc9368 = loc("convert.21620")
#loc9369 = loc("power.21622")
#loc9370 = loc("reduce.21629")
#loc9371 = loc("multiply.21638")
#loc9372 = loc("reshape.21639")
#loc9373 = loc("add.21643")
#loc9374 = loc("rsqrt.21644")
#loc9375 = loc("reshape.21645")
#loc9376 = loc("broadcast.21646")
#loc9377 = loc("multiply.21647")
#loc9378 = loc("convert.21648")
#loc9379 = loc("multiply.21654")
#loc9380 = loc("transpose.21655")
#loc9381 = loc("multiply.21665")
#loc9382 = loc("slice.21657")
#loc9383 = loc("negate.21658")
#loc9384 = loc("slice.21656")
#loc9385 = loc("concatenate.21659")
#loc9386 = loc("multiply.21662")
#loc9387 = loc("add.21668")
#loc9388 = loc("convert.21669")
#loc9389 = loc("multiply.21671")
#loc9390 = loc("broadcast.21600")
#loc9391 = loc("reshape.21601")
#loc9392 = loc("convert.21602")
#loc9393 = loc("transpose.21603")
#loc9394 = loc("multiply.21605")
#loc9395 = loc("dot.21672")
#loc9396 = loc("add.21679")
#loc9397 = loc("convert.21705")
#loc9398 = loc("compare.21707")
#loc9399 = loc("not.21709")
#loc9401 = loc("or.21719")
#loc9402 = loc("select.21720")
#loc9403 = loc("reshape.21725")
#loc9404 = loc("not.21727")
#loc9405 = loc("reshape.21729")
#loc9406 = loc("broadcast.21730")
#loc9407 = loc("reduce.21685")
#loc9408 = loc("broadcast.21686")
#loc9409 = loc("subtract.21687")
#loc9410 = loc("exponential.21688")
#loc9411 = loc("reduce.21694")
#loc9412 = loc("broadcast.21695")
#loc9413 = loc("divide.21696")
#loc9414 = loc("select.21731")
#loc9415 = loc("broadcast.21578")
#loc9416 = loc("reshape.21579")
#loc9417 = loc("convert.21580")
#loc9418 = loc("dot.21732")
#loc9419 = loc("convert.21734")
#loc9420 = loc("transpose.21735")
#loc9421 = loc("reshape.21737")
#loc9422 = loc("reshape.21571")
#loc9423 = loc("reshape.21573")
#loc9424 = loc("transpose.21574")
#loc9426 = loc("reshape.21739")
#loc9427 = loc("add.21742")
#loc9428 = loc("reshape.21773")
#loc9429 = loc("reshape.21775")
#loc9430 = loc("broadcast.21776")
#loc9431 = loc("convert.21743")
#loc9432 = loc("power.21745")
#loc9433 = loc("reduce.21752")
#loc9434 = loc("multiply.21761")
#loc9435 = loc("reshape.21762")
#loc9436 = loc("add.21766")
#loc9437 = loc("rsqrt.21767")
#loc9438 = loc("reshape.21768")
#loc9439 = loc("broadcast.21769")
#loc9440 = loc("multiply.21770")
#loc9441 = loc("convert.21771")
#loc9442 = loc("multiply.21777")
#loc9443 = loc("reshape.21786")
#loc9444 = loc("reshape.21782")
#loc9445 = loc("reshape.21784")
#loc9446 = loc("transpose.21785")
#loc9447 = loc("dot.21787")
#loc9448 = loc("reshape.21788")
#loc9449 = loc("logistic.21789")
#loc9450 = loc("multiply.21790")
#loc9451 = loc("reshape.21562")
#loc9452 = loc("reshape.21564")
#loc9453 = loc("transpose.21565")
#loc9454 = loc("dot.21779")
#loc9455 = loc("reshape.21780")
#loc9456 = loc("multiply.21791")
#loc9457 = loc("reshape.21792")
#loc9458 = loc("reshape.21557")
#loc9459 = loc("reshape.21559")
#loc9460 = loc("transpose.21560")
#loc9462 = loc("reshape.21794")
#loc9463 = loc("add.21797")
#loc9464 = loc("convert.21798")
#loc9465 = loc("power.21800")
#loc9466 = loc("reduce.21807")
#loc9467 = loc("multiply.21816")
#loc9468 = loc("reshape.21817")
#loc9469 = loc("add.21821")
#loc9470 = loc("rsqrt.21822")
#loc9471 = loc("reshape.21823")
#loc9472 = loc("broadcast.21824")
#loc9473 = loc("multiply.21825")
#loc9474 = loc("convert.21826")
#loc9475 = loc("multiply.21832")
#loc9476 = loc("reshape.21833")
#loc9477 = loc("reshape.21548")
#loc9478 = loc("reshape.21550")
#loc9479 = loc("transpose.21551")
#loc9480 = loc("dot.21834")
#loc9481 = loc("reshape.21836")
#loc9482 = loc("convert.21837")
#loc9483 = loc("power.21839")
#loc9484 = loc("reduce.21846")
#loc9485 = loc("multiply.21855")
#loc9486 = loc("reshape.21856")
#loc9487 = loc("add.21860")
#loc9488 = loc("rsqrt.21861")
#loc9489 = loc("reshape.21862")
#loc9490 = loc("broadcast.21863")
#loc9491 = loc("multiply.21864")
#loc9492 = loc("convert.21865")
#loc9493 = loc("multiply.21871")
#loc9494 = loc("transpose.21872")
#loc9495 = loc("multiply.21883")
#loc9496 = loc("slice.21874")
#loc9497 = loc("negate.21875")
#loc9498 = loc("slice.21873")
#loc9499 = loc("concatenate.21876")
#loc9500 = loc("multiply.21879")
#loc9501 = loc("add.21886")
#loc9503 = loc("reshape.21908")
#loc9504 = loc("reshape.21910")
#loc9505 = loc("transpose.21911")
#loc9506 = loc("dot.21913")
#loc9507 = loc("reshape.21915")
#loc9508 = loc("transpose.21916")
#loc9510 = loc("reshape.22262")
#loc9511 = loc("reshape.22264")
#loc9512 = loc("broadcast.22265")
#loc9513 = loc("reshape.22223")
#loc9514 = loc("reshape.22225")
#loc9515 = loc("broadcast.22226")
#loc9516 = loc("reshape.22045")
#loc9517 = loc("reshape.22047")
#loc9518 = loc("broadcast.22048")
#loc9519 = loc("reshape.22007")
#loc9520 = loc("reshape.22009")
#loc9521 = loc("transpose.22010")
#loc9522 = loc("dot.22012")
#loc9523 = loc("reshape.22014")
#loc9524 = loc("convert.22015")
#loc9525 = loc("power.22017")
#loc9526 = loc("reduce.22024")
#loc9527 = loc("multiply.22033")
#loc9528 = loc("reshape.22034")
#loc9529 = loc("add.22038")
#loc9530 = loc("rsqrt.22039")
#loc9531 = loc("reshape.22040")
#loc9532 = loc("broadcast.22041")
#loc9533 = loc("multiply.22042")
#loc9534 = loc("convert.22043")
#loc9535 = loc("multiply.22049")
#loc9536 = loc("transpose.22050")
#loc9537 = loc("multiply.22060")
#loc9538 = loc("slice.22052")
#loc9539 = loc("negate.22053")
#loc9540 = loc("slice.22051")
#loc9541 = loc("concatenate.22054")
#loc9542 = loc("multiply.22057")
#loc9543 = loc("add.22063")
#loc9544 = loc("convert.22064")
#loc9545 = loc("multiply.22066")
#loc9546 = loc("broadcast.21995")
#loc9547 = loc("reshape.21996")
#loc9548 = loc("convert.21997")
#loc9549 = loc("transpose.21998")
#loc9550 = loc("multiply.22000")
#loc9551 = loc("dot.22067")
#loc9552 = loc("add.22074")
#loc9553 = loc("convert.22100")
#loc9554 = loc("compare.22102")
#loc9555 = loc("not.22104")
#loc9557 = loc("or.22114")
#loc9558 = loc("select.22115")
#loc9559 = loc("reshape.22120")
#loc9560 = loc("not.22122")
#loc9561 = loc("reshape.22124")
#loc9562 = loc("broadcast.22125")
#loc9563 = loc("reduce.22080")
#loc9564 = loc("broadcast.22081")
#loc9565 = loc("subtract.22082")
#loc9566 = loc("exponential.22083")
#loc9567 = loc("reduce.22089")
#loc9568 = loc("broadcast.22090")
#loc9569 = loc("divide.22091")
#loc9570 = loc("select.22126")
#loc9571 = loc("broadcast.21973")
#loc9572 = loc("reshape.21974")
#loc9573 = loc("convert.21975")
#loc9574 = loc("dot.22127")
#loc9575 = loc("convert.22129")
#loc9576 = loc("transpose.22130")
#loc9577 = loc("reshape.22132")
#loc9578 = loc("reshape.21966")
#loc9579 = loc("reshape.21968")
#loc9580 = loc("transpose.21969")
#loc9582 = loc("reshape.22134")
#loc9583 = loc("add.22137")
#loc9584 = loc("reshape.22168")
#loc9585 = loc("reshape.22170")
#loc9586 = loc("broadcast.22171")
#loc9587 = loc("convert.22138")
#loc9588 = loc("power.22140")
#loc9589 = loc("reduce.22147")
#loc9590 = loc("multiply.22156")
#loc9591 = loc("reshape.22157")
#loc9592 = loc("add.22161")
#loc9593 = loc("rsqrt.22162")
#loc9594 = loc("reshape.22163")
#loc9595 = loc("broadcast.22164")
#loc9596 = loc("multiply.22165")
#loc9597 = loc("convert.22166")
#loc9598 = loc("multiply.22172")
#loc9599 = loc("reshape.22181")
#loc9600 = loc("reshape.22177")
#loc9601 = loc("reshape.22179")
#loc9602 = loc("transpose.22180")
#loc9603 = loc("dot.22182")
#loc9604 = loc("reshape.22183")
#loc9605 = loc("logistic.22184")
#loc9606 = loc("multiply.22185")
#loc9607 = loc("reshape.21957")
#loc9608 = loc("reshape.21959")
#loc9609 = loc("transpose.21960")
#loc9610 = loc("dot.22174")
#loc9611 = loc("reshape.22175")
#loc9612 = loc("multiply.22186")
#loc9613 = loc("reshape.22187")
#loc9614 = loc("reshape.21952")
#loc9615 = loc("reshape.21954")
#loc9616 = loc("transpose.21955")
#loc9618 = loc("reshape.22189")
#loc9619 = loc("add.22192")
#loc9620 = loc("convert.22193")
#loc9621 = loc("power.22195")
#loc9622 = loc("reduce.22202")
#loc9623 = loc("multiply.22211")
#loc9624 = loc("reshape.22212")
#loc9625 = loc("add.22216")
#loc9626 = loc("rsqrt.22217")
#loc9627 = loc("reshape.22218")
#loc9628 = loc("broadcast.22219")
#loc9629 = loc("multiply.22220")
#loc9630 = loc("convert.22221")
#loc9631 = loc("multiply.22227")
#loc9632 = loc("reshape.22228")
#loc9633 = loc("reshape.21943")
#loc9634 = loc("reshape.21945")
#loc9635 = loc("transpose.21946")
#loc9636 = loc("dot.22229")
#loc9637 = loc("reshape.22231")
#loc9638 = loc("convert.22232")
#loc9639 = loc("power.22234")
#loc9640 = loc("reduce.22241")
#loc9641 = loc("multiply.22250")
#loc9642 = loc("reshape.22251")
#loc9643 = loc("add.22255")
#loc9644 = loc("rsqrt.22256")
#loc9645 = loc("reshape.22257")
#loc9646 = loc("broadcast.22258")
#loc9647 = loc("multiply.22259")
#loc9648 = loc("convert.22260")
#loc9649 = loc("multiply.22266")
#loc9650 = loc("transpose.22267")
#loc9651 = loc("multiply.22278")
#loc9652 = loc("slice.22269")
#loc9653 = loc("negate.22270")
#loc9654 = loc("slice.22268")
#loc9655 = loc("concatenate.22271")
#loc9656 = loc("multiply.22274")
#loc9657 = loc("add.22281")
#loc9659 = loc("reshape.22303")
#loc9660 = loc("reshape.22305")
#loc9661 = loc("transpose.22306")
#loc9662 = loc("dot.22308")
#loc9663 = loc("reshape.22310")
#loc9664 = loc("transpose.22311")
#loc9666 = loc("reshape.22657")
#loc9667 = loc("reshape.22659")
#loc9668 = loc("broadcast.22660")
#loc9669 = loc("reshape.22618")
#loc9670 = loc("reshape.22620")
#loc9671 = loc("broadcast.22621")
#loc9672 = loc("reshape.22440")
#loc9673 = loc("reshape.22442")
#loc9674 = loc("broadcast.22443")
#loc9675 = loc("reshape.22402")
#loc9676 = loc("reshape.22404")
#loc9677 = loc("transpose.22405")
#loc9678 = loc("dot.22407")
#loc9679 = loc("reshape.22409")
#loc9680 = loc("convert.22410")
#loc9681 = loc("power.22412")
#loc9682 = loc("reduce.22419")
#loc9683 = loc("multiply.22428")
#loc9684 = loc("reshape.22429")
#loc9685 = loc("add.22433")
#loc9686 = loc("rsqrt.22434")
#loc9687 = loc("reshape.22435")
#loc9688 = loc("broadcast.22436")
#loc9689 = loc("multiply.22437")
#loc9690 = loc("convert.22438")
#loc9691 = loc("multiply.22444")
#loc9692 = loc("transpose.22445")
#loc9693 = loc("multiply.22455")
#loc9694 = loc("slice.22447")
#loc9695 = loc("negate.22448")
#loc9696 = loc("slice.22446")
#loc9697 = loc("concatenate.22449")
#loc9698 = loc("multiply.22452")
#loc9699 = loc("add.22458")
#loc9700 = loc("convert.22459")
#loc9701 = loc("multiply.22461")
#loc9702 = loc("broadcast.22390")
#loc9703 = loc("reshape.22391")
#loc9704 = loc("convert.22392")
#loc9705 = loc("transpose.22393")
#loc9706 = loc("multiply.22395")
#loc9707 = loc("dot.22462")
#loc9708 = loc("add.22469")
#loc9709 = loc("convert.22495")
#loc9710 = loc("compare.22497")
#loc9711 = loc("not.22499")
#loc9713 = loc("or.22509")
#loc9714 = loc("select.22510")
#loc9715 = loc("reshape.22515")
#loc9716 = loc("not.22517")
#loc9717 = loc("reshape.22519")
#loc9718 = loc("broadcast.22520")
#loc9719 = loc("reduce.22475")
#loc9720 = loc("broadcast.22476")
#loc9721 = loc("subtract.22477")
#loc9722 = loc("exponential.22478")
#loc9723 = loc("reduce.22484")
#loc9724 = loc("broadcast.22485")
#loc9725 = loc("divide.22486")
#loc9726 = loc("select.22521")
#loc9727 = loc("broadcast.22368")
#loc9728 = loc("reshape.22369")
#loc9729 = loc("convert.22370")
#loc9730 = loc("dot.22522")
#loc9731 = loc("convert.22524")
#loc9732 = loc("transpose.22525")
#loc9733 = loc("reshape.22527")
#loc9734 = loc("reshape.22361")
#loc9735 = loc("reshape.22363")
#loc9736 = loc("transpose.22364")
#loc9738 = loc("reshape.22529")
#loc9739 = loc("add.22532")
#loc9740 = loc("reshape.22563")
#loc9741 = loc("reshape.22565")
#loc9742 = loc("broadcast.22566")
#loc9743 = loc("convert.22533")
#loc9744 = loc("power.22535")
#loc9745 = loc("reduce.22542")
#loc9746 = loc("multiply.22551")
#loc9747 = loc("reshape.22552")
#loc9748 = loc("add.22556")
#loc9749 = loc("rsqrt.22557")
#loc9750 = loc("reshape.22558")
#loc9751 = loc("broadcast.22559")
#loc9752 = loc("multiply.22560")
#loc9753 = loc("convert.22561")
#loc9754 = loc("multiply.22567")
#loc9755 = loc("reshape.22576")
#loc9756 = loc("reshape.22572")
#loc9757 = loc("reshape.22574")
#loc9758 = loc("transpose.22575")
#loc9759 = loc("dot.22577")
#loc9760 = loc("reshape.22578")
#loc9761 = loc("logistic.22579")
#loc9762 = loc("multiply.22580")
#loc9763 = loc("reshape.22352")
#loc9764 = loc("reshape.22354")
#loc9765 = loc("transpose.22355")
#loc9766 = loc("dot.22569")
#loc9767 = loc("reshape.22570")
#loc9768 = loc("multiply.22581")
#loc9769 = loc("reshape.22582")
#loc9770 = loc("reshape.22347")
#loc9771 = loc("reshape.22349")
#loc9772 = loc("transpose.22350")
#loc9774 = loc("reshape.22584")
#loc9775 = loc("add.22587")
#loc9776 = loc("convert.22588")
#loc9777 = loc("power.22590")
#loc9778 = loc("reduce.22597")
#loc9779 = loc("multiply.22606")
#loc9780 = loc("reshape.22607")
#loc9781 = loc("add.22611")
#loc9782 = loc("rsqrt.22612")
#loc9783 = loc("reshape.22613")
#loc9784 = loc("broadcast.22614")
#loc9785 = loc("multiply.22615")
#loc9786 = loc("convert.22616")
#loc9787 = loc("multiply.22622")
#loc9788 = loc("reshape.22623")
#loc9789 = loc("reshape.22338")
#loc9790 = loc("reshape.22340")
#loc9791 = loc("transpose.22341")
#loc9792 = loc("dot.22624")
#loc9793 = loc("reshape.22626")
#loc9794 = loc("convert.22627")
#loc9795 = loc("power.22629")
#loc9796 = loc("reduce.22636")
#loc9797 = loc("multiply.22645")
#loc9798 = loc("reshape.22646")
#loc9799 = loc("add.22650")
#loc9800 = loc("rsqrt.22651")
#loc9801 = loc("reshape.22652")
#loc9802 = loc("broadcast.22653")
#loc9803 = loc("multiply.22654")
#loc9804 = loc("convert.22655")
#loc9805 = loc("multiply.22661")
#loc9806 = loc("transpose.22662")
#loc9807 = loc("multiply.22673")
#loc9808 = loc("slice.22664")
#loc9809 = loc("negate.22665")
#loc9810 = loc("slice.22663")
#loc9811 = loc("concatenate.22666")
#loc9812 = loc("multiply.22669")
#loc9813 = loc("add.22676")
#loc9815 = loc("reshape.22698")
#loc9816 = loc("reshape.22700")
#loc9817 = loc("transpose.22701")
#loc9818 = loc("dot.22703")
#loc9819 = loc("reshape.22705")
#loc9820 = loc("transpose.22706")
#loc9822 = loc("reshape.23052")
#loc9823 = loc("reshape.23054")
#loc9824 = loc("broadcast.23055")
#loc9825 = loc("reshape.23013")
#loc9826 = loc("reshape.23015")
#loc9827 = loc("broadcast.23016")
#loc9828 = loc("reshape.22835")
#loc9829 = loc("reshape.22837")
#loc9830 = loc("broadcast.22838")
#loc9831 = loc("reshape.22797")
#loc9832 = loc("reshape.22799")
#loc9833 = loc("transpose.22800")
#loc9834 = loc("dot.22802")
#loc9835 = loc("reshape.22804")
#loc9836 = loc("convert.22805")
#loc9837 = loc("power.22807")
#loc9838 = loc("reduce.22814")
#loc9839 = loc("multiply.22823")
#loc9840 = loc("reshape.22824")
#loc9841 = loc("add.22828")
#loc9842 = loc("rsqrt.22829")
#loc9843 = loc("reshape.22830")
#loc9844 = loc("broadcast.22831")
#loc9845 = loc("multiply.22832")
#loc9846 = loc("convert.22833")
#loc9847 = loc("multiply.22839")
#loc9848 = loc("transpose.22840")
#loc9849 = loc("multiply.22850")
#loc9850 = loc("slice.22842")
#loc9851 = loc("negate.22843")
#loc9852 = loc("slice.22841")
#loc9853 = loc("concatenate.22844")
#loc9854 = loc("multiply.22847")
#loc9855 = loc("add.22853")
#loc9856 = loc("convert.22854")
#loc9857 = loc("multiply.22856")
#loc9858 = loc("broadcast.22785")
#loc9859 = loc("reshape.22786")
#loc9860 = loc("convert.22787")
#loc9861 = loc("transpose.22788")
#loc9862 = loc("multiply.22790")
#loc9863 = loc("dot.22857")
#loc9864 = loc("add.22864")
#loc9865 = loc("convert.22890")
#loc9866 = loc("compare.22892")
#loc9867 = loc("not.22894")
#loc9869 = loc("or.22904")
#loc9870 = loc("select.22905")
#loc9871 = loc("reshape.22910")
#loc9872 = loc("not.22912")
#loc9873 = loc("reshape.22914")
#loc9874 = loc("broadcast.22915")
#loc9875 = loc("reduce.22870")
#loc9876 = loc("broadcast.22871")
#loc9877 = loc("subtract.22872")
#loc9878 = loc("exponential.22873")
#loc9879 = loc("reduce.22879")
#loc9880 = loc("broadcast.22880")
#loc9881 = loc("divide.22881")
#loc9882 = loc("select.22916")
#loc9883 = loc("broadcast.22763")
#loc9884 = loc("reshape.22764")
#loc9885 = loc("convert.22765")
#loc9886 = loc("dot.22917")
#loc9887 = loc("convert.22919")
#loc9888 = loc("transpose.22920")
#loc9889 = loc("reshape.22922")
#loc9890 = loc("reshape.22756")
#loc9891 = loc("reshape.22758")
#loc9892 = loc("transpose.22759")
#loc9894 = loc("reshape.22924")
#loc9895 = loc("add.22927")
#loc9896 = loc("reshape.22958")
#loc9897 = loc("reshape.22960")
#loc9898 = loc("broadcast.22961")
#loc9899 = loc("convert.22928")
#loc9900 = loc("power.22930")
#loc9901 = loc("reduce.22937")
#loc9902 = loc("multiply.22946")
#loc9903 = loc("reshape.22947")
#loc9904 = loc("add.22951")
#loc9905 = loc("rsqrt.22952")
#loc9906 = loc("reshape.22953")
#loc9907 = loc("broadcast.22954")
#loc9908 = loc("multiply.22955")
#loc9909 = loc("convert.22956")
#loc9910 = loc("multiply.22962")
#loc9911 = loc("reshape.22971")
#loc9912 = loc("reshape.22967")
#loc9913 = loc("reshape.22969")
#loc9914 = loc("transpose.22970")
#loc9915 = loc("dot.22972")
#loc9916 = loc("reshape.22973")
#loc9917 = loc("logistic.22974")
#loc9918 = loc("multiply.22975")
#loc9919 = loc("reshape.22747")
#loc9920 = loc("reshape.22749")
#loc9921 = loc("transpose.22750")
#loc9922 = loc("dot.22964")
#loc9923 = loc("reshape.22965")
#loc9924 = loc("multiply.22976")
#loc9925 = loc("reshape.22977")
#loc9926 = loc("reshape.22742")
#loc9927 = loc("reshape.22744")
#loc9928 = loc("transpose.22745")
#loc9930 = loc("reshape.22979")
#loc9931 = loc("add.22982")
#loc9932 = loc("convert.22983")
#loc9933 = loc("power.22985")
#loc9934 = loc("reduce.22992")
#loc9935 = loc("multiply.23001")
#loc9936 = loc("reshape.23002")
#loc9937 = loc("add.23006")
#loc9938 = loc("rsqrt.23007")
#loc9939 = loc("reshape.23008")
#loc9940 = loc("broadcast.23009")
#loc9941 = loc("multiply.23010")
#loc9942 = loc("convert.23011")
#loc9943 = loc("multiply.23017")
#loc9944 = loc("reshape.23018")
#loc9945 = loc("reshape.22733")
#loc9946 = loc("reshape.22735")
#loc9947 = loc("transpose.22736")
#loc9948 = loc("dot.23019")
#loc9949 = loc("reshape.23021")
#loc9950 = loc("convert.23022")
#loc9951 = loc("power.23024")
#loc9952 = loc("reduce.23031")
#loc9953 = loc("multiply.23040")
#loc9954 = loc("reshape.23041")
#loc9955 = loc("add.23045")
#loc9956 = loc("rsqrt.23046")
#loc9957 = loc("reshape.23047")
#loc9958 = loc("broadcast.23048")
#loc9959 = loc("multiply.23049")
#loc9960 = loc("convert.23050")
#loc9961 = loc("multiply.23056")
#loc9962 = loc("transpose.23057")
#loc9963 = loc("multiply.23068")
#loc9964 = loc("slice.23059")
#loc9965 = loc("negate.23060")
#loc9966 = loc("slice.23058")
#loc9967 = loc("concatenate.23061")
#loc9968 = loc("multiply.23064")
#loc9969 = loc("add.23071")
#loc9971 = loc("reshape.23093")
#loc9972 = loc("reshape.23095")
#loc9973 = loc("transpose.23096")
#loc9974 = loc("dot.23098")
#loc9975 = loc("reshape.23100")
#loc9976 = loc("transpose.23101")
#loc9978 = loc("reshape.23447")
#loc9979 = loc("reshape.23449")
#loc9980 = loc("broadcast.23450")
#loc9981 = loc("reshape.23408")
#loc9982 = loc("reshape.23410")
#loc9983 = loc("broadcast.23411")
#loc9984 = loc("reshape.23230")
#loc9985 = loc("reshape.23232")
#loc9986 = loc("broadcast.23233")
#loc9987 = loc("reshape.23192")
#loc9988 = loc("reshape.23194")
#loc9989 = loc("transpose.23195")
#loc9990 = loc("dot.23197")
#loc9991 = loc("reshape.23199")
#loc9992 = loc("convert.23200")
#loc9993 = loc("power.23202")
#loc9994 = loc("reduce.23209")
#loc9995 = loc("multiply.23218")
#loc9996 = loc("reshape.23219")
#loc9997 = loc("add.23223")
#loc9998 = loc("rsqrt.23224")
#loc9999 = loc("reshape.23225")
#loc10000 = loc("broadcast.23226")
#loc10001 = loc("multiply.23227")
#loc10002 = loc("convert.23228")
#loc10003 = loc("multiply.23234")
#loc10004 = loc("transpose.23235")
#loc10005 = loc("multiply.23245")
#loc10006 = loc("slice.23237")
#loc10007 = loc("negate.23238")
#loc10008 = loc("slice.23236")
#loc10009 = loc("concatenate.23239")
#loc10010 = loc("multiply.23242")
#loc10011 = loc("add.23248")
#loc10012 = loc("convert.23249")
#loc10013 = loc("multiply.23251")
#loc10014 = loc("broadcast.23180")
#loc10015 = loc("reshape.23181")
#loc10016 = loc("convert.23182")
#loc10017 = loc("transpose.23183")
#loc10018 = loc("multiply.23185")
#loc10019 = loc("dot.23252")
#loc10020 = loc("add.23259")
#loc10021 = loc("convert.23285")
#loc10022 = loc("compare.23287")
#loc10023 = loc("not.23289")
#loc10025 = loc("or.23299")
#loc10026 = loc("select.23300")
#loc10027 = loc("reshape.23305")
#loc10028 = loc("not.23307")
#loc10029 = loc("reshape.23309")
#loc10030 = loc("broadcast.23310")
#loc10031 = loc("reduce.23265")
#loc10032 = loc("broadcast.23266")
#loc10033 = loc("subtract.23267")
#loc10034 = loc("exponential.23268")
#loc10035 = loc("reduce.23274")
#loc10036 = loc("broadcast.23275")
#loc10037 = loc("divide.23276")
#loc10038 = loc("select.23311")
#loc10039 = loc("broadcast.23158")
#loc10040 = loc("reshape.23159")
#loc10041 = loc("convert.23160")
#loc10042 = loc("dot.23312")
#loc10043 = loc("convert.23314")
#loc10044 = loc("transpose.23315")
#loc10045 = loc("reshape.23317")
#loc10046 = loc("reshape.23151")
#loc10047 = loc("reshape.23153")
#loc10048 = loc("transpose.23154")
#loc10050 = loc("reshape.23319")
#loc10051 = loc("add.23322")
#loc10052 = loc("reshape.23353")
#loc10053 = loc("reshape.23355")
#loc10054 = loc("broadcast.23356")
#loc10055 = loc("convert.23323")
#loc10056 = loc("power.23325")
#loc10057 = loc("reduce.23332")
#loc10058 = loc("multiply.23341")
#loc10059 = loc("reshape.23342")
#loc10060 = loc("add.23346")
#loc10061 = loc("rsqrt.23347")
#loc10062 = loc("reshape.23348")
#loc10063 = loc("broadcast.23349")
#loc10064 = loc("multiply.23350")
#loc10065 = loc("convert.23351")
#loc10066 = loc("multiply.23357")
#loc10067 = loc("reshape.23366")
#loc10068 = loc("reshape.23362")
#loc10069 = loc("reshape.23364")
#loc10070 = loc("transpose.23365")
#loc10071 = loc("dot.23367")
#loc10072 = loc("reshape.23368")
#loc10073 = loc("logistic.23369")
#loc10074 = loc("multiply.23370")
#loc10075 = loc("reshape.23142")
#loc10076 = loc("reshape.23144")
#loc10077 = loc("transpose.23145")
#loc10078 = loc("dot.23359")
#loc10079 = loc("reshape.23360")
#loc10080 = loc("multiply.23371")
#loc10081 = loc("reshape.23372")
#loc10082 = loc("reshape.23137")
#loc10083 = loc("reshape.23139")
#loc10084 = loc("transpose.23140")
#loc10086 = loc("reshape.23374")
#loc10087 = loc("add.23377")
#loc10088 = loc("convert.23378")
#loc10089 = loc("power.23380")
#loc10090 = loc("reduce.23387")
#loc10091 = loc("multiply.23396")
#loc10092 = loc("reshape.23397")
#loc10093 = loc("add.23401")
#loc10094 = loc("rsqrt.23402")
#loc10095 = loc("reshape.23403")
#loc10096 = loc("broadcast.23404")
#loc10097 = loc("multiply.23405")
#loc10098 = loc("convert.23406")
#loc10099 = loc("multiply.23412")
#loc10100 = loc("reshape.23413")
#loc10101 = loc("reshape.23128")
#loc10102 = loc("reshape.23130")
#loc10103 = loc("transpose.23131")
#loc10104 = loc("dot.23414")
#loc10105 = loc("reshape.23416")
#loc10106 = loc("convert.23417")
#loc10107 = loc("power.23419")
#loc10108 = loc("reduce.23426")
#loc10109 = loc("multiply.23435")
#loc10110 = loc("reshape.23436")
#loc10111 = loc("add.23440")
#loc10112 = loc("rsqrt.23441")
#loc10113 = loc("reshape.23442")
#loc10114 = loc("broadcast.23443")
#loc10115 = loc("multiply.23444")
#loc10116 = loc("convert.23445")
#loc10117 = loc("multiply.23451")
#loc10118 = loc("transpose.23452")
#loc10119 = loc("multiply.23463")
#loc10120 = loc("slice.23454")
#loc10121 = loc("negate.23455")
#loc10122 = loc("slice.23453")
#loc10123 = loc("concatenate.23456")
#loc10124 = loc("multiply.23459")
#loc10125 = loc("add.23466")
#loc10127 = loc("reshape.23488")
#loc10128 = loc("reshape.23490")
#loc10129 = loc("transpose.23491")
#loc10130 = loc("dot.23493")
#loc10131 = loc("reshape.23495")
#loc10132 = loc("transpose.23496")
#loc10134 = loc("reshape.23842")
#loc10135 = loc("reshape.23844")
#loc10136 = loc("broadcast.23845")
#loc10137 = loc("reshape.23803")
#loc10138 = loc("reshape.23805")
#loc10139 = loc("broadcast.23806")
#loc10140 = loc("reshape.23625")
#loc10141 = loc("reshape.23627")
#loc10142 = loc("broadcast.23628")
#loc10143 = loc("reshape.23587")
#loc10144 = loc("reshape.23589")
#loc10145 = loc("transpose.23590")
#loc10146 = loc("dot.23592")
#loc10147 = loc("reshape.23594")
#loc10148 = loc("convert.23595")
#loc10149 = loc("power.23597")
#loc10150 = loc("reduce.23604")
#loc10151 = loc("multiply.23613")
#loc10152 = loc("reshape.23614")
#loc10153 = loc("add.23618")
#loc10154 = loc("rsqrt.23619")
#loc10155 = loc("reshape.23620")
#loc10156 = loc("broadcast.23621")
#loc10157 = loc("multiply.23622")
#loc10158 = loc("convert.23623")
#loc10159 = loc("multiply.23629")
#loc10160 = loc("transpose.23630")
#loc10161 = loc("multiply.23640")
#loc10162 = loc("slice.23632")
#loc10163 = loc("negate.23633")
#loc10164 = loc("slice.23631")
#loc10165 = loc("concatenate.23634")
#loc10166 = loc("multiply.23637")
#loc10167 = loc("add.23643")
#loc10168 = loc("convert.23644")
#loc10169 = loc("multiply.23646")
#loc10170 = loc("broadcast.23575")
#loc10171 = loc("reshape.23576")
#loc10172 = loc("convert.23577")
#loc10173 = loc("transpose.23578")
#loc10174 = loc("multiply.23580")
#loc10175 = loc("dot.23647")
#loc10176 = loc("add.23654")
#loc10177 = loc("convert.23680")
#loc10178 = loc("compare.23682")
#loc10179 = loc("not.23684")
#loc10181 = loc("or.23694")
#loc10182 = loc("select.23695")
#loc10183 = loc("reshape.23700")
#loc10184 = loc("not.23702")
#loc10185 = loc("reshape.23704")
#loc10186 = loc("broadcast.23705")
#loc10187 = loc("reduce.23660")
#loc10188 = loc("broadcast.23661")
#loc10189 = loc("subtract.23662")
#loc10190 = loc("exponential.23663")
#loc10191 = loc("reduce.23669")
#loc10192 = loc("broadcast.23670")
#loc10193 = loc("divide.23671")
#loc10194 = loc("select.23706")
#loc10195 = loc("broadcast.23553")
#loc10196 = loc("reshape.23554")
#loc10197 = loc("convert.23555")
#loc10198 = loc("dot.23707")
#loc10199 = loc("convert.23709")
#loc10200 = loc("transpose.23710")
#loc10201 = loc("reshape.23712")
#loc10202 = loc("reshape.23546")
#loc10203 = loc("reshape.23548")
#loc10204 = loc("transpose.23549")
#loc10206 = loc("reshape.23714")
#loc10207 = loc("add.23717")
#loc10208 = loc("reshape.23748")
#loc10209 = loc("reshape.23750")
#loc10210 = loc("broadcast.23751")
#loc10211 = loc("convert.23718")
#loc10212 = loc("power.23720")
#loc10213 = loc("reduce.23727")
#loc10214 = loc("multiply.23736")
#loc10215 = loc("reshape.23737")
#loc10216 = loc("add.23741")
#loc10217 = loc("rsqrt.23742")
#loc10218 = loc("reshape.23743")
#loc10219 = loc("broadcast.23744")
#loc10220 = loc("multiply.23745")
#loc10221 = loc("convert.23746")
#loc10222 = loc("multiply.23752")
#loc10223 = loc("reshape.23761")
#loc10224 = loc("reshape.23757")
#loc10225 = loc("reshape.23759")
#loc10226 = loc("transpose.23760")
#loc10227 = loc("dot.23762")
#loc10228 = loc("reshape.23763")
#loc10229 = loc("logistic.23764")
#loc10230 = loc("multiply.23765")
#loc10231 = loc("reshape.23537")
#loc10232 = loc("reshape.23539")
#loc10233 = loc("transpose.23540")
#loc10234 = loc("dot.23754")
#loc10235 = loc("reshape.23755")
#loc10236 = loc("multiply.23766")
#loc10237 = loc("reshape.23767")
#loc10238 = loc("reshape.23532")
#loc10239 = loc("reshape.23534")
#loc10240 = loc("transpose.23535")
#loc10242 = loc("reshape.23769")
#loc10243 = loc("add.23772")
#loc10244 = loc("convert.23773")
#loc10245 = loc("power.23775")
#loc10246 = loc("reduce.23782")
#loc10247 = loc("multiply.23791")
#loc10248 = loc("reshape.23792")
#loc10249 = loc("add.23796")
#loc10250 = loc("rsqrt.23797")
#loc10251 = loc("reshape.23798")
#loc10252 = loc("broadcast.23799")
#loc10253 = loc("multiply.23800")
#loc10254 = loc("convert.23801")
#loc10255 = loc("multiply.23807")
#loc10256 = loc("reshape.23808")
#loc10257 = loc("reshape.23523")
#loc10258 = loc("reshape.23525")
#loc10259 = loc("transpose.23526")
#loc10260 = loc("dot.23809")
#loc10261 = loc("reshape.23811")
#loc10262 = loc("convert.23812")
#loc10263 = loc("power.23814")
#loc10264 = loc("reduce.23821")
#loc10265 = loc("multiply.23830")
#loc10266 = loc("reshape.23831")
#loc10267 = loc("add.23835")
#loc10268 = loc("rsqrt.23836")
#loc10269 = loc("reshape.23837")
#loc10270 = loc("broadcast.23838")
#loc10271 = loc("multiply.23839")
#loc10272 = loc("convert.23840")
#loc10273 = loc("multiply.23846")
#loc10274 = loc("transpose.23847")
#loc10275 = loc("multiply.23858")
#loc10276 = loc("slice.23849")
#loc10277 = loc("negate.23850")
#loc10278 = loc("slice.23848")
#loc10279 = loc("concatenate.23851")
#loc10280 = loc("multiply.23854")
#loc10281 = loc("add.23861")
#loc10283 = loc("reshape.23883")
#loc10284 = loc("reshape.23885")
#loc10285 = loc("transpose.23886")
#loc10286 = loc("dot.23888")
#loc10287 = loc("reshape.23890")
#loc10288 = loc("transpose.23891")
#loc10290 = loc("reshape.24237")
#loc10291 = loc("reshape.24239")
#loc10292 = loc("broadcast.24240")
#loc10293 = loc("reshape.24198")
#loc10294 = loc("reshape.24200")
#loc10295 = loc("broadcast.24201")
#loc10296 = loc("reshape.24020")
#loc10297 = loc("reshape.24022")
#loc10298 = loc("broadcast.24023")
#loc10299 = loc("reshape.23982")
#loc10300 = loc("reshape.23984")
#loc10301 = loc("transpose.23985")
#loc10302 = loc("dot.23987")
#loc10303 = loc("reshape.23989")
#loc10304 = loc("convert.23990")
#loc10305 = loc("power.23992")
#loc10306 = loc("reduce.23999")
#loc10307 = loc("multiply.24008")
#loc10308 = loc("reshape.24009")
#loc10309 = loc("add.24013")
#loc10310 = loc("rsqrt.24014")
#loc10311 = loc("reshape.24015")
#loc10312 = loc("broadcast.24016")
#loc10313 = loc("multiply.24017")
#loc10314 = loc("convert.24018")
#loc10315 = loc("multiply.24024")
#loc10316 = loc("transpose.24025")
#loc10317 = loc("multiply.24035")
#loc10318 = loc("slice.24027")
#loc10319 = loc("negate.24028")
#loc10320 = loc("slice.24026")
#loc10321 = loc("concatenate.24029")
#loc10322 = loc("multiply.24032")
#loc10323 = loc("add.24038")
#loc10324 = loc("convert.24039")
#loc10325 = loc("multiply.24041")
#loc10326 = loc("broadcast.23970")
#loc10327 = loc("reshape.23971")
#loc10328 = loc("convert.23972")
#loc10329 = loc("transpose.23973")
#loc10330 = loc("multiply.23975")
#loc10331 = loc("dot.24042")
#loc10332 = loc("add.24049")
#loc10333 = loc("convert.24075")
#loc10334 = loc("compare.24077")
#loc10335 = loc("not.24079")
#loc10337 = loc("or.24089")
#loc10338 = loc("select.24090")
#loc10339 = loc("reshape.24095")
#loc10340 = loc("not.24097")
#loc10341 = loc("reshape.24099")
#loc10342 = loc("broadcast.24100")
#loc10343 = loc("reduce.24055")
#loc10344 = loc("broadcast.24056")
#loc10345 = loc("subtract.24057")
#loc10346 = loc("exponential.24058")
#loc10347 = loc("reduce.24064")
#loc10348 = loc("broadcast.24065")
#loc10349 = loc("divide.24066")
#loc10350 = loc("select.24101")
#loc10351 = loc("broadcast.23948")
#loc10352 = loc("reshape.23949")
#loc10353 = loc("convert.23950")
#loc10354 = loc("dot.24102")
#loc10355 = loc("convert.24104")
#loc10356 = loc("transpose.24105")
#loc10357 = loc("reshape.24107")
#loc10358 = loc("reshape.23941")
#loc10359 = loc("reshape.23943")
#loc10360 = loc("transpose.23944")
#loc10362 = loc("reshape.24109")
#loc10363 = loc("add.24112")
#loc10364 = loc("reshape.24143")
#loc10365 = loc("reshape.24145")
#loc10366 = loc("broadcast.24146")
#loc10367 = loc("convert.24113")
#loc10368 = loc("power.24115")
#loc10369 = loc("reduce.24122")
#loc10370 = loc("multiply.24131")
#loc10371 = loc("reshape.24132")
#loc10372 = loc("add.24136")
#loc10373 = loc("rsqrt.24137")
#loc10374 = loc("reshape.24138")
#loc10375 = loc("broadcast.24139")
#loc10376 = loc("multiply.24140")
#loc10377 = loc("convert.24141")
#loc10378 = loc("multiply.24147")
#loc10379 = loc("reshape.24156")
#loc10380 = loc("reshape.24152")
#loc10381 = loc("reshape.24154")
#loc10382 = loc("transpose.24155")
#loc10383 = loc("dot.24157")
#loc10384 = loc("reshape.24158")
#loc10385 = loc("logistic.24159")
#loc10386 = loc("multiply.24160")
#loc10387 = loc("reshape.23932")
#loc10388 = loc("reshape.23934")
#loc10389 = loc("transpose.23935")
#loc10390 = loc("dot.24149")
#loc10391 = loc("reshape.24150")
#loc10392 = loc("multiply.24161")
#loc10393 = loc("reshape.24162")
#loc10394 = loc("reshape.23927")
#loc10395 = loc("reshape.23929")
#loc10396 = loc("transpose.23930")
#loc10398 = loc("reshape.24164")
#loc10399 = loc("add.24167")
#loc10400 = loc("convert.24168")
#loc10401 = loc("power.24170")
#loc10402 = loc("reduce.24177")
#loc10403 = loc("multiply.24186")
#loc10404 = loc("reshape.24187")
#loc10405 = loc("add.24191")
#loc10406 = loc("rsqrt.24192")
#loc10407 = loc("reshape.24193")
#loc10408 = loc("broadcast.24194")
#loc10409 = loc("multiply.24195")
#loc10410 = loc("convert.24196")
#loc10411 = loc("multiply.24202")
#loc10412 = loc("reshape.24203")
#loc10413 = loc("reshape.23918")
#loc10414 = loc("reshape.23920")
#loc10415 = loc("transpose.23921")
#loc10416 = loc("dot.24204")
#loc10417 = loc("reshape.24206")
#loc10418 = loc("convert.24207")
#loc10419 = loc("power.24209")
#loc10420 = loc("reduce.24216")
#loc10421 = loc("multiply.24225")
#loc10422 = loc("reshape.24226")
#loc10423 = loc("add.24230")
#loc10424 = loc("rsqrt.24231")
#loc10425 = loc("reshape.24232")
#loc10426 = loc("broadcast.24233")
#loc10427 = loc("multiply.24234")
#loc10428 = loc("convert.24235")
#loc10429 = loc("multiply.24241")
#loc10430 = loc("transpose.24242")
#loc10431 = loc("multiply.24253")
#loc10432 = loc("slice.24244")
#loc10433 = loc("negate.24245")
#loc10434 = loc("slice.24243")
#loc10435 = loc("concatenate.24246")
#loc10436 = loc("multiply.24249")
#loc10437 = loc("add.24256")
#loc10439 = loc("reshape.24278")
#loc10440 = loc("reshape.24280")
#loc10441 = loc("transpose.24281")
#loc10442 = loc("dot.24283")
#loc10443 = loc("reshape.24285")
#loc10444 = loc("transpose.24286")
#loc10446 = loc("reshape.24632")
#loc10447 = loc("reshape.24634")
#loc10448 = loc("broadcast.24635")
#loc10449 = loc("reshape.24593")
#loc10450 = loc("reshape.24595")
#loc10451 = loc("broadcast.24596")
#loc10452 = loc("reshape.24415")
#loc10453 = loc("reshape.24417")
#loc10454 = loc("broadcast.24418")
#loc10455 = loc("reshape.24377")
#loc10456 = loc("reshape.24379")
#loc10457 = loc("transpose.24380")
#loc10458 = loc("dot.24382")
#loc10459 = loc("reshape.24384")
#loc10460 = loc("convert.24385")
#loc10461 = loc("power.24387")
#loc10462 = loc("reduce.24394")
#loc10463 = loc("multiply.24403")
#loc10464 = loc("reshape.24404")
#loc10465 = loc("add.24408")
#loc10466 = loc("rsqrt.24409")
#loc10467 = loc("reshape.24410")
#loc10468 = loc("broadcast.24411")
#loc10469 = loc("multiply.24412")
#loc10470 = loc("convert.24413")
#loc10471 = loc("multiply.24419")
#loc10472 = loc("transpose.24420")
#loc10473 = loc("multiply.24430")
#loc10474 = loc("slice.24422")
#loc10475 = loc("negate.24423")
#loc10476 = loc("slice.24421")
#loc10477 = loc("concatenate.24424")
#loc10478 = loc("multiply.24427")
#loc10479 = loc("add.24433")
#loc10480 = loc("convert.24434")
#loc10481 = loc("multiply.24436")
#loc10482 = loc("broadcast.24365")
#loc10483 = loc("reshape.24366")
#loc10484 = loc("convert.24367")
#loc10485 = loc("transpose.24368")
#loc10486 = loc("multiply.24370")
#loc10487 = loc("dot.24437")
#loc10488 = loc("add.24444")
#loc10489 = loc("convert.24470")
#loc10490 = loc("compare.24472")
#loc10491 = loc("not.24474")
#loc10493 = loc("or.24484")
#loc10494 = loc("select.24485")
#loc10495 = loc("reshape.24490")
#loc10496 = loc("not.24492")
#loc10497 = loc("reshape.24494")
#loc10498 = loc("broadcast.24495")
#loc10499 = loc("reduce.24450")
#loc10500 = loc("broadcast.24451")
#loc10501 = loc("subtract.24452")
#loc10502 = loc("exponential.24453")
#loc10503 = loc("reduce.24459")
#loc10504 = loc("broadcast.24460")
#loc10505 = loc("divide.24461")
#loc10506 = loc("select.24496")
#loc10507 = loc("broadcast.24343")
#loc10508 = loc("reshape.24344")
#loc10509 = loc("convert.24345")
#loc10510 = loc("dot.24497")
#loc10511 = loc("convert.24499")
#loc10512 = loc("transpose.24500")
#loc10513 = loc("reshape.24502")
#loc10514 = loc("reshape.24336")
#loc10515 = loc("reshape.24338")
#loc10516 = loc("transpose.24339")
#loc10518 = loc("reshape.24504")
#loc10519 = loc("add.24507")
#loc10520 = loc("reshape.24538")
#loc10521 = loc("reshape.24540")
#loc10522 = loc("broadcast.24541")
#loc10523 = loc("convert.24508")
#loc10524 = loc("power.24510")
#loc10525 = loc("reduce.24517")
#loc10526 = loc("multiply.24526")
#loc10527 = loc("reshape.24527")
#loc10528 = loc("add.24531")
#loc10529 = loc("rsqrt.24532")
#loc10530 = loc("reshape.24533")
#loc10531 = loc("broadcast.24534")
#loc10532 = loc("multiply.24535")
#loc10533 = loc("convert.24536")
#loc10534 = loc("multiply.24542")
#loc10535 = loc("reshape.24551")
#loc10536 = loc("reshape.24547")
#loc10537 = loc("reshape.24549")
#loc10538 = loc("transpose.24550")
#loc10539 = loc("dot.24552")
#loc10540 = loc("reshape.24553")
#loc10541 = loc("logistic.24554")
#loc10542 = loc("multiply.24555")
#loc10543 = loc("reshape.24327")
#loc10544 = loc("reshape.24329")
#loc10545 = loc("transpose.24330")
#loc10546 = loc("dot.24544")
#loc10547 = loc("reshape.24545")
#loc10548 = loc("multiply.24556")
#loc10549 = loc("reshape.24557")
#loc10550 = loc("reshape.24322")
#loc10551 = loc("reshape.24324")
#loc10552 = loc("transpose.24325")
#loc10554 = loc("reshape.24559")
#loc10555 = loc("add.24562")
#loc10556 = loc("convert.24563")
#loc10557 = loc("power.24565")
#loc10558 = loc("reduce.24572")
#loc10559 = loc("multiply.24581")
#loc10560 = loc("reshape.24582")
#loc10561 = loc("add.24586")
#loc10562 = loc("rsqrt.24587")
#loc10563 = loc("reshape.24588")
#loc10564 = loc("broadcast.24589")
#loc10565 = loc("multiply.24590")
#loc10566 = loc("convert.24591")
#loc10567 = loc("multiply.24597")
#loc10568 = loc("reshape.24598")
#loc10569 = loc("reshape.24313")
#loc10570 = loc("reshape.24315")
#loc10571 = loc("transpose.24316")
#loc10572 = loc("dot.24599")
#loc10573 = loc("reshape.24601")
#loc10574 = loc("convert.24602")
#loc10575 = loc("power.24604")
#loc10576 = loc("reduce.24611")
#loc10577 = loc("multiply.24620")
#loc10578 = loc("reshape.24621")
#loc10579 = loc("add.24625")
#loc10580 = loc("rsqrt.24626")
#loc10581 = loc("reshape.24627")
#loc10582 = loc("broadcast.24628")
#loc10583 = loc("multiply.24629")
#loc10584 = loc("convert.24630")
#loc10585 = loc("multiply.24636")
#loc10586 = loc("transpose.24637")
#loc10587 = loc("multiply.24648")
#loc10588 = loc("slice.24639")
#loc10589 = loc("negate.24640")
#loc10590 = loc("slice.24638")
#loc10591 = loc("concatenate.24641")
#loc10592 = loc("multiply.24644")
#loc10593 = loc("add.24651")
#loc10595 = loc("reshape.24673")
#loc10596 = loc("reshape.24675")
#loc10597 = loc("transpose.24676")
#loc10598 = loc("dot.24678")
#loc10599 = loc("reshape.24680")
#loc10600 = loc("transpose.24681")
#loc10602 = loc("reshape.25027")
#loc10603 = loc("reshape.25029")
#loc10604 = loc("broadcast.25030")
#loc10605 = loc("reshape.24988")
#loc10606 = loc("reshape.24990")
#loc10607 = loc("broadcast.24991")
#loc10608 = loc("reshape.24810")
#loc10609 = loc("reshape.24812")
#loc10610 = loc("broadcast.24813")
#loc10611 = loc("reshape.24772")
#loc10612 = loc("reshape.24774")
#loc10613 = loc("transpose.24775")
#loc10614 = loc("dot.24777")
#loc10615 = loc("reshape.24779")
#loc10616 = loc("convert.24780")
#loc10617 = loc("power.24782")
#loc10618 = loc("reduce.24789")
#loc10619 = loc("multiply.24798")
#loc10620 = loc("reshape.24799")
#loc10621 = loc("add.24803")
#loc10622 = loc("rsqrt.24804")
#loc10623 = loc("reshape.24805")
#loc10624 = loc("broadcast.24806")
#loc10625 = loc("multiply.24807")
#loc10626 = loc("convert.24808")
#loc10627 = loc("multiply.24814")
#loc10628 = loc("transpose.24815")
#loc10629 = loc("multiply.24825")
#loc10630 = loc("slice.24817")
#loc10631 = loc("negate.24818")
#loc10632 = loc("slice.24816")
#loc10633 = loc("concatenate.24819")
#loc10634 = loc("multiply.24822")
#loc10635 = loc("add.24828")
#loc10636 = loc("convert.24829")
#loc10637 = loc("multiply.24831")
#loc10638 = loc("broadcast.24760")
#loc10639 = loc("reshape.24761")
#loc10640 = loc("convert.24762")
#loc10641 = loc("transpose.24763")
#loc10642 = loc("multiply.24765")
#loc10643 = loc("dot.24832")
#loc10644 = loc("add.24839")
#loc10645 = loc("convert.24865")
#loc10646 = loc("compare.24867")
#loc10647 = loc("not.24869")
#loc10649 = loc("or.24879")
#loc10650 = loc("select.24880")
#loc10651 = loc("reshape.24885")
#loc10652 = loc("not.24887")
#loc10653 = loc("reshape.24889")
#loc10654 = loc("broadcast.24890")
#loc10655 = loc("reduce.24845")
#loc10656 = loc("broadcast.24846")
#loc10657 = loc("subtract.24847")
#loc10658 = loc("exponential.24848")
#loc10659 = loc("reduce.24854")
#loc10660 = loc("broadcast.24855")
#loc10661 = loc("divide.24856")
#loc10662 = loc("select.24891")
#loc10663 = loc("broadcast.24738")
#loc10664 = loc("reshape.24739")
#loc10665 = loc("convert.24740")
#loc10666 = loc("dot.24892")
#loc10667 = loc("convert.24894")
#loc10668 = loc("transpose.24895")
#loc10669 = loc("reshape.24897")
#loc10670 = loc("reshape.24731")
#loc10671 = loc("reshape.24733")
#loc10672 = loc("transpose.24734")
#loc10674 = loc("reshape.24899")
#loc10675 = loc("add.24902")
#loc10676 = loc("reshape.24933")
#loc10677 = loc("reshape.24935")
#loc10678 = loc("broadcast.24936")
#loc10679 = loc("convert.24903")
#loc10680 = loc("power.24905")
#loc10681 = loc("reduce.24912")
#loc10682 = loc("multiply.24921")
#loc10683 = loc("reshape.24922")
#loc10684 = loc("add.24926")
#loc10685 = loc("rsqrt.24927")
#loc10686 = loc("reshape.24928")
#loc10687 = loc("broadcast.24929")
#loc10688 = loc("multiply.24930")
#loc10689 = loc("convert.24931")
#loc10690 = loc("multiply.24937")
#loc10691 = loc("reshape.24946")
#loc10692 = loc("reshape.24942")
#loc10693 = loc("reshape.24944")
#loc10694 = loc("transpose.24945")
#loc10695 = loc("dot.24947")
#loc10696 = loc("reshape.24948")
#loc10697 = loc("logistic.24949")
#loc10698 = loc("multiply.24950")
#loc10699 = loc("reshape.24722")
#loc10700 = loc("reshape.24724")
#loc10701 = loc("transpose.24725")
#loc10702 = loc("dot.24939")
#loc10703 = loc("reshape.24940")
#loc10704 = loc("multiply.24951")
#loc10705 = loc("reshape.24952")
#loc10706 = loc("reshape.24717")
#loc10707 = loc("reshape.24719")
#loc10708 = loc("transpose.24720")
#loc10710 = loc("reshape.24954")
#loc10711 = loc("add.24957")
#loc10712 = loc("convert.24958")
#loc10713 = loc("power.24960")
#loc10714 = loc("reduce.24967")
#loc10715 = loc("multiply.24976")
#loc10716 = loc("reshape.24977")
#loc10717 = loc("add.24981")
#loc10718 = loc("rsqrt.24982")
#loc10719 = loc("reshape.24983")
#loc10720 = loc("broadcast.24984")
#loc10721 = loc("multiply.24985")
#loc10722 = loc("convert.24986")
#loc10723 = loc("multiply.24992")
#loc10724 = loc("reshape.24993")
#loc10725 = loc("reshape.24708")
#loc10726 = loc("reshape.24710")
#loc10727 = loc("transpose.24711")
#loc10728 = loc("dot.24994")
#loc10729 = loc("reshape.24996")
#loc10730 = loc("convert.24997")
#loc10731 = loc("power.24999")
#loc10732 = loc("reduce.25006")
#loc10733 = loc("multiply.25015")
#loc10734 = loc("reshape.25016")
#loc10735 = loc("add.25020")
#loc10736 = loc("rsqrt.25021")
#loc10737 = loc("reshape.25022")
#loc10738 = loc("broadcast.25023")
#loc10739 = loc("multiply.25024")
#loc10740 = loc("convert.25025")
#loc10741 = loc("multiply.25031")
#loc10742 = loc("transpose.25032")
#loc10743 = loc("multiply.25043")
#loc10744 = loc("slice.25034")
#loc10745 = loc("negate.25035")
#loc10746 = loc("slice.25033")
#loc10747 = loc("concatenate.25036")
#loc10748 = loc("multiply.25039")
#loc10749 = loc("add.25046")
#loc10751 = loc("reshape.25068")
#loc10752 = loc("reshape.25070")
#loc10753 = loc("transpose.25071")
#loc10754 = loc("dot.25073")
#loc10755 = loc("reshape.25075")
#loc10756 = loc("transpose.25076")
#loc10758 = loc("reshape.25378")
#loc10759 = loc("reshape.25380")
#loc10760 = loc("broadcast.25381")
#loc10761 = loc("reshape.25200")
#loc10762 = loc("reshape.25202")
#loc10763 = loc("broadcast.25203")
#loc10764 = loc("reshape.25162")
#loc10765 = loc("reshape.25164")
#loc10766 = loc("transpose.25165")
#loc10767 = loc("dot.25167")
#loc10768 = loc("reshape.25169")
#loc10769 = loc("convert.25170")
#loc10770 = loc("power.25172")
#loc10771 = loc("reduce.25179")
#loc10772 = loc("multiply.25188")
#loc10773 = loc("reshape.25189")
#loc10774 = loc("add.25193")
#loc10775 = loc("rsqrt.25194")
#loc10776 = loc("reshape.25195")
#loc10777 = loc("broadcast.25196")
#loc10778 = loc("multiply.25197")
#loc10779 = loc("convert.25198")
#loc10780 = loc("multiply.25204")
#loc10781 = loc("transpose.25205")
#loc10782 = loc("multiply.25215")
#loc10783 = loc("slice.25207")
#loc10784 = loc("negate.25208")
#loc10785 = loc("slice.25206")
#loc10786 = loc("concatenate.25209")
#loc10787 = loc("multiply.25212")
#loc10788 = loc("add.25218")
#loc10789 = loc("convert.25219")
#loc10790 = loc("multiply.25221")
#loc10791 = loc("broadcast.25150")
#loc10792 = loc("reshape.25151")
#loc10793 = loc("convert.25152")
#loc10794 = loc("transpose.25153")
#loc10795 = loc("multiply.25155")
#loc10796 = loc("dot.25222")
#loc10797 = loc("add.25229")
#loc10798 = loc("convert.25255")
#loc10799 = loc("compare.25257")
#loc10800 = loc("not.25259")
#loc10802 = loc("or.25269")
#loc10803 = loc("select.25270")
#loc10804 = loc("reshape.25275")
#loc10805 = loc("not.25277")
#loc10806 = loc("reshape.25279")
#loc10807 = loc("broadcast.25280")
#loc10808 = loc("reduce.25235")
#loc10809 = loc("broadcast.25236")
#loc10810 = loc("subtract.25237")
#loc10811 = loc("exponential.25238")
#loc10812 = loc("reduce.25244")
#loc10813 = loc("broadcast.25245")
#loc10814 = loc("divide.25246")
#loc10815 = loc("select.25281")
#loc10816 = loc("broadcast.25128")
#loc10817 = loc("reshape.25129")
#loc10818 = loc("convert.25130")
#loc10819 = loc("dot.25282")
#loc10820 = loc("convert.25284")
#loc10821 = loc("transpose.25285")
#loc10822 = loc("reshape.25287")
#loc10823 = loc("reshape.25121")
#loc10824 = loc("reshape.25123")
#loc10825 = loc("transpose.25124")
#loc10827 = loc("reshape.25289")
#loc10828 = loc("add.25292")
#loc10829 = loc("reshape.25323")
#loc10830 = loc("reshape.25325")
#loc10831 = loc("broadcast.25326")
#loc10832 = loc("convert.25293")
#loc10833 = loc("power.25295")
#loc10834 = loc("reduce.25302")
#loc10835 = loc("multiply.25311")
#loc10836 = loc("reshape.25312")
#loc10837 = loc("add.25316")
#loc10838 = loc("rsqrt.25317")
#loc10839 = loc("reshape.25318")
#loc10840 = loc("broadcast.25319")
#loc10841 = loc("multiply.25320")
#loc10842 = loc("convert.25321")
#loc10843 = loc("multiply.25327")
#loc10844 = loc("reshape.25336")
#loc10845 = loc("reshape.25332")
#loc10846 = loc("reshape.25334")
#loc10847 = loc("transpose.25335")
#loc10848 = loc("dot.25337")
#loc10849 = loc("reshape.25338")
#loc10850 = loc("logistic.25339")
#loc10851 = loc("multiply.25340")
#loc10852 = loc("reshape.25112")
#loc10853 = loc("reshape.25114")
#loc10854 = loc("transpose.25115")
#loc10855 = loc("dot.25329")
#loc10856 = loc("reshape.25330")
#loc10857 = loc("multiply.25341")
#loc10858 = loc("reshape.25342")
#loc10859 = loc("reshape.25107")
#loc10860 = loc("reshape.25109")
#loc10861 = loc("transpose.25110")
#loc10863 = loc("reshape.25344")
#loc10864 = loc("add.25347")
#loc10865 = loc("convert.25348")
#loc10866 = loc("power.25350")
#loc10867 = loc("reduce.25357")
#loc10868 = loc("multiply.25366")
#loc10869 = loc("reshape.25367")
#loc10870 = loc("add.25371")
#loc10871 = loc("rsqrt.25372")
#loc10872 = loc("reshape.25373")
#loc10873 = loc("broadcast.25374")
#loc10874 = loc("multiply.25375")
#loc10875 = loc("convert.25376")
#loc10876 = loc("multiply.25382")
#loc10877 = loc("reshape.25383")
#loc10878 = loc("reshape.25098")
#loc10879 = loc("reshape.25100")
#loc10880 = loc("transpose.25101")
#loc10881 = loc("dot.25384")
#loc10882 = loc("reshape.25385")
------------------ END OF MLIR MODULE ------------------
