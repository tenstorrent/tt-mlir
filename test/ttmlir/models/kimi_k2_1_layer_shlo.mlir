#loc1 = loc("p0.1")
#loc2 = loc("p1.14")
#loc3 = loc("p2.23")
#loc4 = loc("p3.28")
#loc5 = loc("p4.37")
#loc6 = loc("p5.42")
#loc7 = loc("p6.50")
#loc8 = loc("p7.58")
#loc9 = loc("p8.63")
#loc10 = loc("p9.99")
#loc11 = loc("p10.138")
#loc12 = loc("p11.230")
#loc13 = loc("p12.400")
#loc14 = loc("p13.408")
#loc15 = loc("p14.445")
#loc16 = loc("p15.633")
#loc17 = loc("p16.642")
#loc18 = loc("p17.688")
module @SyncTensorsGraph.699 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]> loc(#loc)
  func.func @main(%arg0: tensor<32xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "self___model_layers_0_self_attn_rotary_emb__buffers__inv_freq"} loc("p0.1"), %arg1: tensor<129280x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___lm_head_weight"} loc("p1.14"), %arg2: tensor<7168x18432xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_mlp_down_proj_weight"} loc("p2.23"), %arg3: tensor<18432x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_mlp_up_proj_weight"} loc("p3.28"), %arg4: tensor<7168x16384xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_self_attn_o_proj_weight"} loc("p4.37"), %arg5: tensor<32768x512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_self_attn_kv_b_proj_weight"} loc("p5.42"), %arg6: tensor<576x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_self_attn_kv_a_proj_with_mqa_weight"} loc("p6.50"), %arg7: tensor<1x32xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p7.58"), %arg8: tensor<129280x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p8.63"), %arg9: tensor<7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_input_layernorm_weight"} loc("p9.99"), %arg10: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_self_attn_kv_a_layernorm_weight"} loc("p10.138"), %arg11: tensor<i1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>} loc("p11.230"), %arg12: tensor<24576x1536xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_self_attn_q_b_proj_weight"} loc("p12.400"), %arg13: tensor<1536x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_self_attn_q_a_proj_weight"} loc("p13.408"), %arg14: tensor<1536xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_self_attn_q_a_layernorm_weight"} loc("p14.445"), %arg15: tensor<7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_post_attention_layernorm_weight"} loc("p15.633"), %arg16: tensor<18432x7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_layers_0_mlp_gate_proj_weight"} loc("p16.642"), %arg17: tensor<7168xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___model_norm_weight"} loc("p17.688")) -> (tensor<32x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x64xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x32x129280xf32> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<-3.38953139E+38> : tensor<32x32xf32> loc(#loc)
    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<32x32xf32> loc(#loc)
    %c = stablehlo.constant dense<[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]> : tensor<32xi64> loc(#loc)
    %cst_1 = stablehlo.constant dense<7.226560e-02> : tensor<1x128x32x32xbf16> loc(#loc)
    %cst_2 = stablehlo.constant dense<0.001953125> : tensor<1x32xf32> loc(#loc)
    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<1x32x512xf32> loc(#loc)
    %c_4 = stablehlo.constant dense<128> : tensor<192xi64> loc(#loc)
    %c_5 = stablehlo.constant dense<127> : tensor<192xi64> loc(#loc)
    %cst_6 = stablehlo.constant dense<"0x000000000000803F0000004000004040000080400000A0400000C0400000E0400000004100001041000020410000304100004041000050410000604100007041000080410000884100009041000098410000A0410000A8410000B0410000B8410000C0410000C8410000D0410000D8410000E0410000E8410000F0410000F84100000042000004420000084200000C4200001042000014420000184200001C4200002042000024420000284200002C4200003042000034420000384200003C4200004042000044420000484200004C4200005042000054420000584200005C4200006042000064420000684200006C4200007042000074420000784200007C42000080420000824200008442000086420000884200008A4200008C4200008E42000090420000924200009442000096420000984200009A4200009C4200009E420000A0420000A2420000A4420000A6420000A8420000AA420000AC420000AE420000B0420000B2420000B4420000B6420000B8420000BA420000BC420000BE420000C0420000C2420000C4420000C6420000C8420000CA420000CC420000CE420000D0420000D2420000D4420000D6420000D8420000DA420000DC420000DE420000E0420000E2420000E4420000E6420000E8420000EA420000EC420000EE420000F0420000F2420000F4420000F6420000F8420000FA420000FC420000FE420000004300000143000002430000034300000443000005430000064300000743000008430000094300000A4300000B4300000C4300000D4300000E4300000F430000104300001143000012430000134300001443000015430000164300001743000018430000194300001A4300001B4300001C4300001D4300001E4300001F430000204300002143000022430000234300002443000025430000264300002743000028430000294300002A4300002B4300002C4300002D4300002E4300002F430000304300003143000032430000334300003443000035430000364300003743000038430000394300003A4300003B4300003C4300003D4300003E4300003F43"> : tensor<192xf32> loc(#loc)
    %c_7 = stablehlo.constant dense<"0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF0000000000000000"> : tensor<192xi1> loc(#loc)
    %c_8 = stablehlo.constant dense<64> : tensor<192xi64> loc(#loc)
    %c_9 = stablehlo.constant dense<63> : tensor<192xi64> loc(#loc)
    %cst_10 = stablehlo.constant dense<"0x000000C30000FEC20000FCC20000FAC20000F8C20000F6C20000F4C20000F2C20000F0C20000EEC20000ECC20000EAC20000E8C20000E6C20000E4C20000E2C20000E0C20000DEC20000DCC20000DAC20000D8C20000D6C20000D4C20000D2C20000D0C20000CEC20000CCC20000CAC20000C8C20000C6C20000C4C20000C2C20000C0C20000BEC20000BCC20000BAC20000B8C20000B6C20000B4C20000B2C20000B0C20000AEC20000ACC20000AAC20000A8C20000A6C20000A4C20000A2C20000A0C200009EC200009CC200009AC2000098C2000096C2000094C2000092C2000090C200008EC200008CC200008AC2000088C2000086C2000084C2000082C2000080C200007CC2000078C2000074C2000070C200006CC2000068C2000064C2000060C200005CC2000058C2000054C2000050C200004CC2000048C2000044C2000040C200003CC2000038C2000034C2000030C200002CC2000028C2000024C2000020C200001CC2000018C2000014C2000010C200000CC2000008C2000004C2000000C20000F8C10000F0C10000E8C10000E0C10000D8C10000D0C10000C8C10000C0C10000B8C10000B0C10000A8C10000A0C1000098C1000090C1000088C1000080C1000070C1000060C1000050C1000040C1000030C1000020C1000010C1000000C10000E0C00000C0C00000A0C0000080C0000040C0000000C0000080BF000000000000803F0000004000004040000080400000A0400000C0400000E0400000004100001041000020410000304100004041000050410000604100007041000080410000884100009041000098410000A0410000A8410000B0410000B8410000C0410000C8410000D0410000D8410000E0410000E8410000F0410000F84100000042000004420000084200000C4200001042000014420000184200001C4200002042000024420000284200002C4200003042000034420000384200003C4200004042000044420000484200004C4200005042000054420000584200005C4200006042000064420000684200006C4200007042000074420000784200007C42"> : tensor<192xf32> loc(#loc)
    %c_11 = stablehlo.constant dense<0> : tensor<192xi64> loc(#loc)
    %c_12 = stablehlo.constant dense<[[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]]> : tensor<1x32x1xi64> loc(#loc)
    %cst_13 = stablehlo.constant dense<6.51041686E-4> : tensor<1x32xf32> loc(#loc)
    %cst_14 = stablehlo.constant dense<2.000000e+00> : tensor<1x32x1536xf32> loc(#loc)
    %cst_15 = stablehlo.constant dense<9.99999997E-7> : tensor<1x32x1xf32> loc(#loc)
    %cst_16 = stablehlo.constant dense<1.39508935E-4> : tensor<1x32xf32> loc(#loc)
    %cst_17 = stablehlo.constant dense<2.000000e+00> : tensor<1x32x7168xf32> loc(#loc)
    %cst_18 = stablehlo.constant dense<0.000000e+00> : tensor<1x128x32x192xbf16> loc(#loc)
    %c_19 = stablehlo.constant dense<"0x00000000000000000000000000000000FFFFFFFFFFFFFFFF"> : tensor<192xi1> loc(#loc)
    %cst_20 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %c_21 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64> loc(#loc)
    %cst_22 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %cst_23 = stablehlo.constant dense<[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00, 7.000000e+00, 8.000000e+00, 9.000000e+00, 1.000000e+01, 1.100000e+01, 1.200000e+01, 1.300000e+01, 1.400000e+01, 1.500000e+01, 1.600000e+01, 1.700000e+01, 1.800000e+01, 1.900000e+01, 2.000000e+01, 2.100000e+01, 2.200000e+01, 2.300000e+01, 2.400000e+01, 2.500000e+01, 2.600000e+01, 2.700000e+01, 2.800000e+01, 2.900000e+01, 3.000000e+01, 3.100000e+01]> : tensor<32xbf16> loc(#loc19)
    %0 = stablehlo.broadcast_in_dim %cst_23, dims = [0] : (tensor<32xbf16>) -> tensor<32x32xbf16> loc(#loc20)
    %1 = stablehlo.reshape %arg0 : (tensor<32xbf16>) -> tensor<1x1x32xbf16> loc(#loc21)
    %2 = stablehlo.reshape %1 : (tensor<1x1x32xbf16>) -> tensor<32xbf16> loc(#loc22)
    %3 = stablehlo.broadcast_in_dim %2, dims = [1] : (tensor<32xbf16>) -> tensor<32x32xbf16> loc(#loc23)
    %4 = stablehlo.multiply %0, %3 : tensor<32x32xbf16> loc(#loc24)
    %5 = stablehlo.concatenate %4, %4, dim = 1 : (tensor<32x32xbf16>, tensor<32x32xbf16>) -> tensor<32x64xbf16> loc(#loc25)
    %6 = stablehlo.cosine %5 : tensor<32x64xbf16> loc(#loc26)
    %7 = stablehlo.sine %5 : tensor<32x64xbf16> loc(#loc27)
    %8 = stablehlo.reshape %arg17 : (tensor<7168xbf16>) -> tensor<1x1x7168xbf16> loc(#loc28)
    %9 = stablehlo.reshape %8 : (tensor<1x1x7168xbf16>) -> tensor<7168xbf16> loc(#loc29)
    %10 = stablehlo.broadcast_in_dim %9, dims = [2] : (tensor<7168xbf16>) -> tensor<1x32x7168xbf16> loc(#loc30)
    %11 = stablehlo.reshape %arg8 : (tensor<129280x7168xbf16>) -> tensor<1x129280x7168xbf16> loc(#loc31)
    %12 = stablehlo.reshape %11 : (tensor<1x129280x7168xbf16>) -> tensor<129280x7168xbf16> loc(#loc32)
    %13 = stablehlo.reshape %arg7 : (tensor<1x32xi64>) -> tensor<1x1x32xi64> loc(#loc33)
    %14 = stablehlo.reshape %13 : (tensor<1x1x32xi64>) -> tensor<32xi64> loc(#loc34)
    %15 = stablehlo.convert %14 : (tensor<32xi64>) -> tensor<32xui32> loc(#loc35)
    %16 = "stablehlo.gather"(%12, %15) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 7168>}> : (tensor<129280x7168xbf16>, tensor<32xui32>) -> tensor<32x7168xbf16> loc(#loc36)
    %17 = stablehlo.reshape %16 : (tensor<32x7168xbf16>) -> tensor<1x32x7168xbf16> loc(#loc37)
    %18 = stablehlo.broadcast_in_dim %arg11, dims = [] : (tensor<i1>) -> tensor<192xi1> loc(#loc38)
    %19 = stablehlo.and %18, %c_19 : tensor<192xi1> loc(#loc39)
    %20 = stablehlo.reshape %19 : (tensor<192xi1>) -> tensor<1x1x1x192xi1> loc(#loc40)
    %21 = stablehlo.reshape %19 : (tensor<192xi1>) -> tensor<1x192xi1> loc(#loc41)
    %22 = stablehlo.broadcast_in_dim %21, dims = [0, 3] : (tensor<1x192xi1>) -> tensor<1x128x32x192xi1> loc(#loc42)
    %23 = stablehlo.not %20 : tensor<1x1x1x192xi1> loc(#loc43)
    %24 = stablehlo.reshape %23 : (tensor<1x1x1x192xi1>) -> tensor<1x192xi1> loc(#loc44)
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 3] : (tensor<1x192xi1>) -> tensor<1x128x32x192xi1> loc(#loc45)
    %26 = stablehlo.reshape %arg14 : (tensor<1536xbf16>) -> tensor<1x1x1536xbf16> loc(#loc46)
    %27 = stablehlo.reshape %26 : (tensor<1x1x1536xbf16>) -> tensor<1536xbf16> loc(#loc47)
    %28 = stablehlo.broadcast_in_dim %27, dims = [2] : (tensor<1536xbf16>) -> tensor<1x32x1536xbf16> loc(#loc48)
    %29 = stablehlo.reshape %arg9 : (tensor<7168xbf16>) -> tensor<1x1x7168xbf16> loc(#loc49)
    %30 = stablehlo.reshape %29 : (tensor<1x1x7168xbf16>) -> tensor<7168xbf16> loc(#loc50)
    %31 = stablehlo.broadcast_in_dim %30, dims = [2] : (tensor<7168xbf16>) -> tensor<1x32x7168xbf16> loc(#loc51)
    %32 = stablehlo.convert %17 : (tensor<1x32x7168xbf16>) -> tensor<1x32x7168xf32> loc(#loc52)
    %33 = stablehlo.power %32, %cst_17 : tensor<1x32x7168xf32> loc(#loc53)
    %34 = stablehlo.reduce(%33 init: %cst_22) applies stablehlo.add across dimensions = [2] : (tensor<1x32x7168xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc54)
    %35 = stablehlo.multiply %34, %cst_16 : tensor<1x32xf32> loc(#loc55)
    %36 = stablehlo.reshape %35 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc56)
    %37 = stablehlo.add %36, %cst_15 : tensor<1x32x1xf32> loc(#loc57)
    %38 = stablehlo.rsqrt %37 : tensor<1x32x1xf32> loc(#loc58)
    %39 = stablehlo.reshape %38 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc59)
    %40 = stablehlo.broadcast_in_dim %39, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x7168xf32> loc(#loc60)
    %41 = stablehlo.multiply %32, %40 : tensor<1x32x7168xf32> loc(#loc61)
    %42 = stablehlo.convert %41 : (tensor<1x32x7168xf32>) -> tensor<1x32x7168xbf16> loc(#loc62)
    %43 = stablehlo.multiply %31, %42 : tensor<1x32x7168xbf16> loc(#loc63)
    %44 = stablehlo.reshape %43 : (tensor<1x32x7168xbf16>) -> tensor<32x7168xbf16> loc(#loc64)
    %45 = stablehlo.reshape %arg13 : (tensor<1536x7168xbf16>) -> tensor<1x1536x7168xbf16> loc(#loc65)
    %46 = stablehlo.reshape %45 : (tensor<1x1536x7168xbf16>) -> tensor<1536x7168xbf16> loc(#loc66)
    %47 = stablehlo.transpose %46, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,1536]{0,1}"} : (tensor<1536x7168xbf16>) -> tensor<7168x1536xbf16> loc(#loc67)
    %48 = stablehlo.dot_general %44, %47, contracting_dims = [1] x [0] : (tensor<32x7168xbf16>, tensor<7168x1536xbf16>) -> tensor<32x1536xbf16> loc(#loc68)
    %49 = stablehlo.reshape %48 : (tensor<32x1536xbf16>) -> tensor<1x32x1536xbf16> loc(#loc69)
    %50 = stablehlo.convert %49 : (tensor<1x32x1536xbf16>) -> tensor<1x32x1536xf32> loc(#loc70)
    %51 = stablehlo.power %50, %cst_14 : tensor<1x32x1536xf32> loc(#loc71)
    %52 = stablehlo.reduce(%51 init: %cst_22) applies stablehlo.add across dimensions = [2] : (tensor<1x32x1536xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc72)
    %53 = stablehlo.multiply %52, %cst_13 : tensor<1x32xf32> loc(#loc73)
    %54 = stablehlo.reshape %53 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc74)
    %55 = stablehlo.add %54, %cst_15 : tensor<1x32x1xf32> loc(#loc75)
    %56 = stablehlo.rsqrt %55 : tensor<1x32x1xf32> loc(#loc76)
    %57 = stablehlo.reshape %56 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc77)
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x1536xf32> loc(#loc78)
    %59 = stablehlo.multiply %50, %58 : tensor<1x32x1536xf32> loc(#loc79)
    %60 = stablehlo.convert %59 : (tensor<1x32x1536xf32>) -> tensor<1x32x1536xbf16> loc(#loc80)
    %61 = stablehlo.multiply %28, %60 : tensor<1x32x1536xbf16> loc(#loc81)
    %62 = stablehlo.reshape %61 : (tensor<1x32x1536xbf16>) -> tensor<32x1536xbf16> loc(#loc82)
    %63 = stablehlo.reshape %arg12 : (tensor<24576x1536xbf16>) -> tensor<1x24576x1536xbf16> loc(#loc83)
    %64 = stablehlo.reshape %63 : (tensor<1x24576x1536xbf16>) -> tensor<24576x1536xbf16> loc(#loc84)
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[1536,24576]{0,1}"} : (tensor<24576x1536xbf16>) -> tensor<1536x24576xbf16> loc(#loc85)
    %66 = stablehlo.dot_general %62, %65, contracting_dims = [1] x [0] : (tensor<32x1536xbf16>, tensor<1536x24576xbf16>) -> tensor<32x24576xbf16> loc(#loc86)
    %67 = stablehlo.reshape %66 : (tensor<32x24576xbf16>) -> tensor<1x32x128x192xbf16> loc(#loc87)
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,32,192]{3,1,2,0}"} : (tensor<1x32x128x192xbf16>) -> tensor<1x128x32x192xbf16> loc(#loc88)
    %69 = stablehlo.slice %68 [0:1, 0:128, 0:32, 128:192] : (tensor<1x128x32x192xbf16>) -> tensor<1x128x32x64xbf16> loc(#loc89)
    %70 = stablehlo.reshape %69 : (tensor<1x128x32x64xbf16>) -> tensor<1x128x32x32x2xbf16> loc(#loc90)
    %71 = stablehlo.transpose %70, dims = [0, 1, 2, 4, 3] {result_layout = dense<[3, 4, 2, 1, 0]> : tensor<5xindex>, xla_shape = "bf16[1,128,32,2,32]{3,4,2,1,0}"} : (tensor<1x128x32x32x2xbf16>) -> tensor<1x128x32x2x32xbf16> loc(#loc91)
    %72 = stablehlo.reshape %71 : (tensor<1x128x32x2x32xbf16>) -> tensor<1x128x32x64xbf16> loc(#loc92)
    %73 = "stablehlo.gather"(%6, %c_12) <{dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 64>}> : (tensor<32x64xbf16>, tensor<1x32x1xi64>) -> tensor<1x32x64xbf16> loc(#loc93)
    %74 = stablehlo.reshape %73 : (tensor<1x32x64xbf16>) -> tensor<1x1x32x64xbf16> loc(#loc94)
    %75 = stablehlo.broadcast_in_dim %73, dims = [0, 2, 3] : (tensor<1x32x64xbf16>) -> tensor<1x128x32x64xbf16> loc(#loc95)
    %76 = stablehlo.multiply %72, %75 : tensor<1x128x32x64xbf16> loc(#loc96)
    %77 = stablehlo.slice %72 [0:1, 0:128, 0:32, 32:64] : (tensor<1x128x32x64xbf16>) -> tensor<1x128x32x32xbf16> loc(#loc97)
    %78 = stablehlo.negate %77 : tensor<1x128x32x32xbf16> loc(#loc98)
    %79 = stablehlo.slice %72 [0:1, 0:128, 0:32, 0:32] : (tensor<1x128x32x64xbf16>) -> tensor<1x128x32x32xbf16> loc(#loc99)
    %80 = stablehlo.concatenate %78, %79, dim = 3 : (tensor<1x128x32x32xbf16>, tensor<1x128x32x32xbf16>) -> tensor<1x128x32x64xbf16> loc(#loc100)
    %81 = "stablehlo.gather"(%7, %c_12) <{dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 64>}> : (tensor<32x64xbf16>, tensor<1x32x1xi64>) -> tensor<1x32x64xbf16> loc(#loc101)
    %82 = stablehlo.reshape %81 : (tensor<1x32x64xbf16>) -> tensor<1x1x32x64xbf16> loc(#loc102)
    %83 = stablehlo.broadcast_in_dim %81, dims = [0, 2, 3] : (tensor<1x32x64xbf16>) -> tensor<1x128x32x64xbf16> loc(#loc103)
    %84 = stablehlo.multiply %80, %83 : tensor<1x128x32x64xbf16> loc(#loc104)
    %85 = stablehlo.add %76, %84 : tensor<1x128x32x64xbf16> loc(#loc105)
    %86 = stablehlo.floor %cst_10 : tensor<192xf32> loc(#loc106)
    %87 = stablehlo.convert %86 : (tensor<192xf32>) -> tensor<192xi64> loc(#loc107)
    %88 = stablehlo.clamp %c_11, %87, %c_9 : tensor<192xi64> loc(#loc108)
    %89 = stablehlo.compare  LT, %88, %c_11 : (tensor<192xi64>, tensor<192xi64>) -> tensor<192xi1> loc(#loc109)
    %90 = stablehlo.add %88, %c_8 : tensor<192xi64> loc(#loc110)
    %91 = stablehlo.select %89, %90, %88 : tensor<192xi1>, tensor<192xi64> loc(#loc111)
    %92 = stablehlo.reshape %91 : (tensor<192xi64>) -> tensor<192x1xi64> loc(#loc112)
    %93 = "stablehlo.gather"(%85, %92) <{dimension_numbers = #stablehlo.gather<offset_dims = [0, 1, 2], collapsed_slice_dims = [3], start_index_map = [3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128, 32, 1>}> : (tensor<1x128x32x64xbf16>, tensor<192x1xi64>) -> tensor<1x128x32x192xbf16> loc(#loc113)
    %94 = stablehlo.select %25, %cst_18, %93 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc114)
    %95 = stablehlo.and %18, %c_7 : tensor<192xi1> loc(#loc115)
    %96 = stablehlo.reshape %95 : (tensor<192xi1>) -> tensor<1x1x1x192xi1> loc(#loc116)
    %97 = stablehlo.reshape %95 : (tensor<192xi1>) -> tensor<1x192xi1> loc(#loc117)
    %98 = stablehlo.broadcast_in_dim %97, dims = [0, 3] : (tensor<1x192xi1>) -> tensor<1x128x32x192xi1> loc(#loc118)
    %99 = stablehlo.not %96 : tensor<1x1x1x192xi1> loc(#loc119)
    %100 = stablehlo.reshape %99 : (tensor<1x1x1x192xi1>) -> tensor<1x192xi1> loc(#loc120)
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 3] : (tensor<1x192xi1>) -> tensor<1x128x32x192xi1> loc(#loc121)
    %102 = stablehlo.slice %68 [0:1, 0:128, 0:32, 0:128] : (tensor<1x128x32x192xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc122)
    %103 = stablehlo.floor %cst_6 : tensor<192xf32> loc(#loc123)
    %104 = stablehlo.convert %103 : (tensor<192xf32>) -> tensor<192xi64> loc(#loc124)
    %105 = stablehlo.clamp %c_11, %104, %c_5 : tensor<192xi64> loc(#loc125)
    %106 = stablehlo.compare  LT, %105, %c_11 : (tensor<192xi64>, tensor<192xi64>) -> tensor<192xi1> loc(#loc126)
    %107 = stablehlo.add %105, %c_4 : tensor<192xi64> loc(#loc127)
    %108 = stablehlo.select %106, %107, %105 : tensor<192xi1>, tensor<192xi64> loc(#loc128)
    %109 = stablehlo.reshape %108 : (tensor<192xi64>) -> tensor<192x1xi64> loc(#loc129)
    %110 = "stablehlo.gather"(%102, %109) <{dimension_numbers = #stablehlo.gather<offset_dims = [0, 1, 2], collapsed_slice_dims = [3], start_index_map = [3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128, 32, 1>}> : (tensor<1x128x32x128xbf16>, tensor<192x1xi64>) -> tensor<1x128x32x192xbf16> loc(#loc130)
    %111 = stablehlo.select %101, %cst_18, %110 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc131)
    %112 = stablehlo.select %98, %111, %cst_18 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc132)
    %113 = stablehlo.select %22, %94, %112 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc133)
    %114 = stablehlo.reshape %arg6 : (tensor<576x7168xbf16>) -> tensor<1x576x7168xbf16> loc(#loc134)
    %115 = stablehlo.reshape %114 : (tensor<1x576x7168xbf16>) -> tensor<576x7168xbf16> loc(#loc135)
    %116 = stablehlo.transpose %115, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,576]{0,1}"} : (tensor<576x7168xbf16>) -> tensor<7168x576xbf16> loc(#loc136)
    %117 = stablehlo.dot_general %44, %116, contracting_dims = [1] x [0] : (tensor<32x7168xbf16>, tensor<7168x576xbf16>) -> tensor<32x576xbf16> loc(#loc137)
    %118 = stablehlo.reshape %117 : (tensor<32x576xbf16>) -> tensor<1x32x576xbf16> loc(#loc138)
    %119 = stablehlo.slice %118 [0:1, 0:32, 512:576] : (tensor<1x32x576xbf16>) -> tensor<1x32x64xbf16> loc(#loc139)
    %120 = stablehlo.reshape %119 : (tensor<1x32x64xbf16>) -> tensor<1x1x32x32x2xbf16> loc(#loc140)
    %121 = stablehlo.transpose %120, dims = [0, 1, 2, 4, 3] {result_layout = dense<[3, 4, 2, 1, 0]> : tensor<5xindex>, xla_shape = "bf16[1,1,32,2,32]{3,4,2,1,0}"} : (tensor<1x1x32x32x2xbf16>) -> tensor<1x1x32x2x32xbf16> loc(#loc141)
    %122 = stablehlo.reshape %121 : (tensor<1x1x32x2x32xbf16>) -> tensor<1x1x32x64xbf16> loc(#loc142)
    %123 = stablehlo.multiply %122, %74 : tensor<1x1x32x64xbf16> loc(#loc143)
    %124 = stablehlo.slice %122 [0:1, 0:1, 0:32, 32:64] : (tensor<1x1x32x64xbf16>) -> tensor<1x1x32x32xbf16> loc(#loc144)
    %125 = stablehlo.negate %124 : tensor<1x1x32x32xbf16> loc(#loc145)
    %126 = stablehlo.slice %122 [0:1, 0:1, 0:32, 0:32] : (tensor<1x1x32x64xbf16>) -> tensor<1x1x32x32xbf16> loc(#loc146)
    %127 = stablehlo.concatenate %125, %126, dim = 3 : (tensor<1x1x32x32xbf16>, tensor<1x1x32x32xbf16>) -> tensor<1x1x32x64xbf16> loc(#loc147)
    %128 = stablehlo.multiply %127, %82 : tensor<1x1x32x64xbf16> loc(#loc148)
    %129 = stablehlo.add %123, %128 : tensor<1x1x32x64xbf16> loc(#loc149)
    %130 = stablehlo.reshape %129 : (tensor<1x1x32x64xbf16>) -> tensor<1x32x64xbf16> loc(#loc150)
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 2, 3] : (tensor<1x32x64xbf16>) -> tensor<1x128x32x64xbf16> loc(#loc151)
    %132 = "stablehlo.gather"(%131, %92) <{dimension_numbers = #stablehlo.gather<offset_dims = [0, 1, 2], collapsed_slice_dims = [3], start_index_map = [3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128, 32, 1>}> : (tensor<1x128x32x64xbf16>, tensor<192x1xi64>) -> tensor<1x128x32x192xbf16> loc(#loc152)
    %133 = stablehlo.select %25, %cst_18, %132 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc153)
    %134 = stablehlo.reshape %arg10 : (tensor<512xbf16>) -> tensor<1x1x512xbf16> loc(#loc154)
    %135 = stablehlo.reshape %134 : (tensor<1x1x512xbf16>) -> tensor<512xbf16> loc(#loc155)
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<512xbf16>) -> tensor<1x32x512xbf16> loc(#loc156)
    %137 = stablehlo.slice %118 [0:1, 0:32, 0:512] : (tensor<1x32x576xbf16>) -> tensor<1x32x512xbf16> loc(#loc157)
    %138 = stablehlo.convert %137 : (tensor<1x32x512xbf16>) -> tensor<1x32x512xf32> loc(#loc158)
    %139 = stablehlo.power %138, %cst_3 : tensor<1x32x512xf32> loc(#loc159)
    %140 = stablehlo.reduce(%139 init: %cst_22) applies stablehlo.add across dimensions = [2] : (tensor<1x32x512xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc160)
    %141 = stablehlo.multiply %140, %cst_2 : tensor<1x32xf32> loc(#loc161)
    %142 = stablehlo.reshape %141 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc162)
    %143 = stablehlo.add %142, %cst_15 : tensor<1x32x1xf32> loc(#loc163)
    %144 = stablehlo.rsqrt %143 : tensor<1x32x1xf32> loc(#loc164)
    %145 = stablehlo.reshape %144 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc165)
    %146 = stablehlo.broadcast_in_dim %145, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x512xf32> loc(#loc166)
    %147 = stablehlo.multiply %138, %146 : tensor<1x32x512xf32> loc(#loc167)
    %148 = stablehlo.convert %147 : (tensor<1x32x512xf32>) -> tensor<1x32x512xbf16> loc(#loc168)
    %149 = stablehlo.multiply %136, %148 : tensor<1x32x512xbf16> loc(#loc169)
    %150 = stablehlo.reshape %149 : (tensor<1x32x512xbf16>) -> tensor<32x512xbf16> loc(#loc170)
    %151 = stablehlo.reshape %arg5 : (tensor<32768x512xbf16>) -> tensor<1x32768x512xbf16> loc(#loc171)
    %152 = stablehlo.reshape %151 : (tensor<1x32768x512xbf16>) -> tensor<32768x512xbf16> loc(#loc172)
    %153 = stablehlo.transpose %152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[512,32768]{0,1}"} : (tensor<32768x512xbf16>) -> tensor<512x32768xbf16> loc(#loc173)
    %154 = stablehlo.dot_general %150, %153, contracting_dims = [1] x [0] : (tensor<32x512xbf16>, tensor<512x32768xbf16>) -> tensor<32x32768xbf16> loc(#loc174)
    %155 = stablehlo.reshape %154 : (tensor<32x32768xbf16>) -> tensor<1x32x128x256xbf16> loc(#loc175)
    %156 = stablehlo.transpose %155, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,32,256]{3,1,2,0}"} : (tensor<1x32x128x256xbf16>) -> tensor<1x128x32x256xbf16> loc(#loc176)
    %157 = stablehlo.slice %156 [0:1, 0:128, 0:32, 0:128] : (tensor<1x128x32x256xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc177)
    %158 = "stablehlo.gather"(%157, %109) <{dimension_numbers = #stablehlo.gather<offset_dims = [0, 1, 2], collapsed_slice_dims = [3], start_index_map = [3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 128, 32, 1>}> : (tensor<1x128x32x128xbf16>, tensor<192x1xi64>) -> tensor<1x128x32x192xbf16> loc(#loc178)
    %159 = stablehlo.select %101, %cst_18, %158 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc179)
    %160 = stablehlo.select %98, %159, %cst_18 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc180)
    %161 = stablehlo.select %22, %133, %160 : tensor<1x128x32x192xi1>, tensor<1x128x32x192xbf16> loc(#loc181)
    %162 = stablehlo.transpose %161, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,128,192,32]{2,3,1,0}"} : (tensor<1x128x32x192xbf16>) -> tensor<1x128x192x32xbf16> loc(#loc182)
    %163 = stablehlo.dot_general %113, %162, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x128x32x192xbf16>, tensor<1x128x192x32xbf16>) -> tensor<1x128x32x32xbf16> loc(#loc183)
    %164 = stablehlo.multiply %163, %cst_1 : tensor<1x128x32x32xbf16> loc(#loc184)
    %165 = stablehlo.broadcast_in_dim %c_21, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc185)
    %166 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc186)
    %167 = stablehlo.compare  LT, %165, %166 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1> loc(#loc187)
    %168 = stablehlo.select %167, %cst_0, %cst : tensor<32x32xi1>, tensor<32x32xf32> loc(#loc188)
    %169 = stablehlo.convert %168 : (tensor<32x32xf32>) -> tensor<32x32xbf16> loc(#loc189)
    %170 = stablehlo.reshape %169 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16> loc(#loc190)
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<1x128x32x32xbf16> loc(#loc191)
    %172 = stablehlo.add %164, %171 : tensor<1x128x32x32xbf16> loc(#loc192)
    %173 = stablehlo.convert %172 : (tensor<1x128x32x32xbf16>) -> tensor<1x128x32x32xf32> loc(#loc193)
    %174 = stablehlo.reduce(%173 init: %cst_20) applies stablehlo.maximum across dimensions = [3] : (tensor<1x128x32x32xf32>, tensor<f32>) -> tensor<1x128x32xf32> loc(#loc194)
    %175 = stablehlo.broadcast_in_dim %174, dims = [0, 1, 2] : (tensor<1x128x32xf32>) -> tensor<1x128x32x32xf32> loc(#loc195)
    %176 = stablehlo.subtract %173, %175 : tensor<1x128x32x32xf32> loc(#loc196)
    %177 = stablehlo.exponential %176 : tensor<1x128x32x32xf32> loc(#loc197)
    %178 = stablehlo.reduce(%177 init: %cst_22) applies stablehlo.add across dimensions = [3] : (tensor<1x128x32x32xf32>, tensor<f32>) -> tensor<1x128x32xf32> loc(#loc198)
    %179 = stablehlo.broadcast_in_dim %178, dims = [0, 1, 2] : (tensor<1x128x32xf32>) -> tensor<1x128x32x32xf32> loc(#loc199)
    %180 = stablehlo.divide %177, %179 : tensor<1x128x32x32xf32> loc(#loc200)
    %181 = stablehlo.convert %180 : (tensor<1x128x32x32xf32>) -> tensor<1x128x32x32xbf16> loc(#loc201)
    %182 = stablehlo.slice %156 [0:1, 0:128, 0:32, 128:256] : (tensor<1x128x32x256xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc202)
    %183 = stablehlo.dot_general %181, %182, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x128x32x32xbf16>, tensor<1x128x32x128xbf16>) -> tensor<1x128x32x128xbf16> loc(#loc203)
    %184 = stablehlo.transpose %183, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,32,128,128]{3,1,2,0}"} : (tensor<1x128x32x128xbf16>) -> tensor<1x32x128x128xbf16> loc(#loc204)
    %185 = stablehlo.reshape %184 : (tensor<1x32x128x128xbf16>) -> tensor<32x16384xbf16> loc(#loc205)
    %186 = stablehlo.reshape %arg4 : (tensor<7168x16384xbf16>) -> tensor<1x7168x16384xbf16> loc(#loc206)
    %187 = stablehlo.reshape %186 : (tensor<1x7168x16384xbf16>) -> tensor<7168x16384xbf16> loc(#loc207)
    %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[16384,7168]{0,1}"} : (tensor<7168x16384xbf16>) -> tensor<16384x7168xbf16> loc(#loc208)
    %189 = stablehlo.dot_general %185, %188, contracting_dims = [1] x [0] : (tensor<32x16384xbf16>, tensor<16384x7168xbf16>) -> tensor<32x7168xbf16> loc(#loc209)
    %190 = stablehlo.reshape %189 : (tensor<32x7168xbf16>) -> tensor<1x32x7168xbf16> loc(#loc210)
    %191 = stablehlo.add %17, %190 : tensor<1x32x7168xbf16> loc(#loc211)
    %192 = stablehlo.reshape %arg15 : (tensor<7168xbf16>) -> tensor<1x1x7168xbf16> loc(#loc212)
    %193 = stablehlo.reshape %192 : (tensor<1x1x7168xbf16>) -> tensor<7168xbf16> loc(#loc213)
    %194 = stablehlo.broadcast_in_dim %193, dims = [2] : (tensor<7168xbf16>) -> tensor<1x32x7168xbf16> loc(#loc214)
    %195 = stablehlo.convert %191 : (tensor<1x32x7168xbf16>) -> tensor<1x32x7168xf32> loc(#loc215)
    %196 = stablehlo.power %195, %cst_17 : tensor<1x32x7168xf32> loc(#loc216)
    %197 = stablehlo.reduce(%196 init: %cst_22) applies stablehlo.add across dimensions = [2] : (tensor<1x32x7168xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc217)
    %198 = stablehlo.multiply %197, %cst_16 : tensor<1x32xf32> loc(#loc218)
    %199 = stablehlo.reshape %198 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc219)
    %200 = stablehlo.add %199, %cst_15 : tensor<1x32x1xf32> loc(#loc220)
    %201 = stablehlo.rsqrt %200 : tensor<1x32x1xf32> loc(#loc221)
    %202 = stablehlo.reshape %201 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc222)
    %203 = stablehlo.broadcast_in_dim %202, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x7168xf32> loc(#loc223)
    %204 = stablehlo.multiply %195, %203 : tensor<1x32x7168xf32> loc(#loc224)
    %205 = stablehlo.convert %204 : (tensor<1x32x7168xf32>) -> tensor<1x32x7168xbf16> loc(#loc225)
    %206 = stablehlo.multiply %194, %205 : tensor<1x32x7168xbf16> loc(#loc226)
    %207 = stablehlo.reshape %206 : (tensor<1x32x7168xbf16>) -> tensor<32x7168xbf16> loc(#loc227)
    %208 = stablehlo.reshape %arg16 : (tensor<18432x7168xbf16>) -> tensor<1x18432x7168xbf16> loc(#loc228)
    %209 = stablehlo.reshape %208 : (tensor<1x18432x7168xbf16>) -> tensor<18432x7168xbf16> loc(#loc229)
    %210 = stablehlo.transpose %209, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,18432]{0,1}"} : (tensor<18432x7168xbf16>) -> tensor<7168x18432xbf16> loc(#loc230)
    %211 = stablehlo.dot_general %207, %210, contracting_dims = [1] x [0] : (tensor<32x7168xbf16>, tensor<7168x18432xbf16>) -> tensor<32x18432xbf16> loc(#loc231)
    %212 = stablehlo.reshape %211 : (tensor<32x18432xbf16>) -> tensor<1x32x18432xbf16> loc(#loc232)
    %213 = stablehlo.logistic %212 : tensor<1x32x18432xbf16> loc(#loc233)
    %214 = stablehlo.multiply %212, %213 : tensor<1x32x18432xbf16> loc(#loc234)
    %215 = stablehlo.reshape %arg3 : (tensor<18432x7168xbf16>) -> tensor<1x18432x7168xbf16> loc(#loc235)
    %216 = stablehlo.reshape %215 : (tensor<1x18432x7168xbf16>) -> tensor<18432x7168xbf16> loc(#loc236)
    %217 = stablehlo.transpose %216, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,18432]{0,1}"} : (tensor<18432x7168xbf16>) -> tensor<7168x18432xbf16> loc(#loc237)
    %218 = stablehlo.dot_general %207, %217, contracting_dims = [1] x [0] : (tensor<32x7168xbf16>, tensor<7168x18432xbf16>) -> tensor<32x18432xbf16> loc(#loc238)
    %219 = stablehlo.reshape %218 : (tensor<32x18432xbf16>) -> tensor<1x32x18432xbf16> loc(#loc239)
    %220 = stablehlo.multiply %214, %219 : tensor<1x32x18432xbf16> loc(#loc240)
    %221 = stablehlo.reshape %220 : (tensor<1x32x18432xbf16>) -> tensor<32x18432xbf16> loc(#loc241)
    %222 = stablehlo.reshape %arg2 : (tensor<7168x18432xbf16>) -> tensor<1x7168x18432xbf16> loc(#loc242)
    %223 = stablehlo.reshape %222 : (tensor<1x7168x18432xbf16>) -> tensor<7168x18432xbf16> loc(#loc243)
    %224 = stablehlo.transpose %223, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[18432,7168]{0,1}"} : (tensor<7168x18432xbf16>) -> tensor<18432x7168xbf16> loc(#loc244)
    %225 = stablehlo.dot_general %221, %224, contracting_dims = [1] x [0] : (tensor<32x18432xbf16>, tensor<18432x7168xbf16>) -> tensor<32x7168xbf16> loc(#loc245)
    %226 = stablehlo.reshape %225 : (tensor<32x7168xbf16>) -> tensor<1x32x7168xbf16> loc(#loc246)
    %227 = stablehlo.add %191, %226 : tensor<1x32x7168xbf16> loc(#loc247)
    %228 = stablehlo.convert %227 : (tensor<1x32x7168xbf16>) -> tensor<1x32x7168xf32> loc(#loc248)
    %229 = stablehlo.power %228, %cst_17 : tensor<1x32x7168xf32> loc(#loc249)
    %230 = stablehlo.reduce(%229 init: %cst_22) applies stablehlo.add across dimensions = [2] : (tensor<1x32x7168xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc250)
    %231 = stablehlo.multiply %230, %cst_16 : tensor<1x32xf32> loc(#loc251)
    %232 = stablehlo.reshape %231 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc252)
    %233 = stablehlo.add %232, %cst_15 : tensor<1x32x1xf32> loc(#loc253)
    %234 = stablehlo.rsqrt %233 : tensor<1x32x1xf32> loc(#loc254)
    %235 = stablehlo.reshape %234 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc255)
    %236 = stablehlo.broadcast_in_dim %235, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x7168xf32> loc(#loc256)
    %237 = stablehlo.multiply %228, %236 : tensor<1x32x7168xf32> loc(#loc257)
    %238 = stablehlo.convert %237 : (tensor<1x32x7168xf32>) -> tensor<1x32x7168xbf16> loc(#loc258)
    %239 = stablehlo.multiply %10, %238 : tensor<1x32x7168xbf16> loc(#loc259)
    %240 = stablehlo.reshape %239 : (tensor<1x32x7168xbf16>) -> tensor<32x7168xbf16> loc(#loc260)
    %241 = stablehlo.reshape %arg1 : (tensor<129280x7168xbf16>) -> tensor<1x129280x7168xbf16> loc(#loc261)
    %242 = stablehlo.reshape %241 : (tensor<1x129280x7168xbf16>) -> tensor<129280x7168xbf16> loc(#loc262)
    %243 = stablehlo.transpose %242, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[7168,129280]{0,1}"} : (tensor<129280x7168xbf16>) -> tensor<7168x129280xbf16> loc(#loc263)
    %244 = stablehlo.dot_general %240, %243, contracting_dims = [1] x [0] : (tensor<32x7168xbf16>, tensor<7168x129280xbf16>) -> tensor<32x129280xbf16> loc(#loc264)
    %245 = stablehlo.reshape %244 : (tensor<32x129280xbf16>) -> tensor<1x32x129280xbf16> loc(#loc265)
    %246 = stablehlo.convert %245 : (tensor<1x32x129280xbf16>) -> tensor<1x32x129280xf32> loc(#loc266)
    return %6, %7, %246 : tensor<32x64xbf16>, tensor<32x64xbf16>, tensor<1x32x129280xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc19 = loc("constant.5")
#loc20 = loc("broadcast.9")
#loc21 = loc("reshape.2")
#loc22 = loc("reshape.4")
#loc23 = loc("broadcast.7")
#loc24 = loc("multiply.10")
#loc25 = loc("concatenate.11")
#loc26 = loc("cosine.12")
#loc27 = loc("sine.13")
#loc28 = loc("reshape.689")
#loc29 = loc("reshape.691")
#loc30 = loc("broadcast.692")
#loc31 = loc("reshape.64")
#loc32 = loc("reshape.66")
#loc33 = loc("reshape.59")
#loc34 = loc("reshape.62")
#loc35 = loc("convert.67")
#loc36 = loc("gather.68")
#loc37 = loc("reshape.69")
#loc38 = loc("broadcast.551")
#loc39 = loc("and.554")
#loc40 = loc("reshape.555")
#loc41 = loc("reshape.563")
#loc42 = loc("broadcast.564")
#loc43 = loc("not.556")
#loc44 = loc("reshape.558")
#loc45 = loc("broadcast.559")
#loc46 = loc("reshape.446")
#loc47 = loc("reshape.448")
#loc48 = loc("broadcast.449")
#loc49 = loc("reshape.100")
#loc50 = loc("reshape.102")
#loc51 = loc("broadcast.103")
#loc52 = loc("convert.70")
#loc53 = loc("power.72")
#loc54 = loc("reduce.79")
#loc55 = loc("multiply.88")
#loc56 = loc("reshape.89")
#loc57 = loc("add.93")
#loc58 = loc("rsqrt.94")
#loc59 = loc("reshape.95")
#loc60 = loc("broadcast.96")
#loc61 = loc("multiply.97")
#loc62 = loc("convert.98")
#loc63 = loc("multiply.104")
#loc64 = loc("reshape.413")
#loc65 = loc("reshape.409")
#loc66 = loc("reshape.411")
#loc67 = loc("transpose.412")
#loc68 = loc("dot.414")
#loc69 = loc("reshape.415")
#loc70 = loc("convert.416")
#loc71 = loc("power.418")
#loc72 = loc("reduce.425")
#loc73 = loc("multiply.434")
#loc74 = loc("reshape.435")
#loc75 = loc("add.439")
#loc76 = loc("rsqrt.440")
#loc77 = loc("reshape.441")
#loc78 = loc("broadcast.442")
#loc79 = loc("multiply.443")
#loc80 = loc("convert.444")
#loc81 = loc("multiply.450")
#loc82 = loc("reshape.451")
#loc83 = loc("reshape.401")
#loc84 = loc("reshape.403")
#loc85 = loc("transpose.404")
#loc86 = loc("dot.452")
#loc87 = loc("reshape.454")
#loc88 = loc("transpose.455")
#loc89 = loc("slice.519")
#loc90 = loc("reshape.520")
#loc91 = loc("transpose.521")
#loc92 = loc("reshape.522")
#loc93 = loc("gather.322")
#loc94 = loc("reshape.323")
#loc95 = loc("broadcast.531")
#loc96 = loc("multiply.532")
#loc97 = loc("slice.524")
#loc98 = loc("negate.525")
#loc99 = loc("slice.523")
#loc100 = loc("concatenate.526")
#loc101 = loc("gather.296")
#loc102 = loc("reshape.297")
#loc103 = loc("broadcast.528")
#loc104 = loc("multiply.529")
#loc105 = loc("add.535")
#loc106 = loc("floor.501")
#loc107 = loc("convert.502")
#loc108 = loc("clamp.505")
#loc109 = loc("compare.514")
#loc110 = loc("add.511")
#loc111 = loc("select.515")
#loc112 = loc("reshape.516")
#loc113 = loc("gather.537")
#loc114 = loc("select.560")
#loc115 = loc("and.475")
#loc116 = loc("reshape.476")
#loc117 = loc("reshape.484")
#loc118 = loc("broadcast.485")
#loc119 = loc("not.477")
#loc120 = loc("reshape.479")
#loc121 = loc("broadcast.480")
#loc122 = loc("slice.456")
#loc123 = loc("floor.383")
#loc124 = loc("convert.384")
#loc125 = loc("clamp.387")
#loc126 = loc("compare.396")
#loc127 = loc("add.393")
#loc128 = loc("select.397")
#loc129 = loc("reshape.398")
#loc130 = loc("gather.458")
#loc131 = loc("select.481")
#loc132 = loc("select.486")
#loc133 = loc("select.565")
#loc134 = loc("reshape.51")
#loc135 = loc("reshape.53")
#loc136 = loc("transpose.54")
#loc137 = loc("dot.106")
#loc138 = loc("reshape.107")
#loc139 = loc("slice.298")
#loc140 = loc("reshape.301")
#loc141 = loc("transpose.302")
#loc142 = loc("reshape.303")
#loc143 = loc("multiply.324")
#loc144 = loc("slice.305")
#loc145 = loc("negate.306")
#loc146 = loc("slice.304")
#loc147 = loc("concatenate.307")
#loc148 = loc("multiply.308")
#loc149 = loc("add.327")
#loc150 = loc("reshape.331")
#loc151 = loc("broadcast.332")
#loc152 = loc("gather.334")
#loc153 = loc("select.357")
#loc154 = loc("reshape.139")
#loc155 = loc("reshape.141")
#loc156 = loc("broadcast.142")
#loc157 = loc("slice.108")
#loc158 = loc("convert.109")
#loc159 = loc("power.111")
#loc160 = loc("reduce.118")
#loc161 = loc("multiply.127")
#loc162 = loc("reshape.128")
#loc163 = loc("add.132")
#loc164 = loc("rsqrt.133")
#loc165 = loc("reshape.134")
#loc166 = loc("broadcast.135")
#loc167 = loc("multiply.136")
#loc168 = loc("convert.137")
#loc169 = loc("multiply.143")
#loc170 = loc("reshape.144")
#loc171 = loc("reshape.43")
#loc172 = loc("reshape.45")
#loc173 = loc("transpose.46")
#loc174 = loc("dot.145")
#loc175 = loc("reshape.147")
#loc176 = loc("transpose.148")
#loc177 = loc("slice.217")
#loc178 = loc("gather.219")
#loc179 = loc("select.243")
#loc180 = loc("select.248")
#loc181 = loc("select.362")
#loc182 = loc("transpose.363")
#loc183 = loc("dot.566")
#loc184 = loc("multiply.569")
#loc185 = loc("broadcast.170")
#loc186 = loc("broadcast.172")
#loc187 = loc("compare.173")
#loc188 = loc("select.175")
#loc189 = loc("convert.176")
#loc190 = loc("reshape.177")
#loc191 = loc("broadcast.573")
#loc192 = loc("add.574")
#loc193 = loc("convert.575")
#loc194 = loc("reduce.581")
#loc195 = loc("broadcast.582")
#loc196 = loc("subtract.583")
#loc197 = loc("exponential.584")
#loc198 = loc("reduce.590")
#loc199 = loc("broadcast.591")
#loc200 = loc("divide.592")
#loc201 = loc("convert.593")
#loc202 = loc("slice.149")
#loc203 = loc("dot.594")
#loc204 = loc("transpose.596")
#loc205 = loc("reshape.598")
#loc206 = loc("reshape.38")
#loc207 = loc("reshape.40")
#loc208 = loc("transpose.41")
#loc209 = loc("dot.599")
#loc210 = loc("reshape.600")
#loc211 = loc("add.603")
#loc212 = loc("reshape.634")
#loc213 = loc("reshape.636")
#loc214 = loc("broadcast.637")
#loc215 = loc("convert.604")
#loc216 = loc("power.606")
#loc217 = loc("reduce.613")
#loc218 = loc("multiply.622")
#loc219 = loc("reshape.623")
#loc220 = loc("add.627")
#loc221 = loc("rsqrt.628")
#loc222 = loc("reshape.629")
#loc223 = loc("broadcast.630")
#loc224 = loc("multiply.631")
#loc225 = loc("convert.632")
#loc226 = loc("multiply.638")
#loc227 = loc("reshape.647")
#loc228 = loc("reshape.643")
#loc229 = loc("reshape.645")
#loc230 = loc("transpose.646")
#loc231 = loc("dot.648")
#loc232 = loc("reshape.649")
#loc233 = loc("logistic.650")
#loc234 = loc("multiply.651")
#loc235 = loc("reshape.29")
#loc236 = loc("reshape.31")
#loc237 = loc("transpose.32")
#loc238 = loc("dot.640")
#loc239 = loc("reshape.641")
#loc240 = loc("multiply.652")
#loc241 = loc("reshape.653")
#loc242 = loc("reshape.24")
#loc243 = loc("reshape.26")
#loc244 = loc("transpose.27")
#loc245 = loc("dot.654")
#loc246 = loc("reshape.655")
#loc247 = loc("add.658")
#loc248 = loc("convert.659")
#loc249 = loc("power.661")
#loc250 = loc("reduce.668")
#loc251 = loc("multiply.677")
#loc252 = loc("reshape.678")
#loc253 = loc("add.682")
#loc254 = loc("rsqrt.683")
#loc255 = loc("reshape.684")
#loc256 = loc("broadcast.685")
#loc257 = loc("multiply.686")
#loc258 = loc("convert.687")
#loc259 = loc("multiply.693")
#loc260 = loc("reshape.694")
#loc261 = loc("reshape.15")
#loc262 = loc("reshape.17")
#loc263 = loc("transpose.18")
#loc264 = loc("dot.695")
#loc265 = loc("reshape.696")
#loc266 = loc("convert.697")
------------------ END OF MLIR MODULE ------------------
