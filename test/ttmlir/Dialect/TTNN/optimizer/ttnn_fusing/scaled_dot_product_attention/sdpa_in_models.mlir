// SDPA patterns extracted from real model IRs for fusing regression tests.
//
// These tests capture the exact SDPA patterns generated by different model frontends
// to ensure fusing patterns remain compatible across framework/version updates.

// REQUIRES: opmodel
// RUN: ttmlir-opt --ttir-to-ttnn-backend-pipeline="system-desc-path=%system_desc_path% enable-optimizer=true" %s | FileCheck %s
module {

  // ===----------------------------------------------------------------------===
  // LLM Decoder Patterns (with KV cache support)
  // ===----------------------------------------------------------------------===

  // Q was prepared with:
  // - Precomputed in the model graph as [batch, q_heads, q_seq, head_dim].
  // - Typecast bf16 -> f32 before attention compute.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - GQA KV head expansion: reshape -> broadcast (repeat factor) -> reshape.
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - GQA KV head expansion: reshape -> broadcast (repeat factor) -> reshape.
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> add mask -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  //
  // LLaMA 3.2 1B prefill pattern
  //
  // CHECK-LABEL: func.func @sdpa_prefill_llama_3_2_1b
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_prefill_llama_3_2_1b(
    %arg0: tensor<32x32x18x64xbf16>,  // Q
    %arg1: tensor<32x8x128x64xbf16>,  // K
    %arg2: tensor<32x8x128x64xbf16>,  // V
    %arg3: tensor<32x1x18x128xbf16>   // Mask
  ) -> tensor<32x32x18x64xbf16> {
    // Scale constants
    %cst_scale_q = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x18x64xf32>}> : () -> tensor<32x32x18x64xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x64x128xf32>}> : () -> tensor<32x32x64x128xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<32x32x18x128xf64>}> : () -> tensor<32x32x18x128xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<32x32x18x128xf32>}> : () -> tensor<32x32x18x128xf32>

    // GQA: Broadcast K from [32, 8, 128, 64] to [32, 32, 128, 64]
    %k_reshaped = "ttir.reshape"(%arg1) <{shape = [32 : i32, 8 : i32, 1 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x128x64xbf16>) -> tensor<32x8x1x128x64xbf16>
    %k_broadcast = "ttir.broadcast"(%k_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<32x8x1x128x64xbf16>) -> tensor<32x8x4x128x64xbf16>
    %k_final = "ttir.reshape"(%k_broadcast) <{shape = [32 : i32, 32 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x4x128x64xbf16>) -> tensor<32x32x128x64xbf16>

    // GQA: Broadcast V from [32, 8, 128, 64] to [32, 32, 128, 64]
    %v_reshaped = "ttir.reshape"(%arg2) <{shape = [32 : i32, 8 : i32, 1 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x128x64xbf16>) -> tensor<32x8x1x128x64xbf16>
    %v_broadcast = "ttir.broadcast"(%v_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<32x8x1x128x64xbf16>) -> tensor<32x8x4x128x64xbf16>
    %v_final = "ttir.reshape"(%v_broadcast) <{shape = [32 : i32, 32 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x4x128x64xbf16>) -> tensor<32x32x128x64xbf16>

    // Q: typecast and scale
    %q_f32 = "ttir.typecast"(%arg0) <{conservative_folding = false}> : (tensor<32x32x18x64xbf16>) -> tensor<32x32x18x64xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<32x32x18x64xf32>, tensor<32x32x18x64xf32>) -> tensor<32x32x18x64xf32>

    // K: typecast, permute (transpose), and scale
    %k_f32 = "ttir.typecast"(%k_final) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<32x32x128x64xf32>) -> tensor<32x32x64x128xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<32x32x64x128xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x64x128xf32>

    // Q @ K^T using dot_general with batch dimensions
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x18x64xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x18x128xf32>

    // Add attention mask (match model IR: typecast -> reshape -> reshape -> broadcast).
    %mask_f32 = "ttir.typecast"(%arg3) <{conservative_folding = false}> : (tensor<32x1x18x128xbf16>) -> tensor<32x1x18x128xf32>
    %mask_3d = "ttir.reshape"(%mask_f32) <{shape = [32 : i32, 18 : i32, 128 : i32]}> : (tensor<32x1x18x128xf32>) -> tensor<32x18x128xf32>
    %mask_4d = "ttir.reshape"(%mask_3d) <{shape = [32 : i32, 1 : i32, 18 : i32, 128 : i32]}> : (tensor<32x18x128xf32>) -> tensor<32x1x18x128xf32>
    %mask_broadcast = "ttir.broadcast"(%mask_4d) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<32x1x18x128xf32>) -> tensor<32x32x18x128xf32>
    %qk_masked = "ttir.add"(%qk, %mask_broadcast) : (tensor<32x32x18x128xf32>, tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf32>

    // NaN-safe softmax: detect all-inf rows using f64 comparison
    %qk_f64 = "ttir.typecast"(%qk_masked) <{conservative_folding = false}> : (tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<32x32x18x128xf64>, tensor<32x32x18x128xf64>) -> tensor<32x32x18x128xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<32x32x18x128xi1>) -> tensor<32x32x18x128xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x18x128xi1>) -> tensor<32x32x18xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [32 : i32, 32 : i32, 18 : i32, 1 : i32]}> : (tensor<32x32x18xi1>) -> tensor<32x32x18x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<32x32x18x1xi1>) -> tensor<32x32x18x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x18x1xi1>) -> tensor<32x32x18x128xi1>

    // Softmax: max -> subtract -> exp -> sum -> div
    %max_val = "ttir.max"(%qk_masked) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x18x128xf32>) -> tensor<32x32x18xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [32 : i32, 32 : i32, 18 : i32, 1 : i32]}> : (tensor<32x32x18xf32>) -> tensor<32x32x18x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x18x1xf32>) -> tensor<32x32x18x128xf32>
    %shifted = "ttir.subtract"(%qk_masked, %max_broadcast) : (tensor<32x32x18x128xf32>, tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x18x128xf32>) -> tensor<32x32x18xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [32 : i32, 32 : i32, 18 : i32, 1 : i32]}> : (tensor<32x32x18xf32>) -> tensor<32x32x18x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x18x1xf32>) -> tensor<32x32x18x128xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<32x32x18x128xf32>, tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf32>

    // Handle all-inf rows: output zeros instead of NaN
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<32x32x18x128xi1>, tensor<32x32x18x128xf32>, tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf32>

    // Attention @ V using dot_general
    %v_f32 = "ttir.typecast"(%v_final) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x18x128xf32>, tensor<32x32x128x64xf32>) -> tensor<32x32x18x64xf32>

    // Typecast back to bf16
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<32x32x18x64xf32>) -> tensor<32x32x18x64xbf16>

    return %result : tensor<32x32x18x64xbf16>
  }

  // Q was prepared with:
  // - Precomputed in the model graph as [batch, q_heads, q_seq, head_dim] with q_seq = 1.
  // - Typecast bf16 -> f32 before attention compute.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - GQA KV head expansion: reshape -> broadcast (repeat factor) -> reshape.
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - GQA KV head expansion: reshape -> broadcast (repeat factor) -> reshape.
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> add mask -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  //
  // LLaMA 3.2 1B decode pattern
  //
  // CHECK-LABEL: func.func @sdpa_decode_llama_3_2_1b_decode
  // CHECK: "ttnn.scaled_dot_product_attention_decode"
  func.func @sdpa_decode_llama_3_2_1b_decode(
    %arg0: tensor<32x32x1x64xbf16>,   // Q
    %arg1: tensor<32x8x128x64xbf16>,  // K
    %arg2: tensor<32x8x128x64xbf16>,  // V
    %arg3: tensor<32x1x1x128xbf16>    // Mask
  ) -> tensor<32x32x1x64xbf16> {
    // Scale constants
    %cst_scale_q = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x1x64xf32>}> : () -> tensor<32x32x1x64xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x64x128xf32>}> : () -> tensor<32x32x64x128xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<32x32x1x128xf64>}> : () -> tensor<32x32x1x128xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<32x32x1x128xf32>}> : () -> tensor<32x32x1x128xf32>

    // GQA: Broadcast K from [32, 8, 128, 64] to [32, 32, 128, 64]
    %k_reshaped = "ttir.reshape"(%arg1) <{shape = [32 : i32, 8 : i32, 1 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x128x64xbf16>) -> tensor<32x8x1x128x64xbf16>
    %k_broadcast = "ttir.broadcast"(%k_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<32x8x1x128x64xbf16>) -> tensor<32x8x4x128x64xbf16>
    %k_final = "ttir.reshape"(%k_broadcast) <{shape = [32 : i32, 32 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x4x128x64xbf16>) -> tensor<32x32x128x64xbf16>

    // GQA: Broadcast V from [32, 8, 128, 64] to [32, 32, 128, 64]
    %v_reshaped = "ttir.reshape"(%arg2) <{shape = [32 : i32, 8 : i32, 1 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x128x64xbf16>) -> tensor<32x8x1x128x64xbf16>
    %v_broadcast = "ttir.broadcast"(%v_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<32x8x1x128x64xbf16>) -> tensor<32x8x4x128x64xbf16>
    %v_final = "ttir.reshape"(%v_broadcast) <{shape = [32 : i32, 32 : i32, 128 : i32, 64 : i32]}> : (tensor<32x8x4x128x64xbf16>) -> tensor<32x32x128x64xbf16>

    // Q: typecast and scale
    %q_f32 = "ttir.typecast"(%arg0) <{conservative_folding = false}> : (tensor<32x32x1x64xbf16>) -> tensor<32x32x1x64xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<32x32x1x64xf32>, tensor<32x32x1x64xf32>) -> tensor<32x32x1x64xf32>

    // K: typecast, permute (transpose), and scale
    %k_f32 = "ttir.typecast"(%k_final) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<32x32x128x64xf32>) -> tensor<32x32x64x128xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<32x32x64x128xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x64x128xf32>

    // Q @ K^T using dot_general with batch dimensions
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x1x64xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x1x128xf32>

    // Add attention mask (match model IR: typecast -> reshape -> reshape -> broadcast).
    %mask_f32 = "ttir.typecast"(%arg3) <{conservative_folding = false}> : (tensor<32x1x1x128xbf16>) -> tensor<32x1x1x128xf32>
    %mask_3d = "ttir.reshape"(%mask_f32) <{shape = [32 : i32, 1 : i32, 128 : i32]}> : (tensor<32x1x1x128xf32>) -> tensor<32x1x128xf32>
    %mask_4d = "ttir.reshape"(%mask_3d) <{shape = [32 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<32x1x128xf32>) -> tensor<32x1x1x128xf32>
    %mask_broadcast = "ttir.broadcast"(%mask_4d) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<32x1x1x128xf32>) -> tensor<32x32x1x128xf32>
    %qk_masked = "ttir.add"(%qk, %mask_broadcast) : (tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>

    // NaN-safe softmax: detect all-inf rows using f64 comparison
    %qk_f64 = "ttir.typecast"(%qk_masked) <{conservative_folding = false}> : (tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<32x32x1x128xf64>, tensor<32x32x1x128xf64>) -> tensor<32x32x1x128xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<32x32x1x128xi1>) -> tensor<32x32x1x128xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x1x128xi1>) -> tensor<32x32x1xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [32 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x32x1xi1>) -> tensor<32x32x1x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<32x32x1x1xi1>) -> tensor<32x32x1x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x1x1xi1>) -> tensor<32x32x1x128xi1>

    // Softmax: max -> subtract -> exp -> sum -> div
    %max_val = "ttir.max"(%qk_masked) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x1x128xf32>) -> tensor<32x32x1xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [32 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x32x1xf32>) -> tensor<32x32x1x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x1x1xf32>) -> tensor<32x32x1x128xf32>
    %shifted = "ttir.subtract"(%qk_masked, %max_broadcast) : (tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x1x128xf32>) -> tensor<32x32x1xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [32 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x32x1xf32>) -> tensor<32x32x1x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x1x1xf32>) -> tensor<32x32x1x128xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>

    // Handle all-inf rows: output zeros instead of NaN
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<32x32x1x128xi1>, tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>

    // Attention @ V using dot_general
    %v_f32 = "ttir.typecast"(%v_final) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x1x128xf32>, tensor<32x32x128x64xf32>) -> tensor<32x32x1x64xf32>

    // Typecast back to bf16
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<32x32x1x64xf32>) -> tensor<32x32x1x64xbf16>
    return %result : tensor<32x32x1x64xbf16>
  }

  // Q was prepared with:
  // - Precomputed in the model graph as [batch, heads, q_seq, head_dim].
  // - Typecast bf16 -> f32 before attention compute.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> add mask -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  //
  // Phi 1 prefill pattern (no GQA)
  //
  // CHECK-LABEL: func.func @sdpa_prefill_phi_1_no_gqa
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_prefill_phi_1_no_gqa(
    %arg0: tensor<32x32x17x64xbf16>,    // Q
    %arg1: tensor<32x32x128x64xbf16>,   // K
    %arg2: tensor<32x32x128x64xbf16>,   // V
    %arg3: tensor<32x1x17x128xbf16>     // Mask / bias
  ) -> tensor<32x32x17x64xbf16> {
    // Scale constants (copied from model IR patterns used in other SDPA tests).
    %cst_scale_q = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x17x64xf32>}> : () -> tensor<32x32x17x64xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x64x128xf32>}> : () -> tensor<32x32x64x128xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<32x32x17x128xf64>}> : () -> tensor<32x32x17x128xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<32x32x17x128xf32>}> : () -> tensor<32x32x17x128xf32>

    // Q: typecast and scale.
    %q_f32 = "ttir.typecast"(%arg0) <{conservative_folding = false}> : (tensor<32x32x17x64xbf16>) -> tensor<32x32x17x64xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<32x32x17x64xf32>, tensor<32x32x17x64xf32>) -> tensor<32x32x17x64xf32>

    // K: typecast, permute (transpose), and scale.
    %k_f32 = "ttir.typecast"(%arg1) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<32x32x128x64xf32>) -> tensor<32x32x64x128xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<32x32x64x128xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x64x128xf32>

    // Q @ K^T.
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x17x64xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x17x128xf32>

    // Add attention mask / bias (match model IR: typecast -> reshape -> reshape -> broadcast).
    %mask_f32 = "ttir.typecast"(%arg3) <{conservative_folding = false}> : (tensor<32x1x17x128xbf16>) -> tensor<32x1x17x128xf32>
    %mask_3d = "ttir.reshape"(%mask_f32) <{shape = [32 : i32, 17 : i32, 128 : i32]}> : (tensor<32x1x17x128xf32>) -> tensor<32x17x128xf32>
    %mask_4d = "ttir.reshape"(%mask_3d) <{shape = [32 : i32, 1 : i32, 17 : i32, 128 : i32]}> : (tensor<32x17x128xf32>) -> tensor<32x1x17x128xf32>
    %mask_broadcast = "ttir.broadcast"(%mask_4d) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<32x1x17x128xf32>) -> tensor<32x32x17x128xf32>
    %qk_masked = "ttir.add"(%qk, %mask_broadcast) : (tensor<32x32x17x128xf32>, tensor<32x32x17x128xf32>) -> tensor<32x32x17x128xf32>

    // NaN-safe softmax guard: detect all -inf rows.
    %qk_f64 = "ttir.typecast"(%qk_masked) <{conservative_folding = false}> : (tensor<32x32x17x128xf32>) -> tensor<32x32x17x128xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<32x32x17x128xf64>, tensor<32x32x17x128xf64>) -> tensor<32x32x17x128xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<32x32x17x128xi1>) -> tensor<32x32x17x128xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x17x128xi1>) -> tensor<32x32x17xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [32 : i32, 32 : i32, 17 : i32, 1 : i32]}> : (tensor<32x32x17xi1>) -> tensor<32x32x17x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<32x32x17x1xi1>) -> tensor<32x32x17x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x17x1xi1>) -> tensor<32x32x17x128xi1>

    // Softmax: max -> subtract -> exp -> sum -> div.
    %max_val = "ttir.max"(%qk_masked) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x17x128xf32>) -> tensor<32x32x17xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [32 : i32, 32 : i32, 17 : i32, 1 : i32]}> : (tensor<32x32x17xf32>) -> tensor<32x32x17x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x17x1xf32>) -> tensor<32x32x17x128xf32>
    %shifted = "ttir.subtract"(%qk_masked, %max_broadcast) : (tensor<32x32x17x128xf32>, tensor<32x32x17x128xf32>) -> tensor<32x32x17x128xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<32x32x17x128xf32>) -> tensor<32x32x17x128xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x17x128xf32>) -> tensor<32x32x17xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [32 : i32, 32 : i32, 17 : i32, 1 : i32]}> : (tensor<32x32x17xf32>) -> tensor<32x32x17x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x17x1xf32>) -> tensor<32x32x17x128xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<32x32x17x128xf32>, tensor<32x32x17x128xf32>) -> tensor<32x32x17x128xf32>

    // Handle all-inf rows: output zeros instead of NaN.
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<32x32x17x128xi1>, tensor<32x32x17x128xf32>, tensor<32x32x17x128xf32>) -> tensor<32x32x17x128xf32>

    // Attention @ V.
    %v_f32 = "ttir.typecast"(%arg2) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x17x128xf32>, tensor<32x32x128x64xf32>) -> tensor<32x32x17x64xf32>

    // Typecast back to bf16.
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<32x32x17x64xf32>) -> tensor<32x32x17x64xbf16>
    return %result : tensor<32x32x17x64xbf16>
  }

  // Q was prepared with:
  // - Precomputed in the model graph as [batch, heads, q_seq, head_dim] with q_seq = 1.
  // - Typecast bf16 -> f32 before attention compute.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> add mask -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  //
  // Phi 1 decode pattern (no GQA)
  //
  // CHECK-LABEL: func.func @sdpa_decode_phi_1_no_gqa
  // CHECK: "ttnn.scaled_dot_product_attention_decode"
  func.func @sdpa_decode_phi_1_no_gqa(
    %arg0: tensor<32x32x1x64xbf16>,    // Q
    %arg1: tensor<32x32x128x64xbf16>,  // K
    %arg2: tensor<32x32x128x64xbf16>,  // V
    %arg3: tensor<32x1x1x128xbf16>     // Mask / bias
  ) -> tensor<32x32x1x64xbf16> {
    // Scale constants (copied from model IR patterns used in other SDPA tests).
    %cst_scale_q = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x1x64xf32>}> : () -> tensor<32x32x1x64xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.353553385> : tensor<32x32x64x128xf32>}> : () -> tensor<32x32x64x128xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<32x32x1x128xf64>}> : () -> tensor<32x32x1x128xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<32x32x1x128xf32>}> : () -> tensor<32x32x1x128xf32>

    // Q: typecast and scale.
    %q_f32 = "ttir.typecast"(%arg0) <{conservative_folding = false}> : (tensor<32x32x1x64xbf16>) -> tensor<32x32x1x64xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<32x32x1x64xf32>, tensor<32x32x1x64xf32>) -> tensor<32x32x1x64xf32>

    // K: typecast, permute (transpose), and scale.
    %k_f32 = "ttir.typecast"(%arg1) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<32x32x128x64xf32>) -> tensor<32x32x64x128xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<32x32x64x128xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x64x128xf32>

    // Q @ K^T.
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x1x64xf32>, tensor<32x32x64x128xf32>) -> tensor<32x32x1x128xf32>

    // Add attention mask / bias (match model IR: typecast -> reshape -> reshape -> broadcast).
    %mask_f32 = "ttir.typecast"(%arg3) <{conservative_folding = false}> : (tensor<32x1x1x128xbf16>) -> tensor<32x1x1x128xf32>
    %mask_3d = "ttir.reshape"(%mask_f32) <{shape = [32 : i32, 1 : i32, 128 : i32]}> : (tensor<32x1x1x128xf32>) -> tensor<32x1x128xf32>
    %mask_4d = "ttir.reshape"(%mask_3d) <{shape = [32 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<32x1x128xf32>) -> tensor<32x1x1x128xf32>
    %mask_broadcast = "ttir.broadcast"(%mask_4d) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<32x1x1x128xf32>) -> tensor<32x32x1x128xf32>
    %qk_masked = "ttir.add"(%qk, %mask_broadcast) : (tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>

    // NaN-safe softmax: detect all-inf rows using f64 comparison.
    %qk_f64 = "ttir.typecast"(%qk_masked) <{conservative_folding = false}> : (tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<32x32x1x128xf64>, tensor<32x32x1x128xf64>) -> tensor<32x32x1x128xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<32x32x1x128xi1>) -> tensor<32x32x1x128xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x1x128xi1>) -> tensor<32x32x1xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [32 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x32x1xi1>) -> tensor<32x32x1x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<32x32x1x1xi1>) -> tensor<32x32x1x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x1x1xi1>) -> tensor<32x32x1x128xi1>

    // Softmax: max -> subtract -> exp -> sum -> div.
    %max_val = "ttir.max"(%qk_masked) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x1x128xf32>) -> tensor<32x32x1xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [32 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x32x1xf32>) -> tensor<32x32x1x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x1x1xf32>) -> tensor<32x32x1x128xf32>
    %shifted = "ttir.subtract"(%qk_masked, %max_broadcast) : (tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x1x128xf32>) -> tensor<32x32x1xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [32 : i32, 32 : i32, 1 : i32, 1 : i32]}> : (tensor<32x32x1xf32>) -> tensor<32x32x1x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<32x32x1x1xf32>) -> tensor<32x32x1x128xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>

    // Handle all-inf rows: output zeros instead of NaN.
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<32x32x1x128xi1>, tensor<32x32x1x128xf32>, tensor<32x32x1x128xf32>) -> tensor<32x32x1x128xf32>

    // Attention @ V.
    %v_f32 = "ttir.typecast"(%arg2) <{conservative_folding = false}> : (tensor<32x32x128x64xbf16>) -> tensor<32x32x128x64xf32>
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x1x128xf32>, tensor<32x32x128x64xf32>) -> tensor<32x32x1x64xf32>

    // Typecast back to bf16.
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<32x32x1x64xf32>) -> tensor<32x32x1x64xbf16>
    return %result : tensor<32x32x1x64xbf16>
  }

  // ===----------------------------------------------------------------------===
  // Encoder Model Patterns (no KV cache)
  // ===----------------------------------------------------------------------===

  // Q was prepared with:
  // - Typecast bf16 -> f32 before attention compute.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - GQA KV head expansion over sequence: reshape -> broadcast (repeat factor) -> reshape.
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> add mask -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  //
  // Qwen embedding encoder pattern (no KV cache, SxS attention, GQA over sequence)
  //
  // CHECK-LABEL: func.func @sdpa_encoder_qwen_embedding_gqa_sxs
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_encoder_qwen_embedding_gqa_sxs(
    %arg0: tensor<32x32x18x128xbf16>,  // Q
    %arg1: tensor<32x8x18x128xbf16>,   // K (KV heads)
    %arg2: tensor<32x8x18x128xbf16>,   // V (KV heads)
    %arg3: tensor<32x1x18x18xbf16>     // Mask / bias (SxS)
  ) -> tensor<32x32x18x128xbf16> {
    // Scale constants (copied from qwen_3_embedding_4b_encoder_layer patterns).
    %cst_scale_q = "ttir.constant"() <{value = dense<0.297301769> : tensor<32x32x18x128xf32>}> : () -> tensor<32x32x18x128xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.297301769> : tensor<32x32x128x18xf32>}> : () -> tensor<32x32x128x18xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<32x32x18x18xf64>}> : () -> tensor<32x32x18x18xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<32x32x18x18xf32>}> : () -> tensor<32x32x18x18xf32>

    // GQA: Broadcast K from [32, 8, 18, 128] to [32, 32, 18, 128].
    %k_reshaped = "ttir.reshape"(%arg1) <{shape = [32 : i32, 8 : i32, 1 : i32, 18 : i32, 128 : i32]}> : (tensor<32x8x18x128xbf16>) -> tensor<32x8x1x18x128xbf16>
    %k_broadcast = "ttir.broadcast"(%k_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<32x8x1x18x128xbf16>) -> tensor<32x8x4x18x128xbf16>
    %k_final = "ttir.reshape"(%k_broadcast) <{shape = [32 : i32, 32 : i32, 18 : i32, 128 : i32]}> : (tensor<32x8x4x18x128xbf16>) -> tensor<32x32x18x128xbf16>

    // GQA: Broadcast V from [32, 8, 18, 128] to [32, 32, 18, 128].
    %v_reshaped = "ttir.reshape"(%arg2) <{shape = [32 : i32, 8 : i32, 1 : i32, 18 : i32, 128 : i32]}> : (tensor<32x8x18x128xbf16>) -> tensor<32x8x1x18x128xbf16>
    %v_broadcast = "ttir.broadcast"(%v_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 4, 1, 1>}> : (tensor<32x8x1x18x128xbf16>) -> tensor<32x8x4x18x128xbf16>
    %v_final = "ttir.reshape"(%v_broadcast) <{shape = [32 : i32, 32 : i32, 18 : i32, 128 : i32]}> : (tensor<32x8x4x18x128xbf16>) -> tensor<32x32x18x128xbf16>

    // Q: typecast and scale.
    %q_f32 = "ttir.typecast"(%arg0) <{conservative_folding = false}> : (tensor<32x32x18x128xbf16>) -> tensor<32x32x18x128xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<32x32x18x128xf32>, tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf32>

    // K: typecast, permute (transpose), and scale.
    %k_f32 = "ttir.typecast"(%k_final) <{conservative_folding = false}> : (tensor<32x32x18x128xbf16>) -> tensor<32x32x18x128xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<32x32x18x128xf32>) -> tensor<32x32x128x18xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<32x32x128x18xf32>, tensor<32x32x128x18xf32>) -> tensor<32x32x128x18xf32>

    // Q @ K^T -> [32, 32, 18, 18].
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x18x128xf32>, tensor<32x32x128x18xf32>) -> tensor<32x32x18x18xf32>

    // Add attention mask / bias (match model IR: typecast -> reshape -> reshape -> broadcast).
    %mask_f32 = "ttir.typecast"(%arg3) <{conservative_folding = false}> : (tensor<32x1x18x18xbf16>) -> tensor<32x1x18x18xf32>
    %mask_3d = "ttir.reshape"(%mask_f32) <{shape = [32 : i32, 18 : i32, 18 : i32]}> : (tensor<32x1x18x18xf32>) -> tensor<32x18x18xf32>
    %mask_4d = "ttir.reshape"(%mask_3d) <{shape = [32 : i32, 1 : i32, 18 : i32, 18 : i32]}> : (tensor<32x18x18xf32>) -> tensor<32x1x18x18xf32>
    %mask_broadcast = "ttir.broadcast"(%mask_4d) <{broadcast_dimensions = array<i64: 1, 32, 1, 1>}> : (tensor<32x1x18x18xf32>) -> tensor<32x32x18x18xf32>
    %qk_masked = "ttir.add"(%qk, %mask_broadcast) : (tensor<32x32x18x18xf32>, tensor<32x32x18x18xf32>) -> tensor<32x32x18x18xf32>

    // NaN-safe softmax: detect all-inf rows using f64 comparison.
    %qk_f64 = "ttir.typecast"(%qk_masked) <{conservative_folding = false}> : (tensor<32x32x18x18xf32>) -> tensor<32x32x18x18xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<32x32x18x18xf64>, tensor<32x32x18x18xf64>) -> tensor<32x32x18x18xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<32x32x18x18xi1>) -> tensor<32x32x18x18xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x18x18xi1>) -> tensor<32x32x18xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [32 : i32, 32 : i32, 18 : i32, 1 : i32]}> : (tensor<32x32x18xi1>) -> tensor<32x32x18x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<32x32x18x1xi1>) -> tensor<32x32x18x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 18>}> : (tensor<32x32x18x1xi1>) -> tensor<32x32x18x18xi1>

    // Softmax: max -> subtract -> exp -> sum -> div.
    %max_val = "ttir.max"(%qk_masked) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x18x18xf32>) -> tensor<32x32x18xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [32 : i32, 32 : i32, 18 : i32, 1 : i32]}> : (tensor<32x32x18xf32>) -> tensor<32x32x18x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 18>}> : (tensor<32x32x18x1xf32>) -> tensor<32x32x18x18xf32>
    %shifted = "ttir.subtract"(%qk_masked, %max_broadcast) : (tensor<32x32x18x18xf32>, tensor<32x32x18x18xf32>) -> tensor<32x32x18x18xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<32x32x18x18xf32>) -> tensor<32x32x18x18xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<32x32x18x18xf32>) -> tensor<32x32x18xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [32 : i32, 32 : i32, 18 : i32, 1 : i32]}> : (tensor<32x32x18xf32>) -> tensor<32x32x18x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 18>}> : (tensor<32x32x18x1xf32>) -> tensor<32x32x18x18xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<32x32x18x18xf32>, tensor<32x32x18x18xf32>) -> tensor<32x32x18x18xf32>

    // Handle all-inf rows: output zeros instead of NaN.
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<32x32x18x18xi1>, tensor<32x32x18x18xf32>, tensor<32x32x18x18xf32>) -> tensor<32x32x18x18xf32>

    // Attention @ V -> [32, 32, 18, 128].
    %v_f32 = "ttir.typecast"(%v_final) <{conservative_folding = false}> : (tensor<32x32x18x128xbf16>) -> tensor<32x32x18x128xf32>
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<32x32x18x18xf32>, tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xf32>

    // Typecast back to bf16.
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<32x32x18x128xf32>) -> tensor<32x32x18x128xbf16>
    return %result : tensor<32x32x18x128xbf16>
  }

  // Q was prepared with:
  // - Precomputed in the model graph as [batch, heads, seq, head_dim].
  // - Typecast bf16 -> f32 before attention compute.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> add mask/bias -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  //
  // BERT encoder MHA pattern (no KV cache)
  //
  // CHECK-LABEL: func.func @sdpa_bert_encoder_mha
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_bert_encoder_mha(
    %arg0: tensor<8x12x384x64xbf16>,   // Q
    %arg1: tensor<8x12x384x64xbf16>,   // K
    %arg2: tensor<8x12x384x64xbf16>,   // V
    %arg3: tensor<8x1x384x384xbf16>    // Mask / bias
  ) -> tensor<8x12x384x64xbf16> {
    // Scale constants
    %cst_scale_q = "ttir.constant"() <{value = dense<0.353553385> : tensor<8x12x384x64xf32>}> : () -> tensor<8x12x384x64xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.353553385> : tensor<8x12x64x384xf32>}> : () -> tensor<8x12x64x384xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<8x12x384x384xf64>}> : () -> tensor<8x12x384x384xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<8x12x384x384xf32>}> : () -> tensor<8x12x384x384xf32>

    // Q: typecast and scale
    %q_f32 = "ttir.typecast"(%arg0) <{conservative_folding = false}> : (tensor<8x12x384x64xbf16>) -> tensor<8x12x384x64xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<8x12x384x64xf32>, tensor<8x12x384x64xf32>) -> tensor<8x12x384x64xf32>

    // K: typecast, permute (transpose), and scale
    %k_f32 = "ttir.typecast"(%arg1) <{conservative_folding = false}> : (tensor<8x12x384x64xbf16>) -> tensor<8x12x384x64xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<8x12x384x64xf32>) -> tensor<8x12x64x384xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<8x12x64x384xf32>, tensor<8x12x64x384xf32>) -> tensor<8x12x64x384xf32>

    // Q @ K^T using dot_general with batch dimensions
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<8x12x384x64xf32>, tensor<8x12x64x384xf32>) -> tensor<8x12x384x384xf32>

    // Add attention mask / bias (match model IR: typecast -> reshape -> reshape -> broadcast).
    %mask_f32 = "ttir.typecast"(%arg3) <{conservative_folding = false}> : (tensor<8x1x384x384xbf16>) -> tensor<8x1x384x384xf32>
    %mask_3d = "ttir.reshape"(%mask_f32) <{shape = [8 : i32, 384 : i32, 384 : i32]}> : (tensor<8x1x384x384xf32>) -> tensor<8x384x384xf32>
    %mask_4d = "ttir.reshape"(%mask_3d) <{shape = [8 : i32, 1 : i32, 384 : i32, 384 : i32]}> : (tensor<8x384x384xf32>) -> tensor<8x1x384x384xf32>
    %mask_broadcast = "ttir.broadcast"(%mask_4d) <{broadcast_dimensions = array<i64: 1, 12, 1, 1>}> : (tensor<8x1x384x384xf32>) -> tensor<8x12x384x384xf32>
    %qk_masked = "ttir.add"(%qk, %mask_broadcast) : (tensor<8x12x384x384xf32>, tensor<8x12x384x384xf32>) -> tensor<8x12x384x384xf32>

    // NaN-safe softmax: detect all-inf rows using f64 comparison
    %qk_f64 = "ttir.typecast"(%qk_masked) <{conservative_folding = false}> : (tensor<8x12x384x384xf32>) -> tensor<8x12x384x384xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<8x12x384x384xf64>, tensor<8x12x384x384xf64>) -> tensor<8x12x384x384xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<8x12x384x384xi1>) -> tensor<8x12x384x384xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<8x12x384x384xi1>) -> tensor<8x12x384xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [8 : i32, 12 : i32, 384 : i32, 1 : i32]}> : (tensor<8x12x384xi1>) -> tensor<8x12x384x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<8x12x384x1xi1>) -> tensor<8x12x384x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 384>}> : (tensor<8x12x384x1xi1>) -> tensor<8x12x384x384xi1>

    // Softmax: max -> subtract -> exp -> sum -> div
    %max_val = "ttir.max"(%qk_masked) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<8x12x384x384xf32>) -> tensor<8x12x384xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [8 : i32, 12 : i32, 384 : i32, 1 : i32]}> : (tensor<8x12x384xf32>) -> tensor<8x12x384x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 384>}> : (tensor<8x12x384x1xf32>) -> tensor<8x12x384x384xf32>
    %shifted = "ttir.subtract"(%qk_masked, %max_broadcast) : (tensor<8x12x384x384xf32>, tensor<8x12x384x384xf32>) -> tensor<8x12x384x384xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<8x12x384x384xf32>) -> tensor<8x12x384x384xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<8x12x384x384xf32>) -> tensor<8x12x384xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [8 : i32, 12 : i32, 384 : i32, 1 : i32]}> : (tensor<8x12x384xf32>) -> tensor<8x12x384x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 384>}> : (tensor<8x12x384x1xf32>) -> tensor<8x12x384x384xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<8x12x384x384xf32>, tensor<8x12x384x384xf32>) -> tensor<8x12x384x384xf32>

    // Handle all-inf rows: output zeros instead of NaN
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<8x12x384x384xi1>, tensor<8x12x384x384xf32>, tensor<8x12x384x384xf32>) -> tensor<8x12x384x384xf32>

    // Attention @ V using dot_general
    %v_f32 = "ttir.typecast"(%arg2) <{conservative_folding = false}> : (tensor<8x12x384x64xbf16>) -> tensor<8x12x384x64xf32>
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<8x12x384x384xf32>, tensor<8x12x384x64xf32>) -> tensor<8x12x384x64xf32>

    // Typecast back to bf16
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<8x12x384x64xf32>) -> tensor<8x12x384x64xbf16>

    return %result : tensor<8x12x384x64xbf16>
  }

  // ===----------------------------------------------------------------------===
  // Vision Model Patterns
  // ===----------------------------------------------------------------------===

  // Q was prepared with:
  // - Linear projection output (flattened) with head-splitting TMs.
  // - Reshape (split heads) -> permute [0,2,1,3] -> typecast bf16 -> f32.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - Linear projection output (flattened) with head-splitting TMs.
  // - Reshape (split heads) -> permute [0,2,1,3] -> typecast bf16 -> f32.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - Linear projection output (flattened) with head-splitting TMs.
  // - Reshape (split heads) -> permute [0,2,1,3] -> typecast bf16 -> f32.
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  // - No explicit additive mask.
  // - Same num_heads for Q/K/V (no GQA).
  //
  // CLIP Resampler self-attention pattern
  //
  // CHECK-LABEL: func.func @sdpa_clip_resampler_self_attention
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_clip_resampler_self_attention(
      %arg0: tensor<1x257x1280xbf16>,  // Q projection output (before head split)
      %arg1: tensor<1x257x1280xbf16>,  // K projection output (before head split)
      %arg2: tensor<1x257x1280xbf16>   // V projection output (before head split)
  ) -> tensor<1x257x1280xbf16> {
    // Constants
    %cst_scale_q = "ttir.constant"() <{value = dense<0.334370166> : tensor<1x16x257x80xf32>}> : () -> tensor<1x16x257x80xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.334370166> : tensor<1x16x80x257xf32>}> : () -> tensor<1x16x80x257xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<1x16x257x257xf64>}> : () -> tensor<1x16x257x257xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x16x257x257xf32>}> : () -> tensor<1x16x257x257xf32>

    // Q: reshape (split heads) -> permute [0,2,1,3] -> typecast -> scale
    %q_reshaped = "ttir.reshape"(%arg0) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16>
    %q_permuted = "ttir.permute"(%q_reshaped) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16>
    %q_f32 = "ttir.typecast"(%q_permuted) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<1x16x257x80xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32>

    // K: reshape (split heads) -> permute [0,2,1,3] -> typecast -> permute (K^T) -> scale
    %k_reshaped = "ttir.reshape"(%arg1) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16>
    %k_permuted = "ttir.permute"(%k_reshaped) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16>
    %k_f32 = "ttir.typecast"(%k_permuted) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x80x257xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<1x16x80x257xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x80x257xf32>

    // Q @ K^T using dot_general
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x80xf32>, tensor<1x16x80x257xf32>) -> tensor<1x16x257x257xf32>

    // No additive mask - only NaN-safe softmax handling
    // NaN detection: typecast -> f64, eq(-inf), logical_not, reduce_or, reshape, logical_not, broadcast
    %qk_f64 = "ttir.typecast"(%qk) <{conservative_folding = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<1x16x257x257xf64>, tensor<1x16x257x257xf64>) -> tensor<1x16x257x257xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<1x16x257x257xi1>) -> tensor<1x16x257x257xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xi1>) -> tensor<1x16x257xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xi1>) -> tensor<1x16x257x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xi1>) -> tensor<1x16x257x257xi1>

    // Decomposed softmax: max -> subtract -> exp -> sum -> div
    %max_val = "ttir.max"(%qk) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32>
    %shifted = "ttir.subtract"(%qk, %max_broadcast) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x16x257x257xf32>) -> tensor<1x16x257xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [1 : i32, 16 : i32, 257 : i32, 1 : i32]}> : (tensor<1x16x257xf32>) -> tensor<1x16x257x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 257>}> : (tensor<1x16x257x1xf32>) -> tensor<1x16x257x257xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32>

    // Post-softmax masking: where(all_inf, zeros, softmax)
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<1x16x257x257xi1>, tensor<1x16x257x257xf32>, tensor<1x16x257x257xf32>) -> tensor<1x16x257x257xf32>

    // V: reshape (split heads) -> permute [0,2,1,3] -> typecast
    %v_reshaped = "ttir.reshape"(%arg2) <{shape = [1 : i32, 257 : i32, 16 : i32, 80 : i32]}> : (tensor<1x257x1280xbf16>) -> tensor<1x257x16x80xbf16>
    %v_permuted = "ttir.permute"(%v_reshaped) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x257x16x80xbf16>) -> tensor<1x16x257x80xbf16>
    %v_f32 = "ttir.typecast"(%v_permuted) <{conservative_folding = false}> : (tensor<1x16x257x80xbf16>) -> tensor<1x16x257x80xf32>

    // Attention @ V
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x16x257x257xf32>, tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xf32>

    // Convert back: typecast -> permute [0,2,1,3] -> reshape (merge heads)
    %attn_bf16 = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<1x16x257x80xf32>) -> tensor<1x16x257x80xbf16>
    %attn_permuted = "ttir.permute"(%attn_bf16) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x257x80xbf16>) -> tensor<1x257x16x80xbf16>
    %result = "ttir.reshape"(%attn_permuted) <{shape = [1 : i32, 257 : i32, 1280 : i32]}> : (tensor<1x257x16x80xbf16>) -> tensor<1x257x1280xbf16>

    return %result : tensor<1x257x1280xbf16>
  }

  // Q was prepared with:
  // - Learnable queries (16 tokens) from linear projection output (flattened).
  // - Reshape (split heads) -> permute [0,2,1,3] -> typecast bf16 -> f32.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - Image features (273 tokens) from linear projection output (flattened).
  // - Reshape (split heads) -> permute [0,2,1,3] -> typecast bf16 -> f32.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  // - Scaling applied to K (multiply by a constant) after transpose and before QK^T.
  //
  // V was prepared with:
  // - Image features (273 tokens) from linear projection output (flattened).
  // - Reshape (split heads) -> permute [0,2,1,3] -> typecast bf16 -> f32.
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  // - No explicit additive mask.
  // - Different sequence lengths: Q_seq=16, KV_seq=273 (cross-attention).
  // - Same num_heads for Q/K/V (no GQA).
  //
  // CLIP Resampler cross-attention pattern
  //
  // CHECK-LABEL: func.func @sdpa_clip_resampler_cross_attention
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_clip_resampler_cross_attention(
      %arg0: tensor<1x16x1280xbf16>,   // Q projection output (learnable queries)
      %arg1: tensor<1x273x1280xbf16>,  // K projection output (image features)
      %arg2: tensor<1x273x1280xbf16>   // V projection output (image features)
  ) -> tensor<1x16x1280xbf16> {
    // Constants - 20 heads, head_dim=64
    %cst_scale_q = "ttir.constant"() <{value = dense<0.353553385> : tensor<1x20x16x64xf32>}> : () -> tensor<1x20x16x64xf32>
    %cst_scale_k = "ttir.constant"() <{value = dense<0.353553385> : tensor<1x20x64x273xf32>}> : () -> tensor<1x20x64x273xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<1x20x16x273xf64>}> : () -> tensor<1x20x16x273xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x20x16x273xf32>}> : () -> tensor<1x20x16x273xf32>

    // Q: reshape (split heads) -> permute [0,2,1,3] -> typecast -> scale
    %q_reshaped = "ttir.reshape"(%arg0) <{shape = [1 : i32, 16 : i32, 20 : i32, 64 : i32]}> : (tensor<1x16x1280xbf16>) -> tensor<1x16x20x64xbf16>
    %q_permuted = "ttir.permute"(%q_reshaped) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x16x20x64xbf16>) -> tensor<1x20x16x64xbf16>
    %q_f32 = "ttir.typecast"(%q_permuted) <{conservative_folding = false}> : (tensor<1x20x16x64xbf16>) -> tensor<1x20x16x64xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<1x20x16x64xf32>, tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xf32>

    // K: reshape (split heads) -> permute [0,2,1,3] -> typecast -> permute (K^T) -> scale
    %k_reshaped = "ttir.reshape"(%arg1) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<1x273x1280xbf16>) -> tensor<1x273x20x64xbf16>
    %k_permuted = "ttir.permute"(%k_reshaped) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16>
    %k_f32 = "ttir.typecast"(%k_permuted) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x20x273x64xf32>) -> tensor<1x20x64x273xf32>
    %k_scaled = "ttir.multiply"(%k_transposed, %cst_scale_k) : (tensor<1x20x64x273xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x64x273xf32>

    // Q @ K^T using dot_general
    %qk = "ttir.dot_general"(%q_scaled, %k_scaled) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x64xf32>, tensor<1x20x64x273xf32>) -> tensor<1x20x16x273xf32>

    // No additive mask - only NaN-safe softmax handling
    %qk_f64 = "ttir.typecast"(%qk) <{conservative_folding = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<1x20x16x273xf64>, tensor<1x20x16x273xf64>) -> tensor<1x20x16x273xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<1x20x16x273xi1>) -> tensor<1x20x16x273xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xi1>) -> tensor<1x20x16xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xi1>) -> tensor<1x20x16x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xi1>) -> tensor<1x20x16x273xi1>

    // Decomposed softmax
    %max_val = "ttir.max"(%qk) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32>
    %shifted = "ttir.subtract"(%qk, %max_broadcast) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x20x16x273xf32>) -> tensor<1x20x16xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [1 : i32, 20 : i32, 16 : i32, 1 : i32]}> : (tensor<1x20x16xf32>) -> tensor<1x20x16x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 273>}> : (tensor<1x20x16x1xf32>) -> tensor<1x20x16x273xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32>

    // Post-softmax masking
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<1x20x16x273xi1>, tensor<1x20x16x273xf32>, tensor<1x20x16x273xf32>) -> tensor<1x20x16x273xf32>

    // V: reshape (split heads) -> permute [0,2,1,3] -> typecast
    %v_reshaped = "ttir.reshape"(%arg2) <{shape = [1 : i32, 273 : i32, 20 : i32, 64 : i32]}> : (tensor<1x273x1280xbf16>) -> tensor<1x273x20x64xbf16>
    %v_permuted = "ttir.permute"(%v_reshaped) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x273x20x64xbf16>) -> tensor<1x20x273x64xbf16>
    %v_f32 = "ttir.typecast"(%v_permuted) <{conservative_folding = false}> : (tensor<1x20x273x64xbf16>) -> tensor<1x20x273x64xf32>

    // Attention @ V
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x20x16x273xf32>, tensor<1x20x273x64xf32>) -> tensor<1x20x16x64xf32>

    // Convert back: typecast -> permute [0,2,1,3] -> reshape (merge heads)
    %attn_bf16 = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<1x20x16x64xf32>) -> tensor<1x20x16x64xbf16>
    %attn_permuted = "ttir.permute"(%attn_bf16) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x20x16x64xbf16>) -> tensor<1x16x20x64xbf16>
    %result = "ttir.reshape"(%attn_permuted) <{shape = [1 : i32, 16 : i32, 1280 : i32]}> : (tensor<1x16x20x64xbf16>) -> tensor<1x16x1280xbf16>

    return %result : tensor<1x16x1280xbf16>
  }

  // Q was prepared with:
  // - Packed QKV projection output followed by head split:
  //   reshape -> permute -> slice_static (Q) -> reshape.
  // - Typecast bf16 -> f32 before attention compute.
  // - Scaling applied to Q (multiply by a constant) before QK^T.
  //
  // K was prepared with:
  // - Packed QKV projection output followed by head split:
  //   reshape -> permute -> slice_static (K) -> reshape.
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  //
  // V was prepared with:
  // - Packed QKV projection output followed by head split:
  //   reshape -> permute -> slice_static (V) -> reshape.
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> add bias -> softmax decomposition -> PV (ttir.dot_general),
  //   including NaN-safe handling (all -inf rows -> zeros).
  //
  // Swin window attention pattern (packed QKV + slice_static split)
  //
  // CHECK-LABEL: func.func @sdpa_swin_window_attention_packed_qkv
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_swin_window_attention_packed_qkv(
    %arg0: tensor<361x49x288xbf16>,    // Packed QKV (3 * heads * head_dim)
    %arg1: tensor<361x1x49x49xbf16>    // Bias (e.g. relative position), per window
  ) -> tensor<361x3x49x32xbf16> {
    // Constants.
    %cst_scale_q = "ttir.constant"() <{value = dense<1.767580e-01> : tensor<361x3x49x32xf32>}> : () -> tensor<361x3x49x32xf32>
    %cst_neg_inf_f64 = "ttir.constant"() <{value = dense<0xFFF0000000000000> : tensor<361x3x49x49xf64>}> : () -> tensor<361x3x49x49xf64>
    %cst_zero = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<361x3x49x49xf32>}> : () -> tensor<361x3x49x49xf32>

    // Split packed QKV -> Q/K/V in [B, H, S, D] where B=361, H=3, S=49, D=32.
    %qkv_5d = "ttir.reshape"(%arg0) <{shape = [361 : i32, 49 : i32, 3 : i32, 3 : i32, 32 : i32]}> : (tensor<361x49x288xbf16>) -> tensor<361x49x3x3x32xbf16>
    %qkv_perm = "ttir.permute"(%qkv_5d) <{permutation = array<i64: 2, 0, 3, 1, 4>}> : (tensor<361x49x3x3x32xbf16>) -> tensor<3x361x3x49x32xbf16>

    %q_slice = "ttir.slice_static"(%qkv_perm) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 361 : i32, 3 : i32, 49 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<3x361x3x49x32xbf16>) -> tensor<1x361x3x49x32xbf16>
    %k_slice = "ttir.slice_static"(%qkv_perm) <{begins = [1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [2 : i32, 361 : i32, 3 : i32, 49 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<3x361x3x49x32xbf16>) -> tensor<1x361x3x49x32xbf16>
    %v_slice = "ttir.slice_static"(%qkv_perm) <{begins = [2 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [3 : i32, 361 : i32, 3 : i32, 49 : i32, 32 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<3x361x3x49x32xbf16>) -> tensor<1x361x3x49x32xbf16>

    %q = "ttir.reshape"(%q_slice) <{shape = [361 : i32, 3 : i32, 49 : i32, 32 : i32]}> : (tensor<1x361x3x49x32xbf16>) -> tensor<361x3x49x32xbf16>
    %k = "ttir.reshape"(%k_slice) <{shape = [361 : i32, 3 : i32, 49 : i32, 32 : i32]}> : (tensor<1x361x3x49x32xbf16>) -> tensor<361x3x49x32xbf16>
    %v = "ttir.reshape"(%v_slice) <{shape = [361 : i32, 3 : i32, 49 : i32, 32 : i32]}> : (tensor<1x361x3x49x32xbf16>) -> tensor<361x3x49x32xbf16>

    // Q: typecast and scale.
    %q_f32 = "ttir.typecast"(%q) <{conservative_folding = false}> : (tensor<361x3x49x32xbf16>) -> tensor<361x3x49x32xf32>
    %q_scaled = "ttir.multiply"(%q_f32, %cst_scale_q) : (tensor<361x3x49x32xf32>, tensor<361x3x49x32xf32>) -> tensor<361x3x49x32xf32>

    // K: typecast and transpose for matmul.
    %k_f32 = "ttir.typecast"(%k) <{conservative_folding = false}> : (tensor<361x3x49x32xbf16>) -> tensor<361x3x49x32xf32>
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<361x3x49x32xf32>) -> tensor<361x3x32x49xf32>

    // Q @ K^T -> [361, 3, 49, 49].
    %qk = "ttir.dot_general"(%q_scaled, %k_transposed) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<361x3x49x32xf32>, tensor<361x3x32x49xf32>) -> tensor<361x3x49x49xf32>

    // Add attention bias (match model IR: typecast -> reshape -> reshape -> broadcast).
    %bias_f32 = "ttir.typecast"(%arg1) <{conservative_folding = false}> : (tensor<361x1x49x49xbf16>) -> tensor<361x1x49x49xf32>
    %bias_3d = "ttir.reshape"(%bias_f32) <{shape = [361 : i32, 49 : i32, 49 : i32]}> : (tensor<361x1x49x49xf32>) -> tensor<361x49x49xf32>
    %bias_4d = "ttir.reshape"(%bias_3d) <{shape = [361 : i32, 1 : i32, 49 : i32, 49 : i32]}> : (tensor<361x49x49xf32>) -> tensor<361x1x49x49xf32>
    %bias_broadcast = "ttir.broadcast"(%bias_4d) <{broadcast_dimensions = array<i64: 1, 3, 1, 1>}> : (tensor<361x1x49x49xf32>) -> tensor<361x3x49x49xf32>
    %qk_biased = "ttir.add"(%qk, %bias_broadcast) : (tensor<361x3x49x49xf32>, tensor<361x3x49x49xf32>) -> tensor<361x3x49x49xf32>

    // NaN-safe softmax: detect all-inf rows using f64 comparison.
    %qk_f64 = "ttir.typecast"(%qk_biased) <{conservative_folding = false}> : (tensor<361x3x49x49xf32>) -> tensor<361x3x49x49xf64>
    %is_neg_inf = "ttir.eq"(%qk_f64, %cst_neg_inf_f64) : (tensor<361x3x49x49xf64>, tensor<361x3x49x49xf64>) -> tensor<361x3x49x49xi1>
    %not_neg_inf = "ttir.logical_not"(%is_neg_inf) : (tensor<361x3x49x49xi1>) -> tensor<361x3x49x49xi1>
    %has_valid = "ttir.reduce_or"(%not_neg_inf) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<361x3x49x49xi1>) -> tensor<361x3x49xi1>
    %has_valid_reshaped = "ttir.reshape"(%has_valid) <{shape = [361 : i32, 3 : i32, 49 : i32, 1 : i32]}> : (tensor<361x3x49xi1>) -> tensor<361x3x49x1xi1>
    %all_inf = "ttir.logical_not"(%has_valid_reshaped) : (tensor<361x3x49x1xi1>) -> tensor<361x3x49x1xi1>
    %all_inf_broadcast = "ttir.broadcast"(%all_inf) <{broadcast_dimensions = array<i64: 1, 1, 1, 49>}> : (tensor<361x3x49x1xi1>) -> tensor<361x3x49x49xi1>

    // Softmax: max -> subtract -> exp -> sum -> div.
    %max_val = "ttir.max"(%qk_biased) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<361x3x49x49xf32>) -> tensor<361x3x49xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [361 : i32, 3 : i32, 49 : i32, 1 : i32]}> : (tensor<361x3x49xf32>) -> tensor<361x3x49x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 49>}> : (tensor<361x3x49x1xf32>) -> tensor<361x3x49x49xf32>
    %shifted = "ttir.subtract"(%qk_biased, %max_broadcast) : (tensor<361x3x49x49xf32>, tensor<361x3x49x49xf32>) -> tensor<361x3x49x49xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<361x3x49x49xf32>) -> tensor<361x3x49x49xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<361x3x49x49xf32>) -> tensor<361x3x49xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [361 : i32, 3 : i32, 49 : i32, 1 : i32]}> : (tensor<361x3x49xf32>) -> tensor<361x3x49x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 49>}> : (tensor<361x3x49x1xf32>) -> tensor<361x3x49x49xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<361x3x49x49xf32>, tensor<361x3x49x49xf32>) -> tensor<361x3x49x49xf32>

    // Handle all-inf rows: output zeros instead of NaN.
    %safe_softmax = "ttir.where"(%all_inf_broadcast, %cst_zero, %softmax) : (tensor<361x3x49x49xi1>, tensor<361x3x49x49xf32>, tensor<361x3x49x49xf32>) -> tensor<361x3x49x49xf32>

    // Attention @ V -> [361, 3, 49, 32].
    %v_f32 = "ttir.typecast"(%v) <{conservative_folding = false}> : (tensor<361x3x49x32xbf16>) -> tensor<361x3x49x32xf32>
    %attn_out = "ttir.dot_general"(%safe_softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<361x3x49x49xf32>, tensor<361x3x49x32xf32>) -> tensor<361x3x49x32xf32>

    // Typecast back to bf16.
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<361x3x49x32xf32>) -> tensor<361x3x49x32xbf16>
    return %result : tensor<361x3x49x32xbf16>
  }

  // Q was prepared with:
  // - Precomputed in the model graph as [batch, heads, q_seq, head_dim], where q_seq is large.
  // - Typecast bf16 -> f32 before attention compute.
  //
  // K was prepared with:
  // - Spatial-reduction attention: key/value sequence is smaller than query sequence.
  // - Typecast bf16 -> f32 before attention compute.
  // - Transpose for matmul: permute [0, 1, 3, 2] to form K^T.
  //
  // V was prepared with:
  // - Spatial-reduction attention: value sequence is smaller than query sequence.
  // - Typecast bf16 -> f32 before PV (attention @ V).
  //
  // SDPA is:
  // - QK^T (ttir.dot_general) -> post-matmul scaling (div) -> softmax decomposition -> PV (ttir.dot_general).
  //
  // SegFormer vision attention pattern (long-seq, no KV cache)
  //
  // CHECK-LABEL: func.func @sdpa_segformer_long_seq_attention
  // CHECK: "ttnn.scaled_dot_product_attention"
  func.func @sdpa_segformer_long_seq_attention(
    %arg0: tensor<1x1x16384x32xbf16>,  // Q
    %arg1: tensor<1x1x256x32xbf16>,    // K
    %arg2: tensor<1x1x256x32xbf16>     // V
  ) -> tensor<1x1x16384x32xbf16> {
    // Post-matmul scaling constant (matches SegFormer IR style: divide after QK^T).
    %cst_scale_div = "ttir.constant"() <{value = dense<5.656250e+00> : tensor<1x1x16384x256xf32>}> : () -> tensor<1x1x16384x256xf32>

    // Q/K/V: typecast to f32.
    %q_f32 = "ttir.typecast"(%arg0) <{conservative_folding = false}> : (tensor<1x1x16384x32xbf16>) -> tensor<1x1x16384x32xf32>
    %k_f32 = "ttir.typecast"(%arg1) <{conservative_folding = false}> : (tensor<1x1x256x32xbf16>) -> tensor<1x1x256x32xf32>
    %v_f32 = "ttir.typecast"(%arg2) <{conservative_folding = false}> : (tensor<1x1x256x32xbf16>) -> tensor<1x1x256x32xf32>

    // K^T: [1, 1, 256, 32] -> [1, 1, 32, 256].
    %k_transposed = "ttir.permute"(%k_f32) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x1x256x32xf32>) -> tensor<1x1x32x256xf32>

    // Q @ K^T -> [1, 1, 16384, 256].
    %qk = "ttir.dot_general"(%q_f32, %k_transposed) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x1x16384x32xf32>, tensor<1x1x32x256xf32>) -> tensor<1x1x16384x256xf32>

    // Scale scores (SegFormer-style: divide after QK^T).
    %qk_scaled = "ttir.div"(%qk, %cst_scale_div) : (tensor<1x1x16384x256xf32>, tensor<1x1x16384x256xf32>) -> tensor<1x1x16384x256xf32>

    // Softmax: max -> subtract -> exp -> sum -> div.
    %max_val = "ttir.max"(%qk_scaled) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x1x16384x256xf32>) -> tensor<1x1x16384xf32>
    %max_reshaped = "ttir.reshape"(%max_val) <{shape = [1 : i32, 1 : i32, 16384 : i32, 1 : i32]}> : (tensor<1x1x16384xf32>) -> tensor<1x1x16384x1xf32>
    %max_broadcast = "ttir.broadcast"(%max_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 256>}> : (tensor<1x1x16384x1xf32>) -> tensor<1x1x16384x256xf32>
    %shifted = "ttir.subtract"(%qk_scaled, %max_broadcast) : (tensor<1x1x16384x256xf32>, tensor<1x1x16384x256xf32>) -> tensor<1x1x16384x256xf32>
    %exp_val = "ttir.exp"(%shifted) : (tensor<1x1x16384x256xf32>) -> tensor<1x1x16384x256xf32>
    %sum_val = "ttir.sum"(%exp_val) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x1x16384x256xf32>) -> tensor<1x1x16384xf32>
    %sum_reshaped = "ttir.reshape"(%sum_val) <{shape = [1 : i32, 1 : i32, 16384 : i32, 1 : i32]}> : (tensor<1x1x16384xf32>) -> tensor<1x1x16384x1xf32>
    %sum_broadcast = "ttir.broadcast"(%sum_reshaped) <{broadcast_dimensions = array<i64: 1, 1, 1, 256>}> : (tensor<1x1x16384x1xf32>) -> tensor<1x1x16384x256xf32>
    %softmax = "ttir.div"(%exp_val, %sum_broadcast) : (tensor<1x1x16384x256xf32>, tensor<1x1x16384x256xf32>) -> tensor<1x1x16384x256xf32>

    // Attention @ V -> [1, 1, 16384, 32].
    %attn_out = "ttir.dot_general"(%softmax, %v_f32) <{batch_dims_lhs = array<i64: 0, 1>, batch_dims_rhs = array<i64: 0, 1>, contract_dims_lhs = array<i64: 3>, contract_dims_rhs = array<i64: 2>}> : (tensor<1x1x16384x256xf32>, tensor<1x1x256x32xf32>) -> tensor<1x1x16384x32xf32>

    // Typecast back to bf16.
    %result = "ttir.typecast"(%attn_out) <{conservative_folding = false}> : (tensor<1x1x16384x32xf32>) -> tensor<1x1x16384x32xbf16>
    return %result : tensor<1x1x16384x32xbf16>
  }

}
