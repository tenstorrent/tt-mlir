// SPDX-FileCopyrightText: (c) 2024 Tenstorrent AI ULC
//
// SPDX-License-Identifier: Apache-2.0

#include "ttmlir/Conversion/StableHLOToTTIR/ShardingUtils.h"

#include "mlir/IR/BuiltinTypes.h"
#include "mlir/IR/Operation.h"
#include "mlir/IR/Value.h"
#include "mlir/Transforms/DialectConversion.h"
#include "llvm/ADT/SmallVector.h"

#include <numeric>

namespace mlir {
namespace tt {
namespace sharding_utils {

// Parse GSPMD devices string and fill out MeshSharding info.
llvm::Expected<bool> MeshSharding::parseGSPMDDevicesStr(StringRef devicesStr) {
  // This function extract dimensions from targetDimsStr "[x,y,z]" and saves it
  // to targetDims.
  auto parseDimsFromDimensionStr = [](StringRef targetDimsStr,
                                      SmallVector<int64_t> &targetDims,
                                      bool checkSquareBracket = true) -> bool {
    if (checkSquareBracket && (!targetDimsStr.consume_front("[") ||
                               !targetDimsStr.consume_back("]"))) {
      return false;
    }
    SmallVector<StringRef> dimsStr;
    targetDimsStr.split(dimsStr, ",");
    targetDims.clear();
    for (auto dim : dimsStr) {
      int64_t d;
      if (dim.getAsInteger<int64_t>(10, d)) {
        return false;
      }
      targetDims.push_back(d);
    }
    return true;
  };

  // devciesStr can be appended by either (1) TileAssignmentDevices or
  // (2) '<=[` IotaReshapeDimensions `]` [`T` (IotaTransposeDimensions)]
  bool reshapeDevicesStrParsing = devicesStr.contains("<=");

  // devicesStr is generated by splitting whole string using space " ". Thus,
  // it is not supposed to include any trailing space. e.g.,
  // "[4,2,1]<=[2,4]T(1,0)" or "[2,4]0,1,2,3,4,5,6,7".
  auto firstClosingBracketIdx = devicesStr.find(']');
  if (firstClosingBracketIdx <= 1 ||
      firstClosingBracketIdx == StringRef::npos) {
    return llvm::createStringError(
        "Fail to parse GSPMD devices string [x,y,..]: " + devicesStr);
  }
  auto axesStr = devicesStr.take_front(firstClosingBracketIdx + 1);
  auto restStr = devicesStr.drop_front(firstClosingBracketIdx + 1);
  // Parse devices string e.g., [4,2,1] or [2,4].
  if (!parseDimsFromDimensionStr(axesStr, shardShape)) {
    return llvm::createStringError("Fail to parse GSPMD devices axes string: " +
                                   axesStr);
  }

  if (reshapeDevicesStrParsing) {
    // Parse devices string after "<=" e.g., [8] or [2,4]T(1,0).
    auto [reshapeStr, unused] = restStr.drop_front(2).split("T");
    // Parse reshape[0] string e.g., [8] or [2,4].
    if (!parseDimsFromDimensionStr(reshapeStr, meshShape)) {
      return llvm::createStringError(
          "Fail to parse GSPMD devices reshape string: " + reshapeStr);
    }
    deviceIds.clear();
    // Parse devices string after "]" e.g., 0,1,2,3,4,5,6,7.
  } else if (!parseDimsFromDimensionStr(restStr, deviceIds, false)) {
    return llvm::createStringError("Fail to parse GSPMD device id string: " +
                                   restStr);
  } else {
    // Set meshShape as the size of deviceIds such as [8] because we cannot
    // determine meshShape from the list of device ids.
    meshShape.clear();
    meshShape.push_back(deviceIds.size());
  }
  return true;
}

// Based on current MeshSharding info, finalize sharding dimensions.
llvm::Expected<bool> MeshSharding::determineGSPMDShardingDims() {
  // This code is based on following assumption.
  // 1. Hardware mesh is two dimenion such as 2x4, 1x2, ...
  // 2. Hardware mesh only supports either line or mesh config
  // e.g., t3k 1x8 or 2x4
  SmallVector<int64_t> orgShardShape = shardShape;
  if (lastTileDimReplicate) {
    shardShape.pop_back();
  }
  // Determine obvious properties first.
  bool reverseOrder = meshShape.size() != 1;
  // totalDevices is the total number of multi-chips such as 8 for t3k. Thus, no
  // overflow is expected with int64_t.
  int64_t totalDevices =
      std::accumulate(meshShape.begin(), meshShape.end(), int64_t{1},
                      std::multiplies<int64_t>());
  // Detect line device config (1xN).
  bool isLineDeviceConfig =
      llvm::any_of(orgShardShape, [&](int64_t s) { return s == totalDevices; });
  // Detect hardware mesh. For reverse order sharding, meshShape already
  // includes hardware mesh. For non reverse order case, extract hardware mesh
  // by traversing from front to back and picking none-zero values.
  if (!reverseOrder) {
    if (isLineDeviceConfig) {
      // Device with line config must be 1xN, not Nx1.
      meshShape = {1, meshShape[0]};
    } else {
      meshShape.clear();
      // e.g., orgShardShape [1,2,4] or [2,1,4] leads to [2,4]
      llvm::copy_if(orgShardShape, std::back_inserter(meshShape),
                    [](int64_t s) { return s != int64_t{1}; });
      if (!deviceIds.empty() && deviceIds[0] + 1 != deviceIds[1]) {
        // transposed shardShape if devicIds are not consecutive, so reverse the
        // meshShape. [4,2] leads to [2,4]
        std::reverse(meshShape.begin(), meshShape.end());
        reverseOrder = true;
      }
    }
  }

  if (meshShape.size() != 2) {
    // Currently, we are only supporting 2d hardware mesh config.
    return llvm::createStringError(
        "Only support 2d hardware mesh config. mesh.size()=%d",
        meshShape.size());
  }

  // Determine shardDims based on the shardShape and meshShape.
  // shard_dims indicate in which dimension we shard the tensor. For T3K,
  // detected meshShape will be [2, 4] and shard_dims will be [ a, b ] depending
  // on the sharding intention.
  // For example, if shardShape is [1,2,1,4], shard_dims is supposed to be [1,
  // 3] or if shardShape is [1,4,1,2], then shard_dims should be [3, 1].
  shardDims.assign(meshShape.size(), -1);
  // Skip the first 1 of 1xN hardware.
  uint64_t shardingCnt = isLineDeviceConfig;
  for (uint64_t i = 0; i < shardShape.size(); ++i) {
    // Check sharding dimension only.
    if (shardShape[i] != 1) {
      auto shardDimIdx =
          (reverseOrder) ? (meshShape.size() - 1 - shardingCnt) : shardingCnt;
      // Positive shardShape[i] and meshShape[shardDimIdx] is supposed to be
      // identical.
      if (shardShape[i] > 0 && shardShape[i] != meshShape[shardDimIdx]) {
        return llvm::createStringError(
            "Fail to determine shardDims. shardShape[%d] (%d) != meshShape[%d] "
            "(%d)",
            i, shardShape[i], shardDimIdx, meshShape[shardDimIdx]);
      }
      shardDims[shardDimIdx] = i;
      shardingCnt++;
    }
  }

  return true;
}

// OpenXLA has its own lexer, but we will use simple string-based parser here.
// This parsing is mainly based on "Sharding Attribute" section in
// https://github.com/sdasgup3/stablehlo/blob/80082431d1af0933e6202ecc8a6f8801e039235b/docs/spec.md#sharding-attribute
llvm::Expected<bool>
MeshSharding::convertGSPMDShardingToMeshSharding(StringRef shardingStr) {
  shardType = mlir::tt::ttcore::MeshShardType::Identity;
  lastTileDimReplicate = false;

  // Parse string and tokenize.
  if (!shardingStr.consume_front("{") || !shardingStr.consume_back("}")) {
    return llvm::createStringError(std::errc::invalid_argument,
                                   "Fail to parse GSPMD sharding.");
  }
  SmallVector<StringRef> shardingStrTokens;
  shardingStr.split(shardingStrTokens, " ");

  // Parse string tokens.
  for (auto str : shardingStrTokens) {
    if (str.contains("manual")) {
      // manual: already sharded, so no action is needed
      if (shardType != ttcore::MeshShardType::Identity) {
        return llvm::createStringError(std::errc::invalid_argument,
                                       "Fail to parse GSPMD sharding.");
      }
      setNonDevicesShardType(ttcore::MeshShardType::Identity);
    } else if (str.contains("replicated")) {
      // replicated: all devices have whole data
      if (shardType != ttcore::MeshShardType::Identity) {
        return llvm::createStringError(std::errc::invalid_argument,
                                       "Fail to parse GSPMD sharding.");
      }
      setNonDevicesShardType(ttcore::MeshShardType::Replicate);
    } else if (str.contains("maximal")) {
      // maximal: one device has whole data
      if (shardType != ttcore::MeshShardType::Identity) {
        return llvm::createStringError(std::errc::invalid_argument,
                                       "Fail to parse GSPMD sharding.");
      }
      setNonDevicesShardType(ttcore::MeshShardType::Maximal);
    } else if (str.consume_front("device=")) {
      // maximal should followed by "device" to put data on
      if (shardType != ttcore::MeshShardType::Maximal) {
        return llvm::createStringError(std::errc::invalid_argument,
                                       "Fail to parse GSPMD sharding.");
      }
      int64_t d;
      if (str.getAsInteger<int64_t>(10, d)) {
        return llvm::createStringError(std::errc::invalid_argument,
                                       "Fail to parse GSPMD sharding.");
      }
      deviceIds.push_back(d);
    } else if (str.consume_front("devices=")) {
      // other: "devices" detail sharding plan
      if (shardType != ttcore::MeshShardType::Identity) {
        return llvm::createStringError(std::errc::invalid_argument,
                                       "Fail to parse GSPMD sharding.");
      }
      shardType = ttcore::MeshShardType::Devices;
      auto error = parseGSPMDDevicesStr(str);
      if (auto e = error.takeError()) {
        return e;
      }
    } else if (str.contains("last_tile_dim_replicate")) {
      // other: replicate last tile dim
      if (shardType != ttcore::MeshShardType::Devices) {
        return llvm::createStringError(
            std::errc::invalid_argument,
            "Fail to parse GSPMD sharding in last_tile_dim_replicate.");
      }
      lastTileDimReplicate = true;
    } else {
      return llvm::createStringError("Unknown GSPMD sharding: " + str);
    }
  }

  // Determine shard dims for devices.
  if (shardType == ttcore::MeshShardType::Devices) {
    auto error = determineGSPMDShardingDims();
    if (auto e = error.takeError()) {
      return e;
    }
  }

  return true;
}

// Determine mesh_shard op creation and shard_type.
//
// foundSharding   shardType
//    true          replicate : no mesh_shard op
//    true          devices   : dummy mesh_shard op (identity)
//    false         replicate : mesh_shard op (replicate)
//    false         devices   : mesh_shard op (devices)
bool MeshSharding::determineMeshShardOpCreationAndShardType(
    bool foundSharding) {
  assert(shardType == mlir::tt::ttcore::MeshShardType::Replicate ||
         shardType == mlir::tt::ttcore::MeshShardType::Devices);

  // If JAX expects pre-sharded input/return (foundSharding) and if it is
  // replicate, do not create mesh_shard op as the input/output shapes are
  // identical.
  if (foundSharding &&
      shardType == mlir::tt::ttcore::MeshShardType::Replicate) {
    return false;
  }

  // If JAX expects pre-sharded input/return (foundSharding) and if it is not
  // replicate, mesh_shard op with dummy operation is still necessary for
  // input/output shape conversion purpose.
  if (foundSharding) {
    setDummyShardingOp(); // shardType = Identity
  }

  return true;
}

// Check and remove arg sharding attribute and determine if mesh_shard op needs
// to be created or not.
bool MeshSharding::checkAndUpdateGSPMDArgSharding(
    mlir::PatternRewriter &rewriter, mlir::stablehlo::CustomCallOp srcOp,
    mlir::StringAttr shardingAttr) {
  auto funcOp = srcOp->getParentOfType<mlir::func::FuncOp>();
  bool foundArgSharding = false;
  mlir::tt::ttcore::TensorMeshShardingAttr tensorMeshShardingAttr;

  if (auto blockArg =
          mlir::dyn_cast<mlir::BlockArgument>(srcOp->getOperand(0))) {
    auto argNum = blockArg.getArgNumber();
    foundArgSharding = checkAndRemoveFuncArgSharding<mlir::StringAttr>(
        rewriter, funcOp, argNum, shardingAttr, tensorMeshShardingAttr,
        mlir::tt::sharding_utils::kXlaShardingAttr);
  }

  return determineMeshShardOpCreationAndShardType(foundArgSharding);
}

// Check and remove ret sharding attribute and determine if mesh_shard op needs
// to be created or not.
bool MeshSharding::checkAndUpdateGSPMDRetSharding(
    mlir::PatternRewriter &rewriter, mlir::stablehlo::CustomCallOp srcOp,
    mlir::StringAttr shardingAttr) {
  auto funcOp = srcOp->getParentOfType<mlir::func::FuncOp>();
  bool foundRetSharding = false;
  mlir::tt::ttcore::TensorMeshShardingAttr tensorMeshShardingAttr;

  // Check if the GSPMD ShardToFull output is one of the return values.
  if (auto *funcReturnOp = funcOp.getBody().front().getTerminator()) {
    auto returnOperands = funcReturnOp->getOperands();
    auto returnOperandIt = llvm::find(returnOperands, srcOp->getResult(0));
    if (returnOperandIt != returnOperands.end()) {
      auto retIdx = std::distance(returnOperands.begin(), returnOperandIt);
      foundRetSharding = checkAndRemoveFuncReturnSharding<mlir::StringAttr>(
          rewriter, funcOp, retIdx, shardingAttr, tensorMeshShardingAttr,
          mlir::tt::sharding_utils::kXlaShardingAttr);
    }
  }

  return determineMeshShardOpCreationAndShardType(foundRetSharding);
}

// Parse Shardy sharding attribute.
llvm::Expected<bool>
MeshSharding::parseSdySharding(mlir::sdy::TensorShardingAttr sdySharding,
                               mlir::sdy::MeshAttr meshAttr) {

  shardShape.assign(sdySharding.getRank(), 1);
  shardDims.assign(meshAttr.getAxes().size(), -1);

  meshShape.clear();
  llvm::SmallDenseMap<::llvm::StringRef, int64_t> axisPosition;
  for (auto [idx, meshAxisAttr] : llvm::enumerate(meshAttr.getAxes())) {
    axisPosition[meshAxisAttr.getName()] = idx;
    meshShape.push_back(meshAxisAttr.getSize());
  }

  if (!sdySharding.isFullyClosed()) {
    return llvm::createStringError(
        "Sharding with open dimension is currently not supported.");
  }

  // Iterate each dimSharding in TensorShardingAttr
  for (auto [dimIdx, dimSharding] :
       llvm::enumerate(sdySharding.getDimShardings())) {
    for (auto [axisIdx, axes] : llvm::enumerate(dimSharding.getAxes())) {
      // Check if there is any subaxis sharding
      if (auto subAxis = axes.getSubAxisInfo()) {
        return llvm::createStringError(
            "Sharding with subaxis partitioning is currently not supported.");
      }
      shardShape[dimIdx] *= axes.getSize(meshAttr);
      // Sharding makes sense when it is higher than 1.
      if (axes.getSize(meshAttr) > 1) {
        shardDims[axisPosition[axes.getName()]] = dimIdx;
      }
    }
  }

  return true;
}

// Convert sdy.sharding to meshSharding based on sdy::MeshAttr.
llvm::Expected<bool> MeshSharding::convertSdyShardingToMeshSharding(
    sdy::TensorShardingAttr sdySharding, sdy::MeshAttr meshAttr,
    ttcore::MeshShardDirection direction) {

  // Empty meshAttr indicates single device, so no need to convert.
  if (meshAttr.empty()) {
    meshShape.clear();
    return true;
  }

  shardDirection = direction;
  meshName = sdySharding.getMeshName();

  if (meshAttr.getAxes().empty()) {
    if (meshAttr.getDeviceIds().empty()) {
      // replicated
      setNonDevicesShardType(mlir::tt::ttcore::MeshShardType::Replicate);
    } else {
      // maximal
      setNonDevicesShardType(mlir::tt::ttcore::MeshShardType::Maximal);
      deviceIds = llvm::SmallVector<int64_t>(meshAttr.getDeviceIds());
    }
    return true;
  }

  shardType = ttcore::MeshShardType::Devices;
  auto error = parseSdySharding(sdySharding, meshAttr);
  if (auto e = error.takeError()) {
    return e;
  }

  // totalPartition is the total number of multi-chips such as 8 for t3k. Thus,
  // no overflow is expected with int64_t.
  int64_t totalPartition =
      std::accumulate(shardShape.begin(), shardShape.end(), int64_t{1},
                      std::multiplies<int64_t>());
  // No partition indicates replicate to all devices.
  if (totalPartition == 1) {
    setNonDevicesShardType(mlir::tt::ttcore::MeshShardType::Replicate);
  }

  return true;
}

// Check and remove arg sharding attribute and determine if mesh_shard op needs
// to be created or not.
bool MeshSharding::checkAndUpdateShardyArgSharding(
    mlir::PatternRewriter &rewriter, mlir::func::FuncOp funcOp,
    mlir::Value argOperand, mlir::sdy::TensorShardingAttr shardingAttr) {

  bool foundArgSharding = false;
  if (auto blockArg = mlir::dyn_cast<mlir::BlockArgument>(argOperand)) {
    auto argNum = blockArg.getArgNumber();
    foundArgSharding =
        checkAndRemoveFuncArgSharding<mlir::sdy::TensorShardingAttr>(
            rewriter, funcOp, argNum, shardingAttr,
            getTensorMeshShardingAttr(rewriter), mlir::sdy::kShardingAttr);
  }

  return determineMeshShardOpCreationAndShardType(foundArgSharding);
}

// Check and remove ret sharding attribute and determine if mesh_shard op needs
// to be created or not.
bool MeshSharding::checkAndUpdateShardyRetSharding(
    mlir::PatternRewriter &rewriter, mlir::func::FuncOp funcOp, uint64_t retIdx,
    sdy::TensorShardingAttr shardingAttr) {

  bool foundRetSharding =
      checkAndRemoveFuncReturnSharding<sdy::TensorShardingAttr>(
          rewriter, funcOp, retIdx, shardingAttr,
          getTensorMeshShardingAttr(rewriter), mlir::sdy::kShardingAttr);

  return determineMeshShardOpCreationAndShardType(foundRetSharding);
}

// Get TensorMeshShardingAttr given MeshSharding info.
mlir::tt::ttcore::TensorMeshShardingAttr
MeshSharding::getTensorMeshShardingAttr(mlir::PatternRewriter &rewriter) {
  // Empty meshShape indicates single device, so no TensorMeshShardingAttr needs
  // to be created.
  if (meshShape.empty()) {
    return nullptr;
  }

  MLIRContext *context = rewriter.getContext();
  auto meshNameStrAttr = mlir::StringAttr::get(context, meshName);
  llvm::SmallVector<llvm::SmallVector<int64_t>> tensorAxes(
      shardShape.size(), llvm::SmallVector<int64_t>{});
  llvm::SmallVector<mlir::tt::ttcore::TensorMeshShardingAxisAttr>
      tensorMeshShardingAxisAttr;

  // Only devices has sharding info, so rest shard_types return empty
  // TensorMeshShardingAxis.
  if (shardType != mlir::tt::ttcore::MeshShardType::Devices) {
    return mlir::tt::ttcore::TensorMeshShardingAttr::get(
        context, meshNameStrAttr, tensorMeshShardingAxisAttr);
  }

  for (auto [shardIdx, shardDim] : llvm::enumerate(shardDims)) {
    // Non-neagtive shardDim means actual sharding in certain hardware
    // dimension. So, we would like to show only such info.
    if (shardDim < 0) {
      continue;
    }
    tensorAxes[shardDim].push_back(shardIdx);
  }

  for (auto [shape, axes] : llvm::zip_equal(shardShape, tensorAxes)) {
    tensorMeshShardingAxisAttr.push_back(
        mlir::tt::ttcore::TensorMeshShardingAxisAttr::get(context, shape,
                                                          axes));
  }

  return mlir::tt::ttcore::TensorMeshShardingAttr::get(
      context, meshNameStrAttr, tensorMeshShardingAxisAttr);
}

// Get MeshSharding given MeshAttr, TensorMeshShardingAttr, and
// MeshShardDirection.
void MeshSharding::extractMeshShardingFromTensorMeshShardingAttr(
    mlir::tt::ttcore::MeshAttr meshAttr,
    mlir::tt::ttcore::TensorMeshShardingAttr tensorMeshShardingAttr,
    mlir::tt::ttcore::MeshShardDirection direction) {
  meshName = meshAttr.getName().str();
  meshShape = llvm::SmallVector<int64_t>(meshAttr.getShape());
  shardDirection = direction;

  auto axes = tensorMeshShardingAttr.getTensorMeshShardingAxis();
  shardShape.resize(axes.size());
  shardDims.resize(meshShape.size(), -1);
  for (auto [dim, axis] : llvm::enumerate(axes)) {
    shardShape[dim] = axis.getShardShape();
    for (auto deviceDim : axis.getAxes()) {
      shardDims[deviceDim] = dim;
    }
  }

  shardType = mlir::tt::ttcore::MeshShardType::Devices;
}

FailureOr<std::unordered_map<std::string, std::string>>
MeshSharding::fillStrategyMapFromSharding(
    const mlir::tt::sharding_utils::MeshSharding &meshSharding,
    size_t num_devices) {
  std::unordered_map<std::string, std::string> strategy;
  mlir::tt::ttcore::MeshShardType meshType = meshSharding.getShardType();
  if (meshType == mlir::tt::ttcore::MeshShardType::Replicate) {
    // If there is only one device, the output will be replicated, but there is
    // no need to replicate.
    if (num_devices == 1) {
      strategy["strategy"] = "identity";
    } else {
      strategy["strategy"] = "replicate";
      strategy["replication_factor"] = std::to_string(num_devices);
    }
  } else if (meshType == mlir::tt::ttcore::MeshShardType::Devices) {
    llvm::ArrayRef<int64_t> meshShape = meshSharding.getMeshShape();
    assert(meshShape.size() == 2);
    strategy["strategy"] = "shard_2d";
    strategy["mesh_shape_y"] = std::to_string(meshShape[0]);
    strategy["mesh_shape_x"] = std::to_string(meshShape[1]);
  } else if (meshType == mlir::tt::ttcore::MeshShardType::Identity) {
    strategy["strategy"] = "identity";
  } else {
    return mlir::failure();
  }
  return strategy;
}

} // namespace sharding_utils
} // namespace tt
} // namespace mlir
