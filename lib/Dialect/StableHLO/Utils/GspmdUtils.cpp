// SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
//
// SPDX-License-Identifier: Apache-2.0

#include "ttmlir/Dialect/StableHLO/Utils/GspmdUtils.h"
#include "ttmlir/Dialect/StableHLO/Utils/MeshShardingUtils.h"
#include "ttmlir/Dialect/TTCore/IR/TTCore.h"
#include "ttmlir/Dialect/TTCore/IR/TTCoreOpsTypes.h"

#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/Dialect/Tensor/IR/Tensor.h"
#include "mlir/IR/Builders.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/BuiltinTypes.h"
#include "mlir/IR/MLIRContext.h"
#include "mlir/IR/Operation.h"
#include "mlir/IR/Value.h"
#include "mlir/Transforms/DialectConversion.h"
#include "llvm/ADT/SmallVector.h"

namespace mlir::tt::gspmd_utils {

#ifdef TTMLIR_ENABLE_STABLEHLO

// Parse meshes from the GSPMD module.
llvm::Expected<llvm::SmallVector<llvm::SmallVector<int64_t>>>
parseMeshesFromGspmdModule(mlir::ModuleOp &module) {
  llvm::SmallVector<llvm::SmallVector<int64_t>> meshes;

  // Walk through the module and find all GSPMD mesh annotations.
  mlir::WalkResult moduleResult = module.walk([&](func::FuncOp funcOp) {
    mlir::WalkResult funcOpResult = funcOp.walk([&](Operation *op) {
      if (!mlir::isa<mlir::stablehlo::CustomCallOp>(op)) {
        return WalkResult::advance();
      }

      mlir::stablehlo::CustomCallOp customCallOp =
          mlir::cast<mlir::stablehlo::CustomCallOp>(op);

      // Check call target name to see if it's the one we are interested in.
      auto callTargetName = customCallOp.getCallTargetNameAttr();
      if (callTargetName != gspmd_utils::kSPMDFullToShardShapeCallTargetName &&
          callTargetName != gspmd_utils::kSPMDShardToFullShapeCallTargetName) {
        return WalkResult::advance();
      }

      // We want to extract the mhlo.sharding attribute from the
      // CustomCallOp.
      auto opShardingAttr = dyn_cast_if_present<mlir::StringAttr>(
          customCallOp.getOperation()->getAttr(
              mlir::tt::gspmd_utils::kXlaShardingAttr));
      if (!opShardingAttr) {
        return WalkResult::interrupt();
      }

      // We also want to extract the mhlo.sharding attribute from this op's
      // @Sharding operand.
      auto shardingOperand = customCallOp->getOperand(0);
      auto definingOp =
          shardingOperand.getDefiningOp<mlir::stablehlo::CustomCallOp>();
      auto operandShardingAttr = dyn_cast_if_present<mlir::StringAttr>(
          definingOp.getOperation()->getAttr(
              mlir::tt::gspmd_utils::kXlaShardingAttr));

      if (!operandShardingAttr) {
        return WalkResult::interrupt();
      }

      // Once extracted, we can generate the GSPMDMeshSharding object.
      llvm::Expected<mlir::tt::gspmd_utils::GSPMDMeshSharding>
          gspmdMeshSharding =
              mlir::tt::gspmd_utils::GSPMDMeshSharding::generate(
                  opShardingAttr.getValue(), operandShardingAttr.getValue(),
                  mlir::tt::ttcore::ShardStatus::Unsharded);
      if (auto err = gspmdMeshSharding.takeError()) {
        return WalkResult::interrupt();
      }

      // Some stablehlo custom calls may not have a mesh shape, so we
      // skip those.
      if (!gspmdMeshSharding->getMeshShape().empty()) {
        meshes.push_back(
            llvm::SmallVector<int64_t>(gspmdMeshSharding->getMeshShape()));
      }

      return WalkResult::advance();
    });

    if (funcOpResult.wasInterrupted()) {
      return WalkResult::interrupt();
    }

    return WalkResult::advance();
  });

  if (moduleResult.wasInterrupted() || meshes.empty()) {
    return llvm::createStringError(
        "Error parsing GSPMD annotations to determine the mesh shape.");
  }

  return meshes;
}

// Parse GSPMD devices string and fill out MeshSharding info.
llvm::Expected<bool>
parseGSPMDDevicesStr(StringRef devicesStr,
                     llvm::SmallVector<int64_t> &shardShape,
                     llvm::SmallVector<int64_t> &meshShape,
                     llvm::SmallVector<int64_t> &deviceIds) {
  // This function extract dimensions from targetDimsStr "[x,y,z]" and saves it
  // to targetDims.
  auto parseDimsFromDimensionStr = [](StringRef targetDimsStr,
                                      SmallVector<int64_t> &targetDims,
                                      bool checkSquareBracket = true) -> bool {
    if (checkSquareBracket && (!targetDimsStr.consume_front("[") ||
                               !targetDimsStr.consume_back("]"))) {
      return false;
    }
    SmallVector<StringRef> dimsStr;
    targetDimsStr.split(dimsStr, ",");
    targetDims.clear();
    for (auto dim : dimsStr) {
      int64_t d;
      if (dim.getAsInteger<int64_t>(10, d)) {
        return false;
      }
      targetDims.push_back(d);
    }
    return true;
  };

  // devciesStr can be appended by either (1) TileAssignmentDevices or
  // (2) '<=[` IotaReshapeDimensions `]` [`T` (IotaTransposeDimensions)]
  bool reshapeDevicesStrParsing = devicesStr.contains("<=");

  // devicesStr is generated by splitting whole string using space " ". Thus,
  // it is not supposed to include any trailing space. e.g.,
  // "[4,2,1]<=[2,4]T(1,0)" or "[2,4]0,1,2,3,4,5,6,7".
  auto firstClosingBracketIdx = devicesStr.find(']');
  if (firstClosingBracketIdx <= 1 ||
      firstClosingBracketIdx == StringRef::npos) {
    return llvm::createStringError(
        "Fail to parse GSPMD devices string [x,y,..]: " + devicesStr);
  }
  auto axesStr = devicesStr.take_front(firstClosingBracketIdx + 1);
  auto restStr = devicesStr.drop_front(firstClosingBracketIdx + 1);
  // Parse devices string e.g., [4,2,1] or [2,4].
  if (!parseDimsFromDimensionStr(axesStr, shardShape)) {
    return llvm::createStringError("Fail to parse GSPMD devices axes string: " +
                                   axesStr);
  }

  if (reshapeDevicesStrParsing) {
    // Parse devices string after "<=" e.g., [8] or [2,4]T(1,0).
    auto [reshapeStr, unused] = restStr.drop_front(2).split("T");
    // Parse reshape[0] string e.g., [8] or [2,4].
    if (!parseDimsFromDimensionStr(reshapeStr, meshShape)) {
      return llvm::createStringError(
          "Fail to parse GSPMD devices reshape string: " + reshapeStr);
    }
    deviceIds.clear();
    // Parse devices string after "]" e.g., 0,1,2,3,4,5,6,7.
  } else if (!parseDimsFromDimensionStr(restStr, deviceIds, false)) {
    return llvm::createStringError("Fail to parse GSPMD device id string: " +
                                   restStr);
  } else {
    // Set meshShape as the size of deviceIds such as [8] because we cannot
    // determine meshShape from the list of device ids.
    meshShape.clear();
    meshShape.push_back(deviceIds.size());
  }
  return true;
}

// Based on current MeshSharding info, finalize sharding dimensions.
llvm::Expected<bool>
determineGSPMDShardingDims(llvm::SmallVector<int64_t> &shardShape,
                           llvm::SmallVector<int64_t> &shardDims,
                           llvm::SmallVector<int64_t> &meshShape,
                           llvm::SmallVector<int64_t> &deviceIds,
                           bool lastTileDimReplicate) {
  // This code is based on following assumption.
  // 1. Hardware mesh is two dimenion such as 2x4, 1x2, ...
  // 2. Hardware mesh only supports either line or mesh config
  // e.g., t3k 1x8 or 2x4
  SmallVector<int64_t> orgShardShape = shardShape;
  if (lastTileDimReplicate) {
    shardShape.pop_back();
  }
  // Determine obvious properties first.
  bool reverseOrder = meshShape.size() != 1;
  // totalDevices is the total number of multi-chips such as 8 for t3k. Thus, no
  // overflow is expected with int64_t.
  int64_t totalDevices =
      std::accumulate(meshShape.begin(), meshShape.end(), int64_t{1},
                      std::multiplies<int64_t>());
  // Detect line device config (1xN).
  bool isLineDeviceConfig =
      llvm::any_of(orgShardShape, [&](int64_t s) { return s == totalDevices; });
  // Detect hardware mesh. For reverse order sharding, meshShape already
  // includes hardware mesh. For non reverse order case, extract hardware mesh
  // by traversing from front to back and picking none-zero values.
  if (!reverseOrder) {
    if (isLineDeviceConfig) {
      // Device with line config must be 1xN, not Nx1.
      meshShape = {1, meshShape[0]};
    } else {
      meshShape.clear();
      // e.g., orgShardShape [1,2,4] or [2,1,4] leads to [2,4]
      llvm::copy_if(orgShardShape, std::back_inserter(meshShape),
                    [](int64_t s) { return s != int64_t{1}; });
      if (!deviceIds.empty() && deviceIds[0] + 1 != deviceIds[1]) {
        // transposed shardShape if devicIds are not consecutive, so reverse the
        // meshShape. [4,2] leads to [2,4]
        std::reverse(meshShape.begin(), meshShape.end());
        reverseOrder = true;
      }
    }
  }

  if (meshShape.size() != 2) {
    // Currently, we are only supporting 2d hardware mesh config.
    return llvm::createStringError(
        "Only support 2d hardware mesh config. mesh.size()=%d",
        meshShape.size());
  }

  // Determine shardDims based on the shardShape and meshShape.
  // shard_dims indicate in which dimension we shard the tensor. For T3K,
  // detected meshShape will be [2, 4] and shard_dims will be [ a, b ] depending
  // on the sharding intention.
  // For example, if shardShape is [1,2,1,4], shard_dims is supposed to be [1,
  // 3] or if shardShape is [1,4,1,2], then shard_dims should be [3, 1].
  shardDims.assign(meshShape.size(), -1);
  // Skip the first 1 of 1xN hardware.
  uint64_t shardingCnt = isLineDeviceConfig;
  for (uint64_t i = 0; i < shardShape.size(); ++i) {
    // Check sharding dimension only.
    if (shardShape[i] != 1) {
      auto shardDimIdx =
          (reverseOrder) ? (meshShape.size() - 1 - shardingCnt) : shardingCnt;
      // Positive shardShape[i] and meshShape[shardDimIdx] is supposed to be
      // identical.
      if (shardShape[i] > 0 && shardShape[i] != meshShape[shardDimIdx]) {
        return llvm::createStringError(
            "Fail to determine shardDims. shardShape[%d] (%d) != meshShape[%d] "
            "(%d)",
            i, shardShape[i], shardDimIdx, meshShape[shardDimIdx]);
      }
      shardDims[shardDimIdx] = i;
      shardingCnt++;
    }
  }

  return true;
}

// OpenXLA has its own lexer, but we will use simple string-based parser here.
// This parsing is mainly based on "Sharding Attribute" section in
// https://github.com/sdasgup3/stablehlo/blob/80082431d1af0933e6202ecc8a6f8801e039235b/docs/spec.md#sharding-attribute
llvm::Expected<gspmd_utils::GSPMDMeshSharding>
gspmd_utils::GSPMDMeshSharding::generate(
    llvm::StringRef opShardingStr, llvm::StringRef operandShardingStr,
    mlir::tt::ttcore::ShardStatus shardStatus) {
  // Need to parse GSPMD sharding string and fill out MeshSharding info.
  mlir::tt::ttcore::MeshShardDirection shardDirection =
      mlir::tt::ttcore::MeshShardDirection::ShardToFull;
  mlir::tt::ttcore::MeshShardType shardType =
      mlir::tt::ttcore::MeshShardType::Identity;
  llvm::SmallVector<int64_t> shardShape = {-1};
  llvm::SmallVector<int64_t> shardDims = {-1};
  llvm::SmallVector<int64_t> meshShape = {-1};
  llvm::SmallVector<int64_t> deviceIds = {-1};
  bool lastTileDimReplicate = false;

  // Parse opShardingStr and tokenize.
  if (!opShardingStr.consume_front("{") || !opShardingStr.consume_back("}")) {
    return llvm::createStringError(
        std::errc::invalid_argument,
        "Fail to parse opShardingStr GSPMD sharding.");
  }
  llvm::SmallVector<llvm::StringRef> opShardingStrTokens;
  opShardingStr.split(opShardingStrTokens, " ");

  // Parse operandShardingStr and tokenize.
  if (!operandShardingStr.consume_front("{") ||
      !operandShardingStr.consume_back("}")) {
    return llvm::createStringError(
        std::errc::invalid_argument,
        "Fail to parse operandShardingStr GSPMD sharding.");
  }
  llvm::SmallVector<llvm::StringRef> operandShardingStrTokens;
  operandShardingStr.split(operandShardingStrTokens, " ");

  // Our goal is to map opShardingStr and operandShardingStr to a mesh shard op
  // which is represented by MeshSharding object. If any argument is
  // pre-sharded, we will create an identity mesh shard op by default. This is
  // to ensure the shapes are consistent in the graph but runtime will not
  // attempt to shard the data again. opShardingStr and operandShardingStr may
  // or may not have last_tile_dim_replicate annotated.

  // opShardingStr and operandShardingStr will have one of the following
  // combinations. All others are not legal. | opShardingStr |
  // operandShardingStr |	support       | shard_type (if not pre-sharded)
  // | replicated	   |  manual	           |  yes	          |
  // replicate | maximal	     |  manual	           |  not supported |
  // n/a | devices	     |  manual	           |  yes	          |
  // devices | manual	       |  replicated	       |  yes	          |
  // replicate | manual	       |  maximal	           |  not supported |
  // n/a | manual	       |  devices	           |  yes |   devices

  auto containsKeyword = [](const llvm::SmallVector<llvm::StringRef> &tokens,
                            std::string keyword) -> bool {
    return llvm::any_of(
        tokens, [&](llvm::StringRef str) { return str.contains(keyword); });
  };

  // Any combination which does not have manual is not supported.
  if (!containsKeyword(opShardingStrTokens, "manual") &&
      !containsKeyword(operandShardingStrTokens, "manual")) {
    return llvm::createStringError(
        std::errc::invalid_argument,
        "Fail to parse GSPMD sharding without manual.");
  }

  // Maximal is not supported with GSPMD sharding.
  if (containsKeyword(opShardingStrTokens, "maximal") ||
      containsKeyword(operandShardingStrTokens, "maximal")) {
    return llvm::createStringError(
        std::errc::invalid_argument,
        "Fail to parse GSPMD sharding with maximal.");
  }

  // Check for replicated sharding.
  if (containsKeyword(opShardingStrTokens, "replicated") ||
      containsKeyword(operandShardingStrTokens, "replicated")) {
    shardType = ttcore::MeshShardType::Replicate;
    shardShape = llvm::SmallVector<int64_t>{1};
    shardDims = llvm::SmallVector<int64_t>{-1};
    meshShape = llvm::SmallVector<int64_t>{-1};
  }

  // Check for last_tile_dim_replicate.
  if (containsKeyword(opShardingStrTokens, "last_tile_dim_replicate") ||
      containsKeyword(operandShardingStrTokens, "last_tile_dim_replicate")) {
    lastTileDimReplicate = true;
  }

  // Check for devices sharding.
  if (containsKeyword(opShardingStrTokens, "devices=") ||
      containsKeyword(operandShardingStrTokens, "devices=")) {
    shardType = ttcore::MeshShardType::Devices;

    // Extract device ids from the devices string.
    llvm::SmallVector<llvm::StringRef> shardingStrTokens =
        containsKeyword(opShardingStrTokens, "devices=")
            ? opShardingStrTokens
            : operandShardingStrTokens;

    for (auto str : shardingStrTokens) {
      if (str.consume_front("devices=")) {
        // Parse the devices string and fill out shardShape, meshShape and
        // deviceIds.
        auto error = mlir::tt::gspmd_utils::parseGSPMDDevicesStr(
            str, shardShape, meshShape, deviceIds);
        if (auto e = error.takeError()) {
          return e;
        }
      }
    }

    // Determine shard dims for devices.
    auto error = mlir::tt::gspmd_utils::determineGSPMDShardingDims(
        shardShape, shardDims, meshShape, deviceIds, lastTileDimReplicate);
    if (auto e = error.takeError()) {
      return e;
    }
  }

  // Check if the input is already pre-sharded. If it is, override shardType to
  // Identity.
  shardType = shardType == ttcore::MeshShardType::Identity
                  ? ttcore::MeshShardType::Identity
                  : shardType;

  return gspmd_utils::GSPMDMeshSharding{
      shardDirection,      shardType,           shardShape,
      shardDims,           meshShape,           deviceIds,
      shardStatus,         opShardingStr.str(), operandShardingStr.str(),
      lastTileDimReplicate};
}

#endif // #ifdef TTMLIR_ENABLE_STABLEHLO

} // namespace mlir::tt::gspmd_utils
