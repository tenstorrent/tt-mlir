#loc = loc(unknown)
#loc1 = loc("variables['params']['Conv_0']['bias']")
#loc2 = loc("variables['params']['Conv_0']['kernel']")
#loc3 = loc("variables['params']['Conv_1']['bias']")
#loc4 = loc("variables['params']['Conv_1']['kernel']")
#loc5 = loc("variables['params']['Conv_2']['bias']")
#loc6 = loc("variables['params']['Conv_2']['kernel']")
#loc7 = loc("variables['params']['Conv_3']['bias']")
#loc8 = loc("variables['params']['Conv_3']['kernel']")
#loc9 = loc("variables['params']['Conv_4']['bias']")
#loc10 = loc("variables['params']['Conv_4']['kernel']")
#loc11 = loc("variables['params']['Dense_0']['bias'].value")
#loc12 = loc("variables['params']['Dense_0']['kernel'].value")
#loc13 = loc("variables['params']['Dense_1']['bias'].value")
#loc14 = loc("variables['params']['Dense_1']['kernel'].value")
#loc15 = loc("variables['params']['Dense_2']['bias'].value")
#loc16 = loc("variables['params']['Dense_2']['kernel'].value")
#loc17 = loc("args[0]")
#loc28 = loc("shard_map")
#loc29 = loc("reduce_window_max")
module @jit__lambda attributes {mhlo.num_partitions = 2 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = <["X"=2]> loc(#loc)
  func.func public @main(%arg0: tensor<64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_0']['bias']"), %arg1: tensor<11x11x3x64xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_0']['kernel']"), %arg2: tensor<192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_1']['bias']"), %arg3: tensor<5x5x64x192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_1']['kernel']"), %arg4: tensor<384xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_2']['bias']"), %arg5: tensor<3x3x192x384xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_2']['kernel']"), %arg6: tensor<256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_3']['bias']"), %arg7: tensor<3x3x384x256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_3']['kernel']"), %arg8: tensor<256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_4']['bias']"), %arg9: tensor<3x3x256x256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Conv_4']['kernel']"), %arg10: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_0']['bias'].value"), %arg11: tensor<6400x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_0']['kernel'].value"), %arg12: tensor<4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_1']['bias'].value"), %arg13: tensor<4096x4096xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_1']['kernel'].value"), %arg14: tensor<1000xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_2']['bias'].value"), %arg15: tensor<4096x1000xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_2']['kernel'].value"), %arg16: tensor<8x224x224x3xi32> {sdy.sharding = #sdy.sharding<@mesh, [{"X"}, {}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>} loc("args[0]")) -> (tensor<8x1000xbf16> {jax.result_info = "result", sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{"X"}]>, <@mesh, [{}, {"X"}]>, <@mesh, [{"X"}]>, <@mesh, [{}, {"X"}]>, <@mesh, [{"X"}]>, <@mesh, [{}, {"X"}]>, <@mesh, [{"X"}, {}, {}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"X"} (%arg17: tensor<64xbf16> loc("shard_map"), %arg18: tensor<11x11x3x64xbf16> loc("shard_map"), %arg19: tensor<192xbf16> loc("shard_map"), %arg20: tensor<5x5x64x192xbf16> loc("shard_map"), %arg21: tensor<384xbf16> loc("shard_map"), %arg22: tensor<3x3x192x384xbf16> loc("shard_map"), %arg23: tensor<256xbf16> loc("shard_map"), %arg24: tensor<3x3x384x256xbf16> loc("shard_map"), %arg25: tensor<256xbf16> loc("shard_map"), %arg26: tensor<3x3x256x256xbf16> loc("shard_map"), %arg27: tensor<2048xbf16> loc("shard_map"), %arg28: tensor<6400x2048xbf16> loc("shard_map"), %arg29: tensor<2048xbf16> loc("shard_map"), %arg30: tensor<4096x2048xbf16> loc("shard_map"), %arg31: tensor<500xbf16> loc("shard_map"), %arg32: tensor<4096x500xbf16> loc("shard_map"), %arg33: tensor<4x224x224x3xi32> loc("shard_map")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc85)
      %cst_0 = stablehlo.constant dense<0xFF80> : tensor<bf16> loc(#loc)
      %1 = stablehlo.convert %arg33 : (tensor<4x224x224x3xi32>) -> tensor<4x224x224x3xbf16> loc(#loc86)
      %2 = stablehlo.convolution(%1, %arg18) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {stride = [4, 4]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x224x224x3xbf16>, tensor<11x11x3x64xbf16>) -> tensor<4x54x54x64xbf16> loc(#loc87)
      %3 = stablehlo.reshape %arg17 : (tensor<64xbf16>) -> tensor<1x1x1x64xbf16> loc(#loc88)
      %4 = stablehlo.broadcast_in_dim %3, dims = [0, 1, 2, 3] : (tensor<1x1x1x64xbf16>) -> tensor<4x54x54x64xbf16> loc(#loc89)
      %5 = stablehlo.add %2, %4 : tensor<4x54x54x64xbf16> loc(#loc89)
      %6 = func.call @relu(%5) : (tensor<4x54x54x64xbf16>) -> tensor<4x54x54x64xbf16> loc(#loc90)
      %7 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<bf16> loc(#loc91)
      %8 = "stablehlo.reduce_window"(%6, %7) <{window_dimensions = array<i64: 1, 3, 3, 1>, window_strides = array<i64: 1, 2, 2, 1>}> ({
      ^bb0(%arg34: tensor<bf16> loc("reduce_window_max"), %arg35: tensor<bf16> loc("reduce_window_max")):
        %65 = stablehlo.maximum %arg34, %arg35 : tensor<bf16> loc(#loc91)
        stablehlo.return %65 : tensor<bf16> loc(#loc91)
      }) : (tensor<4x54x54x64xbf16>, tensor<bf16>) -> tensor<4x26x26x64xbf16> loc(#loc91)
      %9 = stablehlo.convolution(%8, %arg20) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {pad = [[2, 2], [2, 2]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x26x26x64xbf16>, tensor<5x5x64x192xbf16>) -> tensor<4x26x26x192xbf16> loc(#loc92)
      %10 = stablehlo.reshape %arg19 : (tensor<192xbf16>) -> tensor<1x1x1x192xbf16> loc(#loc93)
      %11 = stablehlo.broadcast_in_dim %10, dims = [0, 1, 2, 3] : (tensor<1x1x1x192xbf16>) -> tensor<4x26x26x192xbf16> loc(#loc94)
      %12 = stablehlo.add %9, %11 : tensor<4x26x26x192xbf16> loc(#loc94)
      %13 = func.call @relu_17(%12) : (tensor<4x26x26x192xbf16>) -> tensor<4x26x26x192xbf16> loc(#loc90)
      %14 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<bf16> loc(#loc91)
      %15 = "stablehlo.reduce_window"(%13, %14) <{window_dimensions = array<i64: 1, 3, 3, 1>, window_strides = array<i64: 1, 2, 2, 1>}> ({
      ^bb0(%arg34: tensor<bf16> loc("reduce_window_max"), %arg35: tensor<bf16> loc("reduce_window_max")):
        %65 = stablehlo.maximum %arg34, %arg35 : tensor<bf16> loc(#loc91)
        stablehlo.return %65 : tensor<bf16> loc(#loc91)
      }) : (tensor<4x26x26x192xbf16>, tensor<bf16>) -> tensor<4x12x12x192xbf16> loc(#loc91)
      %16 = stablehlo.convolution(%15, %arg22) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x12x12x192xbf16>, tensor<3x3x192x384xbf16>) -> tensor<4x12x12x384xbf16> loc(#loc95)
      %17 = stablehlo.reshape %arg21 : (tensor<384xbf16>) -> tensor<1x1x1x384xbf16> loc(#loc96)
      %18 = stablehlo.broadcast_in_dim %17, dims = [0, 1, 2, 3] : (tensor<1x1x1x384xbf16>) -> tensor<4x12x12x384xbf16> loc(#loc97)
      %19 = stablehlo.add %16, %18 : tensor<4x12x12x384xbf16> loc(#loc97)
      %20 = func.call @relu_24(%19) : (tensor<4x12x12x384xbf16>) -> tensor<4x12x12x384xbf16> loc(#loc90)
      %21 = stablehlo.convolution(%20, %arg24) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x12x12x384xbf16>, tensor<3x3x384x256xbf16>) -> tensor<4x12x12x256xbf16> loc(#loc98)
      %22 = stablehlo.reshape %arg23 : (tensor<256xbf16>) -> tensor<1x1x1x256xbf16> loc(#loc99)
      %23 = stablehlo.broadcast_in_dim %22, dims = [0, 1, 2, 3] : (tensor<1x1x1x256xbf16>) -> tensor<4x12x12x256xbf16> loc(#loc100)
      %24 = stablehlo.add %21, %23 : tensor<4x12x12x256xbf16> loc(#loc100)
      %25 = func.call @relu_30(%24) : (tensor<4x12x12x256xbf16>) -> tensor<4x12x12x256xbf16> loc(#loc90)
      %26 = stablehlo.convolution(%25, %arg26) dim_numbers = [b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f], window = {pad = [[1, 1], [1, 1]]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<4x12x12x256xbf16>, tensor<3x3x256x256xbf16>) -> tensor<4x12x12x256xbf16> loc(#loc101)
      %27 = stablehlo.reshape %arg25 : (tensor<256xbf16>) -> tensor<1x1x1x256xbf16> loc(#loc102)
      %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1, 2, 3] : (tensor<1x1x1x256xbf16>) -> tensor<4x12x12x256xbf16> loc(#loc103)
      %29 = stablehlo.add %26, %28 : tensor<4x12x12x256xbf16> loc(#loc103)
      %30 = func.call @relu_30(%29) : (tensor<4x12x12x256xbf16>) -> tensor<4x12x12x256xbf16> loc(#loc90)
      %31 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<bf16> loc(#loc91)
      %32 = "stablehlo.reduce_window"(%30, %31) <{window_dimensions = array<i64: 1, 3, 3, 1>, window_strides = array<i64: 1, 2, 2, 1>}> ({
      ^bb0(%arg34: tensor<bf16> loc("reduce_window_max"), %arg35: tensor<bf16> loc("reduce_window_max")):
        %65 = stablehlo.maximum %arg34, %arg35 : tensor<bf16> loc(#loc91)
        stablehlo.return %65 : tensor<bf16> loc(#loc91)
      }) : (tensor<4x12x12x256xbf16>, tensor<bf16>) -> tensor<4x5x5x256xbf16> loc(#loc91)
      %33 = stablehlo.reshape %32 : (tensor<4x5x5x256xbf16>) -> tensor<4x6400xbf16> loc(#loc104)
      %34 = "stablehlo.all_gather"(%33) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, use_global_device_ids}> : (tensor<4x6400xbf16>) -> tensor<8x6400xbf16> loc(#loc105)
      %35 = stablehlo.dot_general %34, %arg28, contracting_dims = [1] x [0] : (tensor<8x6400xbf16>, tensor<6400x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc106)
      %36 = stablehlo.reshape %arg27 : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc107)
      %37 = stablehlo.broadcast_in_dim %36, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc108)
      %38 = stablehlo.add %35, %37 : tensor<8x2048xbf16> loc(#loc108)
      %39 = func.call @relu_39(%38) : (tensor<8x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc90)
      %40 = "stablehlo.all_gather"(%39) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 2, type = 0>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, use_global_device_ids}> : (tensor<8x2048xbf16>) -> tensor<8x4096xbf16> loc(#loc105)
      %41 = stablehlo.dot_general %40, %arg30, contracting_dims = [1] x [0] : (tensor<8x4096xbf16>, tensor<4096x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc109)
      %42 = stablehlo.reshape %arg29 : (tensor<2048xbf16>) -> tensor<1x2048xbf16> loc(#loc110)
      %43 = stablehlo.broadcast_in_dim %42, dims = [0, 1] : (tensor<1x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc111)
      %44 = stablehlo.add %41, %43 : tensor<8x2048xbf16> loc(#loc111)
      %45 = func.call @relu_39(%44) : (tensor<8x2048xbf16>) -> tensor<8x2048xbf16> loc(#loc90)
      %46 = "stablehlo.all_gather"(%45) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 2, type = 0>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, use_global_device_ids}> : (tensor<8x2048xbf16>) -> tensor<8x4096xbf16> loc(#loc105)
      %47 = stablehlo.dot_general %46, %arg32, contracting_dims = [1] x [0] : (tensor<8x4096xbf16>, tensor<4096x500xbf16>) -> tensor<8x500xbf16> loc(#loc112)
      %48 = stablehlo.reshape %arg31 : (tensor<500xbf16>) -> tensor<1x500xbf16> loc(#loc113)
      %49 = stablehlo.broadcast_in_dim %48, dims = [0, 1] : (tensor<1x500xbf16>) -> tensor<8x500xbf16> loc(#loc114)
      %50 = stablehlo.add %47, %49 : tensor<8x500xbf16> loc(#loc114)
      %51 = "stablehlo.all_gather"(%50) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 3, type = 0>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, use_global_device_ids}> : (tensor<8x500xbf16>) -> tensor<8x1000xbf16> loc(#loc105)
      %52 = stablehlo.reduce(%51 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<8x1000xbf16>, tensor<bf16>) -> tensor<8xbf16> loc(#loc115)
      %53 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<8xbf16> loc(#loc116)
      %54 = stablehlo.maximum %53, %52 : tensor<8xbf16> loc(#loc116)
      %55 = stablehlo.broadcast_in_dim %54, dims = [0] : (tensor<8xbf16>) -> tensor<8x1xbf16> loc(#loc117)
      %56 = stablehlo.broadcast_in_dim %55, dims = [0, 1] : (tensor<8x1xbf16>) -> tensor<8x1000xbf16> loc(#loc118)
      %57 = stablehlo.subtract %51, %56 : tensor<8x1000xbf16> loc(#loc118)
      %58 = stablehlo.exponential %57 : tensor<8x1000xbf16> loc(#loc119)
      %59 = stablehlo.convert %58 : (tensor<8x1000xbf16>) -> tensor<8x1000xf32> loc(#loc120)
      %60 = stablehlo.reduce(%59 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<8x1000xf32>, tensor<f32>) -> tensor<8xf32> loc(#loc85)
      %61 = stablehlo.broadcast_in_dim %60, dims = [0] : (tensor<8xf32>) -> tensor<8x1xf32> loc(#loc117)
      %62 = stablehlo.convert %61 : (tensor<8x1xf32>) -> tensor<8x1xbf16> loc(#loc120)
      %63 = stablehlo.broadcast_in_dim %62, dims = [0, 1] : (tensor<8x1xbf16>) -> tensor<8x1000xbf16> loc(#loc121)
      %64 = stablehlo.divide %58, %63 : tensor<8x1000xbf16> loc(#loc121)
      sdy.return %64 : tensor<8x1000xbf16> loc(#loc84)
    } : (tensor<64xbf16>, tensor<11x11x3x64xbf16>, tensor<192xbf16>, tensor<5x5x64x192xbf16>, tensor<384xbf16>, tensor<3x3x192x384xbf16>, tensor<256xbf16>, tensor<3x3x384x256xbf16>, tensor<256xbf16>, tensor<3x3x256x256xbf16>, tensor<4096xbf16>, tensor<6400x4096xbf16>, tensor<4096xbf16>, tensor<4096x4096xbf16>, tensor<1000xbf16>, tensor<4096x1000xbf16>, tensor<8x224x224x3xi32>) -> tensor<8x1000xbf16> loc(#loc84)
    return %0 : tensor<8x1000xbf16> loc(#loc)
  } loc(#loc)
  func.func private @relu(%arg0: tensor<4x54x54x64xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<4x54x54x64xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc132)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<4x54x54x64xbf16> loc(#loc123)
    %1 = stablehlo.maximum %arg0, %0 : tensor<4x54x54x64xbf16> loc(#loc123)
    return %1 : tensor<4x54x54x64xbf16> loc(#loc132)
  } loc(#loc132)
  func.func private @relu_17(%arg0: tensor<4x26x26x192xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<4x26x26x192xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc133)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<4x26x26x192xbf16> loc(#loc125)
    %1 = stablehlo.maximum %arg0, %0 : tensor<4x26x26x192xbf16> loc(#loc125)
    return %1 : tensor<4x26x26x192xbf16> loc(#loc133)
  } loc(#loc133)
  func.func private @relu_24(%arg0: tensor<4x12x12x384xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<4x12x12x384xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc134)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<4x12x12x384xbf16> loc(#loc127)
    %1 = stablehlo.maximum %arg0, %0 : tensor<4x12x12x384xbf16> loc(#loc127)
    return %1 : tensor<4x12x12x384xbf16> loc(#loc134)
  } loc(#loc134)
  func.func private @relu_30(%arg0: tensor<4x12x12x256xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<4x12x12x256xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc135)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<4x12x12x256xbf16> loc(#loc129)
    %1 = stablehlo.maximum %arg0, %0 : tensor<4x12x12x256xbf16> loc(#loc129)
    return %1 : tensor<4x12x12x256xbf16> loc(#loc135)
  } loc(#loc135)
  func.func private @relu_39(%arg0: tensor<8x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<8x2048xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc136)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<bf16>) -> tensor<8x2048xbf16> loc(#loc131)
    %1 = stablehlo.maximum %arg0, %0 : tensor<8x2048xbf16> loc(#loc131)
    return %1 : tensor<8x2048xbf16> loc(#loc136)
  } loc(#loc136)
} loc(#loc)
#loc18 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/workloads/workload.py":71:19 to :61)
#loc19 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/runners/jax_device_runner.py":96:19 to :45)
#loc20 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/multichip/model/jax_multichip_model_tester.py":206:15 to :77)
#loc21 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/multichip/model/jax_multichip_model_tester.py":201:15 to :79)
#loc22 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/multichip/model/jax_multichip_model_tester.py":113:17 to :74)
#loc23 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/single_chip/model/model_tester.py":123:12 to :34)
#loc24 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/n300/models/tensor_parallel/alexnet/test_alexnet_n300.py":53:4 to :27)
#loc25 = loc("/usr/local/lib/python3.11/dist-packages/_pytest/python.py":157:13 to :37)
#loc26 = loc("/usr/local/lib/python3.11/dist-packages/pluggy/_callers.py":121:26 to :51)
#loc27 = loc("/usr/local/lib/python3.11/dist-packages/pluggy/_manager.py":120:15 to :76)
#loc30 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/n300/models/tensor_parallel/alexnet/model_implementation.py":50:12 to :22)
#loc31 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":1216:14 to :44)
#loc32 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":699:13 to :57)
#loc33 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":3022:13 to :78)
#loc34 = loc("/usr/local/lib/python3.11/dist-packages/flax/core/scope.py":1079:10 to :35)
#loc35 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":2240:11 to 2245:44)
#loc36 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/python_package/jax_plugin_tt/monkeypatch.py":285:88 to 287:13)
#loc37 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/n300/models/tensor_parallel/alexnet/model_implementation.py":62:12 to :22)
#loc38 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/n300/models/tensor_parallel/alexnet/model_implementation.py":74:12 to :22)
#loc39 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/n300/models/tensor_parallel/alexnet/model_implementation.py":85:12 to :22)
#loc40 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/n300/models/tensor_parallel/alexnet/model_implementation.py":121:12 to :22)
#loc41 = loc("Workload.execute"(#loc18))
#loc42 = loc("JaxDeviceRunner.run_on_multichip_device"(#loc19))
#loc43 = loc("JaxMultichipModelTester._run_on_multichip_device"(#loc20))
#loc44 = loc("JaxMultichipModelTester._run_on_tt_device"(#loc21))
#loc45 = loc("JaxMultichipModelTester._test_inference"(#loc22))
#loc46 = loc("ModelTester.test"(#loc23))
#loc47 = loc("test_alexnet_multichip_n300_inference"(#loc24))
#loc48 = loc("pytest_pyfunc_call"(#loc25))
#loc49 = loc("_multicall"(#loc26))
#loc50 = loc("PluginManager._hookexec"(#loc27))
#loc51 = loc("AlexNetMultichipModel.__call__"(#loc30))
#loc52 = loc("Module._call_wrapped_method"(#loc31))
#loc53 = loc("wrap_method_once.<locals>.wrapped_module_method"(#loc32))
#loc54 = loc("apply.<locals>.scope_fn"(#loc33))
#loc55 = loc("apply.<locals>.wrapper"(#loc34))
#loc56 = loc("Module.apply"(#loc35))
#loc57 = loc("_create_flax_apply_patch_config.<locals>.<lambda>.<locals>.<lambda>"(#loc36))
#loc58 = loc("AlexNetMultichipModel.__call__"(#loc37))
#loc59 = loc("AlexNetMultichipModel.__call__"(#loc38))
#loc60 = loc("AlexNetMultichipModel.__call__"(#loc39))
#loc61 = loc("AlexNetMultichipModel.__call__"(#loc40))
#loc62 = loc(callsite(#loc49 at #loc50))
#loc63 = loc(callsite(#loc42 at #loc43))
#loc64 = loc(callsite(#loc48 at #loc62))
#loc65 = loc(callsite(#loc41 at #loc63))
#loc66 = loc(callsite(#loc47 at #loc64))
#loc67 = loc(callsite(#loc57 at #loc65))
#loc68 = loc(callsite(#loc46 at #loc66))
#loc69 = loc(callsite(#loc56 at #loc67))
#loc70 = loc(callsite(#loc45 at #loc68))
#loc71 = loc(callsite(#loc55 at #loc69))
#loc72 = loc(callsite(#loc44 at #loc70))
#loc73 = loc(callsite(#loc54 at #loc71))
#loc74 = loc(callsite(#loc43 at #loc72))
#loc75 = loc(callsite(#loc53 at #loc73))
#loc76 = loc(callsite(#loc42 at #loc74))
#loc77 = loc(callsite(#loc52 at #loc75))
#loc78 = loc(callsite(#loc41 at #loc76))
#loc79 = loc(callsite(#loc51 at #loc77))
#loc80 = loc(callsite(#loc58 at #loc77))
#loc81 = loc(callsite(#loc59 at #loc77))
#loc82 = loc(callsite(#loc60 at #loc77))
#loc83 = loc(callsite(#loc61 at #loc77))
#loc84 = loc("jit(<lambda>)/shard_map"(#loc78))
#loc85 = loc("jit(<lambda>)/AlexNetMultichipModel/reduce_sum"(#loc78))
#loc86 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_0/convert_element_type"(#loc78))
#loc87 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_0/conv_general_dilated"(#loc78))
#loc88 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_0/reshape"(#loc78))
#loc89 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_0/add"(#loc78))
#loc90 = loc("jit(<lambda>)/AlexNetMultichipModel/jit(relu)"(#loc78))
#loc91 = loc("jit(<lambda>)/AlexNetMultichipModel/reduce_window_max"(#loc78))
#loc92 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_1/conv_general_dilated"(#loc78))
#loc93 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_1/reshape"(#loc78))
#loc94 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_1/add"(#loc78))
#loc95 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_2/conv_general_dilated"(#loc78))
#loc96 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_2/reshape"(#loc78))
#loc97 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_2/add"(#loc78))
#loc98 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_3/conv_general_dilated"(#loc78))
#loc99 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_3/reshape"(#loc78))
#loc100 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_3/add"(#loc78))
#loc101 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_4/conv_general_dilated"(#loc78))
#loc102 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_4/reshape"(#loc78))
#loc103 = loc("jit(<lambda>)/AlexNetMultichipModel/Conv_4/add"(#loc78))
#loc104 = loc("jit(<lambda>)/AlexNetMultichipModel/reshape"(#loc78))
#loc105 = loc("jit(<lambda>)/AlexNetMultichipModel/all_gather"(#loc78))
#loc106 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_0/dot_general"(#loc78))
#loc107 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_0/reshape"(#loc78))
#loc108 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_0/add"(#loc78))
#loc109 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_1/dot_general"(#loc78))
#loc110 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_1/reshape"(#loc78))
#loc111 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_1/add"(#loc78))
#loc112 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_2/dot_general"(#loc78))
#loc113 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_2/reshape"(#loc78))
#loc114 = loc("jit(<lambda>)/AlexNetMultichipModel/Dense_2/add"(#loc78))
#loc115 = loc("jit(<lambda>)/AlexNetMultichipModel/reduce_max"(#loc78))
#loc116 = loc("jit(<lambda>)/AlexNetMultichipModel/max"(#loc78))
#loc117 = loc("jit(<lambda>)/AlexNetMultichipModel/broadcast_in_dim"(#loc78))
#loc118 = loc("jit(<lambda>)/AlexNetMultichipModel/sub"(#loc78))
#loc119 = loc("jit(<lambda>)/AlexNetMultichipModel/exp"(#loc78))
#loc120 = loc("jit(<lambda>)/AlexNetMultichipModel/convert_element_type"(#loc78))
#loc121 = loc("jit(<lambda>)/AlexNetMultichipModel/div"(#loc78))
#loc122 = loc("jit"(#loc79))
#loc123 = loc("max"(#loc79))
#loc124 = loc("jit"(#loc80))
#loc125 = loc("max"(#loc80))
#loc126 = loc("jit"(#loc81))
#loc127 = loc("max"(#loc81))
#loc128 = loc("jit"(#loc82))
#loc129 = loc("max"(#loc82))
#loc130 = loc("jit"(#loc83))
#loc131 = loc("max"(#loc83))
#loc132 = loc("jit:"(#loc122))
#loc133 = loc("jit:"(#loc124))
#loc134 = loc("jit:"(#loc126))
#loc135 = loc("jit:"(#loc128))
#loc136 = loc("jit:"(#loc130))
