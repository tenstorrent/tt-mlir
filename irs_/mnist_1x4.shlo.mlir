#loc = loc(unknown)
#loc1 = loc("variables['params']['Dense_0']['bias'].value")
#loc2 = loc("variables['params']['Dense_0']['kernel'].value")
#loc3 = loc("variables['params']['Dense_1']['bias'].value")
#loc4 = loc("variables['params']['Dense_1']['kernel'].value")
#loc5 = loc("variables['params']['Dense_2']['bias'].value")
#loc6 = loc("variables['params']['Dense_2']['kernel'].value")
#loc7 = loc("variables['params']['Dense_3']['bias']")
#loc8 = loc("variables['params']['Dense_3']['kernel']")
#loc9 = loc("args[0]")
#loc20 = loc("shard_map")
module @jit__lambda attributes {mhlo.num_partitions = 4 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = <["X"=4]> loc(#loc)
  func.func public @main(%arg0: tensor<1024xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_0']['bias'].value"), %arg1: tensor<784x1024xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_0']['kernel'].value"), %arg2: tensor<512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_1']['bias'].value"), %arg3: tensor<1024x512xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_1']['kernel'].value"), %arg4: tensor<256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_2']['bias'].value"), %arg5: tensor<512x256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"X"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_2']['kernel'].value"), %arg6: tensor<10xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_3']['bias']"), %arg7: tensor<256x10xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>} loc("variables['params']['Dense_3']['kernel']"), %arg8: tensor<32x28x28x1xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>} loc("args[0]")) -> (tensor<32x10xf32> {jax.result_info = "result", sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}) {
    %0 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8) in_shardings=[<@mesh, [{"X"}]>, <@mesh, [{}, {"X"}]>, <@mesh, [{"X"}]>, <@mesh, [{}, {"X"}]>, <@mesh, [{"X"}]>, <@mesh, [{}, {"X"}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={"X"} (%arg9: tensor<256xbf16> loc("shard_map"), %arg10: tensor<784x256xbf16> loc("shard_map"), %arg11: tensor<128xbf16> loc("shard_map"), %arg12: tensor<1024x128xbf16> loc("shard_map"), %arg13: tensor<64xbf16> loc("shard_map"), %arg14: tensor<512x64xbf16> loc("shard_map"), %arg15: tensor<10xf32> loc("shard_map"), %arg16: tensor<256x10xf32> loc("shard_map"), %arg17: tensor<32x28x28x1xf32> loc("shard_map")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc64)
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
      %1 = stablehlo.reshape %arg17 : (tensor<32x28x28x1xf32>) -> tensor<32x784xf32> loc(#loc65)
      %2 = stablehlo.convert %arg10 : (tensor<784x256xbf16>) -> tensor<784x256xf32> loc(#loc66)
      %3 = stablehlo.convert %arg9 : (tensor<256xbf16>) -> tensor<256xf32> loc(#loc66)
      %4 = stablehlo.dot_general %1, %2, contracting_dims = [1] x [0] : (tensor<32x784xf32>, tensor<784x256xf32>) -> tensor<32x256xf32> loc(#loc67)
      %5 = stablehlo.reshape %3 : (tensor<256xf32>) -> tensor<1x256xf32> loc(#loc68)
      %6 = stablehlo.broadcast_in_dim %5, dims = [0, 1] : (tensor<1x256xf32>) -> tensor<32x256xf32> loc(#loc69)
      %7 = stablehlo.add %4, %6 : tensor<32x256xf32> loc(#loc69)
      %8 = func.call @relu(%7) : (tensor<32x256xf32>) -> tensor<32x256xf32> loc(#loc70)
      %9 = "stablehlo.all_gather"(%8) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>, use_global_device_ids}> : (tensor<32x256xf32>) -> tensor<32x1024xf32> loc(#loc71)
      %10 = stablehlo.convert %arg12 : (tensor<1024x128xbf16>) -> tensor<1024x128xf32> loc(#loc72)
      %11 = stablehlo.convert %arg11 : (tensor<128xbf16>) -> tensor<128xf32> loc(#loc72)
      %12 = stablehlo.dot_general %9, %10, contracting_dims = [1] x [0] : (tensor<32x1024xf32>, tensor<1024x128xf32>) -> tensor<32x128xf32> loc(#loc73)
      %13 = stablehlo.reshape %11 : (tensor<128xf32>) -> tensor<1x128xf32> loc(#loc74)
      %14 = stablehlo.broadcast_in_dim %13, dims = [0, 1] : (tensor<1x128xf32>) -> tensor<32x128xf32> loc(#loc75)
      %15 = stablehlo.add %12, %14 : tensor<32x128xf32> loc(#loc75)
      %16 = func.call @relu_15(%15) : (tensor<32x128xf32>) -> tensor<32x128xf32> loc(#loc70)
      %17 = "stablehlo.all_gather"(%16) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 2, type = 0>, replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>, use_global_device_ids}> : (tensor<32x128xf32>) -> tensor<32x512xf32> loc(#loc71)
      %18 = stablehlo.convert %arg14 : (tensor<512x64xbf16>) -> tensor<512x64xf32> loc(#loc76)
      %19 = stablehlo.convert %arg13 : (tensor<64xbf16>) -> tensor<64xf32> loc(#loc76)
      %20 = stablehlo.dot_general %17, %18, contracting_dims = [1] x [0] : (tensor<32x512xf32>, tensor<512x64xf32>) -> tensor<32x64xf32> loc(#loc77)
      %21 = stablehlo.reshape %19 : (tensor<64xf32>) -> tensor<1x64xf32> loc(#loc78)
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x64xf32>) -> tensor<32x64xf32> loc(#loc79)
      %23 = stablehlo.add %20, %22 : tensor<32x64xf32> loc(#loc79)
      %24 = func.call @relu_24(%23) : (tensor<32x64xf32>) -> tensor<32x64xf32> loc(#loc70)
      %25 = "stablehlo.all_gather"(%24) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 3, type = 0>, replica_groups = dense<[[0, 1, 2, 3]]> : tensor<1x4xi64>, use_global_device_ids}> : (tensor<32x64xf32>) -> tensor<32x256xf32> loc(#loc71)
      %26 = stablehlo.dot_general %25, %arg16, contracting_dims = [1] x [0] : (tensor<32x256xf32>, tensor<256x10xf32>) -> tensor<32x10xf32> loc(#loc80)
      %27 = stablehlo.reshape %arg15 : (tensor<10xf32>) -> tensor<1x10xf32> loc(#loc81)
      %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<32x10xf32> loc(#loc82)
      %29 = stablehlo.add %26, %28 : tensor<32x10xf32> loc(#loc82)
      %30 = stablehlo.reduce(%29 init: %cst_0) applies stablehlo.maximum across dimensions = [1] : (tensor<32x10xf32>, tensor<f32>) -> tensor<32xf32> loc(#loc83)
      %31 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<32xf32> loc(#loc84)
      %32 = stablehlo.maximum %31, %30 : tensor<32xf32> loc(#loc84)
      %33 = stablehlo.broadcast_in_dim %32, dims = [0] : (tensor<32xf32>) -> tensor<32x1xf32> loc(#loc85)
      %34 = stablehlo.broadcast_in_dim %33, dims = [0, 1] : (tensor<32x1xf32>) -> tensor<32x10xf32> loc(#loc86)
      %35 = stablehlo.subtract %29, %34 : tensor<32x10xf32> loc(#loc86)
      %36 = stablehlo.exponential %35 : tensor<32x10xf32> loc(#loc87)
      %37 = stablehlo.reduce(%36 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<32x10xf32>, tensor<f32>) -> tensor<32xf32> loc(#loc64)
      %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<32xf32>) -> tensor<32x1xf32> loc(#loc85)
      %39 = stablehlo.broadcast_in_dim %38, dims = [0, 1] : (tensor<32x1xf32>) -> tensor<32x10xf32> loc(#loc88)
      %40 = stablehlo.divide %36, %39 : tensor<32x10xf32> loc(#loc88)
      sdy.return %40 : tensor<32x10xf32> loc(#loc63)
    } : (tensor<1024xbf16>, tensor<784x1024xbf16>, tensor<512xbf16>, tensor<1024x512xbf16>, tensor<256xbf16>, tensor<512x256xbf16>, tensor<10xf32>, tensor<256x10xf32>, tensor<32x28x28x1xf32>) -> tensor<32x10xf32> loc(#loc63)
    return %0 : tensor<32x10xf32> loc(#loc)
  } loc(#loc)
  func.func private @relu(%arg0: tensor<32x256xf32> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<32x256xf32> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc91)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x256xf32> loc(#loc90)
    %1 = stablehlo.maximum %arg0, %0 : tensor<32x256xf32> loc(#loc90)
    return %1 : tensor<32x256xf32> loc(#loc91)
  } loc(#loc91)
  func.func private @relu_15(%arg0: tensor<32x128xf32> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<32x128xf32> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc91)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x128xf32> loc(#loc90)
    %1 = stablehlo.maximum %arg0, %0 : tensor<32x128xf32> loc(#loc90)
    return %1 : tensor<32x128xf32> loc(#loc91)
  } loc(#loc91)
  func.func private @relu_24(%arg0: tensor<32x64xf32> {ttcore.argument_type = #ttcore.argument_type<input>} loc(unknown)) -> tensor<32x64xf32> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc91)
    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x64xf32> loc(#loc90)
    %1 = stablehlo.maximum %arg0, %0 : tensor<32x64xf32> loc(#loc90)
    return %1 : tensor<32x64xf32> loc(#loc91)
  } loc(#loc91)
} loc(#loc)
#loc10 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/workloads/workload.py":71:19 to :61)
#loc11 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/runners/jax_device_runner.py":96:19 to :45)
#loc12 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/multichip/model/jax_multichip_model_tester.py":206:15 to :77)
#loc13 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/multichip/model/jax_multichip_model_tester.py":201:15 to :79)
#loc14 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/multichip/model/jax_multichip_model_tester.py":113:17 to :74)
#loc15 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/infra/testers/single_chip/model/model_tester.py":123:12 to :34)
#loc16 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/llmbox/4_devices/models/tensor_parallel/mnist_mlp/test_mnist_mlp.py":82:8 to :31)
#loc17 = loc("/usr/local/lib/python3.11/dist-packages/_pytest/python.py":157:13 to :37)
#loc18 = loc("/usr/local/lib/python3.11/dist-packages/pluggy/_callers.py":121:26 to :51)
#loc19 = loc("/usr/local/lib/python3.11/dist-packages/pluggy/_manager.py":120:15 to :76)
#loc21 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/tests/jax/multi_chip/n300/models/tensor_parallel/mnist_mlp/model_implementation.py":41:16 to :26)
#loc22 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":1216:14 to :44)
#loc23 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":699:13 to :57)
#loc24 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":3022:13 to :78)
#loc25 = loc("/usr/local/lib/python3.11/dist-packages/flax/core/scope.py":1079:10 to :35)
#loc26 = loc("/usr/local/lib/python3.11/dist-packages/flax/linen/module.py":2240:11 to 2245:44)
#loc27 = loc("/localdev/svuckovic/_workspace/repos/tt-xla/python_package/jax_plugin_tt/monkeypatch.py":285:88 to 287:13)
#loc28 = loc("Workload.execute"(#loc10))
#loc29 = loc("JaxDeviceRunner.run_on_multichip_device"(#loc11))
#loc30 = loc("JaxMultichipModelTester._run_on_multichip_device"(#loc12))
#loc31 = loc("JaxMultichipModelTester._run_on_tt_device"(#loc13))
#loc32 = loc("JaxMultichipModelTester._test_inference"(#loc14))
#loc33 = loc("ModelTester.test"(#loc15))
#loc34 = loc("test_mnist_mlp_multichip_llmbox_1x4_inference_shardy"(#loc16))
#loc35 = loc("pytest_pyfunc_call"(#loc17))
#loc36 = loc("_multicall"(#loc18))
#loc37 = loc("PluginManager._hookexec"(#loc19))
#loc38 = loc("MNISTMLPMultichipModel.__call__"(#loc21))
#loc39 = loc("Module._call_wrapped_method"(#loc22))
#loc40 = loc("wrap_method_once.<locals>.wrapped_module_method"(#loc23))
#loc41 = loc("apply.<locals>.scope_fn"(#loc24))
#loc42 = loc("apply.<locals>.wrapper"(#loc25))
#loc43 = loc("Module.apply"(#loc26))
#loc44 = loc("_create_flax_apply_patch_config.<locals>.<lambda>.<locals>.<lambda>"(#loc27))
#loc45 = loc(callsite(#loc36 at #loc37))
#loc46 = loc(callsite(#loc29 at #loc30))
#loc47 = loc(callsite(#loc35 at #loc45))
#loc48 = loc(callsite(#loc28 at #loc46))
#loc49 = loc(callsite(#loc34 at #loc47))
#loc50 = loc(callsite(#loc44 at #loc48))
#loc51 = loc(callsite(#loc33 at #loc49))
#loc52 = loc(callsite(#loc43 at #loc50))
#loc53 = loc(callsite(#loc32 at #loc51))
#loc54 = loc(callsite(#loc42 at #loc52))
#loc55 = loc(callsite(#loc31 at #loc53))
#loc56 = loc(callsite(#loc41 at #loc54))
#loc57 = loc(callsite(#loc30 at #loc55))
#loc58 = loc(callsite(#loc40 at #loc56))
#loc59 = loc(callsite(#loc29 at #loc57))
#loc60 = loc(callsite(#loc39 at #loc58))
#loc61 = loc(callsite(#loc28 at #loc59))
#loc62 = loc(callsite(#loc38 at #loc60))
#loc63 = loc("jit(<lambda>)/shard_map"(#loc61))
#loc64 = loc("jit(<lambda>)/MNISTMLPMultichipModel/reduce_sum"(#loc61))
#loc65 = loc("jit(<lambda>)/MNISTMLPMultichipModel/reshape"(#loc61))
#loc66 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_0/convert_element_type"(#loc61))
#loc67 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_0/dot_general"(#loc61))
#loc68 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_0/reshape"(#loc61))
#loc69 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_0/add"(#loc61))
#loc70 = loc("jit(<lambda>)/MNISTMLPMultichipModel/jit(relu)"(#loc61))
#loc71 = loc("jit(<lambda>)/MNISTMLPMultichipModel/all_gather"(#loc61))
#loc72 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_1/convert_element_type"(#loc61))
#loc73 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_1/dot_general"(#loc61))
#loc74 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_1/reshape"(#loc61))
#loc75 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_1/add"(#loc61))
#loc76 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_2/convert_element_type"(#loc61))
#loc77 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_2/dot_general"(#loc61))
#loc78 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_2/reshape"(#loc61))
#loc79 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_2/add"(#loc61))
#loc80 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_3/dot_general"(#loc61))
#loc81 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_3/reshape"(#loc61))
#loc82 = loc("jit(<lambda>)/MNISTMLPMultichipModel/Dense_3/add"(#loc61))
#loc83 = loc("jit(<lambda>)/MNISTMLPMultichipModel/reduce_max"(#loc61))
#loc84 = loc("jit(<lambda>)/MNISTMLPMultichipModel/max"(#loc61))
#loc85 = loc("jit(<lambda>)/MNISTMLPMultichipModel/broadcast_in_dim"(#loc61))
#loc86 = loc("jit(<lambda>)/MNISTMLPMultichipModel/sub"(#loc61))
#loc87 = loc("jit(<lambda>)/MNISTMLPMultichipModel/exp"(#loc61))
#loc88 = loc("jit(<lambda>)/MNISTMLPMultichipModel/div"(#loc61))
#loc89 = loc("jit"(#loc62))
#loc90 = loc("max"(#loc62))
#loc91 = loc("jit:"(#loc89))
