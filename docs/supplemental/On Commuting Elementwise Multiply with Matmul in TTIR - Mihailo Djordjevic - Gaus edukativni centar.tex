\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[serbian,english]{babel}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{microtype}
\usepackage{bm}
\usepackage{mathtools}
\let\openbox\relax
\usepackage{amsthm}
\usepackage[a4paper,margin=27mm,headsep=8mm,footskip=12mm]{geometry}
\usepackage{titlesec}
\usepackage{setspace}
\setstretch{1.08}
\titlespacing*{\section}{0pt}{1.3ex plus .5ex}{0.9ex}
\titlespacing*{\subsection}{0pt}{1.1ex plus .5ex}{0.7ex}
\titlespacing*{\subsubsection}{0pt}{1ex plus .3ex}{0.6ex}
\setlength{\parskip}{0.35em}
\setlength{\parindent}{12pt}
\usepackage{enumitem}
\setlist{itemsep=0.2em,topsep=0.3em,leftmargin=2.2em}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[unicode]{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  urlcolor=blue!55!black,
  pdfauthor={Mihailo Đorđević},
  pdftitle={On Commuting Elementwise Multiply with Matmul in TTIR}
}
\usepackage{cleveref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition} 
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}


\newcommand{\R}{\mathbb{R}}
\newcommand{\Had}{\circ}
\newcommand{\Mat}{\,}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\Diag}{\operatorname{Diag}}
\newcommand{\vecop}{\operatorname{vec}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\supp}{\operatorname{supp}}

\title{\vspace{-6mm}\Large On Commuting Elementwise Multiply with Matmul in TTIR:\\
Necessary and Sufficient Conditions, Rewrite Patterns, and Extensions}
\author{\foreignlanguage{serbian}{Mihailo Đorđević}}

\date{}

\begin{document}
\maketitle
\vspace{-6mm}

\begin{abstract}
This note studies when an elementwise product can be moved across a matrix multiplication without changing the numerical result. Two base transformations are considered: commuting the elementwise mask to the first operand of \texttt{matmul} and commuting it to the second operand. I give simple criteria that decide when each rewrite is valid. When the base rewrites fail, I discuss extensions that pre-process the left input through a fixed linear operator so that the expression can still be reordered in a useful way. Proofs and counterexamples are deferred to the appendices.
\end{abstract}

\tableofcontents
\clearpage

\section{Introduction and scope}
\label{sec:intro}
The TTIR graph often contains the pattern
\[
(A \Had B) \, C,
\]
where $A,B,C \in \R^{n\times n}$, $\Had$ denotes the Hadamard (elementwise) product, and juxtaposition denotes standard matrix multiplication. This work provides precise conditions for two shape preserving rewrites:
\begin{itemize}
  \item \textbf{Transformation A} (move the mask to the left input): find $B',C'$ depending only on $B,C$ such that
  \[
    (A \Had B) \, C = (A\,B') \Had C' \quad \text{for all } A.
  \]
  \item \textbf{Transformation B} (move the mask to the output of \texttt{matmul}): find $B',C'$ depending only on $B,C$ such that
  \[
    (A \Had B) \, C = A \Had (B' C') \quad \text{for all } A.
  \]
\end{itemize}
When neither transformation is globally valid, I also describe extended rewrites that introduce a fixed linear pre-processing of $A$ before the \texttt{matmul}.

\subsection{Notation}
All matrices are real $n\times n$ unless stated otherwise. For a vector $u\in\R^n$, $\diag(u)$ is the diagonal matrix with diagonal $u$. For a matrix $X$, $\vecop(X)$ stacks its columns into a vector. The support of a vector $x$ is $\supp(x)=\{i: x_i\ne 0\}$. The $k$th column of $X$ is $X_{\cdot k}$. I use $I$ for the identity, $\bm{1}$ for the all ones vector, and $J=\bm{1}\bm{1}^\top$ for the all ones matrix.

\subsection{Elementary facts used throughout}
I collect the basic identities that will be used in the analysis. Proofs are standard and omitted; they are recalled in Appendix~A for completeness.

\paragraph{Hadamard product.}
For $X,Y\in\R^{n\times n}$, the Hadamard product $X\Had Y$ is defined entrywise, $(X\Had Y)_{ij}=X_{ij}Y_{ij}$. It is bilinear, commutative and associative. Two interactions will be repeatedly used:
\begin{align}
X \Had (u v^\top) &= \diag(u)\,X\,\diag(v), \label{eq:had-rank1}\\
\vecop(X\Had Y) &= \Diag(\vecop(Y))\,\vecop(X). \label{eq:vec-had}
\end{align}
Identity \eqref{eq:had-rank1} says that a rank one mask $uv^\top$ induces left and right diagonal scalings. Identity \eqref{eq:vec-had} converts Hadamard multiplication into a diagonal linear map in the vectorized space.

\paragraph{Matrix multiplication and vectorization.}
For conforming matrices, $\vecop(AXB)=(B^\top\!\otimes A)\vecop(X)$. This turns matching of matrix identities into matching of linear maps in the vectorized form. I will use this to equate coefficients with respect to the basis matrices $E^{(ik)}$ that have a single one in position $(i,k)$.

\paragraph{Linearity in $A$.}
In both transformations, $B$ and $C$ are fixed while $A$ is arbitrary. Each side is linear in $A$. Equality for all $A$ holds if and only if the two linear maps agree on a basis, equivalently if the coefficient of each $a_{ik}$ matches in every output position. This is the key idea behind the criteria in Section~\ref{sec:dev-criteria} and the proofs in Appendix~A.

\subsection{Problem statement}
Given $B,C\in\R^{n\times n}$ and arbitrary $A\in\R^{n\times n}$, decide:
\begin{enumerate}[label=\textbf{Q\arabic*.},wide]
  \item When do there exist $B',C'$ depending only on $B,C$ such that $(A\Had B)C=(AB')\Had C'$ for all $A$?
  \item When do there exist $B',C'$ depending only on $B,C$ such that $(A\Had B)C=A\Had(B'C')$ for all $A$?
  \item If neither is possible in general, what structured cases allow it?
  \item If a base rewrite is not valid, can we insert a fixed linear pre-processing of $A$ to obtain a useful reordered form without changing the final result?
\end{enumerate}
\vspace{3cm}


\section{Main results}
\label{sec:dev-criteria}
This section states the conditions in plain English with minimal notation and many examples. Mathematical proofs are in Appendix~A.


\subsection{Transformation A: when can we move the mask to the left input}
\label{subsec:dev-A}

\paragraph{Target form.}
We want fixed $B',C'$ that depend only on $B,C$ such that
\[
(A \Had B)\,C \;=\; (A\,B') \Had C' \quad \text{for every } A.
\]
No shape changes and no extra operators are allowed.

\subsubsection*{Practical criterion}
Think column by column through $C$.

\begin{enumerate}[label=\arabic*)]
  \item Pick a column $j$ of $C$.
  \item Find the row indices where this column is nonzero:
        \[
        \mathrm{support}(C_{\cdot j})=\{\,k : C_{k j}\ne 0\,\}.
        \]
  \item Look at those columns of $B$: $B_{\cdot k}$ for all $k$ in that set.
  \item \textbf{Requirement.} All these $B_{\cdot k}$ must point in the same direction, that is, they must be scalar multiples of one another.
\end{enumerate}

If the requirement holds for every column $j$ of $C$, then the rewrite is valid. If it fails for any column, the rewrite is not valid.

\paragraph{Intuition.}
Each column of $C$ decides which columns of $B$ are allowed to interact in the same output column. When two or more columns of $B$ are mixed together by the same column of $C$, those $B$-columns must be proportional. If a column of $C$ is 1-sparse, there is nothing to check for that column.

\subsubsection*{Quick checklists}
\begin{itemize}
  \item Dense $C$ (no zeros): then every column of $C$ mixes all columns of $B$, so all columns of $B$ must be mutually proportional. Equivalently, $B$ must be rank 1.
  \item Each column of $C$ has at most one nonzero: the rewrite always passes for any $B$.
  \item Block-sparse $C$ where columns only use indices from disjoint blocks $\mathcal{K}_1,\dots,\mathcal{K}_r$: inside each block $\mathcal{K}_t$, all columns of $B$ must be proportional. Different blocks may use different directions.
\end{itemize}

\subsubsection*{How to build $B'$ and $C'$ when it passes}
For each column $j$ of $C$:
\begin{itemize}
  \item pick a unit direction $u^{(j)}$ equal to the common direction of the involved $B_{\cdot k}$,
  \item write $B_{\cdot k}=\alpha_k u^{(j)}$ for all $k$ with $C_{k j}\ne 0$,
  \item set $C'_{\cdot j}=u^{(j)}$ and $B'_{k j}=\alpha_k C_{k j}$, with zeros elsewhere.
\end{itemize}
Then $(A B') \Had C'$ reproduces $(A \Had B) C$ for all $A$.

\subsubsection*{Examples that pass}
\paragraph{Example A1. Overlapping supports, consistent proportionality.}
\[
B=\begin{bmatrix}
1 & 2 & 1\\
2 & 4 & 2\\
3 & 6 & 3
\end{bmatrix},
\qquad
C=\begin{bmatrix}
1 & 0 & 0\\
1 & 1 & 0\\
0 & 1 & 1
\end{bmatrix}.
\]
Column 1 of $C$ uses $k=\{1,2\}$, and $B_{\cdot 2}=2\,B_{\cdot 1}$.  
Column 2 uses $k=\{2,3\}$, and $B_{\cdot 3}=\tfrac12\,B_{\cdot 2}$.  
Column 3 uses $k=\{3\}$ only.  
All checks pass. The rewrite is valid.

\paragraph{Example A2. Dense $C$ with rank 1 $B$.}
Let $B=uv^\top$ with  
$u=\begin{bmatrix}1\\-2\\4\end{bmatrix}$ and $v=\begin{bmatrix}3\\1\\-5\end{bmatrix}$, and let $C$ be any dense matrix.  
Every column of $B$ is a scalar multiple of $u$, so the requirement holds automatically for every column of $C$. The rewrite is valid.

\subsubsection*{Examples that fail}
\paragraph{Example A3. The classic counterexample.}
\[
B=I_2,\qquad C=\begin{bmatrix}1&1\\[2pt]1&1\end{bmatrix}.
\]
Each column of $C$ uses both $k=1$ and $k=2$, but $B_{\cdot 1}=[1,0]^\top$ and $B_{\cdot 2}=[0,1]^\top$ are not proportional. The rewrite is invalid.

\paragraph{Example A4. Two disjoint mixes with inconsistent directions.}
\[
B=\begin{bmatrix}
1&1&0\\
0&1&1\\
0&0&2
\end{bmatrix},
\qquad
C=\begin{bmatrix}
1&0&0\\
1&1&0\\
0&1&1
\end{bmatrix}.
\]
Column 1 of $C$ uses $k=\{1,2\}$ but $B_{\cdot 1}=[1,0,0]^\top$ and $B_{\cdot 2}=[1,1,0]^\top$ are not proportional. The rewrite is invalid, no need to check other columns.

\bigskip
\subsection{Transformation B: when can we move the mask to the output}
\label{subsec:dev-B}

\paragraph{Target form.}
We want fixed $B',C'$ that depend only on $B,C$ such that
\[
(A \Had B)\,C \;=\; A \Had (B' C') \quad \text{for every } A.
\]

\subsubsection*{Practical criterion}
The output entry $(i,j)$ on the right depends only on $a_{ij}$. On the left, $(i,j)$ collects contributions from all $a_{ik}$ with weights $b_{ik} C_{k j}$. For equality for every $A$, all off diagonal contributions must vanish.

\begin{quote}
For each column $j$ of $C$ and each row index $k\ne j$, if $C_{k j}\ne 0$ then the entire column $k$ of $B$ must be zero.
\end{quote}

If the condition holds for all columns $j$, the rewrite is valid. Otherwise it is invalid.

\paragraph{Intuition.}
The right hand side cannot mix $a_{ik}$ with $k\ne j$ into entry $(i,j)$. Therefore the left hand side must not have such contributions either. The only way to kill them for every $i$ is that the whole column $k$ of $B$ is zero whenever $C_{k j}$ is nonzero with $k\ne j$.

\subsubsection*{Quick checklists}
\begin{itemize}
  \item If $C$ is diagonal, the rewrite always passes for any $B$.
  \item If $C$ has an off diagonal nonzero at $(k,j)$ and the $k$th column of $B$ is not identically zero, the rewrite fails immediately.
  \item You may keep off diagonal nonzeros in column $j$ only in rows $k$ whose column $k$ of $B$ is entirely zero.
\end{itemize}

\subsubsection*{How to build $B'$ and $C'$ when it passes}
Let $D=\diag(C_{11},\dots,C_{nn})$. Set $B'=B$ and $C'=D$. Then
\[
A \Had (B' C') \;=\; A \Had (B D),
\]
and because all off diagonal paths have been eliminated by the condition above, this equals $(A \Had B) C$ for every $A$.

\subsubsection*{Examples that pass}
\paragraph{Example B1. Diagonal $C$, arbitrary $B$.}
\[
C=\begin{bmatrix}
c_{11}&0&0\\
0&c_{22}&0\\
0&0&c_{33}
\end{bmatrix}.
\]
No off diagonal entries exist, so the condition holds. Set $C'=\diag(c_{11},c_{22},c_{33})$ and $B'=B$. The rewrite is valid.

\paragraph{Example B2. Off diagonals target only zero columns of $B$.}
\[
B=\begin{bmatrix}
0&2&0\\
0&5&0\\
0&7&0
\end{bmatrix},
\qquad
C=\begin{bmatrix}
0&1&0\\
0&1&0\\
2&0&3
\end{bmatrix}.
\]
Column 1 of $C$ has an off diagonal at $k=3$ but column 3 of $B$ is zero.  
Column 2 of $C$ has an off diagonal at $k=1$ but column 1 of $B$ is zero.  
Column 3 of $C$ is diagonal only. The rewrite is valid.

\subsubsection*{Examples that fail}
\paragraph{Example B3. Single off diagonal breaks it.}
\[
B=I_3,\qquad
C=\begin{bmatrix}
1&0&0\\
1&1&0\\
0&0&1
\end{bmatrix}.
\]
Column 1 of $C$ has $C_{21}=1$ with $k=2\ne 1$ and column 2 of $B$ is not zero. The rewrite is invalid.

\paragraph{Example B4. Dense $C$ with any nonzero column of $B$.}
\[
B=\begin{bmatrix}
1&0\\
0&1
\end{bmatrix},
\qquad
C=\begin{bmatrix}
1&1\\
1&1
\end{bmatrix}.
\]
Column 1 of $C$ has $C_{21}=1$ but column 2 of $B$ is nonzero. The rewrite is invalid.

\bigskip
\subsection{Summary table for quick decisions}
\label{subsec:summary-table}

\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}p{0.24\linewidth}p{0.70\linewidth}@{}}
\toprule
\textbf{Transformation} & \textbf{Pass condition} \\
\midrule
A: $(A \Had B)C=(AB')\Had C'$ &
For every column $j$ in $C$, all $B$-columns indexed by the nonzeros of $C_{\cdot j}$ are proportional. Dense $C$ forces rank 1 $B$. 1-sparse columns of $C$ are always fine. \\
\addlinespace
B: $(A \Had B)C=A \Had (B' C')$ &
For every column $j$ in $C$ and every off diagonal index $k\ne j$ with $C_{k j}\ne 0$, the entire column $k$ of $B$ must be zero. Diagonal $C$ is always fine. \\
\bottomrule
\end{tabular}
\end{center}
\clearpage

\subsubsection*{Constructing $B',C'$}
Let \(L_j=\supp(C_{\cdot j})\) denote the index set of nonzeros in the $j$-th column of $C$. For each column $j$:
\begin{enumerate}[label=\arabic*)]
  \item Pick a direction $u^{(j)}$. If there exists $k\in L_j$ with $B_{\cdot k}\ne 0$, set $u^{(j)}=\frac{B_{\cdot k}}{\|B_{\cdot k}\|_2}$. If all $B_{\cdot k}=0$, choose any unit vector $u^{(j)}$; that column of the output will be zero regardless.
  \item For every $k\in L_j$, compute $\alpha_k$ so that $B_{\cdot k}=\alpha_k u^{(j)}$ (use $\alpha_k=u^{(j)}{}^\top B_{\cdot k}$ if $u^{(j)}$ is unit length).
  \item Set $C'_{\cdot j}=u^{(j)}$ and $B'_{k j}=\alpha_k\,C_{k j}$, with zeros elsewhere.
\end{enumerate}
Shapes are preserved and $(A B')\Had C'$ equals $(A\Had B)C$ for all $A$.

\section{Extensions with a fixed preprocessing of the left input}
\label{sec:tau-extensions}
This section discusses optional rewrites that first apply a fixed linear preprocessing to $A$ before the matrix multiplication. These rewrites always preserve the final numerical result.

\subsection{Shape preserving preprocessing}
\begin{equation}
\tau(A)_{\cdot j}=\sum_{k} \diag(B_{\cdot k})\,A_{\cdot k}\,C_{k j}.
\label{eq:tau-rank}
\end{equation}
Then for any $A$, $\tau(A)$ equals $(A\Had B)C$. Using $C'=\bm{1}\bm{1}^\top$, we can write
\begin{equation}
(A\Had B)C=\tau(A)\Had C'.
\label{eq:tau-lift}
\end{equation}

\subsection{Low rank structure in the columns of \texorpdfstring{$B$}{B}}\label{sec:ext-lowrank}
Assume the set $\{B_{\cdot k}\}_{k=1}^n$ clusters into $r$ proportional directions:
\[
B_{\cdot k}=\alpha_k\,u^{(t)} \quad \text{for } k\in \mathcal{K}_t,\quad t=1,\dots,r.
\]
Then
\[
\tau(A)_{\cdot j}=\sum_{t=1}^r \diag(u^{(t)})\Big(\sum_{k\in\mathcal{K}_t} A_{\cdot k}\,\alpha_k\,C_{k j}\Big).
\]
If $r=1$ we recover a very simple schedule: one row scale by $u$ and one multiplication with $\diag(v)C$ where $B=uv^\top$.

\subsection{Column sparsity in $C$}\label{sec:ext-sparse}
If each column of $C$ has at most $s$ nonzeros, computing $\tau(A)_{\cdot j}$ touches at most $s$ row scales. This reduces to a small gather-and-accumulate per output column.

\vspace{3cm}


\section{Conclusion}
\label{sec:conclusion}
Two base rewrites that commute elementwise multiply with matrix multiplication were studied in a shape preserving setting. For Transformation A, the necessary and sufficient pass condition is that for every column of $C$, all columns of $B$ indexed by the nonzeros of that column are proportional. This includes dense $C$ paired with rank one $B$ and block sparse $C$ with componentwise rank one columns in $B$. For Transformation B, the necessary and sufficient pass condition is that every off diagonal nonzero of $C$ targets a zero column of $B$, with the diagonal case always allowed.

\clearpage


\appendix
\section{Appendix A: Mathematical analysis and proofs}
\label{app:math}

This appendix presents the formal statements and proofs for the results used in Sections~\ref{subsec:dev-A} and~\ref{subsec:dev-B}. Throughout, $A,B,C\in\R^{n\times n}$ are real matrices. The Hadamard product $X\Had Y$ is defined entrywise. The standard matrix product is denoted by juxtaposition.

\subsection{Preliminaries}
I collect standard identities and conventions.

\begin{definition}[Vectorization and Kronecker]
For a matrix $X\in\R^{n\times n}$, $\vecop(X)\in\R^{n^2}$ stacks columns of $X$. For conformable $A,X,B$,
\[
\vecop(AXB)=(B^\top\!\otimes A)\,\vecop(X).
\]
\end{definition}

\begin{definition}[Diagonalization of elementwise multiplication]
For $X,Y\in\R^{n\times n}$,
\[
\vecop(X\Had Y)=\Diag(\vecop(Y))\,\vecop(X).
\]
\end{definition}

\begin{definition}[Rank one mask identity]
For $u,v\in\R^n$ and $X\in\R^{n\times n}$,
\[
X\Had (uv^\top)=\diag(u)\,X\,\diag(v).
\]
\end{definition}

\begin{remark}[Linearity in the left argument]
Fixing $B$ and $C$, the maps
\[
\mathcal{T}_A: A\mapsto (A\Had B)C
\quad\text{and}\quad
\mathcal{S}_A: A\mapsto (AB')\Had C'
\]
are linear in $A$. Equality $\mathcal{T}_A\equiv\mathcal{S}_A$ for all $A$ is equivalent to equality on a basis of $\R^{n\times n}$, hence to equality of coefficients of each $a_{ik}$ in every output position.
\end{remark}

\subsection{Transformation A}
We seek conditions under which there exist $B',C'$ depending only on $B,C$ such that
\begin{equation}\label{eq:A-target}
(A\Had B)C = (AB')\Had C' \quad \text{for all } A\in\R^{n\times n}.
\end{equation}

\begin{lemma}[Coefficient identity for A]\label{lem:A-coeff}
Equality \eqref{eq:A-target} holds for all $A$ if and only if for all indices $i,k,j$,
\begin{equation}\label{eq:A-master}
b_{ik}\,c_{k j} = b'_{k j}\,c'_{i j}.
\end{equation}
\end{lemma}

\begin{proof}
The $(i,j)$ entry of the left hand side is $\sum_{k} a_{ik} b_{ik} c_{k j}$. The $(i,j)$ entry of the right hand side is $\big(\sum_k a_{ik} b'_{k j}\big)c'_{i j}$. Equality for all $A$ is equivalent to equality of the coefficients of $a_{ik}$, hence \eqref{eq:A-master}.
\end{proof}

For a fixed column $j$, define the matrix $M^{(j)}\in\R^{n\times n}$ by $m^{(j)}_{ik}=b_{ik}c_{k j}$. Equation \eqref{eq:A-master} is equivalent to the requirement that $M^{(j)}$ factor as an outer product of a column vector (depending on $i$) and a row vector (depending on $k$), namely $M^{(j)}=\alpha^{(j)} (\beta^{(j)})^\top$ with $\alpha^{(j)}_i=c'_{i j}$ and $\beta^{(j)}_k=b'_{k j}$.

\begin{lemma}[Rank one characterization]\label{lem:rank1}
For fixed $j$, $M^{(j)}=[b_{ik}c_{k j}]_{i,k}$ has rank one if and only if all nonzero columns of $M^{(j)}$ are pairwise proportional.
\end{lemma}

\begin{proof}
The rank of a matrix equals one if and only if all columns lie in a one dimensional subspace, equivalently all nonzero columns are proportional.
\end{proof}

Observe that the $k$th column of $M^{(j)}$ equals $c_{k j} B_{\cdot k}$. Therefore all nonzero columns of $M^{(j)}$ are scalar multiples of the corresponding columns of $B$. This yields the following necessary and sufficient condition.

\begin{theorem}[Pass condition for A]\label{thm:A-pass}
Equality \eqref{eq:A-target} holds for all $A$ if and only if for every column $j$ of $C$ the family
\[
\{B_{\cdot k} : c_{k j}\ne 0\}
\]
consists of proportional vectors. Equivalently, for each $j$ with support $\supp(C_{\cdot j})$, the set of columns $\{B_{\cdot k} : k\in\supp(C_{\cdot j})\}$ spans a one dimensional subspace of $\R^n$.
\end{theorem}

\begin{proof}
Fix $j$. By Lemma~\ref{lem:A-coeff} and the discussion above, \eqref{eq:A-target} holds if and only if $M^{(j)}$ has rank one. By Lemma~\ref{lem:rank1}, this is equivalent to all nonzero columns of $M^{(j)}$ being proportional, which is equivalent to all $B_{\cdot k}$ with $c_{k j}\ne 0$ being proportional. The equivalence for each $j$ implies the statement.
\end{proof}

\begin{corollary}[Dense $C$]\label{cor:A-denseC}
If $C$ has no zeros, then \eqref{eq:A-target} holds for all $A$ if and only if $B$ has rank one.
\end{corollary}

\begin{proof}
If $C$ has no zeros then for each $j$, $\supp(C_{\cdot j})=\{1,\dots,n\}$. Theorem~\ref{thm:A-pass} implies that all columns of $B$ are proportional, hence $\rank(B)=1$. The converse is immediate.
\end{proof}

\begin{corollary}[One-sparse columns of $C$]\label{cor:A-1sparse}
If each column of $C$ has at most one nonzero entry, then \eqref{eq:A-target} holds for all $A$ for any $B$.
\end{corollary}

\begin{proof}
If $|\supp(C_{\cdot j})|\le 1$, the set in Theorem~\ref{thm:A-pass} has cardinality at most one, hence is trivially proportional.
\end{proof}

\begin{corollary}[Block sparse $C$]\label{cor:A-block}
Suppose the indices $\{1,\dots,n\}$ are partitioned into disjoint blocks $\mathcal{K}_1,\dots,\mathcal{K}_r$ with the property that for every column $j$ of $C$, $\supp(C_{\cdot j})\subseteq \mathcal{K}_t$ for some $t$. Then \eqref{eq:A-target} holds for all $A$ if and only if for each block $\mathcal{K}_t$ the family $\{B_{\cdot k}: k\in\mathcal{K}_t\}$ is proportional.
\end{corollary}

\begin{proof}
Apply Theorem~\ref{thm:A-pass} within each block.
\end{proof}


\begin{theorem}[Construction of $B'$ and $C'$]\label{thm:A-construct}
If the pass condition in Theorem~\ref{thm:A-pass} holds, one valid choice is
\[
C'_{\cdot j}=u^{(j)},\qquad B'_{k j}=\alpha_k C_{k j},
\]
where for each $j$ we select a nonzero vector $u^{(j)}$ spanning the common direction of $\{B_{\cdot k}: c_{k j}\ne 0\}$ and scalars $\alpha_k$ with $B_{\cdot k}=\alpha_k u^{(j)}$ for all $k$ in the support of $C_{\cdot j}$, with $\alpha_k=0$ if $c_{k j}=0$.
\end{theorem}

\begin{proof}
Fix $j$. For each $k$ with $c_{k j}\ne 0$, $B_{\cdot k}=\alpha_k u^{(j)}$ by assumption. Then for all $i$ and $k$,
\[
b_{i k} c_{k j} = \alpha_k u^{(j)}_i c_{k j} = b'_{k j} c'_{i j}
\]
with $b'_{k j}=\alpha_k c_{k j}$ and $c'_{i j}=u^{(j)}_i$. This matches \eqref{eq:A-master} and concludes the proof.
\end{proof}

\begin{proposition}[Failure in general]\label{prop:A-fail}
Transformation A fails in general. For instance, with $n=2$, $B=I_2$ and $C=J_2$ there are no $B',C'$ satisfying \eqref{eq:A-target} for all $A$.
\end{proposition}

\begin{proof}
Here each column of $C$ has support $\{1,2\}$, but the columns of $B$ are $e_1$ and $e_2$, which are not proportional. By Theorem~\ref{thm:A-pass} the condition fails.
\end{proof}

\subsection{Transformation B}
We seek conditions under which there exist $B',C'$ depending only on $B,C$ such that
\begin{equation}\label{eq:B-target}
(A\Had B)C = A\Had (B' C') \quad \text{for all } A\in\R^{n\times n}.
\end{equation}

\begin{lemma}[Coefficient identity for B]\label{lem:B-coeff}
Equality \eqref{eq:B-target} holds for all $A$ if and only if there exists a matrix $D=B' C'$ such that for all $i,k,j$,
\begin{equation}\label{eq:B-master}
b_{i k}\,c_{k j} = \delta_{k j}\, d_{i j},
\end{equation}
where $\delta_{k j}$ is the Kronecker symbol.
\end{lemma}

\begin{proof}
The $(i,j)$ entry of the left hand side is $\sum_k a_{i k}\,b_{i k}\,c_{k j}$. The $(i,j)$ entry of the right hand side is $a_{i j}\,d_{i j}$. Equality for all $A$ is equivalent to matching the coefficient of each $a_{i k}$ in entry $(i,j)$, hence \eqref{eq:B-master}.
\end{proof}

\begin{theorem}[Pass condition for B]\label{thm:B-pass}
Equality \eqref{eq:B-target} holds for all $A$ if and only if for every column $j$ of $C$ and every $k\ne j$ with $c_{k j}\ne 0$, the $k$th column of $B$ is identically zero.
\end{theorem}

\begin{proof}
If $k\ne j$ and $c_{k j}\ne 0$, then by \eqref{eq:B-master} we must have $b_{i k}=0$ for all $i$, hence the entire $k$th column of $B$ is zero. Conversely, if every off diagonal nonzero of $c_{k j}$ targets a zero column of $B$, then \eqref{eq:B-master} holds with $d_{i j}=b_{i j} c_{j j}$, which corresponds to $D=B\diag(C_{11},\dots,C_{n n})$.
\end{proof}

\begin{corollary}[Diagonal case]\label{cor:B-diag}
If $C$ is diagonal then \eqref{eq:B-target} holds for all $A$ for any $B$. One may take $B'=B$ and $C'=\diag(C_{11},\dots,C_{n n})$.
\end{corollary}

\begin{proof}
If $C$ is diagonal then for $k\ne j$ we have $c_{k j}=0$, so \eqref{eq:B-master} imposes no constraints beyond $d_{i j}=b_{i j}c_{j j}$.
\end{proof}

\begin{proposition}[Failure with dense $C$]\label{prop:B-fail}
If $C$ has an off diagonal nonzero at $(k,j)$ and the $k$th column of $B$ is nonzero, then Transformation B fails.
\end{proposition}

\begin{proof}
Immediate from Theorem~\ref{thm:B-pass}.
\end{proof}

\subsection{Vectorized proofs}
For completeness, I give vectorized derivations that encode the proofs above as equalities of linear maps.

\subsubsection*{Transformation A}
We require
\[
\vecop((A\Had B)C) = \vecop((AB')\Had C')
\]
for all $A$. Using identities,
\[
\vecop((A\Had B)C) = (C^\top\!\otimes I)\,\Diag(\vecop(B))\,\vecop(A),
\]
and
\[
\vecop((AB')\Had C') = \Diag(\vecop(C'))\,(B'^\top\!\otimes I)\,\vecop(A).
\]
Therefore equality for all $A$ is equivalent to
\begin{equation}\label{eq:A-vec}
(C^\top\!\otimes I)\,\Diag(\vecop(B)) = \Diag(\vecop(C'))\,(B'^\top\!\otimes I).
\end{equation}
Inspecting the $(i,k;j)$ block of \eqref{eq:A-vec} yields $b_{i k} c_{k j}=b'_{k j} c'_{i j}$, which is \eqref{eq:A-master}. The rank one characterization follows from the observation that for fixed $j$ the block column equals $[c_{1 j}B_{\cdot 1}, \dots, c_{n j}B_{\cdot n}]$, which has rank one if and only if all its nonzero columns are proportional.

\subsubsection*{Transformation B}
We require
\[
\vecop((A\Had B)C) = \vecop(A\Had (B'C')) = \Diag(\vecop(B'C'))\,\vecop(A)
\]
for all $A$, which is equivalent to
\[
(C^\top\!\otimes I)\,\Diag(\vecop(B)) = \Diag(\vecop(B'C')).
\]
For a fixed $j$, the $j$th block column of the left-hand side consists of blocks
\[
\big[c_{1 j}\,\Diag(B_{\cdot 1}),\ \dots,\ c_{n j}\,\Diag(B_{\cdot n})\big].
\]
For this operator to be diagonal, every off-diagonal block $c_{k j}\,\Diag(B_{\cdot k})$ with $k\ne j$ must be the zero matrix, hence if $c_{k j}\ne 0$ then $B_{\cdot k}=0$. The diagonal block equals $c_{j j}\,\Diag(B_{\cdot j})$, which identifies
\[
\Diag(\vecop(B'C'))=\Diag\!\big(\vecop(B\,\diag(C_{11},\dots,C_{nn}))\big),
\]
so one may take $D=B'C'=B\diag(C_{11},\dots,C_{nn})$.

\subsection{Constructive counterexamples}
\begin{example}[A fails with $B=I_2$, $C=J_2$]
Assume $B',C'$ satisfy \eqref{eq:A-target} for all $A$. Comparing coefficients in entry $(1,1)$ yields $b'_{1 1}c'_{1 1}=1$ and $b'_{2 1}c'_{1 1}=0$. In entry $(2,2)$ we obtain $b'_{2 2}c'_{2 2}=1$ and $b'_{1 2}c'_{2 2}=0$. This forces both $b'_{2 1}=0$ and $b'_{2 2}\ne 0$, and simultaneously $c'_{1 1}\ne 0$ and $c'_{2 2}\ne 0$, which conflicts when matching the mixed entries $(1,2)$ and $(2,1)$. No solution exists.
\end{example}

\begin{example}[B fails with $B=I_2$, $C=J_2$]
Equation \eqref{eq:B-master} requires $b_{i 2} c_{2 1}=\delta_{2 1} d_{i 1}=0$ for all $i$. Since $c_{2 1}=1$ and $b_{i 2}=[0,1]^\top$ is not zero, this is impossible.
\end{example}

\subsection{Stability under perturbations}
The pass conditions are structurally stable. For Transformation A, small perturbations of $B$ and $C$ preserve proportionality when the proportional groups in $B$ are separated by a uniform angle margin and when zero entries of $C$ remain below the verifier threshold. For Transformation B, small off diagonal entries of $C$ must remain below the threshold or the corresponding columns of $B$ must be sufficiently close to zero.

\subsection{Uniqueness and degrees of freedom}
In Transformation A, when the pass condition holds, $B'$ and $C'$ are not unique. For each column $j$, any scaling $C'_{\cdot j}\leftarrow \gamma_j C'_{\cdot j}$ and $B'_{\cdot j}\leftarrow \gamma_j^{-1} B'_{\cdot j}$ preserves \eqref{eq:A-master}. In Transformation B, once the matrix $\tilde D := B\,\diag(c_{11},\dots,c_{n n})$ is fixed, any factorization $\tilde D = B' C'$ works; a canonical choice is $B'=B$, $C'=\diag(c_{11},\dots,c_{n n})$.


\subsection{Extension note}
The extended constructions with preprocessing $\tau$ in Section~\ref{sec:tau-extensions} can also be derived in vectorized form by factoring $(C^\top\!\otimes I)\Diag(\vecop(B))$ into a product of block diagonal and skinny mixing operators. The detailed algebra is given in the appendix.

\begin{proof}
By definition of $\tau$ the equality is entrywise true for every $(i,j)$:
\[
[\tau(A)\Had C']_{i j} = \tau(A)_{i j} = \sum_k a_{i k}\, b_{i k}\, c_{k j} = \big[(A \Had B) C\big]_{i j}.
\]
\end{proof}

\begin{theorem}[Correctness of grouped-direction lift]\label{thm:tau2}
Assume $B_{\cdot k}=\alpha_k u^{(t)}$ for $k\in\mathcal{K}_t$, $t=1,\dots,r$. Let $\tau$ be as in \eqref{eq:tau-rank}, $B^\diamond$ as described above and $C^\diamond=\bm{1}\bm{1}^\top$. Then \eqref{eq:tau-lift} holds for all $A$.
\end{theorem}

\begin{proof}
The proof is already given after \eqref{eq:tau-rank}. It is a regrouping of terms indexed by the partition of $\{1,\dots,n\}$ into $\{\mathcal{K}_t\}$.
\end{proof}

\begin{theorem}[Correctness of column-sparsity lift]\label{thm:tau3}
Assume every column of $C$ has at most $s$ nonzeros. Let $\tau$ be as in \eqref{eq:tau-rank}, with $B^\diamond$ routing nonzero row indices of $C$ to distinct blocks and pre-multiplying by $\diag(B_{\cdot k})$. With $C^\diamond=\bm{1}\bm{1}^\top$, \eqref{eq:tau-lift} holds for all $A$.
\end{theorem}

\begin{proof}
Immediate from the construction in Section~\ref{sec:ext-sparse}.
\end{proof}

\subsection{Limitations and non-goals}
These extensions are not substitutes for the base shape-preserving rewrites in Sections~\ref{subsec:dev-A}–\ref{subsec:dev-B}. They are useful only when the verifier detects structural reuse.

\paragraph{No creation of new commutativity.}
If one insists on a final form where raw $A$ appears unchanged on the right hand side, i.e.
\[
(A \Had B) C \stackrel{?}{=} A \Had (B' C'),
\]
no preprocessing $\tau$ can help unless Theorem~\ref{thm:B-pass} holds, because the right hand side depends only on $a_{i j}$ while the left depends on all $a_{i k}$ in the same row.

\paragraph{Avoid wide lifts without reuse.}
The $n^2$-width lift that splits by every column and row is always possible algebraically but is not a practical rewrite. The grouped-direction and column-sparsity lifts are bounded by small $r$ or $s$ and are the only viable variants.

\subsection{Stability and tolerances}
In practice one works with floating point arithmetic and near-proportional directions. The grouped-direction lift is stable if the angle between directions is bounded away from zero and the assignment of columns to directions respects a margin. The column-sparsity lift is stable if the small entries of $C$ can be safely treated as zeros by a numerics policy.

\subsection{Worked examples}
I give two concrete instances with explicit matrices to illustrate the constructions.

\paragraph{Example B.1. Rank one $B$.}
Let $B=uv^\top$ with $u=[1,-2,4]^\top$, $v=[3,1,-5]^\top$ and let $C$ be arbitrary. Then $r=1$ with $u^{(1)}=u$ and $\alpha_k=v_k$. The lift uses
\[
\tau(A) = \big[\, \diag(u) A \,\big] \in \R^{n\times n},\quad
B^\diamond = \diag(v)\,C \in \R^{n\times n},\quad
C^\diamond=\bm{1}\bm{1}^\top.
\]
Hence
\[
(A \Had B) C = \big(\,\diag(u) A\,\big) \, \big(\diag(v) C\big),
\]
followed by a Hadamard with all ones, which is a no-op. This is a very cheap schedule: one row-scale and one matmul.

\paragraph{Example B.2. Two-column sparsity per output column.}
Let $n=4$. Suppose each column of $C$ has exactly two nonzeros. Then $s=2$ and
\[
\tau(A) = [\, A \mid A \,] \in \R^{4\times 8}.
\]
For the $j$th output column, let the nonzeros of $C_{\cdot j}$ occur at rows $k_1,k_2$ with values $\gamma_1,\gamma_2$. The corresponding column of $B^\diamond$ places $\diag(B_{\cdot k_1})$ in the first block and $\diag(B_{\cdot k_2})$ in the second block, with weights $\gamma_1,\gamma_2$. Then the $j$th column of $(\tau(A) B^\diamond)$ equals
\[
\diag(B_{\cdot k_1}) A_{\cdot k_1}\, \gamma_1 + \diag(B_{\cdot k_2}) A_{\cdot k_2}\, \gamma_2,
\]
which is exactly the $j$th column of $(A \Had B) C$.

\subsection{Additional remarks on uniqueness and degrees of freedom}
The factorizations are not unique. In F2, any rescaling $u^{(t)}\leftarrow \gamma_t u^{(t)}$ paired with $\alpha_k\leftarrow \alpha_k/\gamma_t$ for $k\in\mathcal{K}_t$ leaves the construction invariant. In F3, the assignment of nonzero rows to blocks is arbitrary up to permutations and zero padding. One may choose assignments that align with memory layout or kernel preferences.

\subsection{Relation to the base criteria}
When the pass condition for Transformation A holds, the grouped-direction lift with $r$ equal to the number of connected components in the co-support graph of $C$ degenerates to the exact $B',C'$ construction in Theorem~\ref{thm:A-construct}. When Transformation B holds with diagonal $C$, the shape-preserving $\tau$ reduces to a single diagonal post-scaling and becomes redundant.

\subsection{Proof of vec-factorization claims}
For completeness I record a compact derivation of \eqref{eq:vec-master} and the factorizations.

\begin{equation}\label{eq:vec-master}
\vecop\big( (A \Had B) C \big) = (C^\top \otimes I)\, \Diag(\vecop(B))\, \vecop(A).
\end{equation}

\begin{lemma}\label{lem:vec-master}
For all $A,B,C\in\R^{n\times n}$,
\[
\vecop\big( (A \Had B) C \big) = (C^\top \otimes I)\, \Diag(\vecop(B))\, \vecop(A).
\]
\end{lemma}

\begin{proof}
We have $\vecop( (A \Had B) C ) = (C^\top \otimes I)\, \vecop(A \Had B) = (C^\top \otimes I)\, \Diag(\vecop(B))\, \vecop(A)$ by the standard properties of vectorization and the diagonalization of the Hadamard product.
\end{proof}

\begin{lemma}[Grouped-direction factorization]\label{lem:grouped-factor}
Under the assumptions of Section~\ref{sec:ext-lowrank},
\[
\mathcal{M}(B,C) = (B^{\diamond\top} \otimes I)\, \big( I \otimes \mathcal{U} \big)\, \Pi,
\]
where $\mathcal{U}$ is the block diagonal operator with blocks $\diag(u^{(t)})$, $\Pi$ is a permutation that replicates columns according to the partition $\{\mathcal{K}_t\}$, and $B^\diamond$ is as defined earlier.
\end{lemma}

\begin{proof}
The $k$th column of $\Diag(\vecop(B))$ applies the row scale $\diag(B_{\cdot k})$. For $k\in\mathcal{K}_t$ this equals $\alpha_k \diag(u^{(t)})$. Grouping by $t$ and pulling out the common operators yields the claimed factorization, with the mixing weights collected in $B^\diamond$.
\end{proof}

\begin{lemma}[Column-sparsity factorization]\label{lem:s-sparse-factor}
Under the assumptions of Section~\ref{sec:ext-sparse},
\[
\mathcal{M}(B,C) = (B^{\diamond\top} \otimes I)\, \Pi_s\, \Diag(\vecop(B)),
\]
where $\Pi_s$ replicates $\vecop(A)$ into $s$ blocks and $B^\diamond$ routes the at most $s$ nonzeros per column of $C$ into distinct blocks.
\end{lemma}

\begin{proof}
Replication by $\Pi_s$ builds $s$ copies of columns so that at most $s$ terms are needed per output column. The routing encoded by $B^\diamond$ collects the correct pairs of $(B_{\cdot k}, C_{k j})$ into the $j$th output column.
\end{proof}

\subsection{Concluding remark for the appendix}
The extended rewrites follow from linear-algebraic factorizations of the operator $(C^\top \otimes I)\Diag(\vecop(B))$. Their practical value depends on the number of reusable directions among $\{B_{\cdot k}\}$ and on the column sparsity of $C$. These are precisely the two structural features that the verifiers and cost models should detect before emitting an extended rewrite.

\vspace{1.5cm}

\begin{flushright}
\includegraphics[width=4cm]{Gaus_Logo_1920x1080.jpg}
\end{flushright}


\end{document}
